<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">David</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9E56DDB5C2F8C7AB47CE1817D23069B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High resolution theory</term>
					<term>rate distortion theory</term>
					<term>source coding</term>
					<term>quantization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The history of the theory and practice of quantization dates to 1948, although similar ideas had appeared in the literature as long ago as 1898. The fundamental role of quantization in modulation and analog-to-digital conversion was first recognized during the early development of pulsecode modulation systems, especially in the 1948 paper of Oliver, Pierce, and Shannon. Also in 1948, Bennett published the first high-resolution analysis of quantization and an exact analysis of quantization noise for Gaussian processes, and Shannon published the beginnings of rate distortion theory, which would provide a theory for quantization as analog-to-digital conversion and as data compression. Beginning with these three papers of fifty years ago, we trace the history of quantization from its origins through this decade, and we survey the fundamentals of the theory and many of the popular and promising techniques for quantization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE dictionary (Random House) definition of quantization is the division of a quantity into a discrete number of small parts, often assumed to be integral multiples of a common quantity. The oldest example of quantization is rounding off, which was first analyzed by Sheppard <ref type="bibr" target="#b474">[468]</ref> for the application of estimating densities by histograms. Any real number can be rounded off to the nearest integer, say , with a resulting quantization error so that . More generally, we can define a quantizer as consisting of a set of intervals or cells , where the index set is ordinarily a collection of consecutive integers beginning with or , together with a set of reproduction values or points or levels , so that the overall quantizer is defined by for , which can be expressed concisely as <ref type="bibr" target="#b0">(1)</ref> where the indicator function is if and otherwise. For this definition to make sense we assume that is a partition of the real line. That is, the cells are disjoint and exhaustive. The general definition reduces to the rounding off Fig. <ref type="figure">1</ref>. A nonuniform quantizer: a 0 = 1, a 5 = 1.</p><p>example if and for all integers . More generally, the cells might take the form where the 's, which are called thresholds, form an increasing sequence. The width of a cell is its length . The function is often called the quantization rule. A simple quantizer with five reproduction levels is depicted in Fig. <ref type="figure">1</ref> as a collection of intervals bordered by thresholds along with the levels for each interval.</p><p>A quantizer is said to be uniform if, as in the roundoff case, the levels are equispaced, say apart, and the thresholds are midway between adjacent levels. If an infinite number of levels are allowed, then all cells will have width equal to , the separation between levels. If only a finite number of levels are allowed, then all but two cells will have width and the outermost cells will be semi-infinite. An example of a uniform quantizer with cell width and levels is given in Fig. <ref type="figure">2</ref>. Given a uniform quantizer with cell width , the region of the input space within of some quantizer level is called the granular region or simply the support and that outside (where the quantizer error is unbounded) is called the overload or saturation region. More generally, the support or granular region of a nonuniform quantizer is the region of the input space within a relatively small distance of some level, and the overload region is the complement of the granular region. To be concrete, "small" might be defined as half the width of the largest cell of finite width.</p><p>The quality of a quantizer can be measured by the goodness of the resulting reproduction in comparison to the original. One way of accomplishing this is to define a distortion measure that quantifies cost or distortion resulting from reproducing as and to consider the average distortion as a measure of the quality of a system, with smaller average distortion meaning higher quality. The most common distortion measure is the squared error , but we shall encounter others later. In practice, the average will be a sample average when the quantizer is applied to a sequence of real data, but the theory views the data as sharing a common probability density function (pdf) corresponding to a generic random variable and the average distortion becomes an expectation <ref type="bibr" target="#b1">(2)</ref> 0018-9448/98$10.00 Â© 1998 IEEE Fig. <ref type="figure">2</ref>. A uniform quantizer.</p><p>If the distortion is measured by squared error, becomes the mean squared error (MSE), a special case on which we shall mostly focus.</p><p>It is desirable to have the average distortion as small as possible, and in fact negligible average distortion is achievable by letting the cells become numerous and tiny. There is a cost in terms of the number of bits required to describe the quantizer output to a decoder, however, and arbitrarily reliable reproduction will not be possible for digital storage and communication media with finite capacity. A simple method for quantifying the cost for communications or storage is to assume that the quantizer "codes" an input into a binary representation or channel codeword of the quantizer index specifying which reproduction level should be used in the reconstruction. If there are possible levels and all of the binary representations or binary codewords have equal length (a temporary assumption), the binary vectors will need (or the next larger integer, , if is not an integer) components or bits. Thus one definition of the rate of the code in bits per input sample is (3) A quantizer with fixed-length binary codewords is said to have fixed rate because all quantizer levels are assumed to have binary codewords of equal length. Later this restriction will be weakened. Note that all logarithms in this paper will have base , unless explicitly specified otherwise.</p><p>In summary, the goal of quantization is to encode the data from a source, characterized by its probability density function, into as few bits as possible (i.e., with low rate) in such a way that a reproduction may be recovered from the bits with as high quality as possible (i.e., with small average distortion). Clearly, there is a tradeoff between the two primary performance measures: average distortion (or simply distortion, as we will often abbreviate) and rate. This tradeoff may be quantified as the operational distortion-rate function , which is defined to be the least distortion of any scalar quantizer with rate or less. That is, <ref type="bibr" target="#b3">(4)</ref> Alternatively, one can define the operational rate-distortion function as the least rate of any fixed-rate scalar quantizer with distortion or less, which is the inverse of . We have so far described scalar quantization with fixed-rate coding, a technique whereby each data sample is independently encoded into a fixed number of bits and decoded into a reproduction. As we shall see, there are many alternative quantization techniques that permit a better tradeoff of distortion and rate; e.g., less distortion for the same rate, or vice versa. The purpose of this paper is to review the development of such techniques, and the theory of their design and performance. For example, for each type of technique we will be interested in its operational distortion-rate function, which is defined to be the least distortion of any quantizer of the given type with rate or less. We will also be interested in the best possible performance among all quantizers. Both as a preview and as an occasional benchmark for comparison, we informally define the class of all quantizers as the class of quantizers that can 1) operate on scalars or vectors instead of only on scalars (vector quantizers), 2) have fixed or variable rate in the sense that the binary codeword describing the quantizer output can have length depending on the input, and 3) be memoryless or have memory, for example, using different sets of reproduction levels, depending on the past. In addition, we restrict attention to quantizers that do not change with time. That is, when confronted with the same input and the same past history, a quantizer will produce the same output regardless of the time. We occasionally use the term lossy source code or simply code as alternatives to quantizer. The rate is now defined as the average number of bits per source symbol required to describe the corresponding reproduction symbol. We informally generalize the operational distortion-rate function providing the best performance for scalar quantizers, to , which is defined as the infimum of the average distortion over all quantization techniques with rate or less. Thus can be viewed as the best possible performance over all quantizers with no constraints on dimension, structure, or complexity.</p><p>Section II begins with a historical tour of the development of the theory and practice of quantization over the past fifty years, a period encompassing almost the entire literature on the subject. Two complementary approaches dominate the history and present state of the theory, and three of the key papers appeared in 1948, two of them in Volume 27 <ref type="bibr">(1948)</ref> of the Bell Systems Technical Journal. Likely the approach best known to the readers of these TRANSACTIONS is that of rate-distortion theory or source coding with a fidelity criterion-Shannon's information-theoretic approach to source coding-which was first suggested in his 1948 paper <ref type="bibr" target="#b470">[464]</ref> providing the foundations of information theory, but which was not fully developed until his 1959 source coding paper <ref type="bibr" target="#b471">[465]</ref>. The second approach is that of high resolution (or highrate or asymptotic) quantization theory, which had its origins in the 1948 paper on PCM by Oliver, Pierce, and Shannon <ref type="bibr" target="#b398">[394]</ref>, the 1948 paper on quantization error spectra by Bennett <ref type="bibr" target="#b42">[43]</ref>, and the 1951 paper by Panter and Dite <ref type="bibr" target="#b409">[405]</ref>. Much of the history and state of the art of quantization derives from these seminal works.</p><p>In contrast to these two asymptotic theories, there is also a small but important collection of results that are not asymptotic in nature. The oldest such results are the exact analyses for special nonasymptotic cases, such as Clavier, Panter, and Grieg's 1947 analysis of the spectra of the quantization error for uniformly quantized sinusoidal signals <ref type="bibr" target="#b101">[99]</ref>, <ref type="bibr" target="#b102">[100]</ref>, and Bennett's 1948 derivation of the power spectral density of a uniformly quantized Gaussian random process <ref type="bibr" target="#b42">[43]</ref>. The most important nonasymptotic results, however, are the basic optimality conditions and iterative-descent algorithms for quantizer design, such as first developed by <ref type="bibr" target="#b486">Steinhaus (1956</ref>) <ref type="bibr" target="#b486">[480]</ref> and <ref type="bibr" target="#b334">Lloyd (1957)</ref>  <ref type="bibr" target="#b334">[330]</ref>, and later popularized by <ref type="bibr" target="#b353">Max (1960)</ref>  <ref type="bibr" target="#b353">[349]</ref>.</p><p>Our goal in the next section is to introduce in historical context many of the key ideas of quantization that originated in classical works and evolved over the past 50 years, and in the remaining sections to survey selectively and in more detail a variety of results which illustrate both the historical development and the state of the field. Section III will present basic background material that will be needed in the remainder of the paper, including the general definition of a quantizer and the basic forms of optimality criteria and descent algorithms. Some such material has already been introduced and more will be introduced in Section II. However, for completeness, Section III will be largely self-contained. Section IV reviews the development of quantization theories and compares the approaches. Finally, Section V describes a number of specific quantization techniques.</p><p>In any review of a large subject such as quantization there is no space to discuss or even mention all work on the subject. Though we have made an effort to select the most important work, no doubt we have missed some important work due to bias, misunderstanding, or ignorance. For this we apologize, both to the reader and to the researchers whose work we may have neglected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HISTORY</head><p>The history of quantization often takes on several parallel paths, which causes some problems in our clustering of topics. We follow roughly a chronological order within each and order the paths as best we can. Specifically, we will first track the design and analysis of practical quantization techniques in three paths: fixed-rate scalar quantization, which leads directly from the discussion of Section I, predictive and transform coding, which adds linear processing to scalar quantization in order to exploit source redundancy, and variable-rate quantization, which uses Shannon's lossless source coding techniques <ref type="bibr" target="#b470">[464]</ref> to reduce rate. (Lossless codes were originally called noiseless.) Next we follow early forward-looking work on vector quantization, including the seminal work of Shannon and Zador, in which vector quantization appears more to be a paradigm for analyzing the fundamental limits of quantizer performance than a practical coding technique. A surprising amount of such vector quantization theory was developed outside the conventional communications and signal processing literature. Subsequently, we review briefly the developments from the mid-1970's to the mid-1980's which mainly concern the emergence of vector quantization as a practical technique. Finally, we sketch briefly developments from the mid-1980's to the present. Except where stated otherwise, we presume squared error as the distortion measure.</p><p>A. Fixed-Rate Scalar Quantization: PCM and the Origins of Quantization Theory Both quantization and source coding with a fidelity criterion have their origins in pulse-code modulation (PCM), a technique patented in 1938 by Reeves <ref type="bibr" target="#b436">[432]</ref>, who 25 years later wrote a historical perspective on and an appraisal of the future of PCM with Deloraine <ref type="bibr" target="#b122">[120]</ref>. The predictions were surprisingly accurate as to the eventual ubiquity of digital speech and video. The technique was first successfully implemented in hardware by Black, who reported the principles and implementation in 1947 <ref type="bibr" target="#b51">[51]</ref>, as did another Bell Labs paper by Goodall <ref type="bibr" target="#b214">[209]</ref>. PCM was subsequently analyzed in detail and popularized by Oliver, Pierce, and Shannon in 1948 <ref type="bibr" target="#b398">[394]</ref>. PCM was the first digital technique for conveying an analog information signal (principally telephone speech) over an analog channel (typically, a wire or the atmosphere). In other words, it is a modulation technique, i.e., an alternative to AM, FM, and various other types of pulse modulation. It consists of three main components: a sampler (including a prefilter), a quantizer (with a fixed-rate binary encoder), and a binary pulse modulator. The sampler converts a continuous-time waveform into a sequence of samples , where is the sampling frequency. The sampler is ordinarily preceded by a lowpass filter with cutoff frequency . If the filter is ideal, then the Shannon-Nyquist or Shannon-Whittaker-Kotelnikov sampling theorem ensures that the lowpass filtered signal can, in principle, be perfectly recovered by appropriately filtering the samples. Quantization of the samples renders this an approximation, with the MSE of the recovered waveform being, approximately, the sum of the MSE of the quantizer and the high-frequency power removed by the lowpass filter. The binary pulse modulator typically uses the bits produced by the quantizer to determine the amplitude, frequency, or phase of a sinusoidal carrier waveform. In the evolutionary development of modulation techniques it was found that the performance of pulse-amplitude modulation in the presence of noise could be improved if the samples were quantized to the nearest of a set of levels before modulating the carrier (64 equally spaced levels was typical). Though this introduces quantization error, deciding which of the levels had been transmitted in the presence of noise could be done with such reliability that the overall MSE was substantially reduced. Reducing the number of quantization levels made it even easier to decide which level had been transmitted, but came at the cost of a considerable increase in the MSE of the quantizer. A solution was to fix at a value giving acceptably small quantizer MSE and to binary encode the levels, so that the receiver had only to make binary decisions, something it can do with great reliability. The resulting system, PCM, had the best resistance to noise of all modulations of the time.</p><p>As the digital era emerged, it was recognized that the sampling, quantizing, and encoding part of PCM performs an analog-to-digital (A/D) conversion, with uses extending much beyond communication over analog channels. Even in the communications field, it was recognized that the task of analog-to-digital conversion (and source coding) should be factored out of binary modulation as a separate task. Thus PCM is now generally considered to just consist of sampling, quantizing, and encoding; i.e., it no longer includes the binary pulse modulation.</p><p>Although quantization in the information theory literature is generally considered as a form of data compression, its use for modulation or A/D conversion was originally viewed as data expansion or, more accurately, bandwidth expansion. For example, a speech waveform occupying roughly 4 kHz would have a Nyquist rate of 8 kHz. Sampling at the Nyquist rate and quantizing at 8 bits per sample and then modulating the resulting binary pulses using amplitude-or frequency-shift keying would yield a signal occupying roughly 64 kHz, a 16-fold increase in bandwidth! Mathematically this constitutes compression in the sense that a continuous waveform requiring an infinite number of bits is reduced to a finite number of bits, but for practical purposes PCM is not well interpreted as a compression scheme.</p><p>In an early contribution to the theory of quantization, Clavier, Panter, and Grieg (1947) <ref type="bibr" target="#b101">[99]</ref>, <ref type="bibr" target="#b102">[100]</ref> applied Rice's characteristic function or transform method <ref type="bibr" target="#b438">[434]</ref> to provide exact expressions for the quantization error and its moments resulting from uniform quantization for certain specific inputs, including constants and sinusoids. The complicated sums of Bessel functions resembled the early analyses of another nonlinear modulation technique, FM, and left little hope for general closed-form solutions for interesting signals.</p><p>The first general contributions to quantization theory came in 1948 with the papers of Oliver, Pierce, and Shannon <ref type="bibr" target="#b398">[394]</ref> and Bennett <ref type="bibr" target="#b42">[43]</ref>. As part of their analysis of PCM for communications, they developed the oft-quoted result that for large rate or resolution, a uniform quantizer with cell width yields average distortion . If the quantizer has levels and rate , and the source has input range (or support) of width , so that is the natural choice, then the approximation yields the familiar form for the signal-to-noise ratio (SNR) of dB showing that for large rate, the SNR of uniform quantization increases 6 dB for each one-bit increase of rate, which is often referred to as the "6-dB-per-bit rule." The formula is considered a high-resolution formula; indeed, the first such formula, in that it applies to the situation where the cells and average distortion are small, and the rate is large, so that the reproduction produced by the quantizer is quite accurate. The result also appeared many years earlier (albeit in somewhat disguised form) in Sheppard's 1898 treatment <ref type="bibr" target="#b474">[468]</ref>.</p><p>Bennett also developed several other fundamental results in quantization theory. He generalized the high-resolution approximation for uniform quantization to provide an approximation to for companders, systems that preceded a uniform quantizer by a monotonic smooth nonlinearity called a "compressor," say , and used the inverse nonlinearity when reconstructing the signal. Thus the output reproduction given an input was given by , where is a uniform quantizer. Bennett showed that in this case <ref type="bibr" target="#b4">(5)</ref> where , is the cellwidth of the uniform quantizer, and the integral is taken over the granular range of the input. (The constant in the above assumes that maps to the unit interval .) Since, as Bennett pointed out, any nonuniform quantizer can be implemented as a compander, this result, often referred to as "Bennett's integral," provides an asymptotic approximation for any quantizer. It is useful to jump ahead and point out that can be interpreted, as Lloyd would explicitly point out in 1957 <ref type="bibr" target="#b334">[330]</ref>, as a constant times a "quantizer point-density function ," that is, a function with the property that for any region number of quantizer levels in <ref type="bibr" target="#b5">(6)</ref> Since integrating over a region gives the fraction of quantizer reproduction levels in the region, it is evident that is normalized so that . It will also prove useful to consider the unnormalized quantizer point density , which when integrated over gives the total number of levels within rather than the fraction. In the current situation , but the unnormalized density will generalize to the case where is infinite. Rewriting Bennett's integral in terms of the point-density function yields its more common form <ref type="bibr" target="#b6">(7)</ref> The idea of a quantizer point-density function will generalize to vectors, while the compander approach will not in the sense that not all vector quantizers can be represented as companders <ref type="bibr" target="#b197">[192]</ref>.</p><p>Bennett also demonstrated that, under assumptions of high resolution and smooth densities, the quantization error behaved much like random "noise": it had small correlation with the signal and had approximately a flat ("white") spectrum. This led to an "additive-noise" model of quantizer error, since with these properties the formula could be interpreted as representing the quantizer output as the sum of a signal and white noise. This model was later popularized by Widrow <ref type="bibr" target="#b534">[528]</ref>, <ref type="bibr" target="#b535">[529]</ref>, but the viewpoint avoids the fact that the "noise" is in fact dependent on the signal and the approximations are valid only under certain conditions. Signalindependent quantization noise has generally been found to be perceptually desirable. This was the motivation for randomizing the action of quantization by the addition of a dither signal, a method introduced by Roberts <ref type="bibr" target="#b446">[442]</ref> as a means of making quantized images look better by replacing the artifacts resulting from deterministic errors by random noise. We shall return to dithering in Section V, where it will be seen that suitable dithering can indeed make exact the Bennett approximations of uniform distribution and signal independence of the overall quantizer noise. Bennett also used a variation of Rice's method to derive an exact computation of the spectrum of quantizer noise when a Gaussian process is uniformly quantized, providing one of the very few exact computations of quantization error spectra.</p><p>In 1951 Panter and Dite <ref type="bibr" target="#b409">[405]</ref> developed a high-resolution formula for the distortion of a fixed-rate scalar quantizer using approximations similar to Bennett's, but without reference to Bennett. They then used variational techniques to minimize their formula and found the following formula for the operational distortion-rate function of fixed-rate scalar quantization: for large values of <ref type="bibr" target="#b7">(8)</ref> which is now called the Panter and Dite formula. 1 As part of their derivation, they demonstrated that an optimal quantizer resulted in roughly equal contributions to total average distortion from each quantization cell, a result later called the "partial distortion theorem." Though they did not rederive Bennett's integral, they had in effect derived the optimal compressor function for a compander, or, equivalently, the optimal quantizer point density <ref type="bibr" target="#b8">(9)</ref> Indeed, substituting this point density into Bennett's integral and using the fact that yields <ref type="bibr" target="#b7">(8)</ref>. As an example, if the input density is Gaussian with variance , then <ref type="bibr" target="#b9">(10)</ref> The fact that for large rates decreases with as implies that the signal-to-noise ratio increases according to the 6-dB-per-bit rule. Virtually all other high resolution formulas to be given later will also obey this rule. However, the constant that adds to will vary with the source and quantizer being considered.</p><p>The Panter-Dite formula for can also be derived directly from Bennett's integral using variational methods, as did <ref type="bibr" target="#b334">Lloyd (1957)</ref>  <ref type="bibr" target="#b334">[330]</ref>, <ref type="bibr" target="#b480">Smith (1957)</ref>  <ref type="bibr" target="#b480">[474]</ref>, and, much later without apparent knowledge of earlier work, <ref type="bibr" target="#b447">Roe (1964)</ref>  <ref type="bibr" target="#b447">[443]</ref>. It can also be derived without using variational methods by application of HÃ¶lder's inequality to Bennett's integral <ref type="bibr" target="#b226">[222]</ref>, with the additional benefit of demonstrating that the claimed minimum is indeed global. Though not known at the time, it turns out that for a Gaussian source with independent and identically distributed (i.i.d.) samples, the operational distortionrate function given above is times larger than , the least distortion achievable by any quantization technique with rate or less. (It was not until Shannon's 1959 paper <ref type="bibr" target="#b471">[465]</ref> that was known.) Equivalently, the induced signal-to-noise ratio is 4.35 dB less than the best possible, or for a fixed distortion the rate is 0.72 bits/sample larger than that achievable by the best quantizers.</p><p>In 1957, Smith <ref type="bibr" target="#b480">[474]</ref> re-examined companding and PCM. Among other things, he gave somewhat cleaner derivations of 1 They also indicated that it had been derived earlier by P. R. Aigrain.</p><p>Bennett's integral, the optimal compressor function, and the Panter-Dite formula.</p><p>Also in 1957, Lloyd <ref type="bibr" target="#b334">[330]</ref> made an important study of quantization with three main contributions. First, he found necessary and sufficient conditions for a fixed-rate quantizer to be locally optimal; i.e., conditions that if satisfied implied that small perturbations to the levels or thresholds would increase distortion. Any optimal quantizer (one with smallest distortion) will necessarily satisfy these conditions, and so they are often called the optimality conditions or the necessary conditions. Simply stated, Lloyd's optimality conditions are that for a fixed-rate quantizer to be optimal, the quantizer partition must be optimal for the set of reproduction levels, and the set of reproduction levels must be optimal for the partition. Lloyd derived these conditions straightforwardly from first principles, without recourse to variational concepts such as derivatives. For the case of mean-squared error, the first condition implies a minimum distance or nearest neighbor quantization rule, choosing the closest available reproduction level to the source sample being quantized, and the second condition implies that the reproduction level corresponding to a given cell is the conditional expectation or centroid of the source value given that it lies in the specified cell; i.e., it is the minimum meansquared error estimate of the source sample. For some sources there are multiple locally optimal quantizers, not all of which are globally optimal.</p><p>Second, based on his optimality conditions, Lloyd developed an iterative descent algorithm for designing quantizers for a given source distribution: begin with an initial collection of reproduction levels; optimize the partition for these levels by using a minimum distortion mapping, which gives a partition of the real line into intervals; then optimize the set of levels for the partition by replacing the old levels by the centroids of the partition cells. The alternation is continued until convergence to a local, if not global, optimum. Lloyd referred to this design algorithm as "Method I." He also developed a Method II based on the optimality properties. First choose an initial smallest reproduction level. This determines the cell threshold to the right, which in turn implies the next larger reproduction level, and so on. This approach alternately produces a level and a threshold. Once the last level has been chosen, the initial level can then be rechosen to reduce distortion and the algorithm continues. Lloyd provided design examples for uniform, Gaussian, and Laplacian random variables and showed that the results were consistent with the high resolution approximations. Although Method II would initially gain more popularity when rediscovered in 1960 by Max <ref type="bibr" target="#b353">[349]</ref>, it is Method I that easily extends to vector quantizers and many types of quantizers with structural constraints.</p><p>Third, motivated by the work of Panter and Dite but apparently unaware of that of Bennett or Smith, Lloyd rederived Bennett's integral and the Panter-Dite formula based on the concept of point-density function. This was a critically important step for subsequent generalizations of Bennett's integral to vector quantizers. He also showed directly that in situations where the global optimum is the only local optimum, quantizers that satisfy the optimality conditions have, asymptotically, the optimal point density given by <ref type="bibr" target="#b8">(9)</ref>.</p><p>Unfortunately, Lloyd's work was not published in an archival journal at the time. Instead, it was presented at the 1957 Institute of Mathematical Statistics (IMS) meeting and appeared in print only as a Bell Laboratories Technical Memorandum. As a result, its results were not widely known in the engineering literature for many years, and many were independently rediscovered. All of the independent rediscoveries, however, used variational derivations, rather than Lloyd's simple derivations. The latter were essential for later extensions to vector quantizers and to the development of many quantizer optimization procedures. To our knowledge, the first mention of Lloyd's work in the IEEE literature came in 1964 with Fleischer's <ref type="bibr" target="#b175">[170]</ref> derivation of a sufficient condition (namely, that the log of the source density be concave) in order that the optimal quantizer be the only locally optimal quantizer, and consequently, that Lloyd's Method I yields a globally optimal quantizer. (The condition is satisfied for common densities such as Gaussian and Laplacian.) Zador <ref type="bibr" target="#b568">[561]</ref> had referred to Lloyd a year earlier in his Ph.D. dissertation, to be discussed later.</p><p>Later in the same year in another Bell Telephone Laboratories Technical Memorandum, Goldstein <ref type="bibr" target="#b212">[207]</ref> used variational methods to derive conditions for global optimality of a scalar quantizer in terms of second-order partial derivatives with respect to the quantizer levels and thresholds. He also provided a simple counterintuitive example of a symmetric density for which the optimal quantizer was asymmetric.</p><p>In 1959, Shtein <ref type="bibr" target="#b477">[471]</ref> added terms representing overload distortion to the formula and to Bennett's integral and used them to optimize uniform and nonuniform quantizers. Unaware of prior work, except for Bennett's, he rederived the optimal compressor characteristic and the Panter-Dite formula.</p><p>In 1960, Max <ref type="bibr" target="#b353">[349]</ref> published a variational proof of the Lloyd optimality properties for th-power distortion measures, rediscovered Lloyd's Method II, and numerically investigated the design of fixed-rate quantizers for a variety of input densities.</p><p>Also in 1960, Widrow <ref type="bibr" target="#b535">[529]</ref> derived an exact formula for the characteristic function of a uniformly quantized signal when the quantizer has an infinite number of levels. His results showed that under the condition that the characteristic function of the input signal be zero when its argument is greater than , the moments of the quantized random variable are the same as the moments of the signal plus an additive signal-independent random variable uniformly distributed on . This has often been misinterpreted as saying that the quantized random variable can be approximated as being the input plus signal-independent uniform noise, a clearly false statement since the quantizer error is a deterministic function of the signal. The "bandlimited" property of the characteristic function implies from Fourier transform theory that the probability density function must have infinite support since a signal and its transform cannot both be perfectly bandlimited.</p><p>We conclude this subsection by mentioning early work that appeared in the mathematical and statistical literature and which, in hindsight, can be viewed as related to scalar quantization. Specifically, in 1950-1951 Dalenius et al. <ref type="bibr" target="#b120">[118]</ref>, <ref type="bibr" target="#b121">[119]</ref> used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus <ref type="bibr">[336] (1955)</ref> developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd). <ref type="bibr">Cox in 1957 [111]</ref> also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later <ref type="bibr" target="#b486">[480]</ref>, <ref type="bibr" target="#b164">[159]</ref>, <ref type="bibr" target="#b568">[561]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scalar Quantization with Memory</head><p>It was recognized early that common sources such as speech and images had considerable "redundancy" that scalar quantization could not exploit. The term "redundancy" was commonly used in the early days and is still popular in some of the quantization literature. Strictly speaking, it refers to the statistical correlation or dependence between the samples of such sources and is usually referred to as memory in the information theory literature. As our current emphasis is historical, we follow the traditional language. While not disrupting the performance of scalar quantizers, such redundancy could be exploited to attain substantially better rate-distortion performance. The early approaches toward this end combined linear processing with scalar quantization, thereby preserving the simplicity of scalar quantization while using intuitionbased arguments and insights to improve performance by incorporating memory into the overall code. The two most important approaches of this variety were predictive coding and transform coding. A shared intuition was that a preprocessing operation intended to make scalar quantization more efficient should "remove the redundancy" in the data. Indeed, to this day there is a common belief that data compression is equivalent to redundancy removal and that data without redundancy cannot be further compressed. As will be discussed later, this belief is contradicted both by Shannon's work, which demonstrated strictly improved performance using vector quantizers even for memoryless sources, and by the early work of Fejes <ref type="bibr">Toth (1959)</ref>  <ref type="bibr" target="#b164">[159]</ref>. Nevertheless, removing redundancy leads to much improved codes.</p><p>Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo <ref type="bibr" target="#b132">[129]</ref>, but the most commonly cited early references are Cutler's patent <ref type="bibr" target="#b119">[117]</ref> 2 605 361 on "Differential quantization of communication signals" and on DeJager's Philips technical report on delta modulation <ref type="bibr" target="#b131">[128]</ref>. Cutler stated in his patent that it "is the object of the present invention to improve the efficiency of communication systems by taking advantage of correlation in the signals of these systems" and Derjavitch et al. also cited the reduction of redundancy as the key to the reduction of quantization noise. In 1950, Elias <ref type="bibr" target="#b144">[141]</ref> provided an information-theoretic development of the benefits of predictive coding, but the work was not published until 1955 <ref type="bibr" target="#b145">[142]</ref>. Other early references include <ref type="bibr" target="#b399">[395]</ref>, <ref type="bibr" target="#b304">[300]</ref>, <ref type="bibr" target="#b241">[237]</ref>, <ref type="bibr" target="#b517">[511]</ref>, and <ref type="bibr" target="#b579">[572]</ref>. In particular, <ref type="bibr" target="#b517">[511]</ref> claims Bennett-style asymptotics for highresolution quantization error, but as will be discussed later, such approximations have yet to be rigorously derived.</p><p>From the point of view of least squares estimation theory, if one were to optimally predict a data sequence based on its past in the sense of minimizing the mean-squared error, then the resulting error or residual or innovations sequence would be uncorrelated and it would have the minimum possible variance. To permit reconstruction in a coded system, however, the prediction must be based on past reconstructed samples and not true samples. This is accomplished by placing a quantizer inside a prediction loop and using the same predictor to decode the signal. A simple predictive quantizer or differential pulsecoded modulator (DPCM) is depicted in Fig. <ref type="figure" target="#fig_0">3</ref>. If the predictor is simply the last sample and the quantizer has only one bit, the system becomes a delta-modulator. Predictive quantizers are considered to have memory in that the quantization of a sample depends on previous samples, via the feedback loop.</p><p>Predictive quantizers have been extensively developed, for example there are many adaptive versions, and are widely used in speech and video coding, where a number of standards are based on them. In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref type="bibr" target="#b269">[265]</ref>, <ref type="bibr" target="#b378">[374]</ref>, <ref type="bibr" target="#b201">[196]</ref>, <ref type="bibr" target="#b428">[424]</ref>, <ref type="bibr" target="#b50">[50]</ref>, and <ref type="bibr" target="#b464">[458]</ref>, as well as survey papers <ref type="bibr" target="#b268">[264]</ref> and <ref type="bibr" target="#b203">[198]</ref>.</p><p>Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar-quantized. This view stems from the facts that a) it is the prediction errors rather than the source samples that are quantized, b) the overall quantization error precisely equals that of the scalar quantizer operating on the prediction errors, c) the operational distortion-rate function for scalar quantization is proportional to variance (more precisely, a scaling of the random variable being quantized by a factor results in a scaling of by ), and d) the density of the prediction error is usually sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref type="bibr" target="#b354">[350]</ref>, <ref type="bibr" target="#b400">[396]</ref>, <ref type="bibr" target="#b488">[482]</ref>, <ref type="bibr" target="#b401">[397]</ref>, <ref type="bibr" target="#b269">[265]</ref>. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions. However, it is not clear that the accuracy of this approximation increases sufficiently rapidly with finer resolution to ensure that the difference between the operational distortion-rate functions of the two types of prediction errors is small relative to their values, which are themselves decreasing as the resolution becomes finer. Indeed, it is still an open question whether this type of analysis, which typically uses Bennett and Panter-Dite formulas, is asymptotically correct. Nevertheless, the results of such high resolution approximations are widely accepted and often compare well with experimental results <ref type="bibr" target="#b160">[156]</ref>, <ref type="bibr" target="#b269">[265]</ref>. Assuming that they give the correct answer, then for large rates and a stationary, Gaussian source with memory, the distortion of an optimized DPCM quantizer is less than that of a scalar quantizer by the factor , where is the variance of the source and is the one-step prediction error; i.e., the smallest MSE of any prediction of one sample based on previous samples. It turns out that this exceeds by the same factor by which the distortion of optimal fixed-rate scalar quantization exceeds for a memoryless Gaussian source. Hence, it appears that DPCM does a good job of exploiting source memory given that it is based on scalar quantization, at least under the high-resolution assumption.</p><p>Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability <ref type="bibr" target="#b196">[191]</ref>, <ref type="bibr" target="#b285">[281]</ref>, <ref type="bibr" target="#b288">[284]</ref>, i.e., asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes <ref type="bibr" target="#b127">[124]</ref>, <ref type="bibr" target="#b479">[473]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b350">[346]</ref>, <ref type="bibr" target="#b245">[241]</ref>, <ref type="bibr" target="#b266">[262]</ref>, <ref type="bibr" target="#b160">[156]</ref>, <ref type="bibr" target="#b194">[189]</ref>, <ref type="bibr" target="#b195">[190]</ref>, <ref type="bibr" target="#b371">[367]</ref>- <ref type="bibr" target="#b373">[369]</ref>, <ref type="bibr" target="#b297">[293]</ref>, to analyses of distortion when the source is a Wiener process <ref type="bibr" target="#b168">[163]</ref>, <ref type="bibr" target="#b350">[346]</ref>, <ref type="bibr" target="#b244">[240]</ref>, and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral densities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier, and Grieg to quantizers inside a feedback loop <ref type="bibr" target="#b264">[260]</ref>, <ref type="bibr" target="#b73">[71]</ref>, <ref type="bibr" target="#b220">[215]</ref>, <ref type="bibr" target="#b221">[216]</ref>, <ref type="bibr" target="#b74">[72]</ref>. Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization <ref type="bibr" target="#b166">[161]</ref>, <ref type="bibr" target="#b208">[203]</ref>, <ref type="bibr" target="#b40">[41]</ref>, but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization <ref type="bibr" target="#b335">[331]</ref>, <ref type="bibr" target="#b418">[414]</ref>, <ref type="bibr" target="#b152">[148]</ref>, <ref type="bibr" target="#b540">[534]</ref>, <ref type="bibr" target="#b183">[178]</ref>, <ref type="bibr" target="#b385">[381]</ref>, <ref type="bibr" target="#b527">[521]</ref>.</p><p>Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing. Here, the source samples are collected into a vector of, say, dimension that is multiplied by an orthogonal matrix (an orthogonal transform) and the resulting transform coefficients are scalar quantized, usually with a different quantizer for each coefficient. The operation is depicted in Fig. <ref type="figure" target="#fig_1">4</ref>. This style of code was introduced in 1956 by Kramer and Mathews <ref type="bibr" target="#b303">[299]</ref> and analyzed and popularized in 1962-1963 by Huang and Schultheiss <ref type="bibr" target="#b251">[247]</ref>, <ref type="bibr" target="#b252">[248]</ref>. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheiss proved that decorrelating does indeed lead to optimal transform code design, at least in the case of Gaussian sources and high resolution. Transform coding has been extensively developed for coding images and video, where the discrete cosine transform (DCT) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b433">[429]</ref> is most commonly used because of its computational simplicity and its good performance. Indeed, DCT coding is the basic approach dominating current image and video coding standards, including H.261, H.263, JPEG, and MPEG. These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variablerate quantizer. For discussions of transform coding for images see <ref type="bibr" target="#b539">[533]</ref>, <ref type="bibr" target="#b426">[422]</ref>, <ref type="bibr" target="#b379">[375]</ref>, <ref type="bibr" target="#b269">[265]</ref>, <ref type="bibr" target="#b100">[98]</ref>, <ref type="bibr" target="#b378">[374]</ref>, <ref type="bibr" target="#b265">[261]</ref>, <ref type="bibr" target="#b428">[424]</ref>, <ref type="bibr" target="#b201">[196]</ref>, <ref type="bibr" target="#b213">[208]</ref>, <ref type="bibr" target="#b412">[408]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b464">[458]</ref>, and More recently, transform coding has also been widely used in high-fidelity audio coding <ref type="bibr" target="#b276">[272]</ref>, <ref type="bibr" target="#b205">[200]</ref>.</p><p>Unlike predictive quantizers, the transform coding approach lent itself quite well to the Bennett high-resolution approximations, the classical analysis being that of Huang and Schultheiss <ref type="bibr" target="#b251">[247]</ref>, <ref type="bibr" target="#b252">[248]</ref> of the performance of optimized transform codes for fixed-rate scalar quantizers for Gaussian sources, a result which demonstrated that the Karhunen-LoÃ¨ve decorrelating transform was optimum for this application for the given assumptions. If the transform is the Karhunen-LoÃ¨ve transform, then the coefficients will be uncorrelated (and hence independent if the input vector is also Gaussian). The seminal work of Huang and Schultheiss showed that high-resolution approximation theory could provide analytical descriptions of optimal performance and design algorithms for optimizing codes of a given structure. In particular, they showed that under the high-resolution assumptions with Gaussian sources, the average distortion of the best transform code with a given rate is less than that of optimal scalar quantization by the factor</p><p>, where is the average of the variances of the components of the source vector and is its covariance matrix. Note that this reduction in distortion becomes larger for sources with more memory (more correlation) because the covariance matrices of such sources have smaller determinants. When is large, it turns out that the distortion of optimized transform coding with a given rate exceeds by the same factor by which the distortion of optimal fixed-rate scalar quantization exceeds for a memoryless Gaussian source. Hence, like DPCM, transform coding does a good job of exploiting source memory given that it is a system based on scalar quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variable-Rate Quantization</head><p>Shannon's lossless source coding theory (1948) <ref type="bibr" target="#b470">[464]</ref> made it clear that assigning equal numbers of bits to all quantization cells is wasteful if the cells have unequal probabilities. Instead, the number of bits produced by the quantizer will, on the average, be reduced if shorter binary codewords are assigned to higher probability cells. Of course, this means that longer codewords will need to be assigned to the less probable cells, but Shannon's theory shows that, in general, there is a net gain. This leads directly to variable-rate quantization, which has the partition into cells and codebook of levels as before, but now has binary codewords of varying lengths assigned to the cells (alternatively, the levels). Ordinarily, the set of binary codewords is chosen to satisfy the prefix condition that no member is a prefix of another member, in order to insure unique decodability. As will be made precise in the next section, one may view a variable-rate quantizer as consisting of a partition, a codebook, and a lossless binary code, i.e., an assignment of binary codewords.</p><p>For variable-rate quantizers the rate is no longer defined as the logarithm of the codebook size. Rather, the instantaneous rate for a given input is the number of binary symbols in the binary codeword (the length of the binary codeword) and the rate is the average length of the binary codewords, where the average is taken over the probability distribution of the source samples. The operational distortion-rate function using this definition is the smallest average distortion over all (variable-rate) quantizers having rate or less. Since we have weakened the constraint by expanding the allowed set of quantizers, this operational distortion-rate function will ordinarily be smaller than the fixed-rate optimum.</p><p>Huffman's algorithm <ref type="bibr" target="#b255">[251]</ref> provides a systematic method of designing binary codes with the smallest possible average length for a given set of probabilities, such as those of the cells. Codes designed in this way are typically called Huffman codes. Unfortunately, there is no known expression for the resulting minimum average length in terms of the probabilities. However, Shannon's lossless source coding theorem implies that given a source and a quantizer partition, one can always find an assignment of binary codewords (indeed, a prefix set) with average length not more than , and that no uniquely decodable set of binary codewords can have average length less than , where is the Shannon entropy of the quantizer output and is the probability that the source sample lies in the th cell . Shannon also provided a simple way of attaining performance within the upper bound: if the quantizer index is , then assign it a binary codeword with length (the Kraft inequality ensures that this is always possible by simply choosing paths in a binary tree). Moreover, tighter bounds have been developed. For example, Gallager <ref type="bibr" target="#b186">[181]</ref> has shown that the entropy can be at most smaller than the average length of the Huffman code, when , the largest of the 's, is less than . See <ref type="bibr" target="#b75">[73]</ref> for discussion of this and other bounds. Since is ordinarily much smaller than , this shows that is generally a fairly accurate estimate of the average rate, especially in the high-resolution case.</p><p>Since there is no simple formula determining the rate of the Huffman code, but entropy provides a useful estimate, it is reasonable to simplify the variable-length quantizer design problem a little by redefining the instantaneous rate of a variablerate quantizer as for the th quantizer level and hence to define the average rate as , the entropy of its output. As mentioned above, this underestimates the true rate by a small amount that in no case exceeds one. We could again define an operational distortion-rate function as the minimum average distortion over all variable-rate quantizers with output entropy . Since the quantizer output entropy is a lower bound to actual rate, this operational distortion-rate function may be optimistic; i.e., it falls below defined using average length as rate. A quantizer designed to provide the smallest average distortion subject to an entropy constraint is called an entropy-constrained scalar quantizer.</p><p>Variable-rate quantization is also called variable-length quantization or quantization with entropy coding. We will not, except where critical, take pains to distinguish entropyconstrained quantizers and entropy-coded quantizers. And we will usually blur the distinction between average length and entropy as measures of the rate of such quantizers unless, again, it is important in some particular discussion. This is much the same sort of blurring as using instead of as the measure of rate in fixed-rate quantization. It is important to note that the number of quantization cells or levels does not play a primary role in variable-rate quantization because, for example, there can be many levels in places where the source density is small with little effect on either distortion or rate. Indeed, the number of levels can be infinite, which has the advantage of eliminating the overload region and resulting overload distortion.</p><p>A potential drawback of variable-rate quantization is the necessity of dealing with the variable numbers of bits that it produces. For example, if the bits are to be communicated through a fixed-rate digital channel, one will have to use buffering and to take buffer overflows and underflows into account. Another drawback is the potential for error propagation when bits are received by the decoder in error.</p><p>The most basic and simple example of a variable-rate quantizer, and one which plays a fundamental role as a benchmark for comparison, is a uniform scalar quantizer with a variable-length binary lossless code.</p><p>The possibility of applying variable-length coding to quantization may well have occurred to any number of people who were familiar with both quantization and Shannon's 1948 paper. The earliest references to such that we have found are in the 1952 papers by Kretzmer <ref type="bibr" target="#b304">[300]</ref> and Oliver <ref type="bibr" target="#b399">[395]</ref>. In 1960, Max <ref type="bibr" target="#b353">[349]</ref> had such in mind when he computed the entropy of nonuniform and uniform quantizers that had been designed to minimize distortion for a given number of levels. For a Gaussian source, his results showed that variable-length coding would yield rate reductions of about 0.5 bit/sample.</p><p>High-resolution analysis of variable-rate quantization developed in a handful of papers from 1958 to 1968. However, since these papers were widely scattered or unpublished, it was not until 1968 that the situation was well understood in the IEEE community.</p><p>The first high-resolution analysis was that of <ref type="bibr" target="#b468">Schutzenberger (1958)</ref>  <ref type="bibr" target="#b468">[462]</ref> who showed that the distortion of optimized variable-rate quantization (both scalar and vector) decreases with rate as , just as with fixed-rate quantization. But he did not find the multiplicative factors, nor did he describe the nature of the partitions and codebooks that are best for variable-rate quantization.</p><p>In 1959, Renyi <ref type="bibr" target="#b437">[433]</ref> showed that a uniform scalar quantizer with infinitely many levels and small cell width has output entropy given approximately by <ref type="bibr" target="#b10">(11)</ref> where is the differential entropy of the source variable .</p><p>In 1963, Koshelev <ref type="bibr" target="#b586">[579]</ref> discovered the very interesting fact that in the high-resolution case, the mean-squared error of uniform scalar quantization exceeds that of the least distortion achievable by any quantization scheme whatsoever, i.e., , by a factor of only . Equivalently, the induced signal-to-noise ratio is only 1.53 dB less than the best possible, or for a fixed distortion , the rate is only 0.255 bit/sample larger than that achievable by the best quantizers. (For the Gaussian source, it gains 2.82 dB or 0.47 bit/sample over the best fixed-rate scalar quantizer.) It is also of interest to note that this was the first paper to compare the performance of a specific quantization scheme to . Unfortunately, Koshelev's paper was published in a journal that was not widely circulated.</p><p>In an unpublished 1966 Bell Telephone Laboratories Technical Memo <ref type="bibr" target="#b569">[562]</ref>, Zador also studied variable-rate (as well as fixed-rate) quantization. As his focus was on vector quantization, his work will be described later. Here we only point out that for variable-rate scalar quantization with large rate, his results showed that the operational distortion-rate function (i.e., the least distortion of such codes with a given rate) is <ref type="bibr" target="#b11">(12)</ref> Though he was not aware of it, this turns out to be the formula found by Koshelev, therby demonstrating that in the high-resolution case, uniform is the best type of scalar quantizer when variable-rate coding is applied.</p><p>Finally, in 1967 and 1968 two papers appeared in the IEEE literature (in fact in these TRANSACTIONS) on variable-rate quantization, without reference to any of the aforementioned work. The first, by Goblick and Holsinger <ref type="bibr" target="#b210">[205]</ref>, showed by numerical evaluation that uniform scalar quantization with variable-rate coding attains performance within about 1.5 dB (or 0.25 bit/sample) of the best possible for an i.i.d. Gaussian source. The second, by Gish and Pierce <ref type="bibr" target="#b209">[204]</ref>, demonstrated analytically what the first paper had found empirically. Specifically, it derived <ref type="bibr" target="#b10">(11)</ref>, and more generally, the fact that a high-resolution nonuniform scalar quantizer has output entropy <ref type="bibr" target="#b12">(13)</ref> where is the unnormalized point density of the quantizer. They then used these approximations along with Bennett's integral to rederive <ref type="bibr" target="#b11">(12)</ref> and to show that in the highresolution case, uniform scalar quantizers achieve the operational distortion-rate function of variable-rate quantization. Next, by comparing to what is called the Shannon lower bound to , they showed that for i.i.d. sources, the latter is only 1.53 dB (0.255 bit/sample) from the best possible performance of any quantization system whatsoever, which is what Koshelev <ref type="bibr" target="#b586">[579]</ref> found earlier. Their results showed that such good performance was attainable for any source distribution, not just the Gaussian case checked by Goblick and Holsinger. They also generalized the results from squared-error distortion to nondecreasing functions of magnitude error.</p><p>Less well known is their proof of the fact that in the high resolution case, the entropy of successive outputs of a uniformly scalar quantized stationary source, e.g., with memory, is <ref type="bibr" target="#b13">(14)</ref> They used this, and the generalization of (13) to vectors, to show that when rate and are large, uniform scalar quantization with variable-length coding of successive quantizer outputs (block entropy coding) achieves performance that is 1.53 dB (0.255 bit/sample) from , even for sources with memory. (They accomplished this by comparing to Shannon lower bounds.) This important result was not widely appreciated until rediscovered by <ref type="bibr">Ziv (1985)</ref>  <ref type="bibr" target="#b585">[578]</ref>, who also showed that a similar result holds for small rates. Note that although uniform scalar quantizers are quite simple, the lossless code capable of approaching the th-order entropy of the quantized source can be quite complicated. In addition, Gish and Pierce observed that when coding vectors, performance could be improved by using quantizer cells other than the cube implicitly used by uniform scalar quantizers and noted that the hexagonal cell was superior in two dimensions, as originally demonstrated by Fejes Toth <ref type="bibr" target="#b164">[159]</ref> and Newman <ref type="bibr" target="#b389">[385]</ref>.</p><p>Though uniform quantization is asymptotically best for entropy-constrained quantization, at lower rates nonuniform quantization can do better, and a series of papers explored algorithms for designing them. In 1969, Wood <ref type="bibr" target="#b545">[539]</ref> provided a numerical descent algorithm for designing an entropyconstrained scalar quantizer, and showed, as predicted by Gish and Pierce, that the performance was only slightly superior to a uniform scalar quantizer followed by a lossless code.</p><p>In a 1972 paper dealing with a vector quantization technique to be discussed later, Berger <ref type="bibr" target="#b46">[47]</ref> described Lloyd-like conditions for optimality of an entropy-constrained scalar quantizer for squared-error distortion. He formulated the optimization as an unconstrained Lagrangian minimization and developed an iterative algorithm for the design of entropy-constrained scalar quantizers. He showed that Gish and Pierce's demonstration of approximate optimality of uniform scalar quantization for variable-rate quantization holds approximately even when the rate is not large and holds exactly for exponential densities, provided the levels are placed at the centroids. In 1976, Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint <ref type="bibr" target="#b380">[376]</ref>. Yet another approach was taken by <ref type="bibr" target="#b395">Noll and Zelinski (1978)</ref>  <ref type="bibr" target="#b395">[391]</ref>. Berger refined his approach to entropy-constrained quantizer design in <ref type="bibr" target="#b48">[48]</ref>.</p><p>Variable-rate quantization was also extended to DPCM and transform coding, where high-resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing <ref type="bibr" target="#b158">[154]</ref>, <ref type="bibr" target="#b402">[398]</ref>. We note, however, that the variable-rate quantization analysis for DPCM suffers from the same flaws as the fixed-rate quantization analysis for DPCM.</p><p>Numerous extensions of the Bennett-style asymptotic approximations and the approximation of or and the characterizations of properties of optimal high-resolution quantization for both fixed-and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., <ref type="bibr" target="#b503">[497]</ref>, <ref type="bibr" target="#b504">[498]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b473">[467]</ref>, <ref type="bibr" target="#b7">[8]</ref>. An excellent summary of the early work is contained in a 1970 paper by Elias <ref type="bibr" target="#b146">[143]</ref>.</p><p>We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization. It is worth pointing out how the standard resembles and differs from the models considered thus far. As previously stated, the transform coefficients are separately quantized by possibly different uniform quantizers, the bin lengths of the quantizers being determined by a customizable quantization table. This typically produces a quantized transformed image with many zeros. The lossless, variable-length code then scans the image in a zig-zag (or Peano) fashion, producing a sequence of runlengths of the zeros and indices corresponding to nonzero values, which are then Huffman-coded (or arithmetic-coded). This procedure has the effect of coding only the transform coefficients with the largest magnitude, which are the ones most important for reconstruction. The early transform coders typically coded the first, say, coefficients, and ignored the rest. In essence, the method adopted for the standards selectively coded the most important coefficients, i.e., those having the largest magnitude, rather than simply the lowest frequency coefficients. The runlength coding step can in hindsight be viewed as a simple way of locating the most significant coefficients, which in turn are described the most accurately. This implicit "significance" map was an early version of an idea that would later be essential to wavelet coders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Beginnings of Vector Quantization</head><p>As described in the three previous subsections, the 1940's through the early 1970's produced a steady stream of advances in the design and analysis of practical quantization techniques, principally scalar, predictive, transform, and variablerate quantization, with quantizer performance improving as these decades progressed. On the other hand, at roughly the same time there was a parallel series of developments that were more concerned with the fundamental limits of quantization than with practical quantization issues. We speak primarily of the remarkable work of Shannon and the very important work of Zador, though there were other important contributors as well. This work dealt with what is now called vector quantization (VQ) (or block or multidimensional quantization), which is just like scalar quantization except that all components of a vector, of say successive source samples, are quantized simultaneously. As such they are characterized by a -dimensional partition, a -dimensional codebook (containing -dimensional points, reproduction codewords or codevectors), and an assignment of binary codewords to the cells of the partition (equivalently, to the codevectors).</p><p>An immediate advantage of vector quantization is that it provides a model of a general quantization scheme operating on vectors without any structural constraints. It clearly includes transform coding as a special case and can also be considered to include predictive quantization operating locally within the vector. This lack of structural constraints makes the general model more amenable to analysis and optimization. In these early decades, vector quantization served primarily as a paradigm for exploring fundamental performance limits; it was not yet evident whether it would become a practical coding technique.</p><p>Shannon's Source Coding Theory: In his classic 1948 paper, Shannon <ref type="bibr" target="#b470">[464]</ref> sketched the idea of the rate of a source as the minimum bit rate required to reconstruct the source to some degree of accuracy as measured by a fidelity criterion such as mean-squared error. The sketch was fully developed in his 1959 paper <ref type="bibr" target="#b471">[465]</ref> for i.i.d. sources, additive measures of distortion, and block source codes, now called vector quantizers. In this later paper, Shannon showed that when coding at some rate , the least distortion achievable by vector quantizers of any kind is equal to a function , subsequently called the Shannon distortion-rate function, that is determined by the statistics of the source and the measure of distortion. <ref type="foot" target="#foot_0">2</ref>To elaborate on Shannon's theory, we note that one can immediately extend the quantizer notation of (1), the distortion and rate definitions of ( <ref type="formula">2</ref>) and (3), and the operational distortion-rate functions to define the smallest distortion possible for a -dimensional fixed-rate vector quantizer that achieves rate or less. (The distortion between twodimensional vectors is defined to be the numerical average of the distortions between their respective components. The rate is times the (average) number of bits to describe a -dimensional source vector.) We will make the dimension explicit in the notation when we are allowing it to vary and omit it when not. Furthermore, as with Shannon's channel coding and lossless source coding theories, one can consider the best possible performance over codes of all dimensions (assuming the data can be blocked into vectors of arbitrary size) and define an operational distortion-rate function <ref type="bibr" target="#b14">(15)</ref> The operational rate-distortion functions and are defined similarly. For finite dimension , the function will depend on the definition of rate, i.e., whether it is the log of the reproduction size, the average binary codeword length, or the quantizer output entropy. It turns out, however, that is not affected by this choice. That is, it is the same for all definitions of rate.</p><p>For an i.i.d. source , the Shannon distortion-rate function is defined as the minimum average distortion over all conditional distributions of given for which the mutual information is at most , where we emphasize that and are scalar variables here. In his principal result, the coding theorem for source coding with a fidelity criterion, Shannon showed that for every , . That is, no VQ of any dimension with rate could yield smaller average distortion than , and that for some dimension-possibly very large-there exists a VQ with rate no greater than and distortion very nearly . As an illustrative example, the Shannon distortion-rate function of an i.i.d. Gaussian source with variance is <ref type="bibr" target="#b15">(16)</ref> where is the variance of the source. Equivalently, the Shannon rate-distortion function is , . Since it is also known that this represents the best possible performance of any quantization scheme whatsoever, it is these formulas that we used previously when comparing the performance of scalar quantizers to that of the best quantization schemes. For example, comparing <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b15">(16)</ref>, one sees why we made earlier the statement that the operational distortion-rate function of scalar quantization is times larger than . Notice that <ref type="bibr" target="#b15">(16)</ref> shows that for this source the exponential rate of decay of distortion with rate, demonstrated by high resolution arguments for high rates, extends to all rates. This is not usually the case for other sources.</p><p>Shannon's approach was subsequently generalized to sources with memory, cf. <ref type="bibr" target="#b185">[180]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, [218], <ref type="bibr" target="#b555">[549]</ref>, <ref type="bibr" target="#b130">[127]</ref>, <ref type="bibr" target="#b129">[126]</ref>, <ref type="bibr" target="#b286">[282]</ref>, <ref type="bibr" target="#b287">[283]</ref>, <ref type="bibr" target="#b141">[138]</ref>, and <ref type="bibr" target="#b485">[479]</ref>. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and ratedistortion functions in that they are infima of th-order functions. For example, the th-order distortion-rate function of a stationary random process is defined as an infimum of the average distortion over all conditional probability distributions of given for which average mutual information . The distortion-rate function for the process is then given by . For i.i.d. sources</p><p>, where is what we previously called for i.i.d. sources. (The rate-distortion functions and are defined similarly.) A source coding theorem then shows under appropriate conditions that, for sources with memory, for all rates . In other words, Shannon's distortion-rate function represents an asymptotically achievable, but never beatable, lower bound to the performance of any VQ of any dimension. The positive coding theorem demonstrating that the Shannon distortion-rate function is in fact achievable if one allows codes of arbitrarily large dimension and complexity is difficult to prove, but the existence of good codes rests on the law of large numbers, suggesting that large dimensions might indeed be required for good codes, with consequently large demands on complexity, memory, and delay.</p><p>Shannon's results, like those of Panter and Dite, Zador, and Gish and Pierce provide benchmarks for comparison for quantizers. However, Shannon's results provide an interesting contrast with these early results on quantizer performance. Specifically, the early quantization theory had derived the limits of scalar quantizer performance based on the assumption of high resolution and showed that these bounds were achievable by a suitable choice of quantizer. Shannon, on the other hand, had fixed a finite, nonasymptotic rate, but had considered asymptotic limits as the dimension of a vector quantizer was allowed to become arbitrarily large. The former asymptotics, high resolution for fixed dimension, are generally viewed as quantization theory, while the latter, fixed-rate and high dimension, are generally considered to be source coding theory or information theory. Prior to 1960, quantization had been viewed primarily as PCM, a form of analog-todigital conversion or digital modulation, while Shannon's source coding theory was generally viewed as a mathematical approach to data compression. The first to explicitly apply Shannon's source coding theory to the problem of analog-todigital conversion combined with digital transmission appear to be Goblick and Holsinger <ref type="bibr" target="#b210">[205]</ref> in 1967, and the first to make explicit comparisons of quantizer performance to Shannon's rate-distortion function was Koshelev <ref type="bibr" target="#b586">[579]</ref> in 1963.</p><p>A distinct variation on the Shannon approach was introduced to the English literature in 1956 by Kolmogorov <ref type="bibr" target="#b292">[288]</ref>, who described several results by Russian information theorists inspired by Shannon's 1948 treatment of coding with respect to a fidelity criterion. Kolmogorov considered two notions of the rate with respect to a fidelity criterion: His second notion was the same as Shannon's, where a mutual information was minimized subject to a constraint on the average distortion, in this case measured by squared error. The first peformed a similar minimization of mutual information, but with the requirement that maximum distortion between the input and reproduction did not exceed a specified level . Kolmogorov referred to both functions as the " -entropy" of a random object , but the name has subsequently been considered to apply to the maximum distortion being constrained to be less than , rather than the Shannon function, later called the rate-distortion function, which constrained the average distortion. Note that the maximum distortion with respect to a distortion measure can be incorporated in the average distortion formulation if one considers a new distortion measure defined by if otherwise.</p><p>(</p><formula xml:id="formula_0">)<label>17</label></formula><p>As with Shannon's rate-distortion function, this was an information-theoretic definition. As with quantization, there are corresponding operational definitions. The operational epsilon entropy ( -entropy) of a random variable can be defined as the smallest entropy of a quantized output such that the reproduction is no further from the input than (at least with probability ): <ref type="bibr" target="#b17">(18)</ref> This is effectively a variable-rate definition since lossless coding would be required to achieve a bit rate near the entropy. Alternatively, one could define the operational epsilon entropy as , where is the smallest number of reproduction codevectors for which all inputs are (with probability ) within of a codevector. This quantity is clearly infinite if the random object does not have finite support. As in the Shannon case, all these definitions can be made for -dimensional vectors and the limiting behavior can be studied. Results regarding the convergence of such limits and the equality of the informationtheoretic and operational notions of epsilon entropy can be found, e.g., in <ref type="bibr" target="#b425">[421]</ref>, <ref type="bibr" target="#b424">[420]</ref>, <ref type="bibr" target="#b282">[278]</ref>, and <ref type="bibr" target="#b59">[59]</ref>. Much of the theory is concerned with approximating epsilon entropy for small .</p><p>Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability.</p><p>Here the epsilon entropy becomes the log of the smallest number of balls of radius required to cover a compact metric space (e.g., a function space-see, e.g., <ref type="bibr" target="#b526">[520]</ref> and <ref type="bibr" target="#b424">[420]</ref> for a discussion of various notions of epsilon entropy).</p><p>We mention epsilon entropy because of its close mathematical connection to rate-distortion theory. Our emphasis, however, is on codes that minimize average, not maximum, distortion.</p><p>The Earliest Vector Quantization Work: Outside of Shannon's sketch of rate-distortion theory in 1948, the earliest work with a definite vector quantization flavor appeared in the mathematical and statistical literature. Most important was the remarkable work of Steinhaus in 1956 <ref type="bibr" target="#b486">[480]</ref>, who considered a problem equivalent to a three-dimensional generalization of scalar quantization with a squared-error distortion measure.</p><p>Suppose that a mass density is defined on Euclidean space. For any finite , let be a partition of Euclidean space into disjoint bodies (cells) and let be a collection of vectors, one associated with each cell of the partition. What partition and collection of vectors minimizes the sum of the moments of inertia of the cells about the associated vectors? This problem is formally equivalent to a fixed-rate three-dimensional vector quantizer with a squared-error distortion measure and a probability density .</p><p>Steinhaus derived what we now consider to be the Lloyd optimality conditions (centroid and nearest neighbor mapping) from fundamental principles (without variational techniques), proved the existence of a solution, and described the iterative descent algorithm for finding a good partition and vector collection. His derivation applies immediately to any finite-dimensional space and hence, like Lloyd's, extends immediately to vector quantization of any dimension. Steinhaus was aware of the problems with local optima, but stated that "generally" there would be a unique solution. No mention is made of "quantization," but this appears to be the first paper to both state the vector quantization problem and to provide necessary conditions for a solution, which yield a design algorithm.</p><p>In 1959, Fejes Toth described the specific application of Steinhaus' problem in two dimensions to a source with a uniform density on a bounded support region and to quantization with an asymptotically large number of points <ref type="bibr" target="#b164">[159]</ref>. Using an earlier inequality of his <ref type="bibr" target="#b163">[158]</ref>, he showed that the optimal twodimensional quantizer under these assumptions tessellated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. It was rederived in a 1964 Bell Laboratories Technical Memorandum by Newman <ref type="bibr" target="#b389">[385]</ref>; its first appearance in English. It made a particularly important point: even in the simple case of two independent uniform random variables, with no redundancy to remove, the performance achievable by quantizing vectors using a hexagonal-lattice encoding partition is strictly better than that achievable by uniform scalar quantization, which can be viewed as a two-dimensional quantizer with a square encoding lattice.</p><p>The first high-resolution approximations for vector quantization were published by Schutzenberger in 1958 <ref type="bibr" target="#b468">[462]</ref>, who found upper and lower bounds to the least distortion ofdimensional variable-rate vector quantizers, both of the form . Unfortunately, the upper and lower bounds diverge as increases.</p><p>In 1963, Zador <ref type="bibr" target="#b568">[561]</ref> made a very large advance by using high-resolution methods to show that for large rates, the operational distortion-rate function of fixed-rate quantization has the form <ref type="bibr" target="#b18">(19)</ref> where is a term that is independent of the source, is the -dimensional source density, and is the term that depends on the source. This generalized the Panter-Dite formula to the vector case. While the formula for obviously matches the Shannon distortion-rate function when both dimension and rate are large (because in this case both are approximations to ), Zador's formula has the advantage of being applicable for any dimension while the Shannon theory is applicable only for large . On the other hand, Shannon theory is applicable for any rate while high resolution theory is applicable only for large rates. Thus the two theories are complementary. Zador also explicitly extended Lloyd's optimality properties to vectors with distortion measures that were integer powers of the Euclidean norm, thereby also generalizing Steinhaus' results to dimensions higher than three, but he did not specifically consider descent design algorithms. Unfortunately, the results of Zador's thesis were not published until 1982 <ref type="bibr" target="#b570">[563]</ref> and were little known outside of Bell Laboratories until Gersho's important paper of 1979 <ref type="bibr" target="#b198">[193]</ref>, to be described later.</p><p>Zador's dissertation also dealt with the analysis of variablerate vector quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo <ref type="bibr" target="#b569">[562]</ref> to derive the correct formula. (Curiously, his 1982 paper <ref type="bibr" target="#b570">[563]</ref> reports the formula from the thesis rather than the memo.) Again using high-resolution methods, he showed that for large rates, the operational distortion-rate function of variable-rate vector quantization has the form <ref type="bibr" target="#b19">(20)</ref> where is a term that is independent of the source and is the dimension-normalized differential entropy of the source. This completed what he and Schutzenberger had begun.</p><p>In the mid-1960's, the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey <ref type="bibr" target="#b177">[172]</ref>, Ball and Hall <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b234">[230]</ref>, Jancey <ref type="bibr" target="#b267">[263]</ref>, and in 1969 by MacQueen <ref type="bibr" target="#b345">[341]</ref> (the " -means" algorithm). These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represent a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology, cf. Anderberg <ref type="bibr" target="#b8">[9]</ref>, Diday and Simon <ref type="bibr" target="#b136">[133]</ref>, or Hartigan <ref type="bibr" target="#b242">[238]</ref>. MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimumdistortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid. Forgey and Jancey used simultaneous updates of all centroids, as did Steinhaus and Lloyd.</p><p>Unfortunately, many of these early results did not propagate among the diverse groups working on similar problems. Zador's extensions of Lloyd's results were little known outside of Bell Laboratories. The work of Steinhaus has been virtually unknown in the quantization community until recently. The work in the clustering community on what were effectively vector quantizer design algorithms in the context of statistical clustering was little known at the time in the quantization community, and it was not generally appreciated that Lloyd's algorithm was in fact a clustering algorithm. Part of the lack of interest through the 1950's was likely due to the fact that there had not yet appeared any strong motivation to consider the quantization of vectors instead of scalars. This motivation came as a result of Shannon's landmark 1959 paper on source coding with a fidelity criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementable Vector Quantizers</head><p>As mentioned before, it was not evident from the earliest studies that vector quantization could be a practical technique. The only obvious encoding procedure is brute-force nearest neighbor encoding: compare the source vector to be quantized with all reproduction vectors in the codebook. Since a (fixedrate) VQ with dimension and rate has codevectors, the number of computations required to do this grows exponentially with the dimension-rate product , and gets quickly out of hand. For example, if and , there are roughly one million codevectors. Moreover, these codevectors need to be stored, which also consumes costly resources. Finally, the proof of Shannon's source coding theorem relies on the dimension becoming large, suggesting that large dimension might be needed to attain good performance. As a point of reference, we note that in the development of channel codes, for which Shannon's theory had also suggested large dimension, it was common circa 1970 to consider channel codes with dimensions on the order of 100 or more. Thus it no doubt appeared to many that similarly large dimensions might be needed for effective quantization. Clearly, a bruteforce implementation of VQ with such dimensions would be out of the question. On the other hand, the channel codes of this era with large dimension and good performance, e.g., BCH codes, were highly structured so that encoding and decoding need not be done by brute force.</p><p>From the above discussion, it should not be surprising that the first VQ intended as a practical technique had a reproduction codebook that was highly structured in order to reduce the complexity of encoding and decoding. Specifically, we speak of the fixed-rate vector quantizer introduced in 1965 by Dunn <ref type="bibr" target="#b140">[137]</ref> for multidimensional i.i.d. Gaussian vectors. He argued that his code was effectively a permutation code as earlier used by Slepian <ref type="bibr" target="#b478">[472]</ref> for channel coding, in that the reproduction codebook contains only codevectors that are permutations of each other. This leads to a quantizer with reduced (but still fairly large) complexity. Dunn compared numerical computations of the performance of this scheme to the Shannon rate-distortion function. As mentioned earlier, this was the first such comparison. In 1972, Berger, Jelinek, and Wolf <ref type="bibr" target="#b49">[49]</ref>, and Berger <ref type="bibr" target="#b46">[47]</ref> introduced lower complexity encoding algorithms for permutation codes, and Berger <ref type="bibr" target="#b46">[47]</ref> showed that for large dimensions, the operational distortionrate function of permutation codes is approximately equal to that of optimal variable-rate scalar quantizers. While they do not attain performance beyond that of scalar quantization, permutation codes have the advantage of avoiding the buffering and error propagation problems of variable-rate quantization.</p><p>Notwithstanding the skepticism of some about the feasibility of brute-force unstructured vector quantization, serious studies of such began to appear in the mid-1970's, when several independent results were reported describing applications of clustering algorithms, usually -means, to problems of vector quantization. In 1974-1975, Chaffee <ref type="bibr" target="#b78">[76]</ref> and Chaffee and Omura <ref type="bibr" target="#b79">[77]</ref> used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977, Hilbert used clustering algorithms for joint image compression and image classification <ref type="bibr" target="#b246">[242]</ref>. These papers appear to be the first applications of direct vector quantization for speech and image coding applications. Also in 1977, Chen used an algorithm equivalent to a two-dimensional Lloyd algorithm to design two-dimensional vector quantizers <ref type="bibr" target="#b89">[87]</ref>.</p><p>In 1978 and 1979, a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref type="bibr" target="#b224">[220]</ref>, <ref type="bibr" target="#b68">[67]</ref>, <ref type="bibr" target="#b69">[68]</ref>, <ref type="bibr" target="#b227">[223]</ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure <ref type="bibr" target="#b262">[258]</ref>, <ref type="bibr" target="#b263">[259]</ref>, <ref type="bibr" target="#b261">[257]</ref>. Also in 1978, Adoul, Collin, and Dalle <ref type="bibr" target="#b2">[3]</ref> used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 <ref type="bibr" target="#b76">[74]</ref> and Menez, Boeri, and Esteban in 1979 <ref type="bibr" target="#b357">[353]</ref> also considered clustering algorithms for the design of vector quantizers with squared error and magnitude error distortion measures.</p><p>The most important paper on quantization during the 1970's was without a doubt Gersho's paper on "Asymptotically optimal block quantization" <ref type="bibr" target="#b198">[193]</ref>. The paper popularized high resolution theory and the potential performance gains of vector quantization, provided new, simplified variations and proofs of Zador's results and vector extensions of Gish and Pierce's results with squared-error distortion, and introduced lattice vector quantization as a means of achieving the asymptotically optimal quantizer point density for entropy-constrained vector quantization for a random vector with bounded support. The simple derivations combined the vector quantizer point-density approximations with the use of HÃ¶lder's and Jensen's inequalities, generalizing a scalar quantizer technique introduced in 1977 <ref type="bibr" target="#b226">[222]</ref>. One step of the development rested on a still unproved conjecture regarding the asymptotically optimal quantizer cell shapes and Zador's constants, a conjecture which since has borne Gersho's name and which will be considered at some length in Section IV. Portions of this work were extended to nondecreasing functions of norms in <ref type="bibr" target="#b561">[554]</ref>.</p><p>Gersho's work stimulated renewed interest in the theory and design of direct vector quantizers and demonstrated that, contrary to the common impression that very large dimensions were required, significant gains could be achieved over scalar quantization by quantizing vectors of modest dimension and, as a result, such codes might be competitive with predictive and transform codes in some applications.</p><p>In 1980, Linde, Buzo, and Gray explicitly extended Lloyd's algorithm to vector quantizer design <ref type="bibr" target="#b322">[318]</ref>. As we have seen, the clustering approach to vector quantizer design originated years earlier, but the Linde et al. paper introduced it as a direct extension to the original Lloyd optimal PCM design algorithm, extended it to more general distortion measures than had been previously considered (including an input-weighted quadratic distortion useful in speech coding), and succeeded in popularizing the algorithm to the point that it is often referred to as the "LBG algorithm." A "splitting" method for designing the quantizer from scratch was developed, wherein one first designs a quantizer with two words ( -means), then doubles the codebook size by adding a new codevector near each existing codevector, then runs Lloyd's algorithm again, and so on. The numerical examples of quantizer design complemented Gersho's high-resolution results much as Lloyd's had complemented Panter and Dite: it was shown that even with modest dimensions and modest rates, significant gains over scalar quantization could be achieved by direct vector quantization of modest complexity. Later in the same year, Buzo et al. <ref type="bibr" target="#b70">[69]</ref> developed a tree-structured vector quantizer (TSVQ) for ten-dimensional LPC vectors that greatly reduced the encoder complexity from exponential growth with codebook size to linear growth by searching a sequence of small codebooks instead of a single large codebook. The result was an 800-bits/s LPC speech coder with intelligible quality comparable to that of scalar-quantized LPC speech coders of four times the rate. (See also <ref type="bibr" target="#b544">[538]</ref>.) In the same year, Adoul, Debray, and Dalle <ref type="bibr" target="#b3">[4]</ref> also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki <ref type="bibr" target="#b557">[551]</ref>.</p><p>In hindsight, the surprising effectiveness of low-dimensional VQ, e.g., to , can be explained by the fact that in Shannon's theory large dimension is needed to attain performance arbitrarily close to the ideal. In channel coding at rates less than capacity, ideal performance means zero error probability, and large dimension is needed for codes to approach this. However, when quantizing at a given rate , ideal performance means distortion equal to . Since this is not zero, there is really no point to making the difference between actual and ideal performance arbitrarily small. For example, it might be enough to come within 5% to 20% (0.2 to 0.8 dB) of , which does not require terribly large dimension. We will return to this in Section IV with estimates of the required dimension.</p><p>There followed an active period for all facets of quantization theory and design. Many of these results developed early in the decade were fortuitously grouped in the March 1982 special issue on Quantization of these TRANSACTIONS, which published the Bell Laboratories Technical Memos of Lloyd, Newman, and Zador along with Berger's extension of the optimality properties of entropy-constrained scalar quantization to thpower distortion measures and his extensive comparison of minimum-entropy quantizers and fixed-rate permutation codes <ref type="bibr" target="#b48">[48]</ref>, generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima <ref type="bibr" target="#b509">[503]</ref>, results on the asymptotic behavior of Lloyd's algorithm with training-sequence size based on the theory of -means consistency by Pollard <ref type="bibr" target="#b422">[418]</ref>, two seminal papers on lattice quantization by Conway and Sloane <ref type="bibr" target="#b105">[103]</ref>, <ref type="bibr" target="#b106">[104]</ref>, rigorous developments of the Bennett theory for vector quantizers and th-power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations <ref type="bibr" target="#b285">[281]</ref>, Kieffer's study of the convergence rate of Lloyd's algorithm <ref type="bibr" target="#b284">[280]</ref>, and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard <ref type="bibr" target="#b192">[187]</ref>.</p><p>Toward the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref type="bibr" target="#b200">[195]</ref>, <ref type="bibr" target="#b219">[214]</ref>, <ref type="bibr" target="#b346">[342]</ref>, <ref type="bibr" target="#b376">[372]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. The Mid-1980's to the Present</head><p>In the middle to late 1980's, a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing <ref type="bibr" target="#b143">[140]</ref>, <ref type="bibr" target="#b513">[507]</ref>, <ref type="bibr" target="#b174">[169]</ref>, <ref type="bibr" target="#b293">[289]</ref>, deterministic annealing <ref type="bibr" target="#b449">[445]</ref>- <ref type="bibr" target="#b452">[447]</ref>, pairwise nearest neighbor <ref type="bibr" target="#b150">[146]</ref> (which had its origins in earlier clustering techniques <ref type="bibr" target="#b530">[524]</ref>), stochastic relaxation <ref type="bibr" target="#b574">[567]</ref>, <ref type="bibr" target="#b578">[571]</ref>, selforganizing feature maps <ref type="bibr" target="#b294">[290]</ref>, <ref type="bibr" target="#b550">[544]</ref>, <ref type="bibr" target="#b551">[545]</ref>, and other neural nets <ref type="bibr" target="#b501">[495]</ref>, <ref type="bibr" target="#b305">[301]</ref>, <ref type="bibr" target="#b498">[492]</ref>, <ref type="bibr" target="#b341">[337]</ref>, <ref type="bibr" target="#b66">[65]</ref>. A variety of quantization techniques were introduced by constraining the structure of the vector quantization to better balance complexity with performance and these methods were applied to real signals (especially speech and images) as well as to random sources, which permitted comparison to the theoretical high-resolution and Shannon bounds. The literature begins to grow too large to cite all works of possible interest, but several of the techniques will be considered in Section V. Here, we only mention several examples with references and leave further discussion to Section V.</p><p>As will be discussed in some depth in Section V, fast search algorithms were developed for unstructured reproduction codebooks, and even faster searches for reproduction codebooks constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ <ref type="bibr" target="#b169">[164]</ref>, which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm. A tree-structured VQ with far less memory is provided by a multistage or residual VQ. A variety of product vector quantizers use a Cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers, mean-removed vector quantizers, and shape-gain vector quantizers. Trellis encoders and trellis-coded quantizers use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity. Many of the early quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization <ref type="bibr" target="#b490">[484]</ref> and Abut's 1990 IEEE Reprint Collection on Vector Quantization <ref type="bibr" target="#b1">[2]</ref>.</p><p>We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general formulation of optimal quantization in the next section. In 1984 Farvardin and Modestino <ref type="bibr" target="#b159">[155]</ref> extended Berger's <ref type="bibr" target="#b46">[47]</ref> necessary conditions for optimality of an entropy-constrained scalar quantizer to more general distortion measures and described two design algorithms: the first is similar to Berger's iterative algorithm, but the second was a fixed-point algorithm which can be considered as a natural extension of Lloyd's Method I from fixed-rate to variable-rate vector quantization. In 1989, Chou et al. <ref type="bibr" target="#b95">[93]</ref> developed a generalized Lloyd algorithm for entropy-constrained vector quantization that generalized Berger's <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref> Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixedpoint design algorithm <ref type="bibr" target="#b159">[155]</ref> to vectors. Optimality properties for minimizing a Lagrangian distortion were derived, where rate could be either average length or entropy. Lloyd's optimal decoder remained unchanged and the lossless code is easily seen to be an optimal lossless code for the encoded vectors, but this formulation shows that the optimal encoder must simultaneously consider both the distortion and rate resulting from the encoder. In other words, quantizers with variable rate should use an encoder that minimizes a sum of squared error and weighted bit rate, and not only the squared error. Another approach to entropy-constrained scalar quantization is described in <ref type="bibr" target="#b289">[285]</ref>.</p><p>This is a good place to again mention Gish and Pierce's result that if the rate is high, optimal entropy-constrained scalar or vector quantization can provide no more than roughly 1/4-bit improvement over uniform scalar quantization with block entropy coding. Berger <ref type="bibr" target="#b46">[47]</ref> showed that permutation codes achieved roughly the same performance with a fixed-rate vector quantizer. Ziv <ref type="bibr" target="#b585">[578]</ref> showed in 1985 that if subtractive dithering is allowed, dithered uniform quantization followed by block lossless encoding will be at most 0.754 bit worse than the optimal entropy-constrained vector quantizer with the same block size, even if the rate is not high. (Subtractive dithering, as will be discussed later, adds a random dither signal to the input and removes it from the decompressed output.) As previously discussed, these results do not eliminate the usefulness of fixed-rate quantizers, because they may be simpler and avoid the difficulties associated with variablerate codes. These results do suggest, however, that uniform quantization and lossless coding is always a candidate and a benchmark for performance comparison. It is not known if the operational distortion-rate function of variable-rate quantization with dithering is better than that without dithering.</p><p>The present decade has seen continuing activity in developing high resolution theory and design algorithms for a variety of quantization structures, and in applying many of the principles of the theory to optimizing signal processing and communication systems incorporating quantizers. As the arrival of the present is a good place to close our historical tour, many results of the current decade will be sketched through the remaining sections. It is difficult to resist pointing out, however, that in 1990 Lloyd's algorithm was rediscovered in the statistical literature under the name of "principal points," which are distinguished from traditional -means by the assumption of an absolutely continuous distribution instead of an empirical distribution <ref type="bibr" target="#b176">[171]</ref>, <ref type="bibr" target="#b502">[496]</ref>, a formulation included in the VQ formulation for a general distribution. Unfortunately, these works reflect no awareness of the rich quantization literature.</p><p>Most quantizers today are indeed uniform and scalar, but are combined with prediction or transforms. In many niche applications, however, the true vector quantizers, including lattices and other constrained code structures, exhibit advantages, including the coding of speech residuals in code excited linear predictive (CELP) speech coding systems and VXTreme/Microsoft streaming video in WebTheater. Vector quantization, unlike scalar quantization, is usually applied to digital signals, e.g., signals that have already been "finely" quantized by an A/D converter. In this case, quantization (vector or scalar) truly represents compression since it reduces the number of bits required to describe a signal and it reduces the bandwidth required to transmit the signal description if an analog link is used.</p><p>Modern video coding schemes often incorporate the Lagrangian distortion viewpoint for accomplishing rate control, while using predictive quantization in a general sense through motion compensation and uniform quantizers with optimized lossless coding of transform coefficients for the intraframe coding (cf. <ref type="bibr" target="#b206">[201]</ref>, <ref type="bibr" target="#b207">[202]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. QUANTIZATION BASICS: ENCODING, RATE, DISTORTION, AND OPTIMALITY</head><p>This section presents, in a self-contained manner, the basics of memoryless quantization, that is, vector quantizers which operate independently on successive vectors. For brevity, we omit the "memoryless" qualifier for most of the rest of this section. A key characteristic of any quantizer is its dimension , a positive integer. Its input is a -dimensional vector from some alphabet . (Abstract alphabets are also of interest in rate-distortion theory, but virtually all alphabets encountered in quantization are realvalued vector spaces, in which case the alphabet is often called the support of the source distribution.) If the quantizer is scalar; otherwise, it is vector. In any case, the quantizer consists of three components-a lossy encoder , where the index set is an arbitrary countable set, usually taken as a collection of consecutive integers, a reproduction decoder</p><p>, where is the reproduction alphabet, and a lossless encoder , an invertible mapping (at least with probability ) into a collection of variable-length binary vectors that satisfies the prefix condition. Alternatively, a lossy encoder is specified by a partition of , where ; a reproduction decoder is specified by a (reproduction) codebook of points, codevectors, or reproduction codewords; and the lossless encoder can be described by its binary codebook containing binary or channel codewords. The quantization rule is the function or, equivalently, whenever . A -dimensional quantizer is used by applying its lossy and lossless encoders, followed by the corresponding decoders, to a sequence of -dimensional input vectors extracted from the data being encoded. There is not a unique way to do such vector extraction; and the design and performance of the quantizer usually depend significantly on the specific method that is used. For data that naturally forms a sequence of scalar-valued samples, e.g., speech, vector extraction is almost always done by parsing the data into successive -tuples of adjacent samples, i.e.,</p><p>As an example of other possibilities, one could also extract the first even samples, followed by the first odd samples, the next even samples, and so on. This subsampling could be useful for a multiresolution reconstruction, as in interpolative vector quantization <ref type="bibr" target="#b238">[234]</ref>, <ref type="bibr" target="#b199">[194]</ref>. For other types of data there may be no canonical extraction method. For example, in stereo speech thedimensional vectors might consist just of left samples, or just of right samples, or half from each, or from the left followed by from the right, etc. Another example is grayscale imagery where the -dimensional vectors might come from parsing the image into rectangular -by-blocks of pixels, where , or into other tiling polytopes, such as hexagons and other shapes aimed at taking advantage of the eye's insensitivity to noise along diagonals in comparison with along horizontal and vertical lines <ref type="bibr" target="#b230">[226]</ref>. Or the vectors might come from some less regular parsing. If the image has color, with each pixel value represented by some three-dimensional vector, thendimensional vectors can be extracted in even more ways. And if the data is a sequence of color of images, e.g., digital video, the extraction possibilities increase immensely. <ref type="foot" target="#foot_1">3</ref>There are two generic domains in which (memoryless) quantization theory, both analysis and design, can proceed. In the first, which we call the random vector domain, the input data, i.e., source, to be quantized is described by a fixed value of , an alphabet , and a probability distribution on ; and the quantizer must be -dimensional. This is the case when the specific vector dimension and contents are not allowed to vary, e.g., when ten-dimensional speech parameter vectors of line spectral pairs or reflection coefficients are coded together. In the second, which we call the random process domain, the input data is characterized as a discrete parameter random process, i.e., a countable collection (usually infinite) of random variables; and different ways of extracting vectors from its component variables may be considered and compared, including different choices of the dimension . As indicated above, there are in general many ways to do this. However, for concreteness and because it provides the opportunity to make some key points, whenever the random process domain is of interest in this and the next section, we focus exclusively on the canonical case where the data naturally forms a onedimensional, scalar-valued sequence, and successive -tuples of adjacent samples are extracted for quantization. We will also assume that the random process is stationary, unless a specific exception is made. Stationary models can easily be defined to include processes that exhibit distinct local and global stationarity properties (such as speech and images) by the use of models such as composite, hidden Markov, and mixture sources. In the random vector domain, there is no firstorder stationarity assumption; e.g., the individual components within each vector need not be identically distributed. In either domain we presume that the quantizer operates on a -dimensional random vector , usually assumed to be absolutely continuous so that it is described by a probability density function <ref type="bibr">(pdf)</ref> . Densities are usually assumed to have finite variance in order to avoid technical difficulties.</p><p>Memoryless quantizers, as described here, are also referred to as "vanilla" vector quantizers or block-source codes. The alternative is a quantizer with memory. Memory can be incorporated in a variety of ways; it can be used separately for the lossy encoder (for example, different mappings can be used, conditional on the past) or for the lossless encoder (the index produced by a quantizer can be coded conditionally based on previous indices). We shall return to vector quantizers with memory in Section V, but our primary emphasis will remain on memoryless quantizers. We will occasionally use the term code as a generic substitute for quantizer.</p><p>The instantaneous rate of the quantizer applied to a particular input is the normalized length of the channel codeword, the number of bits per source symbol that must be sent to describe the reproduction. An important special case is when all binary codewords have the same length , in which case the quantizer is referred to as fixed-length or fixed-rate.</p><p>To measure the quality of the reproduction, we assume the existence of a nonnegative distortion measure which assigns a distortion or cost to the reproduction of input by . Ideally, one would like a distortion measure that is easy to compute, useful in analysis, and perceptually meaningful in the sense that small (large) distortion means good (poor) perceived quality. No single distortion measure accomplishes all three goals, but the common squared-error distortion satisfies the first two. Although much maligned for lack of perceptual meaningfulness, it often is a useful indicator of perceptual quality and, perhaps more importantly, it can be generalized to a class of distortion measures that have proved useful in perceptual coding, the input-weighted quadratic distortion measures of the form <ref type="bibr" target="#b20">(21)</ref> where is a positive-definite matrix that depends on the input, cf. <ref type="bibr" target="#b262">[258]</ref>, <ref type="bibr" target="#b263">[259]</ref>, <ref type="bibr" target="#b261">[257]</ref>, <ref type="bibr" target="#b228">[224]</ref>, <ref type="bibr" target="#b391">[387]</ref>, <ref type="bibr" target="#b390">[386]</ref>, <ref type="bibr" target="#b154">[150]</ref>, <ref type="bibr" target="#b191">[186]</ref>, <ref type="bibr" target="#b320">[316]</ref>, <ref type="bibr" target="#b327">[323]</ref>, <ref type="bibr" target="#b329">[325]</ref>. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. We also assume that if and only if , an assumption that involves no genuine loss of generality and allows us to consider a lossless code as a code for which for all inputs . There exists a considerable literature for various other distortion measures, including and other norms of differences and convex or nondecreasing functions of norms of differences. These have rarely found application in real systems, however, so our emphasis will be on the MSE with comments on generalizations to input-weighted quadratic distortion measures.</p><p>The overall performance of a quantizer applied to a source is characterized by the normalized rate and the normalized average distortion Every quantizer is thus described by a rate-distortion pair . The goal of compression system design is to optimize the rate-distortion tradeoff. Fixed-rate quantizers constrain this optimization by not allowing a code to assign fewer bits to inputs that might benefit from such, but they provide simpler codes that avoid the necessity of buffering in order to match variable-rate codewords to a possibly fixedrate digital channel.</p><p>The optimal rate-distortion tradeoff for a fixed dimension can be formalized in several ways: by optimizing distortion for a constrained rate, by optimizing rate for a constrained distortion, or by an unconstrained optimization using a Lagrange approach. These approaches lead, respectively, to the operational distortion-rate function the operational rate-distortion function and the operational Lagrangian or weighted distortion-rate function where is a nonnegative number. A small value of leads to a low-distortion, high-rate solution and a large value leads to a low-rate, high-distortion solution. Note that so that the bracketed term can be considered to be a modified or Lagrangian distortion, and that is the smallest average Lagrangian distortion. All of these formalizations of optimal performance have their uses, and all are essentially equivalent: the distortion-rate and rate-distortion functions are duals and every distortion-rate pair on the convex hull of these curves corresponds to the Lagrangian for some value of . Note that if one constrains the problem to fixed-rate codes, then the Lagrangian approach reduces to the distortion-rate approach since no longer depends on the code and can be considered as just a binary indexing of .</p><p>Formal definitions of quantizer optimality easily yield optimality conditions as direct vector extensions and variations on Lloyd's conditions. The conditions all have a common flavor: if two components of the code are fixed, then the third component must have a specific form for the code to be optimal. The resulting optimality properties are summarized below. The proofs are simple and require no calculus of variations or differentiation. Proofs may be found, e.g., in <ref type="bibr" target="#b96">[94]</ref> and <ref type="bibr" target="#b201">[196]</ref>.</p><p>â¢ For a fixed lossy encoder , regardless of the lossless encoder , the optimal reproduction decoder is given by the output minimizing the conditional expectation of the distortion between the output and the input given that the encoder produced index . These vectors are called the Lloyd centroids. Note that the optimal decoder output for a given encoder output is simply the optimal estimate of the input vector given in the sense of minimizing the conditional average distortion. If the distortion is squared-error, the reproduction decoder is simply the conditional expectation of given it was encoded into centroid If the distortion measure is the input-weighted squared error of (21), then <ref type="bibr" target="#b322">[318]</ref>, <ref type="bibr" target="#b228">[224]</ref> centroid â¢ For a fixed lossy encoder , regardless of the reproduction decoder , the optimal lossless encoder is the optimal lossless code for the discrete source , e.g., a Huffman code for the lossy encoded source.</p><p>â¢ For a fixed reproduction decoder , lossless code , and Lagrangian parameter , the optimal lossy encoder is a minimum-distortion (nearest neighbor) encoder for the modified Lagrangian distortion measure</p><p>If the code is constrained to be fixed-rate, then the second property is irrelevant and the third property reduces to the familiar minimum distortion encoding with respect to , as in the original formulation of Lloyd (and implicit in Shannon). (The resulting partition is often called a Voronoi partition.) In the general variable-rate case, the minimum distance (with respect to the distortion measure ) encoder is suboptimal; the optimal rule takes into account both distortion and codeword length. Thus simply cascading a minimum MSE vector quantizer with a lossless code is suboptimal. Instead, in the general case, instantaneous rate should be considered in an optimal encoding, as the goal is to trade off distortion and rate in an optimal fashion. In all of these cases, the encoder can be viewed as a mechanism for controlling the output of the decoder so as to minimize the total Lagrangian distortion.</p><p>The optimality conditions imply a descent algorithm for code design: Given some , begin with an initial code . Optimize the encoder for the other two components, then optimize the reproduction decoder for the remaining components, then optimize the lossless coder for the remaining components. Let denote the overall transformation resulting from these three operations. One such iteration of must decrease or leave unchanged the average Lagrangian distortion. Iterate until convergence or the improvement falls beneath some threshold. This algorithm is an extension and variation on the algorithm for optimal scalar quantizer design introduced for fixed-rate scalar quantization by Lloyd <ref type="bibr" target="#b334">[330]</ref>. The algorithm is a fixed-point algorithm since if it converges to a code, the code must be a fixed point with respect to . This generalized Lloyd algorithm applies to any distribution, including parametric models and empirical distributions formed from training sets of real data. There is no obvious means of choosing the "best" , so the design algorithm might sweep through several values to provide a choice of rate-distortion pairs. We also mention that Lloyd-style iterative algorithms have been used to design many structured forms of quantization. For example, when the codes are constrained to have fixed rate, the algorithm becomes -means clustering, finding a fixed number of representative points that yield the minimum average distortion when a minimum distortion mapping is assumed.</p><p>As mentioned in Section I, a variety of other clustering algorithms exist that can be used to design vector quantizers (or solve any other clustering problems). Although each has found its adherents, none has convincingly yielded significant benefits over the Lloyd algorithm and its variations in terms of trading off rate and distortion, although some have proved much faster (and others much slower). Some algorithms such as simulated and deterministic annealing have been found experimentally to do a better job of avoiding local optima and finding globally optimal distortion-rate pairs than has the basic Lloyd algorithm, but repeated applications of the Lloyd algorithm with different initial conditions has also proved effective in avoiding local optima. We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties <ref type="bibr" target="#b456">[451]</ref>, <ref type="bibr" target="#b422">[418]</ref>, <ref type="bibr" target="#b110">[108]</ref>, <ref type="bibr" target="#b93">[91]</ref>, <ref type="bibr" target="#b103">[101]</ref>, <ref type="bibr" target="#b325">[321]</ref>, <ref type="bibr" target="#b339">[335]</ref>, <ref type="bibr" target="#b134">[131]</ref>, <ref type="bibr" target="#b34">[36]</ref>.</p><p>The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily <ref type="bibr" target="#b140">[137]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b198">[193]</ref>, <ref type="bibr" target="#b189">[184]</ref>, <ref type="bibr" target="#b201">[196]</ref> that</p><formula xml:id="formula_1">â¢</formula><p>, so that the quantizer output can be considered as an unbiased estimator of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â¢</head><p>, for all so that each component of the quantizer output is orthogonal to each component of the quantizer error. This is an example of the well-known fact that the minimum mean-squared error estimate of an unknown, , given an observation, , causes the estimate to be orthogonal to the error. In view of the previous property, this implies that the quantizer error is uncorrelated with the quantizer output rather than, as is often assumed, with the quantizer input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â¢</head><p>, which implies that the energy (or variance) of the quantized signal must be less than that in the original signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â¢</head><p>, which shows that the quantizer error is not uncorrelated with the input. In fact, the correlation is minus the mean-squared error.</p><p>It is instructive to consider the extreme points of the ratedistortion tradeoff, when the distortion is zero (or ) and the rate is (when ). First suppose that . In this case, the rate does not affect the Lagrangian distortion at all, but MSE counts. If the source is discrete, then one can optimize this case by forcing zero distortion, that is, using a lossless code. In this case, Shannon's lossless coding theorem implies that for rate measured by average instantaneous codelength or, if rate is measured by entropy, then simply , the entropy of the vector. In terms of the Lagrangian formulation,</p><p>. Conversely, suppose that . In this case distortion costs a negligible amount and rate costs an enormous amount, so here the optimal is attained by using zero rate and simply tolerating whatever distortion one must suffer. The distortion for a zero-rate code is minimized by the centroid of the unconditional distribution, which is simply the mean in the MSE case. Here the Lagrangian formulation becomes . Both of these extreme points are global optima, albeit the second is useless in practice.</p><p>So far, we have focused on the random vector domain and considered optimality for quantizers of a fixed dimension. In practice, however, and in source coding theory, the dimension may be a parameter of choice, and it is of interest to consider how the optima depend on it. Accordingly, we now focus on the random process domain, assuming that the source is a onedimensional, scalar-valued, stationary random process. In this situation, the various operational optima explicitly note the dimension, e.g., denotes the operational distortion-rate function for dimension and rate and, similarly, and denote the operational rate-distortion and Lagrange functions. Moreover, the overall optimal performance for all quantizers of rate less than or equal to is defined by <ref type="bibr" target="#b21">(22)</ref> Similar definitions hold for the rate-versus-distortion and the Lagrangian viewpoints.</p><p>Using stationarity, it can be shown (cf. <ref type="bibr" target="#b569">[562]</ref>, <ref type="bibr" target="#b584">[577]</ref>, <ref type="bibr" target="#b225">[221]</ref>, <ref type="bibr" target="#b222">[217,</ref><ref type="bibr">Lemma 11.2.3]</ref>) that the operational distortion-rate function is subadditive in the sense that for any positive integers and <ref type="bibr" target="#b22">(23)</ref> which shows the generally decreasing trend of the 's as increases. It is not known whether or not is always less than or equal to . However, it can be shown that subadditivity implies (cf. <ref type="bibr">[180, p. 112]</ref>) <ref type="bibr" target="#b23">(24)</ref> Hence high-dimensional quantizers can do as well as any quantizer. Note that ( <ref type="formula">23</ref>) and ( <ref type="formula">24</ref>) both hold for the special cases of fixed-rate quantizers as well as for variable-rate quantizers.</p><p>It is important to point out that for squared error and most other distortion measures, the " " in ( <ref type="formula">22</ref>) is not a "</p><p>." Specifically, represents performance that cannot be achieved exactly, except in degenerate situations such as when or the source distribution is discrete rather than continuous. Of course, by the infimum definition of , there are always quantizers with performance arbitrarily close to it. We conclude that no quantizers are truly optimal. Thus it is essential to understand that whenever the word "optimal" is used in the random process domain, it is always in the context of some specific constraint or class of quantizers, such as eight-dimensional fixed-rate VQ or entropy-constrained uniform scalar quantization or pyramid coding with dimension , to name a few at random. Indeed, though desirable, "optimality" loses a bit of its lustre when one considers the fact that an optimal code in one class might not work as well as a suboptimal code in another. It should now be evident that the importance of the Lloyd-style optimality principles lies ultimately in their ability to guide the optimization of quantizers within specific constraints or classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HIGH RESOLUTION QUANTIZATION THEORY</head><p>This section presents an overview of high resolution theory and compares its results to those of Shannon rate-distortion theory. For simplicity, we will adopt squared error as the distortion measure until late in the section, where extensions to other distortion measures are discussed. There have been two styles of high resolution theory developments: informal, where simple approximations are made, and rigorous, where limiting formulas are rigorously derived. Here, we proceed with the informal style until later when the results of the rigorous approach are summarized. We will also presume the "random vector domain" of fixed dimension, as described in the previous section, until stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Asymptotic Distortion</head><p>As mentioned earlier, the first and most elementary result in high resolution theory is the approximation to the mean-squared error of a uniform scalar quantizer with step size <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b398">[394]</ref>, <ref type="bibr" target="#b474">[468]</ref>, which we now derive. Consider anlevel uniform quantizer whose levels are , with . When this quantizer is applied to a continuous random variable with probability density , when is small, and when overload distortion can be ignored, the mean-squared error (MSE) distortion may be approximated as follows:</p><p>The first approximation in the above derives from ignoring overload distortion. If the source density is entirely contained in the granular region of the quantizer, then this approximation is not needed. The second approximation derives from observing that the density may be approximated as a constant on a small interval. Usually, as in the mean value theorem of integration, one assumes the density is continuous, but as any measurable function is approximately continuous, when is sufficiently small this approximation is valid even for discontinuous densities. The third approximation derives from recognizing that by the definition of a Riemann integral, is approximately equal to the integral of . Finally, the last approximation derives from again ignoring the overload region. As mentioned in earlier sections, there are situations, such as variable-rate quantization, where an infinite number of levels are permitted. In such cases, if the support of the uniform scalar quantizer contains that of the source density, then there will be no overload distortion to ignore, and again we have . It is important to mention the sense in which is approximated by . After all, when is small, both and will be small, so it is not saying much to assert that their difference is small. Rather, as discussed later in the context of the rigorous framework for high resolution theory, it can be shown that under ordinary conditions, the ratio of and tends to as decreases. Though we will not generally mention it, all future high-resolution approximations discussed in this paper will also hold in this ratio-tending-to-one sense.</p><p>Each of the assumptions and simple approximations made in deriving reoccurs in some guise in the derivation of all subsequent high-resolution formulas, such as for nonuniform, vector, and variable-rate quantizers. Thus they might be said to be principal suppositions. Indeed, the small cell type of supposition is what gives the theory its "high resolution" name.</p><p>In uniform quantization, all cells have the same size and shape and the levels are in the center of each cell (except for the outermost cells which are ignored). Thus the cell size is the key performance determining gross characteristic. In more advanced, e.g., vector, quantization, cells may differ in size and shape, and the codevectors need not be in the centers of the cells. Consequently, other gross characterizations are needed. These are the point density and the inertial profile.</p><p>The point density of a vector quantizer is the direct extension of the point density introduced in Section II. That is, it is a nonnegative, usually smooth function that, when integrated over a region, determines the approximate fraction of codevectors contained in that region. In fixed-rate coding, the point density is usually normalized by the number of codevectors so that its total integral is one. In variablerate coding, where the number of codevectors is not a key performance-determining parameter and may even be infinite, the point density is usually left unnormalized. As we consider fixed-rate coding first, we will presume is normalized, until stated otherwise. There is clearly an inverse relationship between the point density and the volume of cells, namely,</p><p>, where, as before, is the number of codevectors or cells and denotes the cell containing . As with any density that describes a discrete set of points, there is no unique way to define it for a specific quantizer. Rather, the point density is intended as a high-level gross characterization, or a model or target to which a quantizer aspires. It describes the codevectors, in much the way that a probability density describes a set of data points-it does not say exactly where they are located, but roughly characterizes their distribution. Quantizers with different numbers of codevectors can be compared on the basis of their point density, and there is an ideal point density to which quantizers aspire-they cannot achieve it exactly, but may approximate it. Nevertheless, there are times when a concrete definition of the point density of a specific quantizer is needed. In such cases, the following is often used: the specific point density of a quantizer is . This piecewiseconstant function captures all the (fine) detail in the quantizer's partition, in contrast to the usual notion of a point density as a gross characterization. As an example of its use, we mention that for fixed-rate quantization, the ideal point density is usually a smooth function, closely related to the source density, and one may say that a quantizer has point density approximately if for all in some set with high probability (relative to the source density). When a scalar quantizer is implemented as a compander, is proportional to the derivative of the compressor function applied to the input. Though the notion of point density would no doubt have been recognizable to the earliest contributors such as Bennett, Panter, and Dite, as mentioned earlier, it was not explicitly introduced until Lloyd's work <ref type="bibr" target="#b334">[330]</ref>.</p><p>In nonuniform scalar quantization and vector quantization, there is the additional issue of codevector placement within cells and, in the latter case, of cell shape. The effect of point placement and cell shape is exhibited in the following approximation to the contribution of a small cell with codevector to the MSE of a -dimensional vector quantizer (25) <ref type="bibr" target="#b24">(26)</ref> where is the normalized moment of inertia of the cell about the point , defined by</p><p>Normalizing by volume makes independent of the size of the cell. Normalizing by dimension yields a kind of invariance to dimension, namely, that . We often write when is clear from the context. The normalized moment of inertia, and the resulting contribution , is smaller for sphere-like cells with codevectors in the center than for cells that are oblong, have sharply pointed vertices, or have displaced codevectors. In the latter cases, there are more points farther from that contribute substantially to normalized moment of inertia, especially when dimension is large.</p><p>In some quantizers, such as uniform scalar and lattice quantizers, all cells (with the exception of the outermost cells) have the same shape and the same placement of codevectors within cells. In other quantizers, however, cell shape or codevector placement varies with position. In such cases, it is useful to characterize the variation of cell normalized moment of inertia by a nonnegative, usually smooth function , called the inertial profile. That is, when . As with point densities, we do not define to be equal to , because we want it to be a high-level gross characterization or model to which a quantizer aspires. Instead, we let be called the specific inertial profile of the quantizer . This is a piecewise-constant function that captures the fine details of cell normalized moment of inertia.</p><p>Returning to expressed in <ref type="bibr" target="#b24">(26)</ref>, the effect of cell size is obviously in the term . Using the inverse relationship between point density and cell volume yields which shows how point density locally influences distortion. Summing the above over all cells and recognizing the sum as an approximation to an integral yields the following approximation to the distortion of a vector quantizer: <ref type="bibr" target="#b25">(27)</ref> For scalar quantizers with points in the middle of the cells, and the above reduces to <ref type="bibr" target="#b26">(28)</ref> which is what Bennett <ref type="bibr" target="#b42">[43]</ref> found for companders, as restated in terms of point densities by Lloyd <ref type="bibr" target="#b334">[330]</ref>. Both <ref type="bibr" target="#b26">(28)</ref> and the more general formula ( <ref type="formula">27</ref>) are called Bennett's integral. The extension of Bennett's integral to vector quantizers was first made by <ref type="bibr">Gersho (1979)</ref>  <ref type="bibr" target="#b198">[193]</ref> for quantizers with congruent cells for which the concept of inertial profile was not needed, and then to vector quantizers with varying cell shapes (and codevector placements) by <ref type="bibr" target="#b369">Na and Neuhoff (1995)</ref>  <ref type="bibr" target="#b369">[365]</ref>.</p><p>Bennett's integral <ref type="bibr" target="#b25">(27)</ref> can be expected to be a good approximation under the following conditions: i) Most cells are small enough that can be approximated as being constant over the cell. (There can be some large cells where is very small.) Ordinarily, this requires to be large. ii) The specific point density of the quantizer approximately equals on a high probability set of 's. iii) The specific inertial profile approximately equals on a high probability set of 's. iv) Adjacent cells have similar volumes. The last condition rules out quantizers such as a scalar one whose cells have alternating lengths such as . The point density of such a quantizer is , because there are three points in an interval of width . Assuming, for simplicity, that the source density is uniform on , it is easy to compute , whereas Bennett's integral equals . One may obtain the correct distortion by separately applying Bennett's integral to the union of intervals of length and to the union of intervals of length . The problem is that Bennett's integral is not linear in the point density. So for it to be accurate, cell size must change slowly or only occasionally. Since Bennett's integral is linear in the inertial profile, it is not necessary to assume that adjacent cells have similar shapes, although one would normally expect this to be the case in situations where Bennett's integral is applied. Examples of the use of the vector extension of Bennett's integral will be given later.</p><p>Approximating the source density as a constant over each quantization cell, which is a key step in the derivations of ( <ref type="formula">26</ref>) and <ref type="bibr" target="#b26">(28)</ref>, is like assuming that the effect of quantization is to add noise that is uniformly distributed. However, the range of noise values must match the size and shape of the cell. And so when the cells are not all of the same size and shape, such quantization noise is obviously correlated with the vector being quantized. On the other hand, for uniform scalar and lattice vector quantizers, the error and are approximately uncorrelated. A more general result, mentioned in Section III, is that the correlation between the input and the quantization error is approximately equal to the MSE of the quantizer when the codevectors are approximately centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of the Best -Dimensional, Fixed-Rate Quantizers</head><p>Having Bennett's integral for distortion, one can hope to find a formula for , the operational distortion-rate function for -dimensional, fixed-rate vector quantization, by choosing the key characteristics, point density and inertial profile, to minimize <ref type="bibr" target="#b25">(27)</ref>. Unfortunately, it is not known how to find the best inertial profile. Indeed, it is not even known what functions are allowable as inertial profiles. However, Gersho (1979) <ref type="bibr" target="#b198">[193]</ref> made the now widely accepted conjecture that when rate is large, most cells of a -dimensional quantizer with rate and minimum or nearly minimum MSE are approximately congruent to some basic tessellating <ref type="foot" target="#foot_2">4</ref> -dimensional cell shape . In this case, the optimum inertial profile is a constant and Bennett's integral can be minimized by variational techniques or HÃ¶lder's inequality <ref type="bibr" target="#b198">[193]</ref>, <ref type="bibr" target="#b226">[222]</ref>, resulting in the optimal point density <ref type="bibr" target="#b27">(29)</ref> and the following approximation to the operational distortionrate function: for large <ref type="bibr" target="#b28">(30)</ref> where , which is the least normalized moment of inertia of -dimensional tessellating polytopes, and is the term depending on the source distribution. Dividing by variance makes invariant to a scaling of the source. We will refer to , , and as, respectively, Gersho's constant (in dimension ), Zador's factor (for -dimensional, fixed-rate quantization), and the Zador-Gersho function (for -dimensional, fixed-rate quantization). (Zador's role will be described later.) When , reduces to the Panter-Dite formula <ref type="bibr" target="#b7">(8)</ref>.</p><p>From the form of one may straightforwardly deduce that cells are smaller and have higher probability where is larger, and that all cells contribute roughly the same to the distortion; i.e., in <ref type="bibr" target="#b24">(26)</ref> is approximately the same for all , which is the "partial distortion theorem" first deduced for scalar quantization by Panter and Dite.</p><p>A number of properties of and are known; here, we mention just a few. Gersho's constant is known only for and , where is, respectively, an interval and a regular hexagon. It is not known whether the 's are monotonically nonincreasing for all , but it can be shown that they form a subadditive sequence, which is a property strong enough to imply that the infimum over equals the limit as tends to infinity. Though it has long been presumed, only recently has it been directly shown that the 's tend to as increases <ref type="bibr">(Zamir and Feder [564]</ref>), which is the limit of the normalized moment of inertia of -dimensional spheres as tends to infinity. Previously, the assertion that the 's tend to depended on Gersho's conjecture. Zador's factor tends to be smaller for source densities that are more "compact" (lighter tails and more uniform) and have more dependence among the source variables.</p><p>Fortunately, high resolution theory need not rely solely on Gersho's conjecture, because Zador's dissertation <ref type="bibr" target="#b568">[561]</ref> and subsequent memo <ref type="bibr" target="#b569">[562]</ref> showed that for large rate has the form , where is independent of the entirely, translations and rotations of T . The Voronoi cell of any lattice tessellates, but not all tessellations are generated by lattices. Gersho also conjectured that T k would be admissible in the sense that the Voronoi partition for the centroids of the tessellation would coincide with the tessellation. But this is not essential.</p><p>source distribution. Thus Gersho's conjecture is really just a conjecture about .</p><p>In deriving the key result, Zador first showed that for a random vector that is uniformly distributed on the unit cube, has the form when is large, which effectively defines . (In this case, .) He then used this to prove the general result by showing that no quantizer with high rate could do better than one whose partition is hierarchically constructed by partitioning into small equally sized cubes and then subdividing each with the partition of the quantizer that is best for a uniform distribution on that cube, where the number of cells within each cube depends on the source density in that cube. In other words, the local structure of an asymptotically optimal quantizer can be that of the optimum quantizer for a uniform distribution.</p><p>In this light, Gersho's conjecture is true if and only if. at high rates. one may obtain an asymptotically optimal quantizer for a uniform distribution by tessellating with . The latter statement has been proven for (cf. <ref type="bibr">[106, p. 59]</ref>) and for by Fejes Toth (1959) <ref type="bibr" target="#b164">[159]</ref>; see also <ref type="bibr" target="#b389">[385]</ref>. For , it is known that the best lattice tessellation is the body-centered cubic lattice, which is generated by a truncated octahedron <ref type="bibr" target="#b33">[35]</ref>. It has not been proven that this is the best tessellation, though one would suspect that it is. In summary, Gersho's conjecture is known to be true only for and . Might it be false for ? If it is, it might be that the best quantizers for a uniform source have a periodic tessellation in which two or more cell shapes alternate in a periodic fashion, like the hexagons and pentagons on the surface of a soccer ball. If the cells in one period of the tessellation have the same volumes, then one may apply Bennett's integral, and (30) holds with replaced by the average of the normalized moment of inertia of the cells in one period. However, if the cells have unequal volumes, then as in the example given while discussing Condition iv) of Bennett's integral, the MSE will be the average of distortions computed by using Bennett's integral separately on the union of cells of each type, and a macrolevel definition of will be needed. It might also be that the structure of optimal quantizers is aperiodic. However, it seems likely to us that, asymptotically, one could always find a quantizer with a periodic structure that is essentially as good as any aperiodic one.</p><p>It is an open question in dimensions three and above whether the best tessellation is a lattice. In most dimensions, the best known tessellation is a lattice. However, tessellations that are better than the best known lattices have recently been found for dimensions seven and nine by Agrell and Eriksson <ref type="bibr" target="#b153">[149]</ref>.</p><p>From now on, we shall proceed assuming Gersho's conjecture is correct, with the knowledge that if this is not the case, then analyses based on will be wrong (for ) by the factor , which will be larger than (but probably not much larger), and which in any case will converge to one as , as discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of the Best -Dimensional, Variable-Rate Quantizers</head><p>Extensions of high resolution theory to variable-rate quantization can also be based on Bennett's integral, as well as approximations, originally due to Gish and Pierce <ref type="bibr" target="#b209">[204]</ref>, to the entropy of the output of a quantizer. Two such approximations, which can be derived using approximations much like those used to derive Bennett's integral, were stated earlier for scalar quantizers in <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b12">(13)</ref>. However, the approximation <ref type="bibr" target="#b12">(13)</ref>, which says that for quantizers with mostly small cells , where is the unnormalized point density, holds equally well for vector quantizers, when is interpreted as a vector rather than a scalar variable. As mentioned before, unnormalized point density is used because with variable-rate quantization, the number of codevectors is not a primary characteristic and may even be infinite. For example, one can always add levels in a way that has negligible impact on the distortion and entropy.</p><p>We could now proceed to use Bennett's integral and the entropy approximation to find the operational distortion-rate function for variable-rate, -dimensional, memoryless VQ. However, we wish to consider a somewhat more general case. Just as Gish and Pierce found something quite interesting by examining the best possible performance of scalar quantization with block entropy coding, we will now consider the operational distortion-rate function for vector quantization with block entropy coding. Specifically, we seek , which is defined to be the infimum of the distortions of any quantizer with rate or less, whose lossy encoder is -dimensional and memoryless, and whose lossless encoder simultaneously codes a block of successive quantization indices with a variable-length prefix code. In effect, the overall code is a -dimensional, memoryless VQ. However, we will refer to it as a -dimensional (memoryless) quantizer with thorder variable-length coding (or th-order entropy coding). When , the code becomes a conventional memoryless, variable-rate vector quantizer. It is convenient to let connote fixed-length coding, so that means the same as of the previous section. By finding high-resolution approximations to for all values of and , we will be able to compare the advantages of increasing the dimension of the quantizer to those of increasing the order of the entropy coder.</p><p>To find we assume that the source produces a sequence of identical, but not necessarily independent, -dimensional random vectors, each with density . A straightforward generalization of <ref type="bibr" target="#b12">(13)</ref> shows that under high-resolution conditions, the rate is given by <ref type="bibr" target="#b29">(31)</ref> On the other hand, the distortion of such a code may be approximated using Bennett's integral <ref type="bibr" target="#b25">(27)</ref>, with substituted for the normalized point density . Then, as with fixed-rate vector quantization, one would like to find by choosing the inertial profile and the point density to minimize Bennett's integral subject to a constraint on the rate that the right-hand side of (31) be at most .</p><p>Once again, though it is not known how to find the best inertial profile, Gersho's conjecture suggests that when rate is large, the cells of the best rate-constrained quantizers are, mostly, congruent to . Hence, from now on we shall assume that the inertial profile of the best variable-rate quantizers is, approximately, . In this case, using variational techniques or simply Jensen's inequality, one can show that the best point density is uniform on all of (or at least over the support of the source density). In other words, all quantizer cells have the same size, as in a tessellation. Using this fact along with ( <ref type="formula">27</ref>) and ( <ref type="formula">31</ref>) yields <ref type="bibr" target="#b30">(32)</ref> where is the term depending on the source distribution. Dividing by variance makes it invariant to scale. We call the ( th-order) Zador entropy factor and a Zador-Gersho function for variable-rate coding. Since fixed-rate coding is a special case of variable-length coding, it must be that is less than or equal to in <ref type="bibr" target="#b28">(30)</ref>. This can be directly verified using Jensen's inequality <ref type="bibr" target="#b198">[193]</ref>.</p><p>In the case of scalar quantization , the optimality of the uniform point density and the operational distortionrate function were found by Gish and Pierce (1968) <ref type="bibr" target="#b209">[204]</ref>. <ref type="bibr">Zador (1966)</ref>  <ref type="bibr" target="#b569">[562]</ref> considered the case and showed that has the form when is large, where is a constant that is independent of the source density and no larger than the constant that he found for fixed-rate quantization. Gersho <ref type="bibr" target="#b198">[193]</ref> used the argument given above to find the form of given in <ref type="bibr" target="#b30">(32)</ref>. As with fixed-rate quantization, we shall proceed under the assumption that Gersho's conjecture is correct, in which case . If it is wrong, then our analyses will be off by the factor , which, as before, will probably be just a little larger than one, and which in any case will converge to one as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fixed-Rate Quantization with Arbitrary Dimension</head><p>We now restrict attention to the random process domain wherein the source is assumed to be a one-dimensional, scalarvalued, stationary random process. We seek a high-resolution approximation to the operational distortion-rate function , which represents the best possible performance of any fixed-rate (memoryless) quantizer. As mentioned in Section III, for stationary sources . Therefore, taking the limit of the highresolution approximation <ref type="bibr" target="#b28">(30)</ref> for yields the fact that for large <ref type="bibr" target="#b31">(33)</ref> where and is another Zador-Gersho function. This operational distortionrate function was also derived by Zador <ref type="bibr" target="#b568">[561]</ref>, who showed that his unknown factors and converged to . The derivation given here is due to Gersho <ref type="bibr" target="#b198">[193]</ref>. Notice that in this limiting case, there is no doubt about the constant .</p><p>As previously mentioned, the 's are subadditive, so that they are smallest when is large. Similarly, for stationary sources it can be shown that the sequence is also subadditive <ref type="bibr" target="#b198">[193]</ref>, so that they too are smallest when is large. Therefore, another expression for the above Zador-Gersho function is .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Benefits of Increasing Dimension in Fixed-Rate Quantization</head><p>Continuing in the random process domain (stationary sources), the generally decreasing natures of and directly quantify the benefits of increasing dimension in fixed-rate quantization. (Of course, there is also a cost to increasing dimension, namely, the increase in complexity.) For example, decreases from for to the limit . In decibels, this represents a 1.53-dB decrease in MSE. For an i.i.d. Gaussian source, decreases from for to the limit , which represents an additional 2.81-dB gain. In total, high-dimensional quantization gains 4.35 dB over scalar quantization for the i.i.d. Gaussian source. For a Gauss-Markov source with correlation coefficient , decreases from for to the limit or a gain of 10.0 dB, yielding a total high-dimensional VQ gain of 11.5 dB over scalar quantization. Because of the 6-dB-per-bit rule, any gain stated in decibels can be translated to a reduction in rate (bits per sample) by dividing by 6.02.</p><p>On the other hand, it is also important to understand what specific characteristics of vector quantizers improve with dimension and by how much. Motivated by several prior explanations <ref type="bibr" target="#b346">[342]</ref>, <ref type="bibr" target="#b337">[333]</ref>, <ref type="bibr" target="#b369">[365]</ref>, we offer the following. We wish to compare an optimal quantizer with dimension to an optimal -dimensional quantizer with . To simplify the discussion, assume is a multiple of . Though these two quantizers have differing dimensions, their characteristics can be fairly compared by comparing to the "product" VQ that is implicitly formed when is used times in succession. Specifically, the product quantizer has quantization rule where are the successive -tuples of , and reproduction codebook consisting of the concatenations of all possible sequences of codevectors from 's reproduction codebook . The subscripts " " and "</p><p>" will be attached as needed to associate the appropriate features with the appropriate quantizer. The distortion and rate of the product quantizer are easily seen to be those of the -dimensional VQ. Thus the shortcomings of an optimal -dimensional quantizer relative to an optimal high-dimensional quantizer may be identified with those of the product quantizer-in particular, with the latter's suboptimal point density and inertial profile, which we now find.</p><p>To simplify discussion, assume for now that , and let be a fixed-rate scalar quantizer, with large rate, levels in the middle of the cells, and point density . The cells of the product quantizer are -dimensional rectangles formed by Cartesian products of cells from the scalar quantizer. When the scalar cells have the same width, a -dimensional cube is formed; otherwise, a rectangle is formed, i.e., an "oblong" cube. Since the widths of the cells are, approximately, determined by , the point density and inertial profile of are determined by . Specifically, from the rectangular nature of the product cells one obtains <ref type="bibr" target="#b369">[365]</ref>, <ref type="bibr" target="#b382">[378]</ref> (34) and <ref type="bibr" target="#b33">(35)</ref> which derive, respectively, from the facts that the volume of a rectangle is the product of its side lengths, that the normalized moment of inertia of a rectangle is that of a cube times the ratio of the arithmetic mean of the square of the side lengths to their geometric mean, and that the side lengths are determined by the scalar point density. Note that along the diagonal of the first "quadrant" (where ), the product cells are cubes and , the minimum value. Off the diagonal, the cells are usually rectangular and, consequently, is larger.</p><p>To quantify the suboptimality of the product quantizer's principal feature, we factor the ratio of the distortions of and , which is a kind of loss, into terms that reflect the loss due to the inertial profile and point density <ref type="bibr" target="#b369">[365]</ref>, <ref type="bibr" target="#b382">[378]</ref> 5 <ref type="bibr" target="#b34">(36)</ref> where is the part of Bennett's integral that does not depend on , where the cell-shape loss, , is the ratio of the distortion of the product quantizer to that of a hypothetical quantizer with same point density and an optimal inertial profile, and where 5 Na and Neuhoff considered the ratio of the product code distortion to that of an optimal k-dimensional VQ for arbitrary k, not just for large k.</p><p>the point-density loss, , is the ratio of the distortion of a hypothetical quantizer with the point density of the product quantizer and a constant (e.g., optimal) inertial profile to that of a hypothetical quantizer with an optimal point density and the same (constant) inertial profile. Substituting <ref type="bibr" target="#b33">(35)</ref> into <ref type="bibr" target="#b34">(36)</ref> and using the fact that for large , , one finds <ref type="bibr" target="#b35">(37)</ref> where the cell shape loss has been factored into the product of a space-filling loss <ref type="bibr" target="#b337">[333]</ref>, <ref type="foot" target="#foot_3">6</ref>, which is the ratio of the normalized moment of inertia of a cube to that of a highdimensional sphere, and an oblongitis loss, , which is the factor by which the rectangularity of the cells makes the cell shape loss larger than the space-filling loss.</p><p>To proceed further, consider first an i.i.d. source (stationary and memoryless) and consider how to choose the scalar point density in order to minimize . On the one hand, choosing to be uniform on the set where the onedimensional density <ref type="foot" target="#foot_4">7</ref>is not small causes the product cells in the region where the -dimensional density is not small to be cubes and, consequently, makes , which is the smallest possible value. However, it causes the product point density to be poorly matched to the source density and, as a result, is large. On the other hand, choosing causes the product quantizer to have, approximately, the optimal point density <ref type="foot" target="#foot_5">8</ref>where the last step uses the fact that is large. However, this choice causes to be infinite. <ref type="foot" target="#foot_6">9</ref> The best point density, as implicitly found by Panter and Dite, is the compromise as given in <ref type="bibr" target="#b27">(29)</ref>. In the region where is not small, is "more uniform" than that causes Fig. <ref type="figure">5</ref>. Losses of optimal k-dimensional quantization relative to optimal high-dimensional quantization for an i.i.d. Gaussian source. The bottom curve is point-density loss; above that is point-density loss plus oblongitis loss; and the top curve is the total loss. For k 4, the space-filling losses are estimates.</p><p>the product quantizer to have the optimum point density. Therefore, it generates a product quantizer whose cells in the region where is largest are more cubic, which explains why it has less oblongitis loss.</p><p>As an example, for an i.i.d. Gaussian source, the optimal choice of scalar quantizer causes the product quantizer to have 0.94-dB oblongitis loss and 1.88-dB point-density loss. The sum of these, 2.81 dB, which equals , has been called the "shape loss" <ref type="bibr" target="#b337">[333]</ref> because it is determined by the shape of the density-the more uniform the density the less need for compromise because the scalar point densities leading to best product cell shapes and best point density are more similar. Indeed, for a uniform source density, there is no shape loss. In summary, for an i.i.d. source, in comparison to high-dimensional quantization, the shortcomings of scalar quantization with fixed-rate coding are 1) the -dB space-filling loss and 2) the lack of sufficient degrees of freedom to simultaneously attain good inertial profile (small ) and good point density (small ). On the other hand, it is often surprising to newcomers that vector quantization gains anything at all over scalar quantizers for i.i.d. sources, and secondly, that the gain is more than just the recovery of the space-filling loss.</p><p>A similar comparison can be made between -dimensional and high-dimensional VQ, by comparing the product quantizer formed by uses of a -dimensional VQ to an optimal -dimensional quantizer, for large . The results are that as increases 1) the space-filling loss decreases, and 2) there are more degrees of freedom so that less compromise is needed between the -dimensional point density that minimizes oblongitis and the one that gives the optimal point density. As a result, the oblongitis, point density, and shape losses decrease to zero, along with the space-filling loss. For the i.i.d. Gaussian source, these losses are plotted in Fig. <ref type="figure">5</ref>.</p><p>For sources with memory, scalar quantization engenders an additional loss due to its inability to exploit the dependence between source samples. Specifically, when there is dependence/correlation between source samples, the product point density cannot match the ideal point density, not even approximately. See <ref type="bibr" target="#b337">[333]</ref> and <ref type="bibr" target="#b369">[365]</ref> for a definition of memory loss. (One can factor both the point density and oblongitis losses into two terms, one of which is due to the quantizer's inability to exploit memory.) There is also a memory loss for -dimensional quantization, which decreases to as increases. The value of for which the memory loss becomes close to unity (i.e., negligible) can be viewed as kind of "effective memory or correlation length" of the source. It is closely related to the decorrelation/independence length of the process, i.e., the smallest value of such that source samples are approximately uncorrelated when separated by more than .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Variable-Rate Quantization with Arbitrary Quantizer Dimension and Entropy Coding Order</head><p>We continue in the random process domain (stationary sources). To find the best possible performance of vector quantizers with block entropy coding over all possible choices of the dimension of the lossy encoder and the order of the entropy coder, we examine the high-resolution approximation <ref type="bibr" target="#b30">(32)</ref>, which shows that . As mentioned previously, the 's are subadditive, so choosing large makes as small as possible, namely, as small as . Next, for stationary sources, it is well known that th-order differential entropy is monotonically nonincreasing in . Therefore, choosing either or large makes as small as possible, namely, as small as . Interestingly, , as shown by Gersho <ref type="bibr" target="#b198">[193]</ref>, who credits Thomas Liggett. It follows immediately that the best possible performance of vector quantizers with block entropy coding is given by , which is the operational distortion-rate function of fixed-rate quantizers. In other words, entropy coding does not permit performance better than highdimensional fixed-rate quantization.</p><p>Let us now re-examine the situation a bit more carefully. We may summarize the various high-resolution approximations to operational distortion-rate functions as <ref type="bibr" target="#b36">(38)</ref> where by convention refers to fixed-rate coding, refers to th-order entropy coding, and Note that both 's and 's tend to decrease as or increase. (The 's and the 's are subadditive. The 's are nonincreasing.) As an illustration, Fig. <ref type="figure" target="#fig_2">6 plots</ref> (in decibels) versus and for a Gauss-Markov source with correlation coefficient . Consider how decreases, i.e., improves, with and increasing. On the one hand, for fixed , it decreases with increasing (actually, it is monotonically nonincreasing) to <ref type="bibr" target="#b37">(39)</ref> Thus -dimensional quantization with high-order entropy coding suffers only the -dimensional space-filling loss. On the other hand, for fixed , decreases with (actually it is subadditive) to <ref type="bibr" target="#b38">(40)</ref> Hence, high-dimensional quantization suffers no loss relative to the best possible performance, no matter the order or absence of an entropy coder.</p><p>From the above, we see that to attain performance close to , must be large enough that the space-filling loss is approximately one, and the combination of and must be large enough that is also approximately one. Regarding the first of these, even (scalar quantization) yields , representing only a 1.53-dB loss, which may be acceptable in many situations. When it is not acceptable, needs to be increased. Unfortunately, as evident in Fig. <ref type="figure">5</ref>, the space-filling loss decreases slowly with increasing . Regarding the second, we note that one has considerable freedom. There are two extreme cases: 1) large and , i.e., fixed-rate high-dimensional quantization, or 2) large and , i.e., scalar quantization with highorder entropy coding. In fact, uniform scalar quantization will suffice in the second case. Alternatively, one may choose moderate values for both and . Roughly speaking, must be approximately equal to the effective memory length of the source plus the value needed for a memoryless source. In effect, if the source has considerable memory, such memory can be exploited either by the lossy encoder ( large), or the lossless encoder ( large), or both (moderate values of and ). Moreover, in such cases the potential reductions in due to increasing or tend to be much larger than the potential reductions in the space-filling loss. For example, for the Gauss-Markov source of Fig. <ref type="figure" target="#fig_2">6</ref>, decreases 10.0 dB as increases from one to infinity, and has already decreased 8.1 dB when . From the point of view of the lossy encoder, the benefit of entropy coding is that it reduces the dimension required of the lossy encoder. Similarly, from the point of view of the lossless encoder, the benefit of increasing the dimension of the vector quantizer is that it decreases the order required of the lossless encoder. Stated another way, the benefits of entropy coding decrease with increasing quantizer dimension, and the benefits of increasing quantizer dimension decrease with increasing entropy coding order. In summary (cf. <ref type="bibr" target="#b381">[377]</ref>), optimal performance is attainable with and only with a high-dimensional lossy encoder, and with or without entropy coding. However, good performance (within 1.53 dB of the best) is attainable with uniform scalar quantizer and high-order entropy coding. Both of these extreme approaches are quite complex, and so practical systems tend to be compromises with moderate quantizer dimension and entropy coding order.</p><p>As with fixed-rate quantization, it is important to understand what specific characteristics of variable-rate quantizers cause them to perform the way they do. Consequently, we will take another look at variable-rate quantization, this time from the point of view of the point density and inertial profile of the high-dimensional product quantizer induced by an optimal low-dimensional variable-rate quantizer. The situation is simpler than it was for fixed-rate quantization. As mentioned earlier, when rate is large, an optimal -dimensional variablerate quantizer has a uniform point density and a partition and codebook formed by tessellating . Suppose is small and is a large multiple of . From the structure of optimal variablerate quantizers, one sees that using an optimal -dimensional quantizer times yields a -dimensional quantizer having the same (uniform) point density as the optimal -dimensional quantizer and differing, mainly, in that its inertial profile equals the constant , whereas that of the optimaldimensional quantizer equals . Thus the loss due to -dimensional quantization is only the space-filling loss , which explains what Gish and Pierce found for scalar quantizers in 1968 <ref type="bibr" target="#b209">[204]</ref>. We emphasize that there is no point density, oblongitis, or memory loss, even for sources with memory. In effect, the entropy code has eliminated the need to shape the point density, and as a result, there is no need to compromise cell shapes.</p><p>Finally, let us compare the structure of the fixed-rate and variable-rate approaches when dimension is large. On the one hand, optimal quantizers of each type have the same constant inertial profile, namely, . On the other hand, they have markedly different point densities: an optimal fixedrate quantizer has point density , whereas an optimal variable-rate quantizer has point density that is uniform over all of . How is it that two such disparate point densities do in fact yield the same distortion? The answer is provided by the asymptotic equipartition property (AEP) <ref type="bibr" target="#b112">[110]</ref>, which is the key fact upon which most of information theory rests. For a stationary, ergodic source with continuous random variables, the AEP says that when dimension is large, thedimensional probability density is approximately constant, except on a set with small probability. More specifically, it shows , where is a set of typical sequences, where is the differential entropy rate of the source. It follows immediately from the AEP and the fact that that the point density of an optimal fixed-rate quantizer is approximately uniform on and zero elsewhere. Moreover, for an optimal variable-rate quantizer, whose point density is uniform over all of , we see that the cells not in can be ignored, because they have negligible probability, and that the cells in all have the same probability and, consequently, can be assigned codewords of equal length. Thus both approaches lead to quantizers that are identical on (uniform point density and fixed-length codewords) and differ only in what they do on the complement of , a set of negligible probability.</p><p>It is worthwhile emphasizing that in all of the discussion in this section we have restricted attention to quantizers with memoryless lossy encoders and either fixed-rate, memoryless or block lossless encoders. Though there are many lossy and lossless encoders that are not of this form, such as DPCM or finite-state, predictive or address vector VQ, and Lempel-Ziv or arithmetic lossless coding, we believe that the easily analyzed case studied here shows, representatively, the effects of increasing memory in the lossy and lossless encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Other Distortion Measures</head><p>By far the most commonly assumed distortion measure is squared error, which for scalars is defined by and for vectors is defined by where Often the distortion is normalized by . A variety of more general distortion measures have been considered in the literature, but the simplicity and tractability of squared error has long given it a central role. Intuitively, the average squared error is the average energy or power in the quantization noise. The most common extension of distortion measures for scalars is the th-power distortion For example, Roe <ref type="bibr" target="#b447">[443]</ref> generalized Max's formulation to distortion measures of this form. Gish and Pierce <ref type="bibr" target="#b209">[204]</ref> considered a more general distortion measure of the form , where is a monotone increasing function of the magnitude of its argument and with the added property that has the property that is monotone. None of these distortion measures has been widely used, although the magnitude error ( th power with</p><p>) has been used in some studies, primarily because of its simple computation in comparison with the squared error (no multiplications).</p><p>The scalar distortion measures have various generalizations to vectors. If the dimension is fixed, then one needs only a distortion measure, say , defined for all . If the dimension is allowed to vary, however, then one requires a family of distortion measures , which collection is called a fidelity criterion in source coding theory. Most commonly it is assumed that the fidelity criterion is additive or single letter in the sense that <ref type="bibr" target="#b40">(41)</ref> for , or, equivalently,</p><p>Additive distortion measures are particularly useful for proving source coding theorems since the normalized distortion will converge under appropriate conditions as the dimension grows large, thanks to the ergodic theorem. One can also assume more generally that the distortion measure is subbadditive in the sense that <ref type="bibr" target="#b42">(43)</ref> and the subadditive ergodic theorem will still lead to positive and negative coding theorems [218], <ref type="bibr" target="#b344">[340]</ref>. 10 An example of a subadditive distortion measure is the Levenshtein distance <ref type="bibr" target="#b318">[314]</ref> which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. Originally developed for studying errorcorrecting codes, the Levenshtein distance was rediscovered in the computer science community as the "edit distance."</p><p>For a fixed dimension one can observe that the squarederror distortion measure can be written as , where is the norm 10 This differs slightly from the previous definition of subadditive because the d k are not assumed to be normalized. The previous definition applied to d k =k is equivalent to this definition.</p><p>This idea can be extended by using any power of any norm, e.g., where (In this notation the norm is .) If we choose , then this distortion measure (sometimes referred to simply as the th-power distortion) is additive. Zador <ref type="bibr" target="#b569">[562]</ref> defined a very general th-power distortion measure as any distortion measure of the form where for any , , for some . This includes th-power distortion in the narrow sense , as well as the additive distortion measures of the form and even weighted average distortions such as and where the 's are nonnegative.</p><p>A variation on the norm is the norm defined by , which has been proposed as a candidate for a perceptually meaningful norm. Quantizer design algorithms exist for this case, but to date no highresolution quantization theory or rate-distortion theory has been developed for this distortion measure (cf. <ref type="bibr" target="#b351">[347]</ref>, <ref type="bibr" target="#b235">[231]</ref>, and <ref type="bibr" target="#b352">[348]</ref>).</p><p>High resolution theory usually considers a fixed dimension , so neither additivity nor a family of distortion measures is required. However, high resolution theory has tended to concentrate on difference distortion i.e., distortion measures that have the form , where is the usual Euclidean difference and is usually assumed to have nice properties, such as being monotonic in some norm of its argument. The th-power distortion measures (of all types) fall into this category.</p><p>Recently, the basic results of high resolution theory have been extended to a family of nondifference distortion measures that are locally quadratic in the sense that provided , the distortion measure is given approximately by a Taylor series expansion as</p><p>, where is a positive definite weighting matrix that depends on the output. This form is ensured by assuming that the distortion measure has continuous partial derivatives of third order almost everywhere and that the matrix defined as a by dimensional matrix with the th element <ref type="bibr" target="#b43">(44)</ref> is positive definite almost everywhere. The basic idea for this distortion measure was introduced by Gardner and Rao <ref type="bibr" target="#b191">[186]</ref> to model a perceptual distortion measure for speech, where the matrix is referred to as the "sensitivity matrix." The requirement for the existence of the derivatives of third order and for the to be positive definite were added in <ref type="bibr" target="#b320">[316]</ref> as necessary for the analysis. Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion <ref type="bibr" target="#b262">[258]</ref>, <ref type="bibr" target="#b263">[259]</ref>, <ref type="bibr" target="#b261">[257]</ref>, <ref type="bibr" target="#b228">[224]</ref>, which has the form of an input-weighted quadratic distortion measure of the form of <ref type="bibr" target="#b20">(21)</ref>. For this case, the input weighting matrix is related to the partial derivative matrix by , so that positive definiteness of assures that of and the derivative conditions are transferred to . Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher <ref type="bibr" target="#b154">[150]</ref> and Nill <ref type="bibr" target="#b390">[386]</ref>, <ref type="bibr" target="#b391">[387]</ref>. The Bennett integral has been extended to this type of distortion, and approximations for both fixedrate and variable-rate operational distortion-rate functions have been developed <ref type="bibr" target="#b191">[186]</ref>, <ref type="bibr" target="#b320">[316]</ref>. For the fixed-rate case, the result is that <ref type="bibr" target="#b44">(45)</ref> where the modified inertial profile is assumed to be the limit of A natural extension of Gersho's conjecture to the nondifference distortion measures under consideration implies that, as in the squared-error case, the optimal inertial profile is assumed to be constant (which in any case will yield a bound) and minimizing the above (for example, using HÃ¶lder's inequality) yields the optimal point density <ref type="bibr" target="#b45">(46)</ref> and the operational distortion-rate function (analogous to ( <ref type="formula">30</ref>)) <ref type="bibr" target="#b46">(47)</ref> where now <ref type="bibr" target="#b48">(48)</ref> generalizes Zador's factor to the given distortion measure. As shown later in <ref type="bibr" target="#b58">(58)</ref>, can be bounded below by the moment of inertia of a sphere. Similarly, in the variable-rate case <ref type="bibr" target="#b49">(49)</ref> with optimal inertial profile and optimal point density <ref type="bibr" target="#b50">(50)</ref> Both results reduce to the previous results for the special case of a squared-error distortion measure since then . Note in particular that the optimal point density for the entropy-constrained case is not in general a uniform density.</p><p>Parallel results for Shannon lower bounds to the ratedistortion function have been developed for this family of distortion measures by Linder and Zamir <ref type="bibr" target="#b327">[323]</ref> and results for multidimensional companding with lattice codes for similar distortion measures have been developed by Linder, Zamir, and Zeger <ref type="bibr" target="#b329">[325]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Rigorous Approaches to High Resolution Theory</head><p>Over the years, high-resolution analyses have been presented in several styles. Informal analyses of distortion, such as those used in this paper to obtain and Bennett's integral (25), generally ignore overload distortion and estimate granular distortion by approximating the density as being constant within each quantization cell. In contrast, rigorous analyses generally focus on sequences of ever finer quantizers, for which they demonstrate that, in the limit, overload distortion becomes negligible in comparison to granular distortion and the ratio of granular distortion to some function of the fineness parameter tends to a constant. Though informal analyses generally lead to the same basic results as rigorous ones, the latter make it clear that the approximations are good enough that their percentage errors decrease to zero as the quantizers become finer, whereas the former do not. Moreover, the rigorous derivations provide explicit conditions under which the assumption of negligible overload distortion is valid. Some analyses (informal and rigorous) provide corrections for overload distortion, and some even give examples where the overload distortion cannot be asymptotically ignored but can be estimated nevertheless. Similar comments apply to informal versus rigorous analyses of asymptotic entropy. In the following we review the development of rigorous theory.</p><p>Many analyses-informal and rigorous-explicitly assume the source has finite range (i.e., a probability distribution with bounded support); so there is no overload distortion to be ignored <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b409">[405]</ref>, <ref type="bibr" target="#b480">[474]</ref>. In some cases, the source really does have finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. In such cases, the truncation by the measurement device creates an implicit overload distortion that is not affected by the design of the quantizer. It makes little sense, then, to choose a quantizer so fine that its (granular) distortion is significantly less than this implicit overload distortion. This means there is an upper limit to the fineness of quantizers that need be considered, and consequently, one must question whether such fineness is small enough that the source density can be approximated as constant within cells. Some analyses do not explicitly assume the source density has finite support, but merely assert that overload distortion can be ignored. We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest.</p><p>The earliest quantizer distortion analyses to appear in the open literature <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b409">[405]</ref>, <ref type="bibr" target="#b480">[474]</ref> assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. For example, Lloyd <ref type="bibr" target="#b334">[330]</ref> used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is , which means that it tends to zero, even when multiplied by . Algazi <ref type="bibr" target="#b7">[8]</ref>, Roe <ref type="bibr" target="#b447">[443]</ref>, and Wood <ref type="bibr" target="#b545">[539]</ref> also used Taylor series.</p><p>Overload distortion was first explicitly considered in the work of <ref type="bibr" target="#b477">Shtein (1959)</ref>  <ref type="bibr" target="#b477">[471]</ref>, who optimized the cell size of uniform scalar quantization using an explicit formula for the overload distortion (as well as for the granular distortion) and while rederiving the Panter-Dite formula, added an overload distortion term.</p><p>The earliest rigorous analysis <ref type="foot" target="#foot_7">11</ref> is contained in Schutzenberger's 1958 paper <ref type="bibr" target="#b468">[462]</ref>, which showed that fordimensional variable-rate quantization , th-power distortion , and a source with finite differential entropy and for some , there is a , depending on the source and the dimension, such that any -dimensional quantizer with finitely or infinitely many cells, and output entropy , has distortion at least . Moreover, there exists and a sequence of quantizers with increasing output entropies and distortion no more than . In essence, these results show that for all Unfortunately, as Schutzenberger notes, the ratio of to tends to infinity as dimension increases. As he indicates, the problem is that in demonstrating the upper bound, he constructs a sequence of quantizers with cubic cells of equal size and then bounds from above the distortion in each cell by something proportional to its diameter to the th power. If instead one were to bound the distortion by the moment of inertia of the cell times the maximum value of the density within it, then would not tend to infinity. Next, two papers appeared in the same issue of Acta Math. Acad. Sci. Hungar. in 1959. The paper by Renyi <ref type="bibr" target="#b437">[433]</ref> gave, in effect, a rigorous derivation of (11) for a uniform quantizer with infinitely many levels. Specifically, it showed that , provided that the source distribution is absolutely continuous and that and are finite, where denotes a uniform quantizer with step size and denotes a quantity that approaches zero as goes to . They paper also explores what happens when the distribution is not absolutely continuous.</p><p>In the second paper, Fejes Toth <ref type="bibr" target="#b164">[159]</ref> showed that for a twodimensional random vector that is uniformly distributed on the unit square, the mean-squared error of any -point quantizer is bounded from below by hexagon . This result was independently rederived in a simpler fashion by <ref type="bibr" target="#b389">Newman (1964)</ref>  <ref type="bibr" target="#b389">[385]</ref>. Clearly, the lower bound is asymptotically achievable by a lattice with hexagonal cells. It follows then that the ratio of to hexagon tends to one, and also, that Gersho's conjecture holds for dimension two.</p><p>Zador's thesis (1963) <ref type="bibr" target="#b568">[561]</ref> was the next rigorous work. As mentioned earlier, it contains two principal results. For fixed-rate quantization, th-power distortion measures of the form and a source that is uniformly distributed on the unit cube, it first shows ([561, Lemma 2.3]) that the operational distortion-rate function <ref type="foot" target="#foot_8">12</ref>multiplied by approaches a limit as . The basic idea, which Zador attributes to J. M. Hammersley, is the following: For any positive integers and , divide the unit cube into subcubes, each with sides of length . Clearly, the best code with codevectors is at least as good as the code constructed by using the best code with points for each subcube. It follows then that</p><p>, where is the operational distortionrate function of a source that is uniformly distributed on a subcube and where the second relation follows from the fact that this "sub" source is just a scaling of the original source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiplying both sides by yields</head><p>Thus we see that increasing the number of codevectors from to does not increase . A somewhat more elaborate argument shows that this is approximately true for any sufficiently large and, as a result, that i.e., has a limit. One can see how the selfsimilarity of the uniform density (it is divisible into similar subdensities) plays a key role in this argument. Notice also that nowhere do the shapes of the cells or the point density enter into it.</p><p>Zador next addresses nonuniform densities. With denoting , his Theorem 2.2 shows that if the -dimensional source density satisfies and for some , then as . The positive part, namely, that is established by constructing codes in, approximately, the following manner: Given , one chooses a sufficiently large support cube (large enough that overload distortion contributes little), subdivides the cube into equally sized subcubes, and places within each subcube a set of codevectors that are optimal for the uniform distribution on that subcube, where the number of codevectors in a subcube is carefully chosen so that the point density in that subcube approximates the optimal point density for the original source distribution. One then shows that the distortion of this code, multiplied by , is approximately . The best codes are at least this good and it follows that One can easily see how this construction creates codes with essentially optimal point density and cell shape. We will not describe the converse.</p><p>Zador's 1966 Bell Labs Memorandum <ref type="bibr" target="#b569">[562]</ref> reproves these two main results under weaker conditions. The distortion measure is th power in the general sense, which includes as special cases the narrow sense of the th power of the Euclidean norm considered by Schutzenberger <ref type="bibr" target="#b468">[462]</ref>. The requirement on the source density is only that each of its marginals has the property that it is bounded from above by , for some and all of sufficiently large magnitude. This is a pure tail condition, as opposed to the finite moment condition of the thesis, which constrains both the tail and the peak of the density. Note also that it no longer requires that be finite. As indicated earlier, Zador's memorandum also derives the asymptotic form of the operational distortion-rate function of variable-rate quantization. In other words, it finishes what his thesis and Schutzenberger <ref type="bibr" target="#b468">[462]</ref> started, though he was apparently unaware of the latter. Specifically, it shows that as where is some constant no larger than , assuming the same conditions as the fixed-rate result, plus the additional requirement that for any there is a bounded set containing all points such that . Gish and Pierce (1968) <ref type="bibr" target="#b209">[204]</ref>, who discovered that uniform is the asymptotically best type of scalar quantizer for variable-rate coding, presented both informal and rigorous derivations-the latter being the first to appear in these TRANSACTIONS. Specifically, they showed rigorously that for uniform scalar quantization with infinitely many cells of width , the distortion and the output entropy behave as follows:</p><p>(51) <ref type="bibr" target="#b52">(52)</ref> which makes rigorous the formula and ( <ref type="formula">11</ref>), respectively. For this result, they required the density to be continuous except at finitely many points, and to satisfy a tail condition similar to Zador's and another condition about the behavior at points of discontinuity. The paper also outlined a rigorous proof of (32) in the scalar case, i.e., that as . But as to the details it offered only that: "The complete proof is surprisingly long and will not be given here." Though Gish and Pierce were the first to informally derive <ref type="bibr" target="#b12">(13)</ref>, neither this paper nor any paper to date has provided a rigorous derivation. <ref type="bibr">Elias (1970)</ref>  <ref type="bibr" target="#b146">[143]</ref> also made a rigorous analysis of scalar quantization, giving asymptotic bounds to the distortion of scalar quantizers with a rather singularly defined measure of distortion, namely, the th root of the average of the th power of the cell widths. A companion paper <ref type="bibr" target="#b148">[144]</ref> considers similar bounds to the performance of vector quantizers with an analogous average-cell-size distortion measure.</p><p>In 1973, CsiszÃ r <ref type="bibr" target="#b116">[114]</ref> presented a rigorous generalization of (52) to higher dimensional quantizers. Of most interest here is the following special case of his principal result ([114, Theorem 1]): Consider a -dimensional source and a sequence of -dimensional quantizers where has a countably infinite number of cells, each with volume , where the 's and also the maximum of the cell diameters tends to zero. Then under certain conditions, including the condition that there be at least some quantizer with finite output entropy, the output entropy satisfies <ref type="bibr" target="#b53">(53)</ref> Clearly, this result applies to quantizers generated by lattices and, more generally, tessellations. It also applies to quantizers with finitely many cells for sources with compact support. But it does not apply to quantizers with finitely many cells and sources with infinite support, because it does not deal with the overload region of such quantizers.</p><p>In 1977, Babkin et al. <ref type="bibr" target="#b587">[580]</ref> obtained results indicating how rapidly the distortion of fixed-rate lattice quantizers approach as rate and dimension increase, for difference distortion measures. In 1978, these same authors <ref type="bibr" target="#b588">[581]</ref> studied uniform scalar quantization with variable-rate coding, and extended Koshelev's results to th power distortion measures.</p><p>The next contribution is that of <ref type="bibr">Bucklew and Gallagher (1980)</ref>  <ref type="bibr" target="#b63">[63]</ref>, who studied asymptotic properties of fixedrate uniform scalar quantization. With denoting the cell width that minimizes distortion among cell uniform scalar quantizers and denoting the resulting minimum meansquared error, they showed that for a source with a Riemann integrable density and where is the length of the shortest interval with probability one. When the support is finite, i.e., and are finite, the above implies as and so decreases as . This makes the formula rigorous in the finite case, at least when is chosen optimally. However, when the support is infinite, e.g., a Gaussian density, decreases at a rate slower than , and the resulting signal-to-noise ratio versus rate curve separates from any line of slope 6 dB/bit. Consequently, the ratio of the operational distortion-rate functions of uniform and nonuniform scalar quantizers increases without bound as the rate increases; i.e., uniform quantization is asymptotically bad. Moreover, they showed that does not always converge to . Instead, , and they exhibited densities where the inequality is strict. In such cases, the formula is invalidated by the heavy tails of the density. It was not until much later that the asymptotic form of and were found, as will be described later. Formal theory advanced further in papers by Bucklew and Wise, Cambanis and Gerr, and Bucklew. The first of these <ref type="bibr">(1982)</ref> [64] demonstrated Zador's fixed-rate result for thpower distortion , assuming only that for some</p><p>. It also contained a generalization to random vectors without probability densities, i.e., with distributions that are not absolutely continuous or even continuous. The paper also gave the first rigorous approach to the derivation of Bennett's integral for scalar quantization via companding. However, as pointed out by <ref type="bibr" target="#b324">Linder (1991)</ref>  <ref type="bibr" target="#b324">[320]</ref>, there was "a gap in the proof concerning the convergence of Riemann sums with increasing support to a Riemann integral." Linder fixed this and presented a correct derivation with weaker assumptions. <ref type="bibr" target="#b72">Cambanis and Gerr (1983)</ref>  <ref type="bibr" target="#b72">[70]</ref> claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems as <ref type="bibr" target="#b65">[64]</ref>. A subsequent paper by <ref type="bibr">Bucklew (1984)</ref>  <ref type="bibr" target="#b58">[58]</ref> derived a result for vector quantizers that lies between Bennett's integral and Zador's formula. Specifically, it showed that when a sequence of quantizers is asymptotically optimal for one probability density , then its th-power distortion on a source with density is asymptotically given by , where is the optimal point density for . On the one hand, this is like Bennett's integral in that , and consequently , can be arbitrary. On the other hand, it is like Zador's result (or Gersho's generalization of Bennett's integral <ref type="bibr" target="#b198">[193]</ref>) in that, in essence, it is assumed that the quantizers have optimal cell shapes.</p><p>In 1994, Linder and Zeger <ref type="bibr" target="#b330">[326]</ref> rigorously derived the asymptotic distortion of quantizers generated by tessellations by showing that the quantizer formed by tessellating with some basic cell shape scaled by a positive number has average (narrow-sense) th-power distortion satisfying</p><p>They then combined the above with CsiszÃ r's result <ref type="bibr" target="#b53">(53)</ref> to show that under fairly weak conditions (finite differential entropy and finite output entropy for some ) the output entropy and the distortion are asymptotically related via which is what Gersho derived informally <ref type="bibr" target="#b198">[193]</ref>.</p><p>The generalization of Bennett's integral to fixed-rate vector quantizers with rather arbitrary cell shapes was accomplished by <ref type="bibr" target="#b369">Na and Neuhoff (1995)</ref>  <ref type="bibr" target="#b369">[365]</ref>, who presented both informal and rigorous derivations. In the rigorous derivations, it was shown that if a sequence of quantizers , parameterized by the number of codevectors, has specific point density and specific inertial profile converging in probability to a model point density and a model inertial profile, respectively, then converges to Bennett's integral , where distortion is th power . A couple of additional conditions were also required, including one that is, implicitly, a tail condition.</p><p>Though uniform scalar quantization with finitely many levels is the oldest and most elementary form of quantization, the asymptotic form of the optimal step size and resulting mean-squared error has only recently been found for Gaussian and other densities with infinite support. Specifically, Hui and Neuhoff <ref type="bibr" target="#b257">[253]</ref>- <ref type="bibr" target="#b259">[255]</ref> have found that for a Gaussian density with variance and This result was independently found by Eriksson and Agrell <ref type="bibr" target="#b153">[149]</ref>. Moreover, it was shown that overload distortion is asymptotically negligible and that , which is the first time this has been proved for a source with infinite support. It follows from the above that the signalto-noise ratio increases as , which shows concretely how uniform scalar quantization is asymptotically bad. Hui and Neuhoff also considered non-Gaussian sources and provided a fairly general characterization of the asymptotic form of and . It turned out that the overload distortion is asymptotically negligible when and only when the tail parameter equals one, which is the case for all generalized Gaussian densities. For such cases, accurate approximations to and can be given. For densities with , the ratio of overload to granular distortion is , and . There are even densities with tails so heavy that and the granular distortion becomes negligible in comparison to the overload distortion. In a related result, the asymptotic form of the optimal scaling factor for lattice quantizers has also been found recently for an i.i.d. Gaussian source <ref type="bibr" target="#b363">[359]</ref>, <ref type="bibr" target="#b153">[149]</ref>.</p><p>We conclude this subsection by mentioning some gaps in rigorous high resolution theory. One, of course, is a proof or counterproof of Gersho's conjecture in dimensions three and higher. Another is the open question of whether the best tessellation in three or more dimensions is a lattice. Both of these are apparently difficult questions. There have been no rigorous derivations of ( <ref type="formula">11</ref>), or its extension to higher dimensional tesselations, where the quantizers have finitely many levels, and overload distortion must be dealt with. Likewise, there have been no rigorous derivations of (13), or its higher dimensional generalization, except in the case where the point density is constant. Even assuming Gersho's conjecture is correct, there is no rigorous derivation of the Zador-Gersho formulas <ref type="bibr" target="#b28">(30)</ref> and <ref type="bibr" target="#b30">(32)</ref> along the lines of the informal derivations that start with Bennett's integral. We also mention that the tail conditions given in some of the rigorous results (e.g., <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b369">[365]</ref>) are very difficult to check. Simpler ones are needed. Finally, as discussed in Section II there are no convincing (let alone rigorous) asymptotic analyses of the operational distortion-rate function of DPCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparing High Resolution Theory and Shannon Rate Distortion Theory</head><p>It is interesting to compare and contrast the two principal theories of quantization, and we shall do so in a number of different domains.</p><p>Applicability: Sources-Shannon rate-distortion theory applies, fundamentally, to infinite sequences of random variables, i.e., to sources modeled as random processes. Its results derive from the frequencies with which events repeat, as expressed in a law of large numbers, such as the weak law or an ergodic theorem. As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218, p. 16]). Though originally derived for ergodic sources, it has been extended to nonergodic sources <ref type="bibr" target="#b225">[221]</ref>, <ref type="bibr" target="#b475">[469]</ref>, <ref type="bibr" target="#b129">[126]</ref>, <ref type="bibr" target="#b141">[138]</ref>, <ref type="bibr" target="#b485">[479]</ref>. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function was found to equal in this way; see <ref type="bibr" target="#b31">(33)</ref>. Rate distortion theory also has one result relevant to finite-dimensional random vectors, namely, that the operational distortion-rate functions for fixed-and variablerate quantization, and , are (strictly) bounded from below by the th-order Shannon distortion-rate function.</p><p>Both theories have been extended to continuous-time random processes. However, the high-resolution results are somewhat sketchy <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b334">[330]</ref>, <ref type="bibr" target="#b209">[204]</ref>. Both can be applied to twoor higher dimensional sources such as images or video. Both have been developed the most for Gaussian sources in the context of squared-error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.</p><p>Applicability: Distortion Measures-Shannon rate distortion theory applies primarily to additive distortion measures; i.e., distortion measures of the form (or a normalized version), though there are some results for subadditive distortion measures [218], <ref type="bibr" target="#b344">[340]</ref> and some for distortion measures such as <ref type="bibr" target="#b327">[323]</ref>. High resolution theory has the most results for th-power difference distortion measures, and as mentioned previously, some of its results have recently been extended to nondifference distortion measures such as <ref type="bibr" target="#b191">[186]</ref>, <ref type="bibr" target="#b320">[316]</ref>, <ref type="bibr" target="#b329">[325]</ref>. In any event, both theories are the most fully developed for the squared-error distortion measure, especially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure. For squarederror distortion, it is simply that the variance of the source be finite. More generally, it is that for some . In addition, as discussed previously, rigorous high resolution theory results require tail conditions on the source density, for example, for some . Complementarity-The two theories are complementary in the sense that Shannon rate distortion theory prescribes the best possible performance of quantizers with a given rate and asymptotically large dimension, while high resolution theory prescribes the best possible performance of codes with a given dimension and asymptotically large rate. That is, for fixed-rate codes for large and any (54) for large and any <ref type="bibr" target="#b55">(55)</ref> and, similarly, for variable-rate codes for large and any (56) for large and any <ref type="bibr" target="#b57">(57)</ref> When both dimension and rate are large, they all give the same result, i.e.,</p><p>Rates of Convergence-It is useful to know how large and must be, respectively, for high resolution and rate distortion theory formulas to be accurate. As a rule of thumb, high resolution theory is fairly accurate for rates greater than or equal to about . And it is sufficiently accurate at rates about for it to be useful when comparing different sources and codes. For example, Fig. <ref type="figure" target="#fig_3">7</ref> shows signal-to-noise ratios for fixedrate quantizers produced by conventional design algorithms and predictions thereof based on the Zador-Gersho function , for two Gaussian sources: i.i.d. and Markov with correlation coefficient</p><p>. It is apparent from data such as this that the accuracy of the Zador-Gersho function approximation to increases with dimension. The convergence rate of to as tends to infinity has also been studied <ref type="bibr" target="#b417">[413]</ref>, <ref type="bibr" target="#b554">[548]</ref>, <ref type="bibr" target="#b325">[321]</ref>, <ref type="bibr" target="#b583">[576]</ref>. Roughly speaking these results show that for memoryless sources, the convergence rate is between and . Unfortunately, this theory does not enable one to actually predict how large the dimension must be in order that is within some specified percentage, e.g., 10%, of . However, one may use high resolution theory to do this, by comparing (or in the variable-rate case) to . For example, for the i.i.d. Gaussian source Fig. <ref type="figure">5</ref> shows that yields distortions within 1 and 0.2 dB of that predicted by at dimensions and , respectively. For sources with memory, the dimension needs to be larger, by roughly the effective memory length. One may conclude that the Shannon distortion-rate function approximation to is applicable for moderate to large dimensions .</p><p>Quantitative Relationships-For squared-error distortion, the Zador-Gersho function is precisely equal to the well-known Shannon lower bound to the Shannon distortion-rate function. It follows that when rate is not large, is, at least, a lower bound to . Similarly, the Shannon lower bound to the th-order Shannon distortion-rate function equals , from which it follows that may be thought of as the distortion of a fictional quantizer having the distortion of an optimal -dimensional variable-rate quantizer with first-order entropy coding, except that its cells have the normalized moment of inertia of a high-dimensional sphere instead of . It is well known that approaches one as increases Gauss-Markov, correlation coefficient 0:9.</p><p>[327], <ref type="bibr" target="#b271">[267]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b326">[322]</ref>, which is entirely consistent with the fact that approaches one as increases. The relationships among the various distortion-rate functions are summarized below. Inequalities marked with a " " become tight as dimension increases, and those marked with a " " become tight as increases.</p><p>Applicability: Quantizer Types-Rate distortion theory finds the performance of the best quantizers of any type for stationary sources. It has nothing to say about suboptimal, structured or dimension-constrained quantizers except, as mentioned earlier, that quantizers of dimension have distortion bounded from below by the th-order Shannon distortion-rate function. In contrast, high resolution theory can be used to analyze and optimize the performance of a number of families of structured quantizers, such as transform, lattice, product, polar, two-stage, and, most directly, dimension-constrained quantizers. Such analyses are typically based on Bennett's integral. Indeed, the ability to analyze structured or dimension-constrained quantizers is the true forte of high resolution theory.</p><p>Performance versus Complexity: Assessing performance versus complexity should be a major goal of quantization theory. On the one hand, rate distortion theory specifies the fundamental limits to performance without regard to complexity. On the other hand, because high resolution theory can analyze the performance of families of quantizers with complexity-reducing structure, one can learn much from it about how complexity relates to performance. In recent work, Hui and Neuhoff <ref type="bibr" target="#b260">[256]</ref> have combined high resolution theory and Turing complexity theory to show that asymptotically optimal quantization can be implemented with complexity increasing at most polynomially with the rate.</p><p>Computability: First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b500">[494]</ref>, <ref type="bibr" target="#b567">[560]</ref>, <ref type="bibr" target="#b222">[217]</ref>. For other sources it can be computed with Blahut's algorithm <ref type="bibr" target="#b52">[52]</ref>. And in the case of squared error, it can be computed with simpler algorithms <ref type="bibr" target="#b173">[168]</ref>, <ref type="bibr" target="#b448">[444]</ref>. For sources with memory, complete analytical formulas for th-order distortion-rate functions are known only for Gaussian sources. For other cases, the Blahut algorithm <ref type="bibr" target="#b52">[52]</ref> can be used to compute , though its computational complexity becomes overwhelming unless is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. <ref type="bibr" target="#b471">[465]</ref>, <ref type="bibr" target="#b331">[327]</ref>, <ref type="bibr" target="#b271">[267]</ref>, <ref type="bibr" target="#b243">[239]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b217">[212]</ref>, <ref type="bibr" target="#b556">[550]</ref>, <ref type="bibr" target="#b566">[559]</ref>, <ref type="bibr" target="#b222">[217]</ref>). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shannon distortion-rate function ( th-order or in the limit) of any source with the same covariance function.</p><p>To compute a Zador-Gersho function, one needs to find and either or in the fixed-and variable-rate cases, respectively. Though is known only for , there are bounds for other values of . One lower bound is the normalized moment of inertia of a sphere of the same dimension <ref type="bibr" target="#b58">(58)</ref> Another bound is given in <ref type="bibr" target="#b108">[106]</ref>. One upper bound was developed by Zador; others derive from the currently best known tessellations (cf. <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b108">[106]</ref>). The Zador factors and can be computed straightforwardly for and, also, for for i.i.d. sources. In some cases, simple closedform expressions can be found, e.g., for Gaussian, Laplacian, gamma densities. In other cases, numerical integration can be used. Upper bounds to are given in <ref type="bibr" target="#b298">[294]</ref>. To the authors' knowledge, for sources with memory, simple expressions for the Zador factors have been found only for Gaussian sources; they depend on the covariance matrix.</p><p>Underlying Principles: Rate distortion theory is a deep and elegant theory based on the law of large numbers and the key information-theoretic property that derives from it, namely, the AEP. High resolution theory is a simpler, less elegant theory based on geometric characterizations and integral approximations over fine partitions.</p><p>Siblings: Lossless source coding and channel coding are sibling branches of information theory, also based on the law of large numbers and the asymptotic equipartition property. Siblings of high resolution theory include error probability analyses in digital modulation and channel coding based on minimum distance and a high signal-to-noise ratio assumption, and the average power analyses for the additive Gaussian channel based on the continuous approximation.</p><p>Code Design Philosophy: Neither theory is ordinarily considered to be constructive, yet each leads to its own design philosophy. Rate distortion theory shows that, with high probability, a good high-dimensional quantizer can be constructed by randomly choosing codevectors according to the output distribution of the test channel that achieves the Shannon ratedistortion function. As a construction technique, this leaves much to be desired because the dimension of such codes is large enough that the codes so constructed are completely impractical. On the other hand, the AEP indicates that such codevectors will be roughly uniformly distributed over a "typical" set, and this leads to the design philosophy that a good code has its codevectors uniformly distributed throughout this set. In the special case of squared-error distortion and an i.i.d. Gaussian source with variance , the output distribution is i.i.d. Gaussian with variance ; the typical set is a thin shell near the surface of a sphere of radius ; and a good code has its codevectors uniformly distributed on this shell. Since the interior volume of such a (highdimensional) sphere is negligible, it is equally valid for the codevectors to be uniformly distributed throughout the sphere. For other sources, the codevectors will be uniformly distributed over some subset of the shell.</p><p>High resolution theory indicates that for large rate and arbitrary dimension , the quantization cells should be as spherical as possible-preferably shaped like , with normalized moment of inertia . Moreover, the codevectors should be distributed according to the optimal point density . Thus high resolution theory yields a very clear design philosophy. In the scalar case, one can use this philosophy directly to construct a good quantizer, by designing a compander whose nonlinearity has derivative , and extracting the resulting reconstruction levels and thresholds to obtain an approximately optimal point quantizer. This was first mentioned in Panter-Dite <ref type="bibr" target="#b409">[405]</ref> and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis <ref type="bibr" target="#b198">[193]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>. So there is no direct way to construct optimal vector quantizers with the high resolution philosophy.</p><p>When dimension as well as rate is large, the two philosophies merge because the output distribution that achieves the Shannon distortion-rate function converges to the source density itself, as does the optimal point density. However, for small to moderate values of , specifies a better distribution of points than the rate distortion philosophy of uniformly distributing codevectors over the typical set. For example, in the i.i.d. Gaussian case it indicates that the point density should be a Gaussian hill with somewhat larger variance than that of the source density. Which design philosophy is more useful? At low rates (say 1 bit per sample or less), one has no choice but to look to rate distortion theory. But at moderate to high rates, it appears that the high-resolution design philosophy is the better choice. To see this consider an i.i.d. Gaussian source, a target rate , and a -dimensional quantizer with points uniformly distributed throughout a spherical support region. This is the ideal code suggested by rate distortion theory. One obtains a lower bound to its distortion by assuming that source vectors outside the support region are quantized to the closest point on the surface of the sphere, and by assuming that the cells within the support region are -dimensional spheres. In this case, at moderate to large rates (say rate ten), after choosing the diameter of the support region to minimize this lower bound, it has been found that the dimension must be larger than in order that the resulting signal-to-noise ratio be within 1 dB of that predicted by the Shannon distortionrate function <ref type="bibr">[25]</ref>. Similar results were reported by Pepin et al. <ref type="bibr" target="#b413">[409]</ref>. On the other hand, as mentioned earlier, a quantizer dimension can achieve this same distortion. It is clear then that the ability to come fairly close to with moderately large dimension is not due to the rate distortion theory design philosophy, the AEP, nor the use of spherical codes. Rather, it is due to the fact that good codes with small to moderate dimension have appropriately tapered point densities, as suggested by high resolution theory.</p><p>Finally, it is interesting to note that high resolution theory actually contains some analyses of the Shannon random coding approach. For example, Zador's thesis <ref type="bibr" target="#b568">[561]</ref> gives an upper bound on the distortion of a randomly generated vector quantizer.</p><p>Nature of the Error Process: Both theories have something to say about the distribution of quantization errors. Generally speaking, what rate distortion theory has to say comes from assuming that the error distribution caused by a quantizer whose performance is close to is similar to that caused by a test channel that comes close to achieving the Shannon distortion-rate function. This is reasonable because Shannon's random coding argument shows that using such a test channel to randomly generate high-dimensional codevectors leads, with very high probability, to a code whose distortion is close to</p><p>. For example, one may use this sort of argument to deduce that the quantization error of a good high-dimensional quantizer is approximately white and Gaussian when the source is memoryless, the distortion is squared error, and the rate is large, cf. <ref type="bibr" target="#b408">[404]</ref>, which shows Gaussian-like histograms for the quantization error of VQ's with dimensions to . As another example, for a Gaussian source with memory and squared-error distortion, rate distortion theory shows there is a simple relation between the spectra of the source and the spectra of the error produced by an optimal high-dimensional quantizer, cf. <ref type="bibr" target="#b45">[46]</ref>.</p><p>High resolution theory also has a long tradition of analyzing the error process, beginning with Clavier et al. <ref type="bibr" target="#b97">[95]</ref>, <ref type="bibr" target="#b102">[100]</ref>, and Bennett <ref type="bibr" target="#b42">[43]</ref>, and focusing on the distribution of the error, its spectrum, and its correlation with the input. Bennett showed that in the high-resolution case, the power spectral density of the quantizer error with uniform quantization is approximately white (and uniformly distributed) provided the assumptions of the high resolution theory are met and the joint density of sample pairs is smooth. (See also <ref type="bibr" target="#b201">[196,</ref><ref type="bibr">Sec. 5.6</ref>].) Bennett also found exact expressions for the power spectral density of a uniformly quantized Gaussian process. Sripad and Snyder <ref type="bibr" target="#b483">[477]</ref> and Claasen and Jongepier <ref type="bibr" target="#b99">[97]</ref> derived conditions under which the quantization error is white in terms of the joint characteristic functions of pairs of samples, two-dimensional analogs of Widrow's <ref type="bibr" target="#b535">[529]</ref> condition. Zador <ref type="bibr" target="#b569">[562]</ref> found highresolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers. Lee and Neuhoff <ref type="bibr" target="#b316">[312]</ref>, <ref type="bibr" target="#b383">[379]</ref> found high-resolution expressions for the density of the error produced by fairly general (deterministic) scalar and vector quantizers in terms of their point density and their shape profile, which is a function that conveys more cell shape information than the inertial profile. As a side benefit, these expressions indicate that much can be deduced about the point density and cell shapes of a quantizer from a histogram of the lengths of the errors. Zamir and Feder <ref type="bibr" target="#b571">[564]</ref> showed that the error produced by an optimal lattice quantizer with infinitely many small cells is asymptotically white in the sense that its components are uncorrelated with zero means and identical variances. Moreover, they showed that it becomes Gaussian as the dimension increases. The basic ideas are that as dimension increases good lattices have nearly spherical cells and that a uniform distribution over a high-dimensional sphere is approximately Gaussian, cf. <ref type="bibr" target="#b531">[525]</ref>. Since optimal high-dimensional, high-rate VQ's can also be expected to have nearly spherical cells and since the AEP implies that most cells will have the same size, we reach the same conclusion as from rate distortion theory, namely, that good high-rate high-dimensional codes cause the quantization error to be approximately white and Gaussian.</p><p>Successive Approximation: Many vector quantizers operate in a successive approximation or progressive fashion, whereby a low-rate coarse quantization is followed by a sequence of finer and finer quantizations, which add to the rate. Tree-structured, multistage and hierarchical quantizers, to be discussed in the next section, are examples of such. Other methods can be used to design progressive indexing into given codebooks, as in <ref type="bibr">Yamada and Tazaki (1991)</ref>  <ref type="bibr" target="#b559">[553]</ref> and <ref type="bibr" target="#b444">Riskin et al. (1994)</ref>  <ref type="bibr" target="#b444">[440]</ref>.</p><p>Successive approximation is useful in situations where the decoder needs to produce rough approximations of the data from the first bits it receives and, subsequently, to refine the approximation as more bits are received. Moreover, successive approximation quantizers are often structured in a way that makes them simpler than unstructured ones. Indeed, the three examples just cited are known more for their good performance with low complexity than for their progressive nature. An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis <ref type="bibr" target="#b232">[228]</ref>, <ref type="bibr" target="#b295">[291]</ref>, <ref type="bibr" target="#b296">[292]</ref>, <ref type="bibr" target="#b564">[557]</ref>, <ref type="bibr" target="#b151">[147]</ref>, <ref type="bibr" target="#b441">[437]</ref>, <ref type="bibr" target="#b98">[96]</ref> has shown that there are situations where successive approximation can be done without loss of optimality. On the other hand, high-resolution analyses of TSVQ <ref type="bibr" target="#b387">[383]</ref> and two-stage VQ <ref type="bibr" target="#b315">[311]</ref> have quantified the loss of these particular codes, and in the latter case shown ways of modifying the quantizer to eliminate the loss. Thus both theories have something to say about successive refinement.</p><p>V. QUANTIZATION TECHNIQUES This section presents an overview of quantization techniques (mainly vector) that have been introduced, beginning in the 1980's, with the goal of attaining rate/distortion performance better than that attainable by scalar-based techniques such as direct scalar quantization, DPCM, and transform coding, but without the inordinately large complexity of brute-force vector quantization methods. Recall that if the dimension of the source vector is fixed, say at , then the goal is to attain performance close to the optimal performance as expressed by in the fixed-rate case, or (usually ) in the general case where variable-rate codes are permitted. However, if, as in the case of a stationary source, the dimension can be chosen arbitrarily, then in both the fixedand variable-rate cases, the goal is to attain performance close to . In this case, all quantizers with are suboptimal, and quantizers with various dimensions and even memory (which blurs the notion of dimension) can be considered.</p><p>We would have liked to make a carefully categorized, ordered, and ranked presentation of the various methods. However, the literature and variety of such techniques is quite large; there are a number of competing in which to categorize the techniques; complexity is itself a difficult thing to quantify; there are several special cases (e.g., fixed or variable rate, and fixed or choosable dimension); and there has not been much theoretical or even quantitative comparison among them. Consequently, much work is still needed in sorting the wheat from the chaff, i.e., determining which methods give the best performance versus complexity tradeoff in which situations, and in gaining an understanding of why certain complexity-reducing approaches are better than others. Nevertheless, we have attempted to choose a reasonable set of techniques and an ordering of them for discussion. Where possible we will make comments about the efficacies of the techniques. In all cases, we include references.</p><p>We begin with a brief discussion of complexity. Roughly speaking, it has two aspects: arithmetic (or computational) complexity, which is the number of arithmetic operations per sample that must be performed when encoding or decoding, and storage (or memory or space) complexity, which is the amount of auxiliary storage (for example, of codebooks) that is required for encoding or decoding. Rather than trying to combine them, it makes sense to keep separate track, because their associated costs vary with implementation venue, e.g., a PC, UNIX platform, generic DSP chip, specially designed VLSI chip, etc. In some venues, storage is of such low cost that one is tempted to ignore it. However, there are techniques that benefit sufficiently from increased memory that even though the per-unit cost is trivial, to obtain the best performance-complexity tradeoff, memory usage should be increased until the marginal gain-to-cost ratio of further increases is small, at which point the total cost of memory may be signficant. As a result, one might think of a quantizer as being characterized by a four-tuple ; i.e., arithmetic complexity and storage complexity have been added to the usual rate and distortion . As a reminder, given a -dimensional fixed-rate VQ with codebook containing codevectors, brute-force fullsearch encoding finds the closest codevector in by computing the distortion between and each codevector. In other words, it uses the optimal lossy encoder for the given codebook, creating the Voronoi partition. In the case of squared error, this requires computing approximately operations per sample and storing approximately vector components. For example, a codebook with rate 0.25 bits per pixel (bpp) and vector dimension has codevectors, an impractical number for, say, real-time video coding. This exponential explosion of complexity and memory can cause serious problems even for modest dimension and rate, but it can in general make codes completely impractical in either the high-resolution or high-dimension extremes. A brute-force variable-rate scheme of the same rate will be even more complex-typically involving a much greater number of codevectors, a Lagrangian distortion computation, and an entropy coding scheme as well. It is the high complexity of such brute-force techniques that motivates the reduced complexity techniques to be discussed later in this section.</p><p>Simple measures such as arithmetic complexity and storage need a number of qualifications. One must decide whether encoding and decoding complexities need to be counted separately or summed, or, indeed, whether only one of them is important. For example, in record-once-play-many situations, it is the decoder that must have low complexity. Having no particular application in mind, we will focus on the sum of encoder and decoder complexities. For some techniques (perhaps most) it is possible to trade computations for storage by the use of precomputed tables. In such cases a quantizer is characterized, not by a single and but by a curve of such. In some cases, a given set of precomputed tables is the heart of the method. Another issue is the cost of memory accesses. Such operations are usually signficantly less expensive than arithmetic operations. However, some methods do such a good job of reducing arithmetic operations that the cost of memory accesses becomes significant. Techniques that attain smaller values of distortion need higher precision in their arithmetic and storage, which though not usually accounted for in assessments of complexity may sometimes be of significance. For example, a recent study of VQ codebook storage has shown that in routine cases one needs to store codevector components with only about bits per component, where is the rate of the quantizer <ref type="bibr" target="#b256">[252]</ref>. Though this study did not assess the required arithmetic precision, one would guess that it need not be more than a little larger than that of the storage; e.g., plus 5-or 6-bit arithmetic should suffice. Finally, variable-rate coding raises additional issues such as the costs associated with buffering, with storing and accessing variable-length codewords, and with the decoder having to parse binary sequences into variable-length codewords.</p><p>When assessing complexity of a quantization technique, it is interesting to compare the complexity invested in the lossy encoder/decoder versus that in the lossless encoder/decoder. (Recall that good performance can theoretically be attained with either a simple lossy encoder, such as a uniform scalar quantizer, and a sophisticated lossless encoder or, vice versa, as in high-dimensional fixed-rate VQ.) A quantizer is considered to have low complexity only when both encoders have low complexity. In the discussion that follows we focus mainly on quantization techniques where the lossless encoder is conceptually if not quantitatively simple. We wish, however, to mention the indexing problem, which may be considered to lie between the lossless and the lossy encoder. There are certain fixed-rate techniques, such as lattice quantization, pyramid VQ, and scalar-vector quantization, where it is fairly easy to find the cell in which the source vector lies, but the cells are associated with some set of indices that are not simply the integers from to , where is the number of cells, and converting the identity of the cell into a sequence of bits is nontrivial. This is referred to as an indexing problem.</p><p>Finally, we mention two additional issues. The first is that there are some VQ techniques whose implementation complexities are not prohibitive, but which have sufficiently many codevectors that designing them is inordinately complex or requires an inordinate amount of training data. A second issue is that in some applications it is desirable that the output of the encoder be progressively decodable in the sense that a rough reproduction can be made from the first bits that it receives, and improved reproductions are made as more bits are received. Such quantizers are said to be progressive or embedded. Now it is true that a progressive decoder can be designed for any encoder (for example, it can compute the expected value of the source vector given whatever bits it has received so far). However, a "good" progressive code is one for which the intermediate distortions achieved at the intermediate rates are relatively good (though not usually as good as those of quantizers designed for one specific rate) and that rather than restarting from scratch every time the decoder receives a new bit (or group of bits), it uses some simple method to update the current reproduction. It is also desirable in some applications for the encoding to be progressive, as well. Though not designed with them in mind, it turns out that a number of the reduced-complexity VQ approaches also address these last two issues. That is, they are easier to design, as well as progressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fast Searches of Unstructured Codebooks</head><p>Many techniques have been developed for speeding the full (minimum-distortion) search of an arbitrary codebook containing -dimensional codevectors, for example, one generated by a Lloyd algorithm. In contrast to codebooks to be considered later these will be called unstructured. As a group these techniques use substantial amounts of additional memory in order to significantly reduce arithmetic complexity. A variety of such techniques are mentioned in <ref type="bibr" target="#b201">[196,</ref><ref type="bibr">Sec. 12.16]</ref>.</p><p>A number of fast-search techniques are similar in spirit to the following: the Euclidean distances between all pairs of codevectors are precomputed and stored in a table. Now, given a source vector to quantize, some initial codevector is chosen. Then all codevectors whose distance from is greater than are eliminated from further consideration because they cannot be closer than . Those not eliminated are successively compared to until one that is closer than is found, which then replaces , and the process continues. In this way, the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref type="bibr" target="#b366">[362]</ref>, <ref type="bibr" target="#b523">[517]</ref>, <ref type="bibr" target="#b481">[475]</ref>, <ref type="bibr" target="#b482">[476]</ref>, <ref type="bibr" target="#b367">[363]</ref>, <ref type="bibr" target="#b430">[426]</ref>, <ref type="bibr" target="#b253">[249]</ref>, <ref type="bibr" target="#b403">[399]</ref>, <ref type="bibr" target="#b277">[273]</ref>, <ref type="bibr" target="#b249">[245]</ref>, <ref type="bibr" target="#b233">[229]</ref>, <ref type="bibr" target="#b336">[332]</ref>, <ref type="bibr" target="#b311">[307]</ref>, <ref type="bibr" target="#b553">[547]</ref>, <ref type="bibr" target="#b312">[308]</ref>, and <ref type="bibr" target="#b499">[493]</ref>.</p><p>A number of other fast-search techniques begin with a "coarse" prequantization with some very low-complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook that is being searched. The coarse prequantization often involves scalar quantization of some type or a tree-structuring of binary quantizers, such as what are called -trees. Associated with each coarse cell is a bucket containing the indices of each codevector that is the nearest codevector to some source vector in the cell. These buckets are determined in advance and saved as tables. Then to encode a source vector , one applies the prequantization, finds the index of the prequantization cell in which is contained, and performs a full search on the corresponding bucket for the closest codevector to . Techniques of this type may be found in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b181">[176]</ref>, <ref type="bibr" target="#b90">[88]</ref>, <ref type="bibr" target="#b91">[89]</ref>, <ref type="bibr" target="#b338">[334]</ref>, <ref type="bibr" target="#b150">[146]</ref>, <ref type="bibr" target="#b538">[532]</ref>, <ref type="bibr" target="#b427">[423]</ref>, <ref type="bibr" target="#b419">[415]</ref>, <ref type="bibr" target="#b506">[500]</ref>, and <ref type="bibr" target="#b86">[84]</ref>. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.</p><p>Another class of techniques is like the previous except that the low-complexity prequantization has much smaller cells than the Voronoi cells of , i.e., it is finer. In this case, the buckets associated with most "fine" prequantization cells contain just one codevector, i.e., the same codevector in is the closest codevector to each point in the fine cell. The indices of these codevectors, one for each fine cell, are stored in a precomputed table. For each of those relatively few fine cells that have buckets containing more than one codevector, one member of the bucket is chosen and its index is placed in the table as the entry for that fine cell. Quantization of then proceeds by applying the fine prequantizer and then using the index of the fine cell in which lies to address the table containing codevectors from , which then outputs the index of a codeword in . Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in <ref type="bibr" target="#b88">[86]</ref>, <ref type="bibr" target="#b362">[358]</ref>, <ref type="bibr" target="#b361">[357]</ref>, <ref type="bibr" target="#b524">[518]</ref>, <ref type="bibr" target="#b77">[75]</ref>, and <ref type="bibr" target="#b223">[219]</ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in <ref type="bibr" target="#b360">[356]</ref> for a case where the prequantization is a lattice quantizer. Other fast-search methods include the partial distortion method of <ref type="bibr" target="#b90">[88]</ref>, <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b406">[402]</ref> and the transform subspace-domain approach of <ref type="bibr" target="#b80">[78]</ref>.</p><p>Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be. Our experience is that the best tradeoffs come when the prequantization cells are finer rather than coarser, the explanation being that if one has prequantized coarsely and now has to determine which codevector in a bucket is closest to , it is more efficient to use some fast search method than to do full search. Dividing the coarse cells into finer ones is a way of doing just this. Another question that arises for all fast search techniques is whether it is worth the effort to perform a full search or whether one should instead stop short of this, as in the methods with fine prequantization cells. Our experience is that it is usually not worth the effort to do a full search, because by suffering only a very small increase in MSE one can achieve a significant reduction in arithmetic complexity and storage. Moreover, in the case of stationary sources where the dimension is subject to choice, for a given amount of arithmetic complexity and storage, one almost always gets better performance by doing a suboptimal search of a higher dimensional codebook than a full search of a lower dimensional one.</p><p>Fast search methods based on fine prequantization can be improved by optimizing the codebook for the given prequantizer. Each cell of the partition corresponding to induced by prequantization followed by table lookup is the union of some number of fine cells of the prequantizer. Thus the question becomes: what is the best partition into cells, each of which is the union of some number of fine cells. The codevectors in should then be the centroids of these cells. Such techniques have been exploited in <ref type="bibr" target="#b88">[86]</ref> and <ref type="bibr" target="#b362">[358]</ref>. One technique worth particular mention is called hierarchical table lookup VQ <ref type="bibr" target="#b88">[86]</ref>, <ref type="bibr" target="#b524">[518]</ref>, <ref type="bibr" target="#b77">[75]</ref>, <ref type="bibr" target="#b223">[219]</ref>. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high-rate scalar quantizer times. The next level of prequantization applies a two-dimensional VQ to each of pairs of scalar quantizer outputs. The next level applies a four-dimensional VQ to each of pairs of outputs from the two-dimensional quantizers, and so on. Hence the method is hierarchical. Because each of the quantizers can be implemented entirely with table lookup, this method eliminates all arithmetic complexity except memory accesses. It has been successfully used for video coding <ref type="bibr" target="#b524">[518]</ref>, <ref type="bibr" target="#b77">[75]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Structured Quantizers</head><p>We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in <ref type="bibr" target="#b201">[196]</ref> and <ref type="bibr" target="#b464">[458]</ref>.</p><p>Lattice Quantizers: Lattice quantization can be viewed as a vector generalization of uniform scalar quantization. It constrains the reproduction codebook to be a subset of a regular lattice, where a lattice is the set of all vectors of the form , where are integers and the are linearly independent (usually nondegenerate, i.e.,</p><p>). The resulting Voronoi partition is a tessellation with all cells (except for those overlapping the overload region) having the same shape, size, and orientation. Lattice quantization was proposed by Gersho <ref type="bibr" target="#b198">[193]</ref> because of its near optimality for high-resolution variable-rate quantization and, also, its near optimality for high-resolution fixed-rate quantization of uniformly distributed sources. (These assume that Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has led to algorithms for implementing their lossy encoders with very low arithmetic and storage complexity <ref type="bibr" target="#b105">[103]</ref>- <ref type="bibr" target="#b107">[105]</ref>, <ref type="bibr" target="#b465">[459]</ref>, <ref type="bibr" target="#b108">[106]</ref>, <ref type="bibr" target="#b204">[199]</ref>. These find the integers associated with the closest lattice point. Conway and Sloane <ref type="bibr" target="#b106">[104]</ref>, <ref type="bibr" target="#b108">[106]</ref> have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. Some important -dimensional lattices are the root lattices , , and , the Barnes-Wall lattice in dimension , and the Leech lattice in 24 dimensions. These latter give the best sphere packings and coverings in their respective dimensions. Recently, Agrell and Eriksson <ref type="bibr" target="#b4">[5]</ref> have found improved lattices in dimensions and .</p><p>Though low complexity algorithms have been found for the lossy encoder, there are other issues that affect the performance and complexity of lattice quantizers. For variable-rate coding, one must scale the lattice to obtain the desired distortion and rate, and one must implement an algorithm for mapping the 's to the variable-length binary codewords. The latter could potentially add much complexity. For fixed-rate coding with rate , the lattice must be scaled and a subset lattice points must be identified as the codevectors. This induces a support region. If the source has finite support, the lattice quantizer will ordinarily be chosen to have the same support. If not, then the scaling factor and lattice subset are usually chosen so that the resulting quantizer support region has large probability. In either case, a low complexity method is needed for assigning binary sequences to the chosen codevectors; i.e., for indexing. Conway and Sloane <ref type="bibr" target="#b107">[105]</ref> found such a method for the important case that the support has the shape of an enlarged cell. For sources with infinite support, such as i.i.d. Gaussian, there is also the difficult question of how to quantize a source vector lying outside the support region. For example, one might scale so that it lies on or just inside the boundary of the support region, and then quantize the scaled vector in the usual way. Unfortunately, this simple method does not always find the closest codevector to . Indeed, it often increases overload distortion substantially over that of the minimumdistance quantization rule. To date, there is apparently no low complexity method that does not substantially increase overload distortion.</p><p>High resolution theory applies immediately to lattice VQ when the entire lattice is considered to be the codebook. The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the codebook and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. <ref type="bibr" target="#b587">[580]</ref>, <ref type="bibr" target="#b155">[151]</ref>, <ref type="bibr" target="#b363">[359]</ref>, <ref type="bibr" target="#b153">[149]</ref>, <ref type="bibr" target="#b413">[409]</ref>. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support. But as discussed earlier, for sources with unbounded support such as i.i.d. Gaussian, they require very large dimensions to achieve performance close to . Product Quantizers: A product quantizer uses a reproduction codebook that is the Cartesian product of lower dimensional reproduction codebooks. For example, the application of a scalar quantizer to successive samples can be viewed as a product quantizer operating on thedimensional vector . The product structure makes searching easier and, unlike the special case of a sequence of scalar quantizers, the search need not be comprised of independent searches. Products of vector quantizers are also possible. Typically, the product quantizer is applied, not to the original vector of samples, but to some functions or features extracted from the vector. The complexities of a product quantizer (arithmetic and storage, encoding and decoding) are the sums of those of the component quantizers. As such, they are ordinarily much less than the complexities of an unstructured quantizer with the same number of codevectors, whose complexities equal the product of those of the components of a product quantizer.</p><p>A shape-gain vector quantizer <ref type="bibr" target="#b454">[449]</ref>, <ref type="bibr" target="#b455">[450]</ref> is an example of a product quantizer. It uses a product reproduction codebook consisting of a gain codebook of positive scalars and a shape codebook of unit norm -dimensional vectors, and the overall reproduction vector is defined by . It is easy to see the minimum-squared-error reproduction codeword for an input vector is found by the following encoding algorithm: First choose the index that maximizes the correlation , then for this chosen choose the index minimizing . This sequential rule gives the minimum-squarederror reproduction codeword without explicitly normalizing the input vector (which would be computationally expensive). The encoder and decoder are depicted in Fig. <ref type="figure" target="#fig_4">8</ref>.</p><p>A potential advantage of such a system is that by separating these two "features," one is able to use a scalar quantizer for the gain feature and a lower rate codebook for the shape feature, which can then have a higher dimension, for the same search complexity. A major issue arises here: given a total rate constraint, how does one best divide the bits between the two codebooks? This is an example of a rate-allocation problem that arises in all product codebooks and about which more will be said shortly.</p><p>It is important to notice that the use of a product quantizer does not mean the use of independent quantizers for each component. As with shape-gain VQ, the optimal lossy encoder will in general not view only one coordinate at a time. Separate and independent quantization of the components provides a low-complexity but generally suboptimal encoder. In the case of the shape-gain VQ, the optimal lossy encoder is happily a simple sequential operation, where the gain quantizer is scalar, but the selection of one of its quantization levels depends on the result of another quantizer, the shape quantizer. Similar ideas can be used for mean-removed VQ <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and mean/gain/shape VQ <ref type="bibr" target="#b396">[392]</ref>. The most general formulation of product codes has been given by Chan and Gersho <ref type="bibr" target="#b84">[82]</ref>. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later.</p><p>Fischer's pyramid VQ <ref type="bibr" target="#b169">[164]</ref> is also a kind of shape-gain VQ. In this case, the codevectors of the shape codebook are constrained to lie on the surface of a -dimensional pyramid, namely, the set of all vectors whose components have magnitudes summing to one. Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed <ref type="bibr" target="#b187">[182]</ref>, <ref type="bibr" target="#b188">[183]</ref>, <ref type="bibr" target="#b411">[407]</ref>, <ref type="bibr" target="#b410">[406]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b536">[530]</ref>, <ref type="bibr" target="#b495">[489]</ref>, <ref type="bibr" target="#b496">[490]</ref>, <ref type="bibr" target="#b489">[483]</ref>, <ref type="bibr" target="#b491">[485]</ref>, <ref type="bibr" target="#b494">[488]</ref>, <ref type="bibr" target="#b364">[360]</ref>. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. Early versions of polar quantization used independent quantization of the magnitude and phase information, but later versions used the better method described above, and some even allowed the phase quantizers to have a resolution that depends on the outcome of the magnitude quantizer. Such polar quantizers are called "unrestricted" <ref type="bibr" target="#b494">[488]</ref>, <ref type="bibr" target="#b536">[530]</ref>. High-resolution analysis can be used to study the rate-distortion performance of these quantizers <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b489">[483]</ref>, <ref type="bibr" target="#b491">[485]</ref>, <ref type="bibr" target="#b494">[488]</ref>, <ref type="bibr" target="#b364">[360]</ref>. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers. However, recently it has been shown that Bennett's integral can be applied to analyze polar quantization in a straightforward way <ref type="bibr" target="#b384">[380]</ref>. It turns out that for an i.i.d. Gaussian source, optimized conventional polar quantization gains about 0.41 dB over direct scalar quantization, and optimized unrestricted polar quantization gains another 0.73 dB. Indeed, the latter has, asymptotically, square cells and the optimal two-dimensional point density, and loses only 0.17 dB relative to optimal two-dimensional vector quantization, but is still 3.11 dB from . Product quantizers be used for any set of features deemed natural for decomposing a vector. Perhaps the most famous example is one we have seen already and now revisit: coding. Transform Coding: Though the goal of this section is mainly to discuss techniques beyond scalar quantization, DPCM and transform coding, we discuss the latter here because of its relationships to other techniques and because we wish to discuss work on the bit-allocation problem.</p><p>Traditional transform coding can be viewed as a product quantizer operating on the transform coefficients resulting from a linear transform on the original vector. We have already mentioned the traditional high-resolution fixed-rate analysis and the more recent high-resolution entropy-constrained analysis for separate lossless coding of each quantized transform coefficient. An asymptotic low-resolution analysis <ref type="bibr" target="#b342">[338]</ref>, <ref type="bibr" target="#b343">[339]</ref> has also been performed. In almost all actual implementations, however, scalar quantizers are combined with a block lossless code, where the lossless code is allowed to effectively operate on an entire block of quantized coefficients at once, usually by combining run-length coding with Huffman or arithmetic coding. As a result, the usual high-resolution analyses are not directly applicable.</p><p>Although high resolution theory shows that the Karhunen-LoÃ¨ve transform is optimal for Gaussian sources, and the asymptotic low-resolution analysis does likewise, the dominant transform for many years has been the discrete cosine transform (DCT) used in most current image and video coding standards. The primary competition for future standards comes from discrete wavelet transforms, which will be considered shortly. One reason for the use of the DCT is its lower complexity. An "unstructured" transform like the Karhunen-LoÃ¨ve requires approximately operations per sample, which is small compared to the arithmetic complexity of unstructured VQ, but large compared to the approximately operations per sample for a DCT. Another motivation for the DCT is that in some sense it approximates the behavior of the Karhunen-LoÃ¨ve transform for certain sources. And a final motivation is that the frequency decomposition done by the DCT mimics, to some extent, that done by the human visual system and so one may quantize the DCT coefficients taking perception into account. We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high-resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Paretooptimality considered in <ref type="bibr" target="#b532">[526]</ref>, <ref type="bibr" target="#b476">[470]</ref>, <ref type="bibr" target="#b96">[94]</ref>, <ref type="bibr" target="#b443">[439]</ref>, <ref type="bibr" target="#b442">[438]</ref>, and <ref type="bibr" target="#b469">[463]</ref>. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see <ref type="bibr" target="#b109">[107]</ref> or <ref type="bibr" target="#b201">[196,</ref><ref type="bibr">Ch. 10]</ref>. A combinatorial optimization method is given in <ref type="bibr" target="#b552">[546]</ref>.</p><p>As a final comment on traditional transform coding, the code can be considered as being suboptimal as a -dimensional quantizer because of the constrained structure (transform and product code). It gains, however, in having a low complexity, and transform codes remain among the most popular compression systems because of their balance of performance and complexity.</p><p>Subband/Wavelet/Pyramid Quantization: Subband codes, wavelet codes, and pyramid codes are intimately related and all are cousins of a transform code. The oldest of these methods (so far as quantization is concerned) is the pyramid code of Burt and Adelsen <ref type="bibr" target="#b67">[66]</ref> (which is quite different from Fischer's pyramid VQ). The Burt and Adelsen pyramid is constructed from an image first by forming a Gaussian pyramid by successively lowpass filtering and downsampling, and then by forming a Laplacian pyramid which replaces each layer of the Gaussian pyramid by a residual image formed by subtracting a prediction of that layer based on the lower resolution layers. The resulting pyramid of images can then be quantized, e.g., by scalar quantizers. The approximation for any layer can be reconstructed by using the inverse quantizers (reproduction decoders) and upsampling and combining the reconstructed layer and all lower resolution reconstructed layers. Note that as one descends the pyramid, one easily combines the new bits for that layer with the bits already used to produce a higher resolution spatially and in amplitude. The pyramid code can be viewed as one of the original multiresolution codes. It can be viewed as a transform code because the entire original structure can be viewed as a linear transform of the original image, but observe that the number of pixels has been roughly doubled.</p><p>Subband codes decompose an image into separate images by using a bank of linear filters, hence once again performing a linear transformation on the data prior to quantizing it. Traditional subband coding used filters of equal or roughly equal bandwidth. Wavelet codes can be viewed as subband codes of logarithmically varying bandwidths instead of equal bandwidths, where the filters used satisfy certain properties. Since the introduction of subband codes in the late 1980's and wavelet codes in the early 1990's, the field has blossomed and produced several of the major contenders for the best speech and image compression systems. The literature is beyond the scope of this article to survey, and much is far more concerned with the transforms, filters, or basis functions used and the lossless coding used following quantization than with the quantization itself. Hence we content ourselves with the mention of a few highlights. The interested reader is referred to the book by Vetterli and KovaceviÄ on wavelets and subband coding <ref type="bibr" target="#b522">[516]</ref>.</p><p>Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. <ref type="bibr" target="#b115">[113]</ref>. The extension of subband filtering from 1-D to 2-D was made by Vetterli <ref type="bibr" target="#b521">[515]</ref> and 2-D subband filtering was first applied to image coding by Woods et al. <ref type="bibr" target="#b547">[541]</ref>, <ref type="bibr" target="#b533">[527]</ref>, <ref type="bibr" target="#b546">[540]</ref>. Early wavelet-coding techniques emphasized scalar or lattice vector quantization <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b133">[130]</ref>, <ref type="bibr" target="#b469">[463]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b190">[185]</ref>, and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding <ref type="bibr" target="#b370">[366]</ref>, residual vector quantization <ref type="bibr" target="#b299">[295]</ref>, and other methods <ref type="bibr" target="#b109">[107]</ref>. A major breakthrough in performance and complexity came with the introduction of zerotrees <ref type="bibr" target="#b319">[315]</ref>, <ref type="bibr" target="#b472">[466]</ref>, <ref type="bibr" target="#b463">[457]</ref>, which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding. As done by JPEG in a primitive way, the zerotree approach led to a code which first sent bits about the transform coefficients with the largest magnitude, and then sent subsequent bits describing these significant coefficients to greater accuracy as well as bits about originally less significant coefficients that became significant as the accuracy improved. The zerotree approach has been extended to vector quantization (e.g., <ref type="bibr" target="#b111">[109]</ref>), but the slight improvement comes at a significant cost in added complexity. Rate-distortion ideas have been used to optimize the rate-distortion tradeoffs using wavelet packets by minimizing a Lagrangian distortion over code trees and bit assignments <ref type="bibr" target="#b431">[427]</ref>. Recently, competitive schemes have demonstrated that separate scalar quantization of individual subbands coupled with a sophisticated but lowcomplexity lossless coding algorithm called stack-run coding can provide performance nearly as good as EZW <ref type="bibr" target="#b510">[504]</ref>.</p><p>The best wavelet codes tend to use very smart lossless codes, lossless codes which effectively code very large vectors. While wavelet advocates may credit the decomposition itself for the gains in compression, the theory suggests that rather it is the fact that vector entropy coding for very large vectors is feasible.</p><p>Scalar-Vector Quantization: Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's <ref type="bibr" target="#b309">[305]</ref> scalar-vector quantization attempts to match the performance of an optimal entropy-constrained scalar quantizer with a low-complexity fixed-rate structured vector quantizer. A derivative technique called block-constrained quantization <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b26">[28]</ref> is simpler and easier to describe. Here the reproduction codebook is a subset of thefold product of some scalar codebook. Variable-length binary codewords are associated with the scalar levels, and given some target rate , the -dimensional codebook contains only those sequences of quantization levels for which the sum of the lengths of the binary codewords associated with the levels is at most . The minimum distortion codevector can be found using dynamic programming. Alternatively, an essentially optimal search can be performed with very low complexity using a knapsack packing or Lagrangian approach. The output of the encoder is the sequence of binary codewords corresponding to the codevector that was found, plus some padded bits if the total does not equal . The simplest method requires approximately operations per sample and storage for approximately numbers, where is the number of scalar quantization levels. The original scalar-vector method differs in that rational lengths rather than binary codewords are assigned to the scalar quantizer levels, dynamic programming is used to find the best codevector, and the resulting codevectors are losslessly encoded with a kind of lexicographic encoding. For i.i.d. Gaussian sources these methods attain SNR within about 2 dB of with on the order of , which is about 0.5 dB from the goal of 1.53 dB larger than . A high-resolution analysis is given in <ref type="bibr" target="#b24">[26]</ref> and <ref type="bibr" target="#b22">[23]</ref>. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform <ref type="bibr" target="#b309">[305]</ref>.</p><p>Tree-Structured Quantization: In its original and simplest form, a -dimensional tree-structured vector quantizer (TSVQ) <ref type="bibr" target="#b70">[69]</ref> is a fixed-rate quantizer with, say, rate whose encoding is guided by a balanced (fixed-depth) binary tree of depth . There is a codevector associated with each of its terminal nodes (leaves), and a -dimensional testvector associated with each of its internal nodes. Quantization of a source vector proceeds in a tree-structured search by finding which of the two nodes stemming from the root node has the closer testvector to , then finding which of the two nodes stemming from this node has the closer testvector, and so on, until a terminal node and codevector are found. The binary encoding of this codevector consists of the sequence of binary decisions that lead to it. Decoding is done by table lookup as in unstructured VQ. As in successive approximation scalar quantization, TSVQ yields an embedded code with a naturally progressive structure.</p><p>With this method, encoding requires storing the tree of testvectors and codevectors, demanding approximately twice the storage of an unstructured codebook. However, encoding requires only distortion calculations, which is a tremendous decrease over the required by full search of an unstructured codebook. In the case of squared-error distortion, instead of storing testvectors and computing the distortion between and each of them, at each internal node one may store the normal to the hyperplane bisecting the testvectors at the two nodes stemming from it, and determine on which side of the hyperplane lies by comparing an inner product of with the normal to a threshold that is also stored. This reduces the arithmetic complexity and storage roughly in half to approximately operations per sample and vectors. Further reductions in storage are possible, as described in <ref type="bibr" target="#b256">[252]</ref>.</p><p>The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ <ref type="bibr" target="#b70">[69]</ref>, <ref type="bibr" target="#b229">[225]</ref> is first to design the testvectors stemming from the root node using the Lloyd algorithm on a training set. Then design the two testvectors stemming from, say, the left one of these by running the Lloyd algorithm on the training vectors that were mapped to the left one, and so on.</p><p>In the scalar case, a tree can be found that implements any quantizer, indeed, the optimal quantizer. So tree-structuring loses nothing, though the above design algorithm does not necessarily generate the best possible quantizers. In the multidimensional case, one cannot expect that the greedy algorithm will produce a TSVQ that is as good as the best unstructured VQ or even the best possible TSVQ. Nevertheless, it seems to work pretty well. It has been observed that in the highresolution case, the cells of the resulting TSVQ's are mostly a mixture of cubes, cubes cut in half, the latter cut in half again, and so on until smaller cubes are formed. And it has been found for i.i.d. Gauss and Gauss-Markov sources that the performances of TSVQ's with moderate to high rates designed by the greedy algorithm are fairly well predicted by Bennett's integral, assuming the point density is optimum and the cells are an equal mixture of cubes, cubes cut in half, and so on. This sort of analysis indicates that the primary weakness of TSVQ is in the shapes of the cells that it produces. Specifically, its loss relative to optimal -dimensional fixed-rate VQ ranges from 0.7 dB for to 2.2 dB for very large dimensions. Part of the loss is , the ratio of the normalized moment of inertia of a cube to that of the best -dimensional cell shape, which approaches 1.53 dB for large , and the remainder, about 0.5 to 0.7 dB, is due to the oblongitis caused by the cubes being cut into pieces <ref type="bibr" target="#b387">[383]</ref>. A paper investigating the nature of TSVQ cells is <ref type="bibr" target="#b576">[569]</ref>.</p><p>Our experience has been that when taking both performance and complexity into account, TSVQ is a very competitive VQ method. For example, we assert that for most of the fast search methods, one can find a TSVQ (with quite possibly a different dimension) that dominates it in the sense that and are all at least as good. Indeed, many of the fast-search approaches use a tree-structured prequantization. However, in TSVQ the searching tree and codebook are matched in size and character in a way that makes them work well together. A notable exception is the hierarchical table lookup VQ which attains a considerably smaller arithmetic complexity than attainable with TSVQ, at the expense of higher storage. The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining.</p><p>TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref type="bibr" target="#b346">[342]</ref>, <ref type="bibr" target="#b96">[94]</ref>, <ref type="bibr" target="#b443">[439]</ref>, <ref type="bibr" target="#b201">[196]</ref> and with larger branching factors than two or even variable branching factors <ref type="bibr" target="#b466">[460]</ref>. However, it should be recalled that the goodness of the original TSVQ means that the gains of such are not likely to be substantial except in the low-resolution case or if variable-rate coding is used or if the source has some complex structure that the usual greedy algorithm cannot exploit.</p><p>A tree-structured quantizer is analogous to a classification or regression tree, and as such unbalanced TSVQ's can be designed by algorithms based on a gardening metaphor of growing and pruning. The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone <ref type="bibr" target="#b53">[53]</ref>, and the variation of CART for designing TSVQ's bears their initials: the BFOS algorithm <ref type="bibr" target="#b96">[94]</ref>, <ref type="bibr" target="#b443">[439]</ref>, <ref type="bibr" target="#b201">[196]</ref>. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. One can grow a balanced tree by splitting all nodes in each level of the tree, or by splitting one node at a time, e.g., by splitting the node with the largest contribution to the distortion <ref type="bibr" target="#b346">[342]</ref> or in a greedy fashion to maximize the decrease in distortion for the increase in rate <ref type="bibr" target="#b443">[439]</ref>. Once grown, the tree can be pruned by removing all descendants of any internal node, thereby making it a leaf. This will increase average distortion, but will also decrease the rate. Once again, one can select for pruning the node that offers the best tradeoff in terms of the least increase in distortion per decrease in bits. It can be shown that, for quite general measures of distortion, pruning can be done in an optimal fashion and the optimal subtrees of decreasing rate are nested <ref type="bibr" target="#b96">[94]</ref> (see also <ref type="bibr" target="#b359">[355]</ref>). It seems likely that in the moderateto high-rate case, pruning removes leaves corresponding to cells that are oblong such as cubes cut in half, leaving mainly cubic cells. We also wish to emphasize that if variable-rate quantization is desired, the pruning can be done so as to optimize the tradeoff between distortion and leaf entropy.</p><p>There has been a flurry of recent work on the theory of treegrowing algorithms for vector quantizers, which are a form of recursive partitioning. See, for example, the work of Nobel and Olshen <ref type="bibr" target="#b394">[390]</ref>, <ref type="bibr" target="#b392">[388]</ref>, <ref type="bibr" target="#b393">[389]</ref>. For other work on tree growing and pruning see <ref type="bibr" target="#b397">[393]</ref>, <ref type="bibr" target="#b443">[439]</ref>, <ref type="bibr" target="#b280">[276]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and <ref type="bibr" target="#b359">[355]</ref>.</p><p>Multistage Vector Quantization: Multistage (or multistep, or cascade, or residual) vector quantization was introduced by Juang and Gray <ref type="bibr" target="#b278">[274]</ref> as a form of tree-structured quantization with much reduced arithmetic complexity and storage. Instead of having a separate reproduction codebook for each branch in the tree, a single codebook could be used for all branches of a common length by coding the residual error accumulated to that point instead of coding the input vector directly. In other words, the quantization error (or residual) from the previous stage is quantized in the usual way by the following stage, and a reproduction is formed by summing the previous reproduction and the newly quantized residual. An example of a two-stage quantizer is depicted in Fig. <ref type="figure" target="#fig_5">9</ref>. The rate of the multistage quantizer is the sum of the rates of the stages, and the distortion is simply that of the last stage. (It is easily seen that the overall error is just that of the last stage.) A multistage quantizer has a direct sum reproduction codebook in the sense that it contains all codevectors formed by summing codevectors from the reproduction codebooks used at each stage. One may also view it as a kind of product code in the sense that the reproduction codebook is determined by the Cartesian product of the stage codebooks. And like product quantization, its complexities (arithmetic and storage, encoding and decoding) are the sum of those of the stage quantizers plus a small amount for computing the residuals at the encoder or the sums at the decoder. In contrast, a conventional single-stage quantizer with the same rate and dimension has complexities equal to the product of those of the stage quantizers.</p><p>Since the total rate is the sum of the stage rates, a bitallocation problem arises. In two-stage quantization using fixed-rate, unstructured, -dimensional VQ's in both stages, it usually happens that choosing both stages to have the same rate leads to the best performance versus complexity tradeoff. In this case, the complexities are approximately the square root of what they would be for a single-stage quantizer.</p><p>Though we restrict attention here to the case where all stages are fixed-rate vector quantizers with the same dimension, there is no reason why they need have the same dimension, have fixed rate, or have any similarity whatsoever. In other words, multistage quantization can be used (and often is) with very different kinds of quantizers in its stages (different dimensions and much different structures, e.g., DPCM or wavelet coding). For example, structuring the stage quantizers leads to good performance and further substantial reductions in complexity, e.g., <ref type="bibr" target="#b247">[243]</ref>, <ref type="bibr" target="#b81">[79]</ref>.</p><p>Of course, the multistage structuring leads to a suboptimal VQ for its given dimension. In particular, the direct-sum form of the codebook is not usually optimal, and the greedysearch algorithm described above, in which the residual from one stage is quantized by the next, does not find the closest codevector in the direct-sum codebook. Moreover, the usual greedy design method, which uses a Lloyd algorithm to design the first stage in the usual way and then to design the second stage to minimize distortion when operating on the errors of the first, and so on, does not, in general, design an optimal multistage VQ, even for greedy search. However, two-stage VQ's designed in this way work fairly well.</p><p>A high-resolution analysis of two-stage VQ using Bennett's integral on the second stage can be found in <ref type="bibr" target="#b315">[311]</ref> and <ref type="bibr" target="#b313">[309]</ref>. In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error-density analysis of vector quantization in <ref type="bibr" target="#b316">[312]</ref> and <ref type="bibr" target="#b383">[379]</ref>.</p><p>Multistage quantizers have been improved in a number of ways. More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal of) increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b182">[177]</ref>, <ref type="bibr" target="#b83">[81]</ref>, <ref type="bibr" target="#b29">[31]</ref>, <ref type="bibr" target="#b31">[33]</ref>. Variablerate multistage quantizers have been developed <ref type="bibr" target="#b247">[243]</ref>, <ref type="bibr" target="#b301">[297]</ref>, <ref type="bibr" target="#b302">[298]</ref>, <ref type="bibr" target="#b445">[441]</ref>, <ref type="bibr" target="#b300">[296]</ref>.</p><p>Another way of improving multistage VQ is to adapt each stage to the outcome of the previous. One such scheme, introduced by Lee and Neuhoff <ref type="bibr" target="#b314">[310]</ref>, <ref type="bibr" target="#b313">[309]</ref>, was motivated by the observation that if the first stage quantizer has high rate, say , then by Gersho's conjecture, the first stage cells all have approximately the shape of , the tesselating polytope with least normalized moment of inertia, and the source density is approximately constant on them. This implies that the conditional distribution of the residual given that the source vector lies in the th cell differs from that for the th only by a scaling and rotation, because cell differs from by just a scaling and rotation. Therefore, if first-stage-dependent scaling and rotation are done prior to second-stage quantization, the conditional distribution of the residual will be the same for all cells, and the second stage can be designed for this distribution, rather than having to be a compromise, as is otherwise the case in two-stage VQ. Moreover, since this distribution is essentially uniform on a support region shaped like , the second stage can itself be a uniform tesselation. The net effect is a quantizer that inherits the optimal point density of the first stage <ref type="foot" target="#foot_9">13</ref> and the optimal cell shapes of the second. Therefore, in the high-resolution case, this cell-conditioned two-stage VQ works essentially as well as an optimal (single-stage) VQ, but with much less complexity.</p><p>Direct implementation of cell-conditioned two-stage VQ, requires the storing of a scale factor and a rotation for each first stage cell, which operate on the first stage residual before quantization by the second stage. Their inverses are applied subsequently. However, since the first stage cells are so nearly spherical, the rotations gain only a small amount, typically about 0.1 dB, and may be omitted. Moreover, since the best known lattice tesselations are so close to the best known tesselations, one may use lattice VQ as the second stage, which further reduces complexity. Good schemes of this sort have even been developed for low to moderate rates by Gibson <ref type="bibr" target="#b274">[270]</ref>, <ref type="bibr" target="#b275">[271]</ref> and Pan and Fischer <ref type="bibr" target="#b407">[403]</ref>, <ref type="bibr" target="#b408">[404]</ref>.</p><p>Cell-conditioned two-stage quantizers can be viewed as having a piecewise-constant point density of the sort proposed earlier by Kuhlmann and Bucklew <ref type="bibr" target="#b306">[302]</ref> as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in <ref type="bibr" target="#b493">[487]</ref>.</p><p>Another scheme for adapting each stage to the previous is called codebook sharing, as introduced by Chan and Gersho <ref type="bibr" target="#b82">[80]</ref>, <ref type="bibr" target="#b84">[82]</ref>. With this approach, each stage has a finite set of reproduction codebooks, one of which is used to quantize the residual, depending on the sequence of outcomes from the previous stages. Thus each codebook is shared among some subset of the possible sequences of outcomes from the previous stages. This method lies between conventional multistage VQ in which each stage has one codebook that is shared among all sequences of outcomes from previous stages, and TSVQ in which, in effect, a different codebook is used for each sequence of outcomes from the previous stages. Chan and Gersho introduced a Lloyd-style iterative design algorithm for designing shared codebooks; they showed that by controlling the number and rate of the codebooks one could optimize multistage VQ with a constraint on storage; and they used this method to good effect in audio coding <ref type="bibr" target="#b82">[80]</ref>. In the larger scheme of things, TSVQ, multistage VQ, and codebook sharing all fit within the broad family of generalized product codes that they introduced in <ref type="bibr" target="#b84">[82]</ref>.</p><p>Feedback Vector Quantization: Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Fig. <ref type="figure" target="#fig_0">3</ref>  <ref type="bibr" target="#b239">[235]</ref>, <ref type="bibr" target="#b118">[116]</ref>, <ref type="bibr" target="#b87">[85]</ref>, <ref type="bibr" target="#b421">[417]</ref>. Alternatively, the encoder and decoder can share a finite set of states and a quantizer custom designed for each state. Both encoder and decoder must be able to track the state in the absence of channel errors, so that the state must be determinable from knowledge of an initial state combined with the binary codewords transmitted to the decoder. The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Fig. <ref type="figure" target="#fig_6">10</ref>. Although little theory has been developed for finite-state quantizers <ref type="bibr" target="#b166">[161]</ref>, <ref type="bibr" target="#b183">[178]</ref>, <ref type="bibr" target="#b184">[179]</ref>, a variety of design methods exist <ref type="bibr" target="#b179">[174]</ref>, <ref type="bibr" target="#b180">[175]</ref>, <ref type="bibr" target="#b139">[136]</ref>, <ref type="bibr" target="#b240">[236]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b290">[286]</ref>, <ref type="bibr" target="#b201">[196]</ref>, Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state. The optimal lossy encoder is not easily described, however, as the next state must be chosen in a way that ensures good future behavior, and not just in a greedy fashion that minimizes the current squared error. If look-ahead is allowed, however, then a tree or trellis search can be used to pick a long-term minimum distortion path, as will be considered in the next subsection.</p><p>Both predictive and finite-state vector quantizers typically use memory in the lossy encoder, but use a memoryless lossless code independently applied to each successive binary codeword. One can, of course, also make the lossless code depend on the state, or be conditional on the previous binary codeword. One can also use a memoryless VQ combined with a conditional lossless code (conditioned on the previous binary codeword) designed with a conditional entropy constraint <ref type="bibr" target="#b97">[95]</ref>, <ref type="bibr" target="#b193">[188]</ref>. A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar. This is a kind of interblock lossless coding <ref type="bibr" target="#b388">[384]</ref>, <ref type="bibr" target="#b414">[410]</ref>, <ref type="bibr" target="#b432">[428]</ref>.</p><p>Address-vector quantization, introduced by Nasrabadi and Feng <ref type="bibr" target="#b375">[371]</ref> (see also <ref type="bibr" target="#b165">[160]</ref> and <ref type="bibr" target="#b377">[373]</ref>), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity. With this approach, in addition to the usual reproduction codebook , there is an address codebook containing permissible sequences of indices of codevectors in . The address codebook plays the same role as the outer code in a concatenated channel code (or the trellis in trellis-encoded quantization discussed below), namely, it limits the allowable sequences of codewords from the inner code, which in this case is . In this way, address-vector quantization can exploit the property that certain sequences of codevectors are much more probable than others; these will be the ones contained in . As with DPCM, the introduction of memory into the lossy encoder seriously complicates the theory of such codes, which likely explains why there is so little.</p><p>Tree/Trellis-Encoded Quantization: Channel coding has often inspired source coding or quantization structures. Channel coding matured much earlier and the dual nature of channel and source coding suggests that a good channel code can be turned into a good source code by reversing the order of encoder and decoder. This role reversal was natural for the codes which eased search requirements by imposition of a tree or trellis structure. Unlike the tree-structured vector quantizers, these earlier systems imposed the tree structure on the sequence of symbols instead of on a single vector of symbols. For the channel coding case, the encoder was a convolutional code, input symbols shifted into a shift register as output symbols, formed by linear combinations (in some field) of the shift-register contents, shifted out. Sequences of output symbols produced in this fashion could be depicted with a tree structure, where each node of the tree corresponded to the state of the shift register (all but the final or oldest symbol) and the branches connecting nodes were determined by the most recent symbol to enter the shift register and were labeled by the corresponding output, the output symbol resulting if that branch is taken. The goal of a channel decoder is to take such a sequence of tree branch labels that has been corrupted by noise, and find a minimum-distance valid sequence of branch labels. This could be accomplished by a tree-search algorithm such as the Fano, stack, or -algorithm. Since the shift register is finite, the tree becomes redundant and new nodes will correspond to previously seen states so that the tree diagram becomes a merged tree or trellis, which can be searched by a dynamic programming algorithm, the Viterbi algorithm, cf. <ref type="bibr" target="#b178">[173]</ref>. In the early 1970's, the algorithms for tree-decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref type="bibr" target="#b272">[268]</ref>, <ref type="bibr" target="#b273">[269]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b135">[132]</ref>, <ref type="bibr" target="#b126">[123]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Later, trellis channel-decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura <ref type="bibr" target="#b525">[519]</ref>. While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref type="bibr" target="#b323">[319]</ref>, <ref type="bibr" target="#b537">[531]</ref>, <ref type="bibr" target="#b487">[481]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[40]</ref>. Observe that the reproduction decoder of a finite-state VQ can be used as the decoder in a trellis-encoding system, where the finite-state encoder is replaced by a minimum-distortion search of the decoder trellis implied by the finite-state VQ decoder, which is an optimal encoding for a sequence of inputs.</p><p>Tree-and trellis-encoded quantizers can both be considered as a VQ with large blocklength and a reproduction codebook constrained to be the possible outputs of a nonlinear filter or a finite-state quantizer or vector quantizer of smaller dimension. Both structures produce long codewords with a trellis structure, i.e., successive reproduction symbols label the branches of a trellis and the encoder is just a minimum-distortion trellis search algorithm such as the Viterbi algorithm.</p><p>Trellis-Coded Quantization: Trellis-coded quantization, both scalar and vector, improves upon traditional trellisencoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref type="bibr" target="#b349">[345]</ref>, <ref type="bibr" target="#b348">[344]</ref>, <ref type="bibr" target="#b171">[166]</ref>, <ref type="bibr" target="#b172">[167]</ref>, <ref type="bibr" target="#b528">[522]</ref>, <ref type="bibr" target="#b347">[343]</ref>, <ref type="bibr" target="#b484">[478]</ref>, <ref type="bibr" target="#b520">[514]</ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance. As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis-coded quantization was motivated by Ungerboeck's enormously successful coded-modulation approach to channel coding for narrowband channels <ref type="bibr" target="#b511">[505]</ref>, <ref type="bibr" target="#b512">[506]</ref>.</p><p>Recent combinations of TCQ to coding wavelet coefficients <ref type="bibr" target="#b484">[478]</ref> have yielded excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.</p><p>Gaussian Quantizers: Shannon <ref type="bibr" target="#b471">[465]</ref> showed that a Gaussian i.i.d. source had the worst rate-distortion function of any i.i.d. source with the same variance, thereby showing that the Gaussian source was an extremum in a source coding sense. It was long assumed and eventually proved by <ref type="bibr">Sakrison in 1975 [456]</ref> that this provided a robust approach to quantization in the sense there exist vector quantizers designed for the i.i.d. Gaussian source with a given average distortion which will provide no worse distortion when applied to any i.i.d. source with the same variance. This provided an approach to robust vector quantization, having a code that might not be optimal for the actual source, but which would perform no worse than it would on the Gaussian source for which it was designed.</p><p>Sakrison extended the extremal properties of the rate distortion functions to sources with memory <ref type="bibr" target="#b458">[453]</ref>- <ref type="bibr" target="#b461">[455]</ref> and Lapidoth <ref type="bibr" target="#b310">[306]</ref> (1997) showed that a code designed for a Gaussian source would yield essentially the same performance when applied to another process with the same covariance structure.</p><p>These results are essentially Shannon theory and hence should be viewed as primarily of interest for high-dimensional quantizers.</p><p>In a different approach toward using a Gaussian quantizer on an arbitrary source, <ref type="bibr" target="#b423">Popat and Zeger (1992)</ref> took advantage of the central limit theorem and the known structure of an optimal scalar quantizer for a Gaussian random variable to code a general process by first filtering it to produce an approximately Gaussian density, scalar-quantizing the result, and then inverse-filtering to recover the original <ref type="bibr" target="#b423">[419]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robust Quantization</head><p>The Gaussian quantizers were described as being robust in a minimax average sense: a vector quantizer suitably designed for a Gaussian source will yield no worse average distortion for any source in the class of all sources with the same second-order properties. An alternative formulation of robust quantization is obtained if instead of dealing with average distortion, as is done in most of this paper, one places a maximum distortion requirement on quantizer design. Here a quantizer is considered to be robust if it bounds the maximum distortion for a class of sources. Morris and Vandelinde (1974) <ref type="bibr" target="#b365">[361]</ref> developed the theory of robust quantization and provide conditions under which the uniform quantizer is optimum in this minimax sense. This can be viewed as a variation on epsilon entropy since the goal is to minimize the maximum distortion. Further results along this line may be found in <ref type="bibr" target="#b35">[37]</ref>, <ref type="bibr" target="#b279">[275]</ref>, <ref type="bibr" target="#b497">[491]</ref>. Because these are minimax results aimed at scalar quantization, these results apply to any rate or dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Universal Quantization</head><p>The minimax approaches provide one means of designing a fixed-rate quantizer for a source with unknown or partially known statistics: a quantizer can be designed that will perform no worse than a fixed value of distortion for all sources in some collection. An alternative approach is to be more greedy and try to design a code that yields nearly optimal performance regardless of which source within some collection is actually coded. This is the idea behind universal quantization.</p><p>Universal quantization or universal source coding had its origins in an approach to universal lossless compression developed by Rice and Plaunt <ref type="bibr" target="#b439">[435]</ref>, <ref type="bibr" target="#b440">[436]</ref> and dubbed the "Rice machine." Their idea was to have a lossless coder that would work well for distinct sources by running multiple lossless codes in parallel and choosing the one producing the fewest bits for a period of time, sending a small amount of overhead to inform the decoder which code the encoder was using. The classic work on lossy universal source codes was Ziv's 1972 paper <ref type="bibr" target="#b584">[577]</ref>, which proved the existence of fixedrate universal lossy codes under certain assumptions on the source statistics and the source and codebook alphabets. The multiple codebook idea was also used in 1974 <ref type="bibr" target="#b225">[221]</ref> to extend the Shannon source coding theorem to nonergodic stationary sources by using the ergodic decomposition to interpret a nonergodic source as a universal coding problem for a family of ergodic sources. The idea is easily described and provides one means of constructing universal codes. Suppose that one has a collection of -dimensional codebooks with codevectors, each designed for a different type of local behavior. For example, one might have different codebooks in an image coder for edges, textures, and gradients. The union codebook then contains all the codevectors in all of the codes, for a total of codevectors. Thus for example, if all of the subcodebooks have equal rate , then the rate of the universal code is bits per symbol, which can be small if the dimension is moderately large. This does not mean that it is necessary to use a large-dimensional VQ, since the VQ can be a product VQ, e.g., for an image one could have by coding each square of dimension using four applications of a VQ of dimension . If one had, say, four different codes, the resulting rate would be , which would be a small increase over the original rate if the original rate is, say, . A universal code is in theory more complicated than an ordinary code, but in practice it can mean codes with smaller dimension might be more efficient since separate codebooks can be used for distinct short-term behavior.</p><p>Subsequently, a variety of notions of fixed-rate universal codes were considered and compared <ref type="bibr" target="#b386">[382]</ref>, and fixeddistortion codes with variable rate were developed by Mackenthun and Pursley <ref type="bibr" target="#b344">[340]</ref> and Kieffer <ref type="bibr" target="#b281">[277]</ref>, <ref type="bibr" target="#b283">[279]</ref>.</p><p>As with the early development of block source codes, universal quantization during its early days in the 1970's was viewed as more of a method for developing the theory than as a practical code-design algorithm. The Rice machine, however, proved the practicality and importance of a simple multiple codebook scheme for handling composite sources.</p><p>These works all assumed the encoder and decoder to possess copies of the codebooks being used. Zeger, Bist, and Linder <ref type="bibr" target="#b573">[566]</ref> considered systems where the codebooks are designed at the encoder, but must be also coded and transmitted to the decoder, as is commonly done in codebook replenishment <ref type="bibr" target="#b211">[206]</ref>.</p><p>A good review of the history of universal source coding through the early 1990's may be found in <ref type="bibr">Kieffer (1993)</ref>  <ref type="bibr" target="#b287">[283]</ref>.</p><p>Better performance tradeoffs can be achieved by allowing both rate and distortion to vary, and in 1996, Chou et al. <ref type="bibr" target="#b94">[92]</ref> formulated the universal coding problem as an entropy-constrained vector quantization problem for a family of sources and provided existence proofs and Lloyd-style design algorithms for the collection of codebooks subject to a Lagrangian distortion measure, yielding a fixed ratedistortion slope optimization rather than fixed distortion or fixed rate. The clustering of codebooks was originally due to Chou <ref type="bibr" target="#b92">[90]</ref> in 1991. High-resolution quantization theory was used to study rates of convergence with blocklength to the optimal performance, yielding results consistent with earlier convergence results developed by other means, e.g., Linder et al. <ref type="bibr" target="#b325">[321]</ref>. The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. <ref type="bibr" target="#b565">[558]</ref>.</p><p>A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu <ref type="bibr" target="#b333">[329]</ref>, <ref type="bibr" target="#b582">[575]</ref>, <ref type="bibr" target="#b581">[574]</ref>. Their approach, dubbed "gold washing," did not involve training, but rather created and removed codevectors according to the data received and an auxiliary random process in a way that could be tracked by a decoder without side information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dithering</head><p>Dithered quantization was introduced by Roberts <ref type="bibr" target="#b446">[442]</ref> in 1962 as a means of randomizing the effects of uniform quantization so as to minimize visual artifacts. It was further developed for images by <ref type="bibr">Limb (1969)</ref>  <ref type="bibr" target="#b321">[317]</ref> and for speech by <ref type="bibr" target="#b270">Jayant and Rabiner (1972)</ref>  <ref type="bibr" target="#b270">[266]</ref>. Intuitively, the goal was to cause the reconstruction error to look more like signalindependent additive white noise. It turns out that for one type of dithering, this intuition is true. In a dithered quantizer, instead of quantizing an input signal directly, one quantizes a signal</p><p>, where is a random process, independent of the signal , called a dither process. The dither process is usually assumed to be i.i.d.. There are two approaches to dithering. Roberts considered subtractive dithering, where the final reconstruction is formed as . An obvious problem is the need for the decoder to possess a copy of the dither signal. Nonsubtractive dithering forms the reproduction as . The principal theoretical property of nonsubtractive dithering was developed by Schuchman <ref type="bibr" target="#b467">[461]</ref>, who showed that the quantizer error is uniformly distributed on and is independent of the original input signal if and only if the quantizer does not overload and the characteristic function satisfies . Schuchman's conditions are satisfied, for example, if the dither signal has a uniform probability density function on . It follows from the work of Jayant and Rabiner <ref type="bibr" target="#b270">[266]</ref> and Sripad and Snyder <ref type="bibr" target="#b483">[477]</ref> (see also <ref type="bibr" target="#b221">[216]</ref>) that Schuchman's condition implies that the sequence of quantization errors is independent. The case of uniform dither remains by far the most widely studied in the literature.</p><p>The subtractive dither result is nice mathematically because it promises a well-behaved quantization noise as well as quantization error. It is impractical in many applications, however, for two reasons. First, the receiver will usually not have a perfect analog link to the transmitter (or else the original signal could be sent in analog form) and hence a pseudorandom deterministic sequence must be used at both transmitter and receiver as proposed by Roberts. In this case, however, there will be no mathematical guarantee that the quantization error and noise have the properties which hold for genuinely random i.i.d. dither. Second, subtractive dither of a signal that indeed resembles a sample function of a memoryless random process is complicated to implement, requiring storage of the dither signal, high-precision arithmetic, and perfect synchronization. As a result, it is of interest to study the behavior of the quantization noise in a simple nonsubtractive dithered quantizer. Unlike subtractive dither, nonsubtractive dither is not capable of making the reconstruction error independent of the input signal (although claims to the contrary have been made in the literature). Proper choice of dithering function can, however, make the conditional moments of the reproduction error independent of the input signal. This can be practically important. For example, it can make the perceived quantization noise energy constant as an input signal fades from high intensity to low intensity, where otherwise it can (and does) exhibit strongly signaldependent behavior. The properties of nonsubtractive dither were originally developed in unpublished work by Wright <ref type="bibr" target="#b548">[542]</ref> in 1979 and Brinton <ref type="bibr" target="#b54">[54]</ref> in 1984, and subsequently extended and refined with a variety of proofs <ref type="bibr" target="#b519">[513]</ref>, <ref type="bibr" target="#b518">[512]</ref>, <ref type="bibr" target="#b332">[328]</ref>, <ref type="bibr" target="#b231">[227]</ref>. For any necessary and sufficient conditions on the characteristic function are known which ensure that the th moment of the quantization noise conditional on does not depend on . A sufficient condition is that the dither signal consists of the sum of independent uniformly distributed random variables on . Unfortunately, this conditional independence of moments comes at the expense of a loss of fidelity. For example, if then the quantizer noise power (the mean-squared error) will be This means that the power in the dither signal is directly added to that of the quantizer error in order to form the overall mean-squared error.</p><p>In addition to its role in whitening quantization noise and making the noise or its moments independent of the input, dithering has played a role in proofs of "universal quantization" results in information theory. For example, Ziv <ref type="bibr" target="#b585">[578]</ref> showed that even without high resolution theory, uniform scalar quantization combined with dithering and vector lossless coding could yield performance within 0.75 bit/symbol of the rate-distortion function. Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder <ref type="bibr" target="#b572">[565]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Quantization for Noisy Channels</head><p>The separation theorem of information theory <ref type="bibr" target="#b470">[464]</ref>, <ref type="bibr" target="#b185">[180]</ref> states that nearly optimal communication of an information source over a noisy channel can be accomplished by separately quantizing or source coding the source and channel coding or error-control coding the resulting encoded source for reliable transmission over a noisy channel. Moreover, these two coding functions can be designed separately, without knowledge of each other. The result is only for point-to-point communications, however, and it is a limiting result in the sense that large blocklengths and hence large complexity must be permitted. If one wishes to perform near the Shannon limit for moderate delay or blocklengths, or in multiuser situations, it is necessary to consider joint source and channel codes, codes which jointly consider quantization and reliable communication. It may not actually be necessary to combine the source and channel codes, but simply to jointly design them. There are a variety of code structures and design methods that have been considered for this purpose, many of which involve issues of channel coding which are well beyond the focus of this paper. Here we mention only schemes which can be viewed as quantizers which are modified for use on a noisy channel and not those schemes which involve explicit channel codes. More general discussions can be found, e.g., in <ref type="bibr" target="#b125">[122]</ref>.</p><p>One approach to designing quantizers for use on noisy channels is to replace the distortion measure with respect to which a quantizer is optimized by the expected distortion over the noisy channel. This simple modification of the distortion measure allows the channel statistics to be included in an optimal quantizer design formulation. Recently, the method has been referred to as "channel-optimized quantization," where the quantization might be scalar, vector, or trellis.</p><p>This approach was introduced in 1969 by Kurtenbach and Wintz <ref type="bibr" target="#b308">[304]</ref> for scalar quantizers. A Shannon source coding theorem for trellis encoders using this distortion measure was proved in 1981 <ref type="bibr" target="#b138">[135]</ref> and a Lloyd-style design algorithm for such encoders provided in 1987 <ref type="bibr" target="#b18">[19]</ref>. A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasahara, and Namekawa <ref type="bibr" target="#b307">[303]</ref> and further studied in <ref type="bibr" target="#b162">[157]</ref>, <ref type="bibr" target="#b156">[152]</ref>, and <ref type="bibr" target="#b157">[153]</ref>. The method has also been applied to tree-structured VQ <ref type="bibr" target="#b416">[412]</ref>. It can be combined with a maximum-likelihood detector to further improve performance and permit progressive transmission over a noisy channel <ref type="bibr" target="#b415">[411]</ref>, <ref type="bibr" target="#b529">[523]</ref>. Simulated annealing has also been used to design such quantizers <ref type="bibr" target="#b143">[140]</ref>, <ref type="bibr" target="#b156">[152]</ref>, <ref type="bibr" target="#b358">[354]</ref>.</p><p>Another approach to joint source and channel coding based on a quantizer structure and not explicitly involving typical channel-coding techniques is to design a scalar or vector quantizer for the source without regard to the channel, but then code the resulting indices in a way that ensures that small (large) Hamming distance of the channel codewords corresponds to small (large) distortion between the resulting reproduction codewords, essentially forcing the topology on the channel codewords to correspond to that of the resulting reproduction codewords. The codes that do this are often called index assignments. Several specific index assignment methods were considered by Rydbeck and Sundberg <ref type="bibr" target="#b453">[448]</ref>. DeMarca and Jayant in 1987 <ref type="bibr" target="#b123">[121]</ref> introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho <ref type="bibr" target="#b575">[568]</ref>, who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include <ref type="bibr" target="#b215">[210]</ref>, <ref type="bibr" target="#b549">[543]</ref>, <ref type="bibr" target="#b291">[287]</ref>. For binary-symmetric channels and certain special sources and quantizers, analytical results have been obtained <ref type="bibr" target="#b562">[555]</ref>, <ref type="bibr" target="#b563">[556]</ref>, <ref type="bibr" target="#b254">[250]</ref>, <ref type="bibr" target="#b507">[501]</ref>, <ref type="bibr" target="#b114">[112]</ref>, <ref type="bibr" target="#b355">[351]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b236">[232]</ref>, <ref type="bibr" target="#b237">[233]</ref>, <ref type="bibr" target="#b356">[352]</ref>. For example, it was shown by <ref type="bibr">Crimmins et al. in 1969 [112]</ref> that the index assignment that minimizes meansquared error for a uniform scalar quantizer used on a binarysymmetric channel is the natural binary assignment. However, this result remained relatively unknown until rederived and generalized in <ref type="bibr" target="#b355">[351]</ref>.</p><p>When source and channel codes are considered together, a key issue is the determination of the quantization rate to be used when the total of number of channel symbols per source symbol is held fixed. For example, as quantization rate is increased, the quantization noise decreases, but channelinduced noise increases because the ability of the channel code to protect the bits is reduced. Clearly, there is an optimal choice of quantization rate. Another issue is the determination of the rate at which overall distortion decreases in an optimal system as the total number of channel uses per source symbol increases. These issues have been addressed in recent papers by Zeger and Manzella <ref type="bibr" target="#b577">[570]</ref> and Hochwald and Zeger <ref type="bibr" target="#b248">[244]</ref>, which use both exponential formulas produced by high resolution quantization theory and exponential bounds to channel coding error probability.</p><p>There are a variety of other approaches to joint source and channel coding, including the use of codes with a channel encoder structure optimized for the source or with a special decoder matched to the source, using unequal error protection to better protect more important (lower resolution) reproduction indices, jointly optimized combinations of source and channel codes, and combinations of channel-optimized quantizers with source-optimized channel codes, but we leave these to the literature as they involve a heavy dose of channel coding ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Quantizing Noisy Sources</head><p>A parallel problem to quantizing for a noisy channel is quantizing for a noisy source. The problem can be seen as trying to compress a dirty source into a clean reproduction, or as doing estimation of the original source based on a quantized version of a noise-corrupted version. If the underlying statistics are known or can be estimated by a training sequence, then this can be treated as a quantization problem with a modified distortion measure, where now the distortion between a noisecorrupted observation of an unseen original and a reconstruction based on the encoded and decoded is given as the conditional expectation . The usefulness of this modified distortion for source-coding noisy sources was first seen by Dobrushin and Tsybakov (1962) <ref type="bibr" target="#b137">[134]</ref> and was used by <ref type="bibr">Fine (1965)</ref>  <ref type="bibr" target="#b167">[162]</ref> and <ref type="bibr" target="#b457">Sakrison (1968)</ref> [452] to obtain information-theoretic bounds an quantization and source coding for noisy sources. <ref type="bibr">Berger (1971)</ref>  <ref type="bibr" target="#b45">[46]</ref> explicitly used the modified distortion in his study of Shannon source coding theorems for noise-corrupted sources.</p><p>In 1970, Wolf and Ziv <ref type="bibr" target="#b543">[537]</ref> used the modified distortion measure for a squared-error distortion to prove that the optimal quantizer for the modified distortion could be decomposed into the cascade of a minimum mean-squared error estimator followed by an optimal quantizer for the estimated original source. This result was subsequently extended to a more general class of distortion measures include the input-weighted quadratic distortion of Ephraim and Gray <ref type="bibr" target="#b149">[145]</ref>, where a generalized Lloyd algorithm for design was presented.</p><p>Related results and approaches can be found in Witsenhausen's (1980) <ref type="bibr" target="#b541">[535]</ref> treatment of rate-distortion theory with modified (or "indirect") distortion measures, and in the Occam filters of <ref type="bibr" target="#b374">Natarajan (1995)</ref>  <ref type="bibr" target="#b374">[370]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Multiple Description Quantization</head><p>A topic closely related to quantization for noisy channels is multiple description quantization. The problem is usually formulated as a source-coding or quantization problem over a network, but it is most easily described in terms of packet communications. In the simplest case, suppose that two packets of information, each of rate , are transmitted to describe a reproduction of a single random vector . The encoder might receive one or the other packet or the two together and wishes to provide the best reconstruction possible for the bit rate it receives. This can be viewed as a network problem with one receiver seeing only one channel, another receiver seeing the second channel, and a third reciever seeing both channels, and the goal is that each have an optimal reconstruction for the total received bitrate. Clearly, one can do no better than having each packet alone result in in a reproduction with distortion near the Shannon distortion-rate function while simultaneously having the two packets together yield a reproduction with distortion near , but this optimistic performance is in general not possible. This problem was first tackled in the information theory community in 1980 by Wolf, Wyner, and Ziv <ref type="bibr" target="#b542">[536]</ref> and Ozarow <ref type="bibr" target="#b405">[401]</ref> who developed achievable rate regions and lower bounds to performance. The results were extended by <ref type="bibr" target="#b5">Ahlswede (1985)</ref>  <ref type="bibr" target="#b5">[6]</ref>, El Gamal and Cover (1982) <ref type="bibr" target="#b142">[139]</ref>, and <ref type="bibr" target="#b580">Zhang and Berger (1987)</ref>  <ref type="bibr" target="#b580">[573]</ref>.</p><p>In 1993, Vaishampayan et al. used a Lloyd algorithm to actually design fixed-rate <ref type="bibr" target="#b514">[508]</ref> and entropy-constrained <ref type="bibr" target="#b515">[509]</ref> scalar quantizers for the multiple description problem. Highresolution quantization ideas were used to evaluate achievable performance in 1998 by Vaishampayan and Batllo <ref type="bibr" target="#b516">[510]</ref> and Linder, Zamir, and Zeger <ref type="bibr" target="#b328">[324]</ref>. An alternative approach to multiple-description quantization using transform coding has also been considered, e.g., in <ref type="bibr" target="#b36">[38]</ref> and <ref type="bibr" target="#b216">[211]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Other Applications</head><p>We have not treated many interesting variations and applications of quantization, several of which have been successfully analyzed or designed using the tools described here. Examples which we would have included had time, space, and patience been more plentiful include mismatch results for quantizers designed for one distribution and applied to another, quantizers designed to provide inputs to classification, detection, or estimation systems, quantizers in multiuser systems such as simple networks, quantizers implicit in finite-precision arithmetic (the modern form of roundoff error), and quantization in noiseshaping analog-to-digital and digital-to-analog converters such as -modulators. Doubtless we have failed to mention a few, but this list suffices to demonstrate how rich the theoretical and applied fields of quantization have become in their half century of active development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Predictive quantizer encoder/decoder.</figDesc><graphic coords="7,43.38,59.58,250.32,196.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Transform code.</figDesc><graphic coords="8,46.92,59.58,243.36,149.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. 10 log 10 k; L for a Gauss-Markov source with correlation coefficient 0:9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Signal-to-noise ratios for optimal VQ's (dots) and predictions thereof based on the Zador-Gersho formula (straight lines). (a) i.i.d. Gaussian. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Shape-gain VQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Two-stage VQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Finite-state vector quantizer.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Actually, Shannon described the solution to the equivalent problem of minimizing rate subject to a distortion constraint and found that the answer was given by a function R(D), subsequently called the Shannon rate-distortion function, which is the inverse of D(R). Accordingly, the theory is often called rate-distortion theory, cf.<ref type="bibr" target="#b45">[46]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For example, the video community has had a longstanding debate between progressive versus interlaced scanning-two different extraction methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>A cell T "tessellates" if there exists a partition of &lt; k whose cells are,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Actually, Lookabaugh and Gray defined the inverse as a vector quantizer advantage. The space-filling loss was called a cubic loss in<ref type="bibr" target="#b369">[365]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Dimension will be added as a subscript to f in places where the dimension of X needs to be emphasized.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>The fact that product quantizers can have the optimal point density is often overlooked.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>This implies that distortion will not decrease as 2 02R .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>Though Lloyd<ref type="bibr" target="#b334">[330]</ref> gave a fairly rigorous analysis of distortion, we do not include his paper in this category because it ignored overload distortion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>We abuse notation slightly and let k (N ) denote the least distortion of k-dimensional quantizers with N codevectors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>Since the second stage uniformly refines the first stage cells, the overall point density is approximately that of the first stage.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors gratefully acknowledge the many helpful comments, corrections, and suggestions from colleagues, students, and reviewers. Of particular assistance were A. Gersho, B. Girod, N. Kashyap, T. Linder, N. Moayeri, P. Moo, Y. Shtarkov, S. VerdÃº, M. Vetterli, and K. Zeger.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation under Grants NCR-941574 and MIP-931190.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some notes on optimal quantization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Communications</title>
		<meeting>Int. Conf. Communications</meeting>
		<imprint>
			<date type="published" when="1981-06">June 1981</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vector Quantization (IEEE Reprint Collection)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>Piscataway, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Block encoding and its application to data compression of PCM speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Adoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Canadian Communications and EHV Conf</title>
		<meeting>Canadian Communications and EHV Conf<address><addrLine>Montreal, Que., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral distance measure applied to the optimum design of DPCM coders with L predictors</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Adoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Debray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="512" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimization of lattices for quantization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lattice-based quantization, Part I&quot; Dept. Inform</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1814" to="1828" />
			<date type="published" when="1998-09">Sept. 1998</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Inform. Theory</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The rate-distortion region for multiple descriptions without excess rate</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="721" to="726" />
			<date type="published" when="1985-11">Nov. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Useful approximation to optimum quantization</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Algazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1966-06">June 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderberg</surname></persName>
		</author>
		<title level="m">Cluster Analysis for Applications</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tree encoding of speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Bodie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A 2-cycle algorithm for source coding with a fidelity criterion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="77" to="92" />
			<date type="published" when="1973-01">Jan. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image coding using vector quantization in the wavelet transform domain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="2297" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image coding using lattice vector quantization of wavelet coefficients</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2273" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image coding using wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-rate image coding with finite-state vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image compression based on vector quantization with finite memory</title>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="570" to="580" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantization error in predictive coders</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Arnstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="423" to="429" />
			<date type="published" when="1975-04">Apr. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The design of predictive trellis waveform coders using the generalized Lloyd algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>AyanÈglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1073" to="1080" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The design of joint source and channel trellis waveform coders</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="855" to="865" />
			<date type="published" when="1987-11">Nov. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image compression using nonadaptive spatial vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Rec. 16th Asilomar Conf. Circuits Systems and Computers</title>
		<meeting><address><addrLine>Asilomar, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-11">Nov. 1982</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Differential vector quantization of achromatic imagery</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Picture Coding Symp</title>
		<meeting>Int. Picture Coding Symp</meeting>
		<imprint>
			<date type="published" when="1983-03">Mar. 1983</date>
			<biblScope unit="page" from="105" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variable-rate treestructured vector quantizers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="917" to="930" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Block-constrained methods of fixed-rate entropy constrained quantization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Balamesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-01">Jan. 1993</date>
			<pubPlace>Michigan, Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">New methods of fixed-rate entropycoded quantization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Balamesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 Conf. Information Sciences and Systems</title>
		<meeting>1992 Conf. Information Sciences and Systems<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
			<biblScope unit="page" from="665" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Block-constrained quantization: Asymptotic analysis</title>
	</analytic>
	<monogr>
		<title level="j">DI-MACS Ser. Discr. Math. and Theoretical Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new fixed-rate quantization scheme based on arithmetic coding</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-01">Jan. 1993</date>
			<biblScope unit="page">435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Block-constrained methods of fixed-rate entropy-coded, scalar quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data analysis in the social sciences: What about the details?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fall Joint Computing Conf</title>
		<meeting>Fall Joint Computing Conf<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>Spartan</publisher>
			<date type="published" when="1965">1965</date>
			<biblScope unit="page" from="533" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramidal lattice vector quantization for multiscale image coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>SolÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">New multiple path search technique for residual vector quantizers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<meeting>Data Compression Conf<address><addrLine>Snowbird, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vector quantizers with direct sum codebooks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Frost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="565" to="580" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Advances in residual vector quantization: A review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="226" to="262" />
			<date type="published" when="1996-02">Feb. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the statistics of fixedpoint roundoff error</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="595" to="606" />
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The optimal lattice quantizer in three dimensions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Alg. Discr. Methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="1983-03">Mar. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The minimax distortion redundancy in empirical quantizer design</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
			<date type="published" when="1998-09">Sept. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust memoryless quantization for minimum signal distortion</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Vandelinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="296" to="306" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Asymptotic performance of multiple description codes</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Batllo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="703" to="707" />
			<date type="published" when="1997-03">Mar. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An improvement of the minimum distortion encoding algorithm for vector quantization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1132" to="1133" />
			<date type="published" when="1985-10">Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simulation of vector trellis encoding systems</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="214" to="218" />
			<date type="published" when="1986-03">Mar. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Statistical delta modulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1967-03">Mar. 1967</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="308" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the performance of a vector quantizer under channel errors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Proc. VI: Theories and Applications, Proc. EUSIPCO&apos;92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1685" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectra of quantized signals</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="446" to="472" />
			<date type="published" when="1948-07">July 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1975-09">Sept. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rate distortion theory for sources with abstract alphabet and memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="254" to="273" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">Rate Distortion Theory</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimum quantizers and permutation codes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="759" to="765" />
			<date type="published" when="1972-11">Nov. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Minimum entropy quantizers and permutation codes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="149" to="157" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Permutation codes for sources</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="160" to="169" />
			<date type="published" when="1972-01">Jan. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Image and Video Compression Standards</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bhaskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konstantinides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pulse code modulation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Lab. Rec</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1947-07">July 1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Computation of channel capacity and rate-distortion functions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Blahut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1972-07">July 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Classification and Regression Trees</title>
		<meeting><address><addrLine>Belmont, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Wadsworth</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nonsubtractive dither</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Brinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984-08">Aug. 1984</date>
			<pubPlace>Salt Lake City, UT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Elec. Eng. Dept., Univ. Utah</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the optimum quantization of stationary signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conv. Rec</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="124" />
			<date type="published" when="1964">1964. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Companding and random quantization in several dimensions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="207" to="211" />
			<date type="published" when="1981-03">Mar. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A note on optimal multidimensional companders</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">279</biblScope>
			<date type="published" when="1983-03">Mar. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Two results on the asymptotic performance of quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="341" to="348" />
			<date type="published" when="1984-03">Mar. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A note on the absolute epsilon entropy</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="142" to="144" />
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A note on optimum quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gallagher</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="365" to="366" />
			<date type="published" when="1979-05">May 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Quantization schemes for bivariate Gaussian random variables</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="537" to="543" />
			<date type="published" when="1979-09">Sept. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Two-dimensional quantization of bivariate circularly symmetric densities</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="667" to="671" />
			<date type="published" when="1979-11">Nov. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Some properties of uniform step size quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="610" to="613" />
			<date type="published" when="1980-09">Sept. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multidimensional asymptotic quantization theory with rth power distortion measures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vector quantization with complexity costs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>KÃ¼hnel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1133" to="1145" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983-04">Apr. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Optimal quantizations of coefficient vectors in LPC speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1978 Joint Meet</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>Acoustical Society of America and the Acoustical Society of Japan</publisher>
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Optimal quantizations of coefficient vectors in LPC, speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979-04">Apr. 1979</date>
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Speech coding based upon vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><surname>Acoust</surname></persName>
		</author>
		<title level="m">Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1980-10">Oct. 1980</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="562" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A simple class of asymptotically optimal quantizers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cambanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="1983-09">Sept. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The structure of quantization noise from Sigma-Delta modulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Candy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1316" to="1323" />
			<date type="published" when="1981-09">Sept. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Oversampling Delta-Sigma Data Converters</title>
		<editor>J. Candy and G. Temes</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Variations on a theme by Gallager</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Capocelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desantis</surname></persName>
		</author>
		<editor>Image and Text Compression, J. A. Storer</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="181" to="213" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Optimum quantization for minimum distortion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Caprio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Westin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int Telemetering Conf</title>
		<meeting>Int Telemetering Conf</meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hierarchical vector quantization of perceptually weighted block transforms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chaddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Compression Conf</title>
		<meeting>Compression Conf<address><addrLine>Snowbird, UT; Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Applications of rate distortion theory to the bandwidth compression</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chaffee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<pubPlace>Los Angeles</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Elec. Eng. Dept., Univ. California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A very low rate voice compression system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chaffee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Omura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstracts of Papers IEEE Int. Symp. Information Theory</title>
		<imprint>
			<date type="published" when="1974-10">Oct. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A complexity reduction technique for image vector quantization</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Po</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="312" to="321" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">High fidelity audio transform coding with vector quantization</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1109" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Constrained-storage vector quantization in high fidelity audio transform coding</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustiics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustiics, Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="3597" to="3600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Enhanced multistage vector quantization by joint codebook design</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1693" to="1697" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Generalized product code vector quantization: a family of efficient techniques for signal compression</title>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="95" to="126" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Enhanced multistage vector quantization by joint codebook design</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1693" to="1697" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">In search of the optimal searching sequence for VQ encoding</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2891" to="2893" />
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Gradient algorithms for designing predictive vector quantizers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="679" to="690" />
			<date type="published" when="1986-08">Aug. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hierarchical vector quantizers with table-lookup encoders</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1985 IEEE Int. Conf. Communications</title>
		<meeting>1985 IEEE Int. Conf. Communications</meeting>
		<imprint>
			<date type="published" when="1985-06">June 1985</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1452" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On two or more dimensional optimum quantizers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Hartford, CT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="640" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fast search algorithms for vector quantization and pattern matching</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramamurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust.ics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust.ics, Speech, and Signal essing (ICASSP)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984-03">Mar. 1984</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="911" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A fast codebook search algorithm for nearest-neighbor pattern matching</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986-04">Apr. 1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Code clustering for weighted universal VQ and other applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The distortion of vector quantizers trained on n vectors decreases to the optimum as Op(1=n)</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A vector quantization approach to universal noiseless coding and quantization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1109" to="1138" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Entropy-constrained vector quantization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lookabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="1989-01">Jan. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Optimal pruning with applications to tree-structured source coding and modeling</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="299" to="315" />
			<date type="published" when="1989-03">Mar. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Conditional entropy-constrained vector quantization of linear predictive coefficients</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lookabaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Failure of successive refinement for symmetric Gaussian mixtures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="350" to="352" />
			<date type="published" when="1957-01">Jan. 1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Model for the power spectral density of quantization noise</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A C M</forename><surname>Claasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jongepier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="914" to="917" />
			<date type="published" when="1981-08">Aug. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Transform Coding of Images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Academic</publisher>
			<pubPlace>Orlando, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Distortion in a pulse count modulation system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Clavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Panter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Grieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIEE Trans</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">PCM, distortion analysis</title>
	</analytic>
	<monogr>
		<title level="j">Elec. Eng</title>
		<imprint>
			<biblScope unit="page" from="1110" to="1122" />
			<date type="published" when="1947-11">Nov. 1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Theory and practice of vector quantizers trained on small training sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Entropy-based algorithms for best basis selection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Wickerhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="713" to="718" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Voronoi regions of lattices, second moments of polytopes, and quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Fast quantizing and decoding algorithms for lattice quantizers and codes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="227" to="232" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A fast encoding method for lattice codes and quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="820" to="824" />
			<date type="published" when="1983-11">Nov. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m">Sphere Packings,Lattices and Groups</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Vector quantization of image subbands: A survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="202" to="225" />
			<date type="published" when="1996-02">Feb. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Training sequence size and vector quantizer performance</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Perlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Perlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annu. Asilomar Conf. Signals, Systems, and Computers</title>
		<meeting>25th Annu. Asilomar Conf. Signals, Systems, and Computers<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
			<biblScope unit="page" from="434" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Tree-structured vector quantization with significance map for wavelet image coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Perlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Perlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1995 IEEE Data Compression Conf. (DCC)</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>1995 IEEE Data Compression Conf. (DCC)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Note on grouping</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Minimization of mean-squared error for data transmitted via group codes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Crimmins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Palermo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="72" to="78" />
			<date type="published" when="1969-01">Jan. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Digital coding of speech in sub-bands</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Crochiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K L</forename><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1069" to="1086" />
			<date type="published" when="1976-10">Oct. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Generalized entropy and quantization problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>CsiszÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Prague Conf</title>
		<meeting>6th Prague Conf</meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Information Theory: Coding Theorems for Discrete Memoryless Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>CsiszÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KÃ¶rner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Vector predictive coding of speech at 16 Kb/s</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Differential quantization of communication signals</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="605" to="361" />
			<date type="published" when="1952-07-29">July 29, 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">The problem of optimum stratification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dalenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Skand. Aktuarietidskrift</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="213" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The problem of optimum stratification II</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dalenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gurney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Skand. Aktuarietidskrift</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="203" to="213" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The 25th anniversary of pulse code modulation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Deloraine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="1965-05">May 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">An algorithm for assigning binary indices to the codevectors of multidimensional quantizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R B</forename><surname>Demarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Communications</title>
		<imprint>
			<biblScope unit="page" from="1128" to="1132" />
			<date type="published" when="1987-06">June 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Joint source/channel coding for variable length codes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 IEEE Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>1998 IEEE Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Soc. Press</publisher>
			<date type="published" when="1998-03">Mar. 1998</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">On tree coding with a fidelity criterion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="373" to="378" />
			<date type="published" when="1975-07">July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Information rates for data compression</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davission</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WESCON</title>
		<imprint>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m">Benchmark Papers in Electrical Engineering and Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Dowden, Hutchinson, and Ross</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Data Compression</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">New results on coding of stationary nonergodic sources</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="1979-03">Mar. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A direct proof of the coding theorem for discrete sources with memory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Pursley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="1975-05">May 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Delta modulation, a method of PCM transmission using a one-unit code</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dejager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philips Res. Repts</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Derjavitch</surname></persName>
			<affiliation>
				<orgName type="collaboration">French Patent 932 140</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Deloraine</surname></persName>
			<affiliation>
				<orgName type="collaboration">French Patent 932 140</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mierlo</surname></persName>
			<affiliation>
				<orgName type="collaboration">French Patent 932 140</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1946-08">Aug. 1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Image compression through wavelet transform coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jawerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="719" to="746" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>GyÃ¶rfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<title level="m">A Probabilistic Theory of Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Tree encoding of Gaussian sources</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="332" to="336" />
			<date type="published" when="1974-05">May 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Clustering analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Diday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Pattern Recognition</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Information transmission with additional noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dobrushin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="293" to="S304" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Joint source and noisy channel trellis encoding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="516" to="519" />
			<date type="published" when="1981-07">July 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">An algorithm for the design of labeled-transition finite-state vector quantizers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Ostendorf</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="83" to="89" />
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">The performance of a class of n dimensional quantizers for a Gaussian source</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Columbia Symp. Signal Transmission Processing</title>
		<title level="s">Benchmark Papers in Electrical Engineering and Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</editor>
		<meeting>Columbia Symp. Signal Transmission essing<address><addrLine>New York; Stroudsberg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Hutchinson and Ross</publisher>
			<date type="published" when="1965">1965. 1975</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="76" to="81" />
		</imprint>
		<respStmt>
			<orgName>Columbia Univ.</orgName>
		</respStmt>
	</monogr>
	<note>reprinted in Data Compression</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Variable-rate source coding theorems for stationary nonergodic sources</title>
		<author>
			<persName><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1920" to="1925" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Achievable rates for multiple descriptions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="851" to="857" />
			<date type="published" when="1982-11">Nov. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Using simulated annealing to design good codes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hemachandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shperling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="1987-01">Jan. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Predictive coding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Elias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Harvard Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Predictive coding I, and II</title>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Bounds on performance of optimum quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="172" to="184" />
			<date type="published" when="1970-03">Mar. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Bounds and asymptotes for the performance of multivariate quantizers</title>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1249" to="1259" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A unified approach for encoding clean and noisy sources by means of waveform and autoregressive vector quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="826" to="834" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A new vector quantization clustering algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Equitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1568" to="1575" />
			<date type="published" when="1989-10">Oct. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Successive refinement of information</title>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="269" to="275" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A result on delay-less information transmission</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ericson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstracts IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>Grignano, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979-06">June, 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Lattice-based quantization, Part II</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rep. 18, Dept. Inform. Theory</title>
		<meeting><address><addrLine>Goteborg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
		<respStmt>
			<orgName>Chalmers Univ. Technol.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Image quality measures and their performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>EskicioÇ§lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2959" to="2965" />
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Lattice and trellis quantization with lattice-and trellis-bounded codebooks-high-rate theory for memoryless sources</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vedat EyuboÇ§lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="46" to="59" />
			<date type="published" when="1993-01">Jan. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">A study of vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="799" to="809" />
			<date type="published" when="1990-07">July 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">On the performance and complexity of channel optimized vector quantizers</title>
	</analytic>
	<monogr>
		<title level="m">Speech Recognition and Coding: New Advances and Trends</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="699" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Performance of entropy-constrained block transform quantizers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1433" to="1439" />
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Optimal quantizer performance for a class of non-Gaussian memoryless sources</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Modestino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="485" to="497" />
			<date type="published" when="1984-05">May 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Rate-distortion performance of DPCM schemes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="402" to="418" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Optimal quantizer design for noisy channels: An approach to combined source-channel coding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="827" to="838" />
			<date type="published" when="1987-11">Nov. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Lagerungen in der Ebene, auf der Kugel und im Raum</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Fejes</forename><surname>Toth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953">1953</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Sur la representation d&apos;une population infinie par un nombre fini d&apos;elements</title>
	</analytic>
	<monogr>
		<title level="j">Acta Math. Acad. Sci. Hung</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Dynamic address-vector quantization of RGB color images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Elec. Eng., Part I, Commun. Speech Vision</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="225" to="231" />
			<date type="published" when="1991-08">Aug. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Properties of an optimal digital system and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Fine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="1964-10">Oct. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Optimum mean-square quantization of a noisy input</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="293" to="294" />
			<date type="published" when="1965-04">Apr. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">The response of a particular nonlinear system with feedback to each of two random processes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="255" to="264" />
			<date type="published" when="1968-03">Mar. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">A pyramid vector quantizer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="568" to="583" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Geometric source coding and vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Trellis-coded vector quantization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1551" to="1566" />
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Entropy-constrained trellis-coded quantization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="415" to="426" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Rate distortion functions for continuous alphabet memoryless sources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Vector quantization codebook generation using simulated annealing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Flanagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Morrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
			<biblScope unit="page" from="1759" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Sufficient conditions for achieving minimum distortion in a quantizer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fleischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conv. Rec</title>
		<imprint>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Principal points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Flury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: Efficiency vs. interpretability of classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Forgey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">1965</biblScope>
		</imprint>
	</monogr>
	<note>abstract</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">The Viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1973-03">Mar. 1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Finite-state vector quantization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstracts 1982 IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>Les Arcs France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-06">June 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Finite-state vector quantization for waveform coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Ostendorf</forename><surname>Dunham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="348" to="359" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">An algorithm for finding nearest neighbors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baskett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Shustek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1000" to="1006" />
			<date type="published" when="1975-10">Oct. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Design and performance of residual quantizers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Reif</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1991-04">Apr. 1991</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">On optimal finite-state digital transmission systems</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Gaarder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="167" to="186" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Gabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gyorfi</surname></persName>
		</author>
		<title level="m">Recursive Source Coding</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<title level="m">Information Theory and Reliable Communication</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Variations on a theme by Huffman</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="668" to="674" />
			<date type="published" when="1978-11">Nov. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Discrete spectral phase coding</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gallagher</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="622" to="624" />
			<date type="published" when="1976-09">Sept. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Quantizing schemes for the discrete Fourier transform of a random time-series</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="156" to="163" />
			<date type="published" when="1978-03">Mar. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Properties of minimum mean squared error block quantizers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="105" to="107" />
			<date type="published" when="1982-01">Jan. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">A comparison of the Z; E 8 , and Leech lattices for image subband quantization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Belzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Villasenor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1995 IEEE Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>1995 IEEE Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1995-03">Mar. 1995</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Theoretical analysis of the high-rate vector quantization of LPC parameters</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">The complexity of the generalized Lloyd-Max problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Witsenhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="255" to="266" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Conditional entropyconstrained vector quantization of frame difference subband signals</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>De Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="745" to="749" />
		</imprint>
	</monogr>
	<note>pt. 1 (of 3</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Analysis of delayed delta modulation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Gerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cambanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="496" to="512" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Analysis of adaptive differential PCM of a stationary Gauss-Markov input</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="350" to="359" />
			<date type="published" when="1987-05">May 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Stochastic stability of delta modulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="821" to="841" />
			<date type="published" when="1972-04">Apr. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Principles of quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="427" to="436" />
			<date type="published" when="1978-07">July 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Asymptotically optimal block quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Optimal nonlinear interpolative vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1285" to="1287" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Vector quantization: A pattern-matching technique for speech coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="1983-12">Dec. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Vector Quantization and Signal Compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Image coding using vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramamurthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-04">Apr. 1982</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="428" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Adaptive prediction in speech differential encoding systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1980-04">Apr. 1980</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="488" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Lattice quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Electron. Electron Phys</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="259" to="330" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Collected Papers on Digital Audio Bit-Rate Reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gilchrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grewin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Audio Eng. Soc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Rate-constrained motion estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communication and Image Processing VCIP&apos;94, Proc. SPIE, A. K. Katsaggelos</title>
		<imprint>
			<date type="published" when="1994-09">Sept. 1994</date>
			<biblScope unit="volume">2308</biblScope>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">The past, present, and future of image and multidimensional signal processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KovaceviÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Proc</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Mag</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Chellappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Girod</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Munson</surname><genName>Jr</genName></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Telkap</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vetterli</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998-03">Mar. 1998</date>
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
	<note>Image and video coding,&quot; part of</note>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Optimum quantization of random sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967-03">Mar. 1967</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Harvard Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Asymptotically efficient quantizing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="676" to="683" />
			<date type="published" when="1968-09">Sept. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Analog source digitization: A comparison of theory and practice</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Goblick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Holsinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="323" to="326" />
			<date type="published" when="1967-04">Apr. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Image sequence coding using vector quantization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="703" to="710" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Quantization noise in P.C.M</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Telephone Lab. Tech. Memo</title>
		<imprint>
			<date type="published" when="1957-10-18">Oct. 18, 1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Telephony by pulse code modulation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="395" to="409" />
			<date type="published" when="1947-07">July 1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Using simulated annealing to design transmission codes for analogue sources</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Moulsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="617" to="618" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Optimal multiple description transform coding of Gaussian vectors</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KovaceviÄ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Comp. Soc. Press</publisher>
			<date type="published" when="1998-04">Mar./Apr. 1998</date>
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Information rates of autoregressive processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="516" to="523" />
			<date type="published" when="1971-03">Mar. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">A new class of lower bounds to information rates of stationary sources via conditional rate-distortion functions</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="480" to="489" />
			<date type="published" when="1973-07">July 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mag</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Oversampled sigma-delta modulation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="481" to="489" />
			<date type="published" when="1987-04">Apr. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Quantization noise spectra</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1220" to="1244" />
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m">Source Coding Theory</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Combined compression and segmentation of images</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 Int. Workshop Mobile Multimedia Communication</title>
		<meeting>1997 Int. Workshop Mobile Multimedia Communication<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10">Sept./Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Source coding and speech compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Telemetering Conf</title>
		<meeting>Int. Telemetering Conf<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978-11">Nov. 1978</date>
			<biblScope unit="volume">XIV</biblScope>
			<biblScope unit="page" from="871" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Source coding theorems without the ergodic assumption</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davission</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1974-07">July 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Asymptotically optimal quantizers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="143" to="144" />
			<date type="published" when="1977-02">Feb. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Optimal speech compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rebolledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Asilomar Conf. Circuits Systems and Computers</title>
		<meeting>13th Asilomar Conf. Circuits Systems and Computers<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Multiple local optima in vector quantizers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Karnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="708" to="721" />
			<date type="published" when="1981-11">Nov. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Vector quantizers and predictive quantizers for Gauss-Markov sources</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="1982-02">Feb. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Tiling shapes for image vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Advances in Commun. and Control Systems (COMCON III)</title>
		<meeting>3rd Int. Conf. Advances in Commun. and Control Systems (COMCON III)<address><addrLine>Victoria, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Dithered quantizers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Stockham</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="805" to="812" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Source coding over simple networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1681" to="1721" />
			<date type="published" when="1974-11">Nov. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Equal-average hyperplane partitioning method for vector quantization of image data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="605" to="609" />
			<date type="published" when="1992-10">Oct. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">ISODATA: A novel method of data analysis and pattern classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stanford Res. Inst</title>
		<imprint>
			<date type="published" when="1965">1965</date>
			<pubPlace>Menlo Park, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Distortion-limited vector quantization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf.-DCC&apos;96</title>
		<meeting>Data Compression Conf.-DCC&apos;96<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Robust vector quantization by linear mappings of block-codes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-01">Jan. 1993</date>
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Design methods for VQ by linear mappings of block codes</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Interpolative vector quantization of color images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haskell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="465" to="470" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Predictive vector quantization of images</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1208" to="1219" />
			<date type="published" when="1985-11">Nov. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Predictive vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Messerschmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984-03">Mar. 1984</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Experiments with linear prediction in television</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="764" to="783" />
			<date type="published" when="1952-07">July 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">The computation and bounding of rate-distortion functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haskell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="525" to="531" />
			<date type="published" when="1969-09">Sept. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Differential pulse code modulation of the Wiener process</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="881" to="887" />
			<date type="published" when="1978-06">June 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Differential pulse code modulation of stationary Gaussian inputs</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1137" to="1147" />
			<date type="published" when="1978-08">Aug. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Cluster compression algorithm: a joint clustering/data compression concept</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Hilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jet Propulsion Lab</title>
		<imprint>
			<biblScope unit="page" from="77" to="120" />
			<date type="published" when="1977-12">Dec. 1977</date>
			<pubPlace>Pasadena, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Variable-rate multi-stage vector quantization for image coding</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="1156" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Tradeoff between source and channel coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hochwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1412" to="1424" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Fast codebook generation algorithm for vector quantization of images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="605" to="609" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Lossless compression of VQ index with search-order coding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1579" to="1582" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<monogr>
		<title level="m" type="main">Quantization of correlated random variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Engi., Yale Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Block quantization of correlated Gaussian random variables</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Schultheiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="289" to="296" />
			<date type="published" when="1963-09">Sept. 1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Fast encoding algorithm for VQ-based encoding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1618" to="1619" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Optimum binary code</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Res. Lab. Electron., Quart. Progr. Rep</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="223" to="225" />
			<date type="published" when="1966-07-15">July 15, 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum redundancy codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint>
			<date type="published" when="1952-09">Sept. 1952</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Reduced storage VQ via secondary quantization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="477" to="495" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Asymptotic analysis of optimum uniform scalar quantizers for generalized Gaussian distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1994 IEEE Int. Symp. Information Theory</title>
		<meeting>1994 IEEE Int. Symp. Information Theory<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="page">461</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">When is overload distortion negligible in uniform scalar quantization</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 IEEE Int. Symp. Information Theory</title>
		<meeting>1997 IEEE Int. Symp. Information Theory<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page">517</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Asymptotic analysis of optimal fixed-rate uniform scalar quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">On the complexity of scalar quantization</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1995 IEEE Int. Symp. Information Theory</title>
		<meeting>1995 IEEE Int. Symp. Information Theory<address><addrLine>Whistler, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
			<biblScope unit="page">372</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Maximum prediction residual principle applied to speech recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Itakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="67" to="72" />
			<date type="published" when="1975-02">Feb. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Analysis synthesis telephony based on the maximum likelihood method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Itakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Congr. Acoustics</title>
		<meeting>6th Int. Congr. Acoustics<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1968-08">Aug. 1968</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">A statistical method for estimation of speech spectral density and formant frequencies</title>
	</analytic>
	<monogr>
		<title level="j">Electron. Commun. Japan</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Calculated quantizing noise of single-integration deltamodulation coders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iwersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2359" to="2389" />
			<date type="published" when="1969-09">Sept. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Fundamentals of Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Differentical PCM systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Janardhanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="82" to="93" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Multidimensional group analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jancey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Austrailian J. Botany</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="127" to="130" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Digital coding of speech waveforms: PCM, DPCM, and DM quantizers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1974-05">May 1974</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="611" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<monogr>
		<title level="m" type="main">Digital Coding of Waveforms: Principles and Applications to Speech and Video</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">The application of dither to the quantization of speech signals</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1293" to="1304" />
			<date type="published" when="1972-08">July/Aug. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Evaluation of rate distortion functions for low distortions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE (Lett.)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2067" to="2068" />
			<date type="published" when="1967-11">Nov. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Tree encoding of memoryless time-discrete sources with a fidelity criterion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="584" to="590" />
			<date type="published" when="1969-09">Sept. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Instrumentable tree encoding of information sources</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="118" to="119" />
			<date type="published" when="1971-01">Jan. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Image coding with uniform and piecewise-uniform vector quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1993-05">May. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Transform coding of audio signals using perceptual noise criteria</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="314" to="323" />
			<date type="published" when="1988-02">Feb. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">A new MMSE encoding algorithm for vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Poonacha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="645" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Multiple stage vector quantization for speech coding</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acouststics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acouststics, Speech, and Signal essing (ICASSP)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-04">Apr. 1982</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">New results on robust quantization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kazakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="page" from="965" to="974" />
			<date type="published" when="1983-08">Aug. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Recursive optimal pruning with applications to tree structured vector quantizers</title>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Kiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">A generalization of the Pursley-Davisson-Mackenthun universal variable-rate coding theorem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kieffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="694" to="697" />
			<date type="published" when="1977-11">Nov. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Block coding for an ergodic source relative to a zero-one valued fidelity criterion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="422" to="437" />
			<date type="published" when="1978-07">July 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">A unified approach to weak universal source coding</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="674" to="682" />
			<date type="published" when="1978-11">Nov. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Exponential rate of convergence for Lloyd&apos;s method I</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="205" to="210" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Stochastic stability for feedback quantization schemes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="248" to="254" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">History of source coding</title>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory Soc. Newslett</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">A survey of the theory of source coding with a fidelity criterion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1473" to="1490" />
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">On a type of stochastic stability for a class of encoding schemes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kieffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dunham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="703" to="797" />
			<date type="published" when="1983-11">Nov. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">New results on optimal entropy-constrained quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kieffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jahns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Obuljen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1250" to="1258" />
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Side match and overlap match vector quantizers for images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="170" to="185" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">The Hadamard transform-A tool for index assignment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Knagenhjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1139" to="1151" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">On the Shannon theory of information transmission in the case of continuous signals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="1956-09">Sept. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">A construction of optimum vector quantizers by simulated annealing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kodama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wakasugi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kasahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Electron., Inform. Commun. Eng. B-I</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<title level="m">Self-Organization and Associative Memory</title>
		<meeting><address><addrLine>Berlin Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Hierarchical coding of discrete sources</title>
		<author>
			<persName><forename type="first">V</forename><surname>KoshÃ©lev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Pered. Inform</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="49" />
			<date type="published" when="1980-09">July-Sept. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Estimation of mean error for a discrete successiveapproximation scheme</title>
	</analytic>
	<monogr>
		<title level="j">Probl. Pered. Inform</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="1981-09">July-Sept. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">On the statistics of the error in predictive coding for stationary Ornstein-Uhlenbeck processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cambanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1029" to="1040" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">On quantizer distortion and the upper bound for exponential entropy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-E</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1168" to="1172" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Subband image coding using entropy-constrained residual vector quantization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kossentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Processing and Manag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="896" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Conditional entropy-constrained residual VQ with application to image coding</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="311" to="320" />
			<date type="published" when="1996-02">Feb. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Image coding using entropy-constrained residual vector quantization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kossentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1349" to="1357" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">Necessary conditions for the optimality of variable-rate residual vector quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1903" to="1914" />
			<date type="published" when="1995-11">Nov. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">A linear coding for transmitting a set of correlated signals</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="46" />
			<date type="published" when="1956-09">Sept. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Statistics of television signals</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Kretzmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="751" to="763" />
			<date type="published" when="1952-07">July 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Neural networks for vector quantization of speech and images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Ahalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="1990-10">Oct. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Piecewise uniform vector quantizers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1259" to="1263" />
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">A construction of vector quantizers for noisy channels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kumazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kasahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Namekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. and Eng. Japan</title>
		<editor>
			<persName><forename type="first">Denshi</forename><surname>Tsushin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gakkai</forename><surname>Ronbunshi</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1984-01">1984. Jan. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kurtenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Wintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun. Technol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="291" to="302" />
			<date type="published" when="1969-04">Apr. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">A structured fixed-rate vector quantizer derived from a variable-length scalar quantizer. I. Memoryless sources. II. Vector sources</title>
		<author>
			<persName><forename type="first">R</forename><surname>Laroia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="851" to="876" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">On the role of mismatch in rate distortion theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lapidoth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="38" to="47" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Fast closest codeword search algorithm for vector quantization</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Inst. Elec. Eng.-Vis. Image Signal Processing</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="143" to="148" />
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">A fast search algorithm for vector quantization using mean pyramids of codewords</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1697" to="1702" />
			<date type="published" when="1995-04">Feb.-Apr. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<monogr>
		<title level="m" type="main">Asymptotic quantization error and cell-conditioned twostage vector quantization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-12">Dec. 1990</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Conditionally corrected two-stage vector quantization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Information Sciences and Systems</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-03">Mar. 1990</date>
			<biblScope unit="page" from="802" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of two-stage vector quantization</title>
	</analytic>
	<monogr>
		<title level="m">1991 IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page">316</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Asymptotic distribution of the errors in scalar and vector quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="446" to="460" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Cell-conditioned twostage vector quantization of speech</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sov. Phys.-Dokl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Image compression using the 2-D, wavelet transform</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="244" to="250" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Asymptotic performance of vector quantizers with a perceptual distortion measure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chaddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<ptr target="http://www-isl.stanford.edu/gray/compression.html" />
	</analytic>
	<monogr>
		<title level="m">1997 IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
	<note>full paper submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Design of dithered waveforms for quantized visual signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Limb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2555" to="2582" />
			<date type="published" when="1968-09">Sept. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">A fake process approach to data compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="840" to="847" />
			<date type="published" when="1978-06">June 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">On asymptotically optimal companding quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Contr. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="465" to="484" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1728" to="1740" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">On the asymptotic tightness of the Shannon lower bound</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2026" to="2031" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">High-resolution source coding for nondifference distortion measures: The rate distortion function</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 IEEE Int. Symp. Information Theory</title>
		<meeting>1997 IEEE Int. Symp. Information Theory<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page">187</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">The multiple description rate region for high resolution source coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Comp. Soc. Press</publisher>
			<date type="published" when="1998-04">Mar./Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">High resolution source coding for nondifference distortion measures: multidimensional companding</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Asymptotic entropy-constrained performance of tessellating and universal randomized lattice quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="575" to="579" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Evaluation of epsilon entropy of random variables for small epsilon</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Linkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Inform. Transm</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="26" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
	<note>Pered. Inform.</note>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Quantization and dither: A theoretical survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lipshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Wannamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderkooy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Audio Eng. Soc</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="355" to="375" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">A fixed-slope universal sequential algorithm for lossy source coding based on Gold-Washing mechanism</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Annu. Allerton Conf. Communication, Control, and Computing</title>
		<meeting>33rd Annu. Allerton Conf. Communication, Control, and Computing<address><addrLine>Monticello, IL; Urbana-Champaign, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-10">Oct. 1995</date>
			<biblScope unit="page" from="466" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Also, IEEE Trans. Inform. Theory (Special Issue on Quantization)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1957-03">Sept. 1957. Mar. 1982</date>
			<pubPlace>Atlantic City, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>Mathematical Statistics Meet.</note>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Rate versus fidelity for the binary source</title>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="1977-03">Mar. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">Subcodebook searching algorithm for efficient VQ encoding of images</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Elec. Eng.-Vis. Image Signal Processing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="327" to="330" />
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">High-resolution quantization theory and the vector quantizer advantage</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Lookabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1020" to="1033" />
			<date type="published" when="1989-09">Sept. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Binary search trees for vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Millar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="2206" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Consistency of data-driven histogram methods for density estimation and classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="687" to="706" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main">On measuring by comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Åukaszewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zastosowania Matematyki</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="231" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
	<note>Polish</note>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Self-supervised training of hierarchical vector quantizers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Luttrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">II Int. Conf. Artificial Neural Networks</title>
		<title level="s">Conf. Publ.</title>
		<meeting><address><addrLine>London, U.K., IEE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<monogr>
		<title level="m" type="main">Fundamental limits of low-rate transform codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Lyons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main">A coding theorem for low-rate transform codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-01">Jan. 1993</date>
			<biblScope unit="page">333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">Strongly and weakly universal source coding</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Mackenthun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Pursley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1977 Conf. Information Science and Systems</title>
		<meeting>1977 Conf. Information Science and Systems<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>The Johns Hopkins Univ</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Berkeley Symp. on Mathematical Statistics and Probability</title>
		<meeting>5th Berkeley Symp. on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Vector quantization in speech coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1985-11">Nov. 1985</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1551" to="1588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">On entropy-constrained trellis-coded quantization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="14" to="16" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Trellis coded quantization of memoryless and Gauss-Markov sources</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="82" to="93" />
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Predictive trellis coded quantization of speech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Delta modulation of the Wiener process</title>
		<author>
			<persName><forename type="first">E</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cambanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1297" to="1300" />
			<date type="published" when="1975-11">Nov. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Vector quantization of images using the L 1 distortion measure</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-10">Oct. 1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Vector quantization using the L 1 distortion measure</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="33" to="35" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Quantizing for minimum distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1960-03">Mar. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Signal-to-noise and idle channel performance of DPCM systems with particular application to voice signals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1123" to="1151" />
			<date type="published" when="1966-09">Sept. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Optimal binary index assignments for a class of equiprobable scalar and vector quantizers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ashley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2031" to="2037" />
			<date type="published" when="1995-11">Nov. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">Binary lattice vector quantization with linear block codes and affine index assignments</title>
		<author>
			<persName><forename type="first">A</forename><surname>MÃ©hes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="79" to="94" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Optimum quantizer algorithm for real-time block quantizing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Menez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Esteban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1979 IEEE Int. Conf. Acoustics, Speech, and Signal Processin</title>
		<meeting>1979 IEEE Int. Conf. Acoustics, Speech, and Signal essin</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="980" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Combined source-channel vector quantization using deterministic annealing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="1994-04">Feb.-Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Some issues related to fixed-rate pruned tree-structured vector quantizers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1523" to="1531" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Theory of lattice-based fine-coarse vector quantization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1072" to="1084" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Time-memory tradeoffs in vector quantizer codebook searching based on decision trees</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="490" to="506" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<analytic>
		<title level="a" type="main">Fine-coarse vector quantization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1503" to="1515" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of fixed-rate lattice vector quantization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Moo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Information Theory and Its Applications</title>
		<meeting>Int. Symp. Information Theory and Its Applications<address><addrLine>Victoria, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-09">Sept. 1996</date>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<analytic>
		<title level="a" type="main">Uniform polar quantization revisited</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">Aug. 17-21, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">Robust quantization of discretetime signals with independent samples</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Vandelinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1897" to="1901" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b366">
	<analytic>
		<title level="a" type="main">On a fast vector quantization algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Motoishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Misumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VIIth Symp. Information Theory and Its Applications</title>
		<meeting>VIIth Symp. Information Theory and Its Applications</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
	<note>not in INSPEC</note>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Fast vector quantization algorithm by using an adaptive searching technique</title>
	</analytic>
	<monogr>
		<title level="m">Abstracts IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b368">
	<analytic>
		<title level="a" type="main">Vector quantizer of video signals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1005" to="1006" />
			<date type="published" when="1982-11">Nov. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b369">
	<analytic>
		<title level="a" type="main">Bennett&apos;s integral for vector quantizers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="886" to="900" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b370">
	<analytic>
		<title level="a" type="main">Tree coding of image subbands</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="147" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Mismatched DPCM encoding of autoregressive processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naraghi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="296" to="304" />
			<date type="published" when="1990-03">Mar. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<analytic>
		<title level="a" type="main">On the continuity of the stationary state distribution of DPCM</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="305" to="311" />
			<date type="published" when="1990-03">Mar. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b373">
	<analytic>
		<title level="a" type="main">Convergence of the projection method for an autoregressive process and a matched DPCM code</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1255" to="1264" />
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b374">
	<analytic>
		<title level="a" type="main">Filtering random noise from deterministic signals via data compression</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="1995-11">Nov. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b375">
	<analytic>
		<title level="a" type="main">Image compression using address-vector quantization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2166" to="2173" />
			<date type="published" when="1990-12">Dec. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b376">
	<analytic>
		<title level="a" type="main">Image coding using vector quantization: A review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="957" to="971" />
			<date type="published" when="1988-08">Aug. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b377">
	<analytic>
		<title level="a" type="main">An interframe hierarchical address-vector quantization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="960" to="967" />
			<date type="published" when="1992-06">June 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b378">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Haskell</surname></persName>
		</author>
		<title level="m">Digital Pictures: Representation and Compression</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum</publisher>
			<date type="published" when="1988">1988. 1995</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b379">
	<analytic>
		<title level="a" type="main">Picture coding: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Limb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1980-03">Mar. 1980</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="366" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b380">
	<analytic>
		<title level="a" type="main">Optimal quantizer design using a fixedpoint algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
			<date type="published" when="1976-11">Nov. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">Source coding strategies: Simple quantizers vs. simple noiseless codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1986 Conf. Information Sciences and Systems</title>
		<meeting>1986 Conf. Information Sciences and Systems</meeting>
		<imprint>
			<date type="published" when="1986-03">Mar. 1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="267" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<analytic>
		<title level="a" type="main">Why vector quantizers outperform scalar quantizers on stationary memoryless sources</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. Information Theory</title>
		<meeting><address><addrLine>Whistler, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
			<biblScope unit="page">438</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b383">
	<analytic>
		<title level="a" type="main">On the asymptotic distribution of the errors in vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="461" to="468" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b384">
	<analytic>
		<title level="a" type="main">Polar quantization revisited</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b385">
	<analytic>
		<title level="a" type="main">Causal source codes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="701" to="713" />
			<date type="published" when="1982-09">Sept. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b386">
	<analytic>
		<title level="a" type="main">Fixed rate universal block source coding with a fidelity criterion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="511" to="523" />
			<date type="published" when="1975-09">Sept. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b387">
	<analytic>
		<title level="a" type="main">On the performance of tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2277" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b388">
	<analytic>
		<title level="a" type="main">Tree searched vector quantization with interblock noiseless coding</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Information Science and Systems</title>
		<meeting>Conf. Information Science and Systems<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-03">Mar. 1988</date>
			<biblScope unit="page" from="781" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">The hexagon theorem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="137" to="139" />
			<date type="published" when="1964-03">1964. Mar. 1982</date>
		</imprint>
	</monogr>
	<note>Bell Lab. Tech. Memo.</note>
</biblStruct>

<biblStruct xml:id="b390">
	<analytic>
		<title level="a" type="main">A visual model weighted cosine transform for image compression and quality assessment</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Nill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="551" to="557" />
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">Objective image quality measure derived from digital image power spectra</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Nill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bouxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="813" to="825" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<analytic>
		<title level="a" type="main">Vanishing distortion and shrinking cells</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Nobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1303" to="1305" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b393">
	<analytic>
		<title level="a" type="main">Recursive partitioning to reduce distortion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1122" to="1133" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<analytic>
		<title level="a" type="main">Termination and continuity of greedy growing for tree-structured vector quantizers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Nobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b395">
	<analytic>
		<title level="a" type="main">Bounds on quantizer performance in the low bit-rate region</title>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zelinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="300" to="305" />
			<date type="published" when="1978-02">Feb. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b396">
	<analytic>
		<title level="a" type="main">Mean-gain-shape vector quantization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Oehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-04">Apr. 1993</date>
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b397">
	<analytic>
		<title level="a" type="main">Unbalanced tree-growing algorithms for practical image compression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Oehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Riskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="2293" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b398">
	<analytic>
		<title level="a" type="main">The philosophy of PCM</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint>
			<date type="published" when="1948-11">Nov. 1948</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1324" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b399">
	<analytic>
		<title level="a" type="main">Efficient coding</title>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="724" to="750" />
			<date type="published" when="1952-07">July 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b400">
	<analytic>
		<title level="a" type="main">A bound on signal-to-quantizing noise ratios for digital encoding systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>O'neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1967-03">Mar. 1967</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b401">
	<analytic>
		<title level="a" type="main">Signal to quantization noise ratio for differential PCM</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="568" to="569" />
			<date type="published" when="1971-08">Aug. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b402">
	<analytic>
		<title level="a" type="main">Entropy coding in speech and television differential PCM systems</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="758" to="761" />
			<date type="published" when="1971-11">Nov. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b403">
	<analytic>
		<title level="a" type="main">A fast nearest neighbor search algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Toronto, Ont., Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="2297" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b404">
	<analytic>
		<title level="a" type="main">Color quantization of images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2677" to="2690" />
			<date type="published" when="1991-12">Dec. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b405">
	<analytic>
		<title level="a" type="main">On a source-coding problem with two channels and three receivers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ozarow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1909" to="1921" />
			<date type="published" when="1980-12">Dec. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b406">
	<analytic>
		<title level="a" type="main">Effect of ordering the codebook on the efficiency of the partial distance search algorithm for vector quantization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="538" to="540" />
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b407">
	<analytic>
		<title level="a" type="main">Vector quantization-lattice vector quantization of speech LPC coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<idno>pt. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust.ics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust.ics, Speech, and Signal essing (ICASSP)<address><addrLine>Adelaide, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b408">
	<analytic>
		<title level="a" type="main">Two-stage vector quantization-lattice vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b409">
	<analytic>
		<title level="a" type="main">Quantizing distortion in pulse-count modulation with nonuniform spacing of levels</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Panter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint>
			<date type="published" when="1951-01">Jan. 1951</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="44" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b410">
	<analytic>
		<title level="a" type="main">Polar quantization of a complex Gaussian random variable</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="892" to="899" />
			<date type="published" when="1979-06">June 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b411">
	<analytic>
		<title level="a" type="main">Source coding of the discrete Fourier transform</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="683" to="692" />
			<date type="published" when="1978-11">Nov. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b412">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">JPEG Still Image Compression Standard</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b413">
	<analytic>
		<title level="a" type="main">Quantization of both stationary and nonstationary Gaussian sources with Voronoi constellations</title>
		<author>
			<persName><forename type="first">C</forename><surname>PÃ©pin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Belfiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boutros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory</title>
		<meeting>IEEE Int. Symp. Information Theory<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b414">
	<analytic>
		<title level="a" type="main">Coding of speech LSP parameters using TSVQ with interblock noiseless coding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Phamdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">Optimal detection of discrete Markov sources over discrete memoryless channels-Applications to combined source-channel coding</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="186" to="193" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<analytic>
		<title level="a" type="main">A unified approach to treestructured and multistage vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Phamdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b417">
	<analytic>
		<title level="a" type="main">The transmission distortion of a source as a function of the encoding block length</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pilc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="827" to="885" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Causal sliding block encoders with feedback</title>
		<author>
			<persName><forename type="first">P</forename><surname>Piret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="237" to="240" />
			<date type="published" when="1979-03">Mar. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Fast algorithm for full-search VQ encoding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1141" to="1142" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<analytic>
		<title level="a" type="main">Generalized-cost-measure-based address-predictive vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="49" to="55" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<analytic>
		<title level="a" type="main">Pruned tree-structured vector quantization of medical images with segmentation and improved prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="734" to="742" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b422">
	<analytic>
		<title level="a" type="main">Quantization and the method of k-means</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="199" to="205" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<analytic>
		<title level="a" type="main">Robust quantization of memoryless sources using dispersive FIR filters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1670" to="1674" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b424">
	<analytic>
		<title level="a" type="main">Epsilon entropy and data compression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodemich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2079" to="2125" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b425">
	<analytic>
		<title level="a" type="main">Epsilon entropy of stochastic processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodemich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rumsey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1000" to="1020" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b426">
	<monogr>
		<title level="m" type="main">Image Transmission Techniques</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b427">
	<analytic>
		<title level="a" type="main">A fast mean-distance-ordered partial codebook search algorithm for image vector quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="576" to="579" />
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b428">
	<analytic>
		<title level="a" type="main">TT7 of Tutorial Texts in Optical Engineering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Compression Techniques</title>
		<meeting><address><addrLine>Bellingham, WA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE Opt. Eng. Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">An optimized k-d tree algorithm for fast vector quantization of speech</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Signal Processing Conf</title>
		<meeting>Euro. Signal essing Conf<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="875" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<analytic>
		<title level="a" type="main">An efficient approximation-elimination algorithm for fast nearest-neighbor search based on a spherical distance coordinate formulation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Signal Processing Conf</title>
		<meeting>Euro. Signal essing Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Best wavelet packet bases in a ratedistortion sense</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="160" to="176" />
			<date type="published" when="1993-04">Apr. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<analytic>
		<title level="a" type="main">Combined VQ-DCT coding of images using interblock noiseless coding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="2281" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b433">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yip</surname></persName>
		</author>
		<title level="m">Discrete Cosine Transform</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b434">
	<analytic>
		<title level="a" type="main">A method for computing the DFT of vector quantized data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chabries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b435">
	<analytic>
		<title level="a" type="main">A multirate voice digitizer based upon vector quantization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rebolledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Burg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="721" to="727" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b436">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Reeves</surname></persName>
			<affiliation>
				<orgName type="collaboration">French Patent 852 183</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1938-10-03">Oct. 3, 1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b437">
	<analytic>
		<title level="a" type="main">On the dimension and entropy of probability distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>RÃ©nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Math. Acad. Sci. Hungar</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="193" to="215" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b438">
	<analytic>
		<title level="a" type="main">Mathematical analysis of random noise</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Stochastic Processes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Wax</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wax</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="133" to="294" />
			<date type="published" when="1944">1944. 1945. 1954</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b439">
	<analytic>
		<title level="a" type="main">The Rice machine: Television data compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Plaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jet Propulsion Lab</title>
		<imprint>
			<date type="published" when="1970-09">Sept. 1970</date>
			<pubPlace>Pasadena, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 900-408</note>
</biblStruct>

<biblStruct xml:id="b440">
	<analytic>
		<title level="a" type="main">Adaptive variable-length coding for efficient compression of spacecraft television data</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="889" to="897" />
			<date type="published" when="1971-12">Dec. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b441">
	<analytic>
		<title level="a" type="main">Successive refinement of information: Characterization of the achievable rates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rimoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="253" to="259" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b442">
	<analytic>
		<title level="a" type="main">Optimal bit allocation via the generalized BFOS algorithm</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Riskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="400" to="402" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b443">
	<analytic>
		<title level="a" type="main">A greedy tree growing algorithm for the design of variable rate vector quantizers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Riskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2500" to="2507" />
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b444">
	<analytic>
		<title level="a" type="main">Index assignment for progressive transmission of full-search vector quantization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Riskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="307" to="312" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b445">
	<analytic>
		<title level="a" type="main">Entropy-constrained predictive residual vector quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b446">
	<analytic>
		<title level="a" type="main">Picture coding using pseudo-random noise</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="1962-02">Feb. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b447">
	<analytic>
		<title level="a" type="main">Quantizing for minimum distortion</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="384" to="385" />
			<date type="published" when="1964-10">Oct. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b448">
	<analytic>
		<title level="a" type="main">Mapping approach to rate-distortion computation and analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1939" to="1952" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b449">
	<analytic>
		<title level="a" type="main">A deterministic annealing approach to clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b450">
	<analytic>
		<title level="a" type="main">Vector quantization by deterministic annealing</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b451">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1249" to="1257" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b452">
	<analytic>
		<title level="a" type="main">Constrained clustering as an optimization method</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="785" to="794" />
			<date type="published" when="1993-08">Aug. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b453">
	<analytic>
		<title level="a" type="main">Analysis of digital errors in nonlinear PCM systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rydbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-E</forename><forename type="middle">W</forename><surname>Sundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="1976-01">Jan. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b454">
	<analytic>
		<title level="a" type="main">Product code vector quantizers for speech waveform coding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Rec. GLOBECOM</title>
		<imprint>
			<date type="published" when="1982-12">Dec. 1982</date>
			<biblScope unit="page" from="1087" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b455">
	<analytic>
		<title level="a" type="main">Product code vector quantizers for waveform and voice coding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="474" to="488" />
			<date type="published" when="1984-06">June 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b456">
	<analytic>
		<title level="a" type="main">Global convergence and empirical consistency of the generalized Lloyd algorithm</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="148" to="155" />
			<date type="published" when="1986-03">Mar. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b457">
	<analytic>
		<title level="a" type="main">Source encoding in the presence of random disturbance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="165" to="167" />
			<date type="published" when="1968-01">Jan. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b458">
	<analytic>
		<title level="a" type="main">The rate distortion function of a Gaussian process with a weighted square error criterion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="506" to="508" />
			<date type="published" when="1968-05">May 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b459">
	<analytic>
		<title level="a" type="main">The rate distortion function for a class of sources</title>
	</analytic>
	<monogr>
		<title level="j">Inform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b460">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969-08">Aug. 1969</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="165" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b461">
	<analytic>
		<title level="a" type="main">Addendum to &apos;The rate distortion function of a Gaussian process with a weighted-square error criterion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="610" to="611" />
			<date type="published" when="1969-09">Sept. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b462">
	<analytic>
		<title level="a" type="main">Worst sources and robust codes for difference distortion measures</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="301" to="309" />
			<date type="published" when="1975-05">May 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b463">
	<analytic>
		<title level="a" type="main">A new, fast, and efficient image codec based on set partitioning in hierarchical trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. for Video Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b464">
	<monogr>
		<title level="m" type="main">Introduction to Data Compression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b465">
	<analytic>
		<title level="a" type="main">An algorithm for uniform vector quantizer design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="805" to="814" />
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b466">
	<analytic>
		<title level="a" type="main">Unbalanced nonbinary tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schmidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Asilomar Conf. on Signals, Systems, and Computers</title>
		<meeting>27th Asilomar Conf. on Signals, Systems, and Computers<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-11">Oct./Nov. 1993</date>
			<biblScope unit="page" from="1519" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b467">
	<analytic>
		<title level="a" type="main">Dither signals and their effects on quantization noise</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schuchman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="162" to="165" />
			<date type="published" when="1964-12">Dec. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b468">
	<analytic>
		<title level="a" type="main">On the quantization of finite dimensional messages</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Schutzenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b469">
	<analytic>
		<title level="a" type="main">Vector quantization for entropy coding of image subbands</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="526" to="532" />
			<date type="published" when="1992-10">Oct. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b470">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b471">
	<analytic>
		<title level="a" type="main">Coding theorems for a discrete source with a fidelity criterion</title>
	</analytic>
	<monogr>
		<title level="j">Conv. Rec</title>
		<editor>
			<persName><forename type="first">Nat</forename><surname>Ire</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="142" to="163" />
		</imprint>
	</monogr>
	<note>Pt. 4, 1959</note>
</biblStruct>

<biblStruct xml:id="b472">
	<analytic>
		<title level="a" type="main">Embedded image coding using zerotrees of wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3445" to="3462" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b473">
	<monogr>
		<title level="m" type="main">Topics in statistical quantization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Shaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965-05">May 1965</date>
			<biblScope unit="volume">7050</biblScope>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Syst. Theory Lab., Stanford Electron. Lab., Stanford Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b474">
	<analytic>
		<title level="a" type="main">On the calculation of the most probable values of frequency constants for data arranged according to equidistant divisions of a scale</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. London Math. Soc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="380" />
			<date type="published" when="1898">1898</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b475">
	<analytic>
		<title level="a" type="main">The distortion-rate function for nonergodic sources</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ledrappier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Probab</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="138" to="143" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b476">
	<analytic>
		<title level="a" type="main">Efficient bit allocation for an arbitrary set of quantizers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1445" to="1453" />
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b477">
	<analytic>
		<title level="a" type="main">On group transmission with frequency division of channels by the pulse-code modulation method</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Shtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elektrosvyaz</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="1959">1959. 1959</date>
		</imprint>
	</monogr>
	<note>Telecommun.</note>
</biblStruct>

<biblStruct xml:id="b478">
	<analytic>
		<title level="a" type="main">A class of binary signaling alphabets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="203" to="234" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b479">
	<analytic>
		<title level="a" type="main">On delta modulation</title>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2101" to="2136" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b480">
	<analytic>
		<title level="a" type="main">Instantaneous companding of quantized signals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="653" to="709" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b481">
	<analytic>
		<title level="a" type="main">An efficient nearest neighbor search method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Morgera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="677" to="679" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b482">
	<analytic>
		<title level="a" type="main">A fast MMSE encoding algorithm for vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="656" to="659" />
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b483">
	<analytic>
		<title level="a" type="main">A necessary and sufficient condition for quantization errors to be uniform and white</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sripad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="442" to="448" />
			<date type="published" when="1977-10">Oct. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b484">
	<analytic>
		<title level="a" type="main">Image coding using wavelet transforms and entropy-constrained trellis-coded quantization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcellin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="725" to="733" />
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b485">
	<analytic>
		<title level="a" type="main">Simulation of random processes and ratedistortion theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>VerdÃº</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b486">
	<analytic>
		<title level="a" type="main">Sur la division des corp materiels en parties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Acad. Polon. Sci., C1. III</title>
		<imprint>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="801" to="804" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b487">
	<analytic>
		<title level="a" type="main">The design of trellis waveform coders</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b488">
	<analytic>
		<title level="a" type="main">Optimum and adaptive differential pulse code modulation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Stroh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polytech. Inst. Brooklyn</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<pubPlace>Brooklyn, NY</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b489">
	<analytic>
		<title level="a" type="main">Uniform spherical coordinate quantization of spherically symmetric sources</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="518" to="521" />
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b490">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
		<title level="m">Quantization (Benchmark Papers in Electrical Engineering and Computer Science)</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b491">
	<analytic>
		<title level="a" type="main">Asymptotic performance of Dirichlet rotated polar quantizers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swaszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="537" to="540" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b492">
	<analytic>
		<title level="a" type="main">A vector quantizer for the Laplace source</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1355" to="1365" />
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b493">
	<analytic>
		<title level="a" type="main">Unrestricted multistage vector quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1169" to="1174" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b494">
	<analytic>
		<title level="a" type="main">Asymptotic performance of unrestricted polar quantizers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="330" to="333" />
			<date type="published" when="1986-03">Mar. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b495">
	<analytic>
		<title level="a" type="main">Optimal circularly symmetric quantizers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Franklin Inst. J</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b496">
	<analytic>
		<title level="a" type="main">Multidimensional spherical coordinates quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="570" to="576" />
			<date type="published" when="1983-07">July 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b497">
	<analytic>
		<title level="a" type="main">Design of quantizers from histograms</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="240" to="245" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b498">
	<analytic>
		<title level="a" type="main">Vector quantization of images using the competitive networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Attikiouzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crebbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Austrailian Conf. Neural Networks, ACNN&apos;91</title>
		<meeting>2nd Austrailian Conf. Neural Networks, ACNN&apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="258" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b499">
	<analytic>
		<title level="a" type="main">Two fast nearest neighbor searching algorithms for image vector quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1623" to="1628" />
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b500">
	<analytic>
		<title level="a" type="main">Evaluation of rate-distortion functions for a class of independent identically distributed sources under an absolute magnitude criterion</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="1975-01">Jan. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b501">
	<analytic>
		<title level="a" type="main">Simple &apos;neural&apos; optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b502">
	<analytic>
		<title level="a" type="main">Principal points and self-consistent points of elliptical distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tarpey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Flury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b503">
	<analytic>
		<title level="a" type="main">Optimal threshold and level selection for quantizing data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Titsworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JPL Space Programs Summary 37-23</title>
		<meeting><address><addrLine>Pasadena, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1963-10">Oct. 1963</date>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b504">
	<analytic>
		<title level="a" type="main">Asymptotic results for optimum equally spaced quantization of Gaussian data</title>
	</analytic>
	<monogr>
		<title level="m">JPL Space Programs Summary</title>
		<meeting><address><addrLine>Pasadena, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1964-10">Oct. 1964</date>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="242" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b505">
	<analytic>
		<title level="a" type="main">Roundoff error statistics for a continuous range of multiplier coefficients</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tokaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="1987-01">Jan. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b506">
	<analytic>
		<title level="a" type="main">An improvement on codebook search for vector quantization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huhuet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="208" to="210" />
			<date type="published" when="1994-04">Feb.-Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b507">
	<analytic>
		<title level="a" type="main">Reconstruction error in waveform transmission</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Totty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="336" to="338" />
			<date type="published" when="1967-04">Apr. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b508">
	<analytic>
		<title level="a" type="main">Optimal bit allocation algorithm for quantizing a random vector</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Trushkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Inform. Transm</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="156" to="161" />
			<date type="published" when="1981-09">July-Sept. 1981</date>
		</imprint>
	</monogr>
	<note>translated from Russian</note>
</biblStruct>

<biblStruct xml:id="b509">
	<analytic>
		<title level="a" type="main">Sufficient conditions for uniqueness of a locally optimal quantizer for a class of convex error weighting functions</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b510">
	<analytic>
		<title level="a" type="main">Stack-run image coding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Villasenor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="519" to="521" />
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b511">
	<analytic>
		<title level="a" type="main">Channel coding with multilevel/phase signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ungerboeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1982-01">Jan. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b512">
	<analytic>
		<title level="a" type="main">Trellis-coded modulation with redundant signal sets, Parts I and II</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5" to="21" />
			<date type="published" when="1987-02">Feb. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b513">
	<analytic>
		<title level="a" type="main">Simulated annealing and codebook design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vaisey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
			<biblScope unit="page" from="1176" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b514">
	<analytic>
		<title level="a" type="main">Design of multiple description scalar quantizers</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="821" to="824" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b515">
	<analytic>
		<title level="a" type="main">Design of entropy-constrained multiple-description scalar quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="245" to="250" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b516">
	<analytic>
		<title level="a" type="main">Asymptotic analysis of multiple description quantizers</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vaishampayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Batllo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="278" to="284" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b517">
	<analytic>
		<title level="a" type="main">Quantization noise of a single integration delta modulation system with an N-digit code</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van De Weg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phillips Res. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="568" to="569" />
			<date type="published" when="1971-08">Aug. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b518">
	<analytic>
		<title level="a" type="main">Dither in digital audio</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderkooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lipshitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Audio Eng. Soc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="966" to="975" />
			<date type="published" when="1987-12">Dec. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b519">
	<analytic>
		<title level="a" type="main">Resolution below the least significant bit in digital systems with dither</title>
	</analytic>
	<monogr>
		<title level="j">J. Audio Eng. Soc</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">889</biblScope>
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b520">
	<analytic>
		<title level="a" type="main">Construction and evaluation of Trellis-coded quantizers for memoryless sources</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Der Vleuten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="853" to="859" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b521">
	<analytic>
		<title level="a" type="main">Multi-dimensional sub-band coding: Some theory and algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b522">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KovaceviÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wavelets</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Subband</forename><surname>Coding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b523">
	<analytic>
		<title level="a" type="main">An algorithm for finding nearest neighbors in (approximately) constant average time complexity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b524">
	<analytic>
		<title level="a" type="main">Efficient algorithm for hierarchical compression of video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing<address><addrLine>Austin, TX; Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comp. Soc. Press</publisher>
			<date type="published" when="1994-11">Nov. 1994. 1994</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="275" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b525">
	<analytic>
		<title level="a" type="main">Trellis encoding of memoryless discretetime sources with a fidelity criterion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Omura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="325" to="332" />
			<date type="published" when="1974-05">May 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b526">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vitushkin</surname></persName>
		</author>
		<title level="m">Translation by R. Feinstein of Otsenka Slozhnosti Zadachi Tabulirovaniya</title>
		<meeting><address><addrLine>New York; Moscow, USSR</addrLine></address></meeting>
		<imprint>
			<publisher>Fizmatgiz</publisher>
			<date type="published" when="1959">1961. 1959</date>
		</imprint>
	</monogr>
	<note>Theory of the Transmission and Processing of Information</note>
</biblStruct>

<biblStruct xml:id="b527">
	<analytic>
		<title level="a" type="main">Optimal causal coding-decoding problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Walrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="814" to="820" />
			<date type="published" when="1983-11">Nov. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b528">
	<analytic>
		<title level="a" type="main">Trellis coded vector quantization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b529">
	<analytic>
		<title level="a" type="main">Codebook organization to enhance maximum a posteriori detection of progressive transmission of vector quantized images over noisy channels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Riskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b530">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963-03">Mar. 1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b531">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
		<title level="m">Statistics on Spheres</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b532">
	<analytic>
		<title level="a" type="main">An optimal bit allocation algorithm for sub-band coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Westerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Boekee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="757" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b533">
	<analytic>
		<title level="a" type="main">Subband coding of images using vector quantization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Westerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Boekee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="713" to="719" />
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b534">
	<analytic>
		<title level="a" type="main">A study of rough amplitude quantization by means of Nyquist sampling theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Circuit Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="266" to="276" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b535">
	<analytic>
		<title level="a" type="main">Statistical analysis of amplitude quantized sampled data systems</title>
	</analytic>
	<monogr>
		<title level="j">Trans. AIEE, Pt. II: Appl. Ind</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="555" to="568" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b536">
	<analytic>
		<title level="a" type="main">Magnitude/phase quantization of independent Gaussian variates</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1924" to="1929" />
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b537">
	<analytic>
		<title level="a" type="main">Trellis encoding of continuous-amplitude memoryless sources</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Lytle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b538">
	<analytic>
		<title level="a" type="main">Fast search methods for vector lookup in vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2311" to="2312" />
			<date type="published" when="1992-12">Dec. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b539">
	<analytic>
		<title level="a" type="main">Transform picture coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Wintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1972-07">July 1972</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="809" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b540">
	<analytic>
		<title level="a" type="main">On the structure of real-time source coders</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Witsenhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1437" to="1451" />
			<date type="published" when="1979-08">Jul./Aug. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b541">
	<analytic>
		<title level="a" type="main">Indirect rate-distortion problems</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="518" to="521" />
			<date type="published" when="1980-09">Sept. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b542">
	<analytic>
		<title level="a" type="main">Source coding for multiple descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1417" to="1426" />
			<date type="published" when="1980-10">Oct. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b543">
	<analytic>
		<title level="a" type="main">Transmission of noisy information to a noisy receiver with minimum distortion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="406" to="411" />
			<date type="published" when="1970-07">July 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b544">
	<analytic>
		<title level="a" type="main">An 800 bit/s vector quantization LPC vocoder</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="770" to="779" />
			<date type="published" when="1982-10">Oct. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b545">
	<analytic>
		<title level="a" type="main">On optimal quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="248" to="252" />
			<date type="published" when="1969-03">Mar. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b546">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Subband Image Coding</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b547">
	<analytic>
		<title level="a" type="main">Subband coding of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1278" to="1288" />
			<date type="published" when="1986-10">Oct. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b548">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Wright</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>unpublished work</note>
</biblStruct>

<biblStruct xml:id="b549">
	<analytic>
		<title level="a" type="main">Index allocation in vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1318" to="1320" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b550">
	<analytic>
		<title level="a" type="main">On the design of connectionist vector quantizers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fallside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Speech Language</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="207" to="229" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b551">
	<analytic>
		<title level="a" type="main">Source coding and vector quantization with codebook-excited neural networks</title>
	</analytic>
	<monogr>
		<title level="j">Comp. Speech Language</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="43" to="276" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b552">
	<analytic>
		<title level="a" type="main">Globally optimum bit allocation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<meeting>Data Compression Conf<address><addrLine>Snowbird, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b553">
	<analytic>
		<title level="a" type="main">Acceleration of the LBG algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1518" to="1523" />
			<date type="published" when="1994-04">Feb.-Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b554">
	<analytic>
		<title level="a" type="main">Communication of analog data from a Gaussian source over a noisy channel</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="801" to="812" />
			<date type="published" when="1968-06">May/June 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b555">
	<analytic>
		<title level="a" type="main">Recent results in the Shannon theory</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b556">
	<analytic>
		<title level="a" type="main">Bounds on the rate-distortion function for stationary sources with memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="508" to="513" />
			<date type="published" when="1971-09">Sept. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b557">
	<analytic>
		<title level="a" type="main">Vector quantization of video signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. IECE</title>
		<meeting>Annu. Conf. IECE</meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page">1031</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b558">
	<analytic>
		<title level="a" type="main">Vector quantizer design for video signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IECE Trans</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="965" to="972" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b559">
	<monogr>
		<title level="m" type="main">Recursive vector quantization for monochrome video signals</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b560">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieice Trans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-02">Feb. 1991</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="399" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b561">
	<analytic>
		<title level="a" type="main">Asymptotic performance of block quantizers with a difference distortion measure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b562">
	<analytic>
		<title level="a" type="main">Optimum fixed-length binary code</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Res. Lab. Electron., Quart. Progr. Rep</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="231" to="233" />
			<date type="published" when="1965-07-15">July 15, 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b563">
	<analytic>
		<title level="a" type="main">Optimum binary code</title>
	</analytic>
	<monogr>
		<title level="j">MIT Res. Lab. Electron., Quart. Progr. Rep</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="214" to="217" />
			<date type="published" when="1965-07-25">July 25, 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b564">
	<analytic>
		<title level="a" type="main">Source coding theory for cascade and branching communication systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="1981-05">May 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b565">
	<analytic>
		<title level="a" type="main">Fixed-slope universal lossy data compression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1465" to="1476" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b566">
	<analytic>
		<title level="a" type="main">Some comments on the generalized Shannon lower bound for stationary finite-alphabet sources with memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="815" to="817" />
			<date type="published" when="1973-11">Nov. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b567">
	<analytic>
		<title level="a" type="main">Absolute error rate-distortion functions for sources with constrained magnitudes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="499" to="503" />
			<date type="published" when="1978-07">July 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b568">
	<monogr>
		<title level="m" type="main">Development and evaluation of procedures for quantizing multivariate distributions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Zador</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
		<respStmt>
			<orgName>Stanford Univ ; Stanford Univ. Dept. Statist. Tech. Rep</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b569">
	<analytic>
		<title level="a" type="main">Topics in the asymptotic quantization of continuous random variables</title>
	</analytic>
	<monogr>
		<title level="j">Bell Lab. Tech. Memo</title>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b570">
	<analytic>
		<title level="a" type="main">Asymptotic quantization error of continuous signals and the quantization dimension</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
	<note>revised version of</note>
</biblStruct>

<biblStruct xml:id="b571">
	<analytic>
		<title level="a" type="main">On lattice quantization noise</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1152" to="1159" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b572">
	<analytic>
		<title level="a" type="main">Information rates of pre/post-filtered dithered quantizers</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1340" to="1353" />
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b573">
	<analytic>
		<title level="a" type="main">Universal source coding with codebook transmission</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commum</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="336" to="346" />
			<date type="published" when="1994-02">Feb. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b574">
	<analytic>
		<title level="a" type="main">A stochastic relaxation algorithm for improved vector quantiser design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="896" to="898" />
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b575">
	<analytic>
		<title level="a" type="main">Pseudo-Gray coding</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2147" to="2156" />
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b576">
	<analytic>
		<title level="a" type="main">Average number of facets per cell in tree-structured vector quantizer partitions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kantorovitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1053" to="1055" />
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b577">
	<analytic>
		<title level="a" type="main">Asymptotic bounds on optimal noisy channel quantization via random coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manzella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1926" to="1938" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b578">
	<analytic>
		<title level="a" type="main">Globally optimal vector quantizer design by stochastic relaxation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vaisey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="310" to="322" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b579">
	<analytic>
		<title level="a" type="main">A comparison between delta and pulse code modulation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zetterberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ericsson Technics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="154" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b580">
	<analytic>
		<title level="a" type="main">New results in binary multiple descriptions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="502" to="521" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b581">
	<analytic>
		<title level="a" type="main">An on-line universal lossy data compression algorithm via continuous codebook refinement. I. Basic results</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="803" to="821" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b582">
	<analytic>
		<title level="a" type="main">An on-line universal lossy data compression algorithm via continuous codebook refinement. II. Optimality for phimixing source models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="822" to="836" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b583">
	<analytic>
		<title level="a" type="main">The redundancy of source coding with a fidelity criterion-Part One: Known statistics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="71" to="91" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b584">
	<analytic>
		<title level="a" type="main">Coding sources with unknown statistics-Part II: Distortion relative to a fidelity criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="389" to="394" />
			<date type="published" when="1972-05">May 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b585">
	<analytic>
		<title level="a" type="main">Universal quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="344" to="347" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b586">
	<analytic>
		<title level="a" type="main">Quantizataion with minimal entropy</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Koshelev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Pered. Inform</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="151" to="156" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b587">
	<analytic>
		<title level="a" type="main">About fixed rate lattice coding of sources with difference fidelity criterion</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Babkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">M</forename><surname>Shtarkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Redundancy in Inform Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10" to="30" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>Voprosi Kibernetikia</note>
</biblStruct>

<biblStruct xml:id="b588">
	<analytic>
		<title level="a" type="main">About coding of sequence of independent continuously distributed random values after quantizing</title>
	</analytic>
	<monogr>
		<title level="j">Probl. Redundancy in Comp. Networks</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="132" to="137" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note>Voprosi Kibernetikia</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
