<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video fingerprinting for copy identification: from research to industry applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Inc</forename><surname>Vobile</surname></persName>
						</author>
						<title level="a" type="main">Video fingerprinting for copy identification: from research to industry applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62B6D66C4E28A71C3ADE30FD22411F8A</idno>
					<idno type="DOI">10.1117/12.805709</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video fingerprinting</term>
					<term>video copy detection</term>
					<term>video sharing</term>
					<term>UGC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research that began a decade ago in video copy detection has developed into a technology known as "video fingerprinting". Today, video fingerprinting is an essential and enabling tool adopted by the industry for video content identification and management in online video distribution. This paper provides a comprehensive review of video fingerprinting technology and its applications in identifying, tracking, and managing copyrighted content on the Internet. The review includes a survey on video fingerprinting algorithms and some fundamental design considerations, such as robustness, discriminability, and compactness. It also discusses fingerprint matching algorithms, including complexity analysis, and approximation and optimization for fast fingerprint matching. On the application side, it provides an overview of a number of industry-driven applications that rely on video fingerprinting. Examples are given based on real-world systems and workflows to demonstrate applications in detecting and managing copyrighted content, and in monitoring and tracking video distribution on the Internet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In February 1999, a few graduate students at Stanford University wrote a technical report entitled, "Finding pirated video sequences on the Internet". <ref type="bibr" target="#b0">1</ref> The work contained in that technical report eventually became part of two distinguished Ph.D. dissertations by Shivakumar 2 and Indyk <ref type="bibr" target="#b2">3</ref> , respectively. And the technique that was used for identifying pirated videos developed into a technology known as "video fingerprinting".</p><p>To be sure, Shivakumar and Indyk's work was not the only one on video fingerprinting and copy identification. There were quite a number of related publications by other researchers around the same time in late 1990s and early 2000s. <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> However, Shivakumar and Indyk's work was particularly notable in two respects. First, it was the first to use video fingerprinting in identifying unauthorized copyrighted videos on the Internet. Years later, such application became the driving force in the development and deployment of video fingerprinting technology in the era of YouTube. Secondly, it was the first to use Locality Sensitive Hashing (LSH) in fingerprint matching. To date, the LSH algorithm remains the state-of-the-art in similarity search in high dimensions; new techniques for fingerprint matching are often compared to LSH.</p><p>Research and development activities in video fingerprinting subsided for a period of time after the burst of the first Internet bubble in early 2000s. In the last few years, however, the proliferation of online video in the peer-to-peer (P2P) and user-generated-content (UGC) networks has brought renewed interest in video fingerprinting technology for solving the copyright violation problems. At issue is unauthorized distribution of copyrighted video content in the P2P and UGC networks. In recent high-profile legal cases such as MGM v. Grokster and Viacom v. YouTube, the plaintiffs argued that the sites that host video content or a search index of video content should proactively police their sites by identifying and filtering out copyright-infringing content. The enabling technology recommended by the plaintiffs for identification and filtering of copyright-infringing content at large-scale is fingerprinting.</p><p>In late 2006, the Motion Picture Association of America (MPAA) and the Motion Picture Laboratories (MovieLabs) initiated a Content Recognition Systems Study that focused specifically on evaluating fingerprinting technologies for video identification. The Study lasted more than 6 months and was joined by 12 participants including large corporations, start-up companies, and a university. Today, all of major Hollywood film and TV studios have adopted video fingerprinting technology. In practical applications, video fingerprinting is used in identifying, monitoring, tracking, as well as filtering of unauthorized, illegal, or offensive content. Researchers and practitioners are also exploring other applications of video fingerprinting, such as video asset management, contextual advertising, and content-based video search.</p><p>Despite the resurgence in research activities in video fingerprinting and its adoption by the industry, there has not been a comprehensive review about video fingerprinting technology and its applications. This paper attempts to fill that gap by providing a review of both research work and real-world applications. Before getting down to the details, it is helpful to review and clarify on related terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Terminology</head><p>A video fingerprint is an identifier that is extracted from a piece of video content. The process of extracting a fingerprint from the video content is referred to as fingerprinting the video or video fingerprinting. There is an obvious analogy to human fingerprint and fingerprinting. Just like human fingerprint that can uniquely identify a human being, video fingerprint can uniquely identify a piece of video content. The analogy extends to the process of subject identification by fingerprint: first, known fingerprints must be stored in a database; then, a subject's fingerprint is queried against the database for match.</p><p>In a broad sense, the term video fingerprinting has been used to refer to the technology encompassing algorithms, systems, and workflows that use video fingerprint for video identification. It should be evident from the context if the term is used to refer to the fingerprinting process or more broadly the technology.</p><p>In research literature, video fingerprinting and fingerprint-based video identification are also commonly known as video copy detection or more generally content-based copy detection (CBCD). Indeed, copy detection is the application that motivated development of video fingerprinting. Here, "copy" has a quite broad meaning. It could be a small segment cut from the original, lasting only a few seconds, and possibly embedded in a long edit or "mash-up". It could be transformed into different formats, codecs, resolutions, frame rates, and bitrates. And it could be modified and distorted by scaling, cropping, frame dropping, and overlay of text and graphics.</p><p>Another term that is used to describe video fingerprinting is robust video hashing. It comes from the observation that conventional cryptographic hashing such as MD5 is fragile and sensitive to even a single bit change in the content. The idea is to design hashing schemes that are robust to distortions that do not change our perception of the video content. For this reason, it is sometimes also called perceptual hashing. However, the use of "hashing" can be confusing because of the additional security requirements that are often imposed on hash functions. For example, one desired property for a hash function is uniform distribution of hash values in order to minimize collisions, the incident that has two different entities hashed into the same point in the hash space. Yet for video fingerprinting, it can be ideal to have different versions (can be infinite in number) of the same video content hashed into the same point in the fingerprint space. In such observation, "robust hashing" sounds like a self-conflicting proposition.</p><p>Lastly, it is worth noting that the word fingerprinting has also been used in the research literature of watermarking to describe the process of adding an identifier (watermark) to the content. To date, the industry has an unambiguous view of what it calls watermarking and fingerprinting. When an identifier or signature is added to the content and thereby changing the content, it is watermarking; when an identifier or signature is extracted from the content without changing the content, it is fingerprinting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization of Sections</head><p>The rest of this paper is organized as follows. Section 2 reviews research work on video fingerprinting and fingerprint matching algorithms and designs. First, a set of desired properties and common metrics for fingerprinting algorithms are introduced. Then, fingerprinting algorithms are grouped into several categories and reviewed based on their use of spatial, temporal, color, and transform-domain signatures. For fingerprint matching, a complexity analysis is given for exhaustive fingerprint search; general strategies for reducing complexities are discussed. A few existing and new algorithms for fast approximate fingerprint matching are reviewed. The last part of Section 2 contains the author's observations and remarks on the video fingerprinting research. Section 3 provides an overview of a few industry-driven applications that rely on video fingerprinting. Examples are given based on real-world systems and workflows to demonstrate applications in identifying and managing copyrighted content, and in monitoring and tracking video distribution on the Internet. Finally, a few promising new applications are previewed. Section 4 concludes the paper with a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RESEARCH IN VIDEO FINGERPRINTING</head><p>This section reviews research work on video fingerprinting, including algorithms and designs for video fingerprinting and fingerprint matching. Before considering specific algorithms and designs, it is helpful to examine what we aim to achieve with video fingerprinting and how to measure the effectiveness of designs and implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Properties and Metrics</head><p>Ideally, a design of a video fingerprint should have the following characteristics that hold true for a large corpus of video content of diverse types.</p><p>Robust. A video fingerprint should stay largely invariant for the same video content under various types of processing, transformations and manipulations, such as format conversion, transcoding, and content editing.</p><p>Discriminating. The video fingerprints for different video content should be distinctly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compact.</head><p>A video fingerprint should be minuscule in data size, comparing to the data size of the original video content.</p><p>Low complexity. The algorithm for extracting video fingerprints should have low computational complexity so that a video fingerprint can be computed fast.</p><p>Efficient for matching and search. Although there are generic algorithms that treat all fingerprints as a string of bits in matching and search, a good design of video fingerprint should facilitate approximation and optimization to improve the efficiency in matching and search.</p><p>Because video fingerprints are generally not perfectly identical for different versions of the same content, fingerprint matching is not a simple table lookup in the database. Instead, it is a similarity search problem. Typically, a distance metric is defined to quantify the similarity between two video fingerprints being compared. Commonly used distance metrics include Manhattan (L1) distance and Euclidean (L2) distance, where a normalized L1 or L2 distance provides a good measure of similarity. When a video fingerprint consists of binary signatures, Hamming distance is often used, and a normalized Hamming distance or Bit Error Rate (BER) provides a good measure of similarity.</p><p>In most applications of video identification, having a quantified measure of similarity is not sufficient. An explicit judgment of whether two videos contain the same content is required. Thus, the effectiveness of a video fingerprint design and implementation can be measured by the rate of correct returns to fingerprint queries. A pair of commonly used measures is precision and recall rates that are defined as follows:</p><formula xml:id="formula_0">P r (%) = N tp N p 100<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">R e (%) = N tp N ep 100<label>(2)</label></formula><p>where P r is precision rate, R e is recall rate, N tp is number of true positives or correct matches, N p is total number of positives or matches, and N ep is number of expected positives or matches.</p><p>Corresponding to the desired properties of video fingerprint, precision rate is a measure of discriminability, and recall rate is a measure of robustness. Another pair of related measures is false positive (FP) and false negative (FN) rates that are defined as follows:</p><formula xml:id="formula_2">R fp (%) = N fp N en 100<label>(3)</label></formula><p>and</p><formula xml:id="formula_3">R fn (%) = N fn N ep 100 (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where R fp and R fn are false positive and false negative rates, respectively; N fp is number of false positives, N fn is number of false negatives, N ep is number of expected positives as previously defined, and N en is number of expected negatives. N ep and N en add up to the total number of fingerprint queries in the test, denoted by N q .</p><p>The above definitions for false positive and false negative rates assume the knowledge of expected positive and negative numbers, N ep and N en . This is usually true for controlled tests. For a real-world running system that receives a large volume of queries, it is hard to know or verify the expected positive and negative numbers. Therefore, the operating R fp and R fn of a fingerprinting system are often computed by replacing N en in (3) and N ep in (4) with N q , the total number of fingerprint queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Fingerprinting Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Overview of Video Signatures</head><p>In its raw form, a video fingerprint is just a string of bits that represent the "signatures" of the video data. Different designs vary in the type of signatures that are chosen to characterize the video data, and the way to compute them. Almost all of the video signatures that have been proposed to date can be classified into four types: spatial, temporal, color, and transform-domain. In some designs, different types of signatures are combined to form video fingerprints.</p><p>A spatial signature characterizes spatial features of a video frame and is computed independent of other frames.</p><p>Examples of spatial features include luminance patterns, differential luminance or gradient patterns, and edges. A temporal signature describes temporal features of a video and is computed between two frames in the temporal direction.</p><p>Examples of temporal features include frame difference measures, motion vector patterns, and shot durations. A color signature captures color characteristics of a video frame and is computed in a color space such as RGB or YUV. Many color signatures are an abstraction of patterns in the color histogram. A transform-domain signature is computed from coefficients of an image or video transform such as a DCT or wavelet transform. Transform-domain signatures provide a different characterization and representation of some spatial and/or temporal features in the transform domain.</p><p>In addition to various types of video signatures, video fingerprints differ in granularity that is the smallest unit of video that a video signature characterizes. Spatial granularity can vary from entire video frame to subdivided blocks to points of interest in a frame. Temporal granularity can be key frames only, group of frames, downsampled single frames, or every single frame.</p><p>Different granularities of video fingerprint provide a tradeoff between discriminability, robustness, and compactness. For example, by dividing a video frame into multiple blocks and computing temporal and color signatures for each block, we gain finer spatial granularity or resolution in temporal and color signatures at the cost of additional storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Spatial Signatures</head><p>A class of spatial signatures is designed to characterize luminance patterns in a video frame. In such designs, a video image is first converted to the YUV color space; the luminance (Y) component is kept, and the chrominance components (U, V) are discarded. The luminance image is further subdivided into a fixed-sized grid of blocks (e.g., a 4x4 grid of blocks) independent of frame resolutions, as shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>Note that unlike in image and video compression where a frame subdivision is by fixed-sized blocks (e.g., 8x8 blocks), here the frame subdivision is designed to produce a fixed-sized grid of blocks. The subdivision of a video frame serves two purposes. First, it leads up to block-based signatures that are robust to changes in pixel values; second, it produces a compact and fixed-sized frame fingerprint consisting of fixed number of block signatures.</p><p>One popular block-based luminance signature is based on ordinal ranking. It was designed by Bhat and Nayar <ref type="bibr" target="#b6">7</ref> for image identification and first used by <ref type="bibr">Mohan 8</ref> in video fingerprinting and matching. The simplicity of ordinal ranking is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(b)-(c). After frame subdivision, the average pixel value for each block is computed, and an abstraction follows by ranking the blocks by their average pixel values. The rank of each block in ordinal position is assigned to the block as its signature. Video fingerprints based on ordinal signatures have been studied and experimented extensively. <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> They were found to be more robust than some temporal and color signatures. <ref type="bibr" target="#b8">9</ref> They are also compact in size: for a frame subdivision containing M blocks, the required number of bits for a frame fingerprint is M * ceiling(log 2 M). One drawback of ordinal signatures has to do with the global ranking. The rank of each block is relative to all other blocks in the frame. This means local luminance variations such as a logo insertion that should change the rank of one block could actually change the ranks of multiple blocks or all blocks. Block-based differential luminance signatures such as those proposed <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> are more robust to local luminance variations while maintaining a compact representation similar in data size to the ordinal signatures. Oostveen et al <ref type="bibr" target="#b11">12</ref> computed block difference in one spatial direction (horizontal) followed by a binary abstraction (greater than or not). Lee and Yoo <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> computed luminance gradients in each block and condensed them to the centroid (geometric average) of gradient orientations. Iwamoto et al <ref type="bibr" target="#b14">15</ref>  Block-based spatial signatures such as ordinal and differential signatures are susceptible to geometric transformations such as rotation, cropping, and scaling that changes aspect ratios. Figure <ref type="figure" target="#fig_2">2</ref> illustrates the mismatch of content in blocks between two transformed and the original frame images after rotation and cropping. This difficulty has motivated designs of spatial signatures that are resilient to geometric transformations. Many proposed algorithms employ a special image transform, such as polar Fourier Transform, <ref type="bibr" target="#b15">16</ref> Radon Transform, <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b21">22</ref> or Singular Value Decomposition (SVD). <ref type="bibr" target="#b18">19</ref> They have reportedly good resilience to affine transformations such as shift and rotation. However, they are still prone to cropping; see, e.g., Seo et al <ref type="bibr" target="#b16">17</ref> for some experimental results. Unfortunately, in practical applications involving videos, cropping often accompanies shift, rotation, and scaling due to fixed video frame size, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Additionally, most of the above techniques have high computational complexities that make them impractical for many applications of video fingerprinting in the real world.</p><p>Another approach that is fundamentally different from block-based designs is to compute spatial signatures around points of interest in a video frame. This approach is often combined with the use of key frames on which points of interest are computed. Examples of spatial signatures that are based on points of interest include those that use Harris points, <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> scale-invariant feature points, <ref type="bibr" target="#b22">23</ref> and the Difference-of-Gaussian scale-space feature points. <ref type="bibr" target="#b23">24</ref> Unlike blockbased designs, spatial signatures based on points of interest lead to frame fingerprints of variable sizes, because the number of points of interest in a frame is content-dependent, and can be potentially very large. The variable number and configuration of points of interest in a frame necessitate an alternative way for similarity definition and search. The computational complexity of the above methods for extracting spatial signatures based on points of interest is significantly higher than that of block-based signatures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Temporal Signatures</head><p>The groundbreaking work by Shivakumar 1-2 used temporal signatures. The design is quite straightforward. First, a video sequence is segmented into shots. Then, the duration of each shot is taken as a temporal signature, and the sequence of concatenated shot durations form the fingerprint of the video.</p><p>Related to shots are key frames where the content has abrupt changes, such as the boundary that separates two shots. Key frames are important anchors in a video sequence that are often used in video fingerprinting. <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> The locations of key frames in a video provide a natural temporal signature. However, not all designs make use of the temporal positions of key frames. For example, in Cheung and Zakhor's work, <ref type="bibr" target="#b25">26</ref> the temporal information of key frames is discarded; the similarity between two video sequences is measured by the degree of match to a group of key frames. In such design, it is not possible to determine the location or offset of a match in the query and reference videos.</p><p>More commonly, temporal signatures are computed on adjacent frames in a video. Chen and Stentiford <ref type="bibr" target="#b26">27</ref> proposed to use temporal ordinal signatures. Similar to the steps of deriving spatial ordinal signatures, a frame is subdivided into a fixed-sized grid of blocks, and the average pixel value of each block is computed. Unlike spatial ordinal ranking that ranks blocks in a frame, temporal ordinal ranking in Chen and Stentiford 27 ranks blocks in same spatial position across the frames in a temporal window; the temporal ordinal ranks of the blocks are used as temporal signatures.</p><p>With an approach similar to the way of computing spatial differential signatures, temporal differential signatures are explored. In such an approach, <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b27">28</ref> luminance difference between two adjacent frames or same-positioned blocks in two adjacent frames is computed, followed by an abstraction that quantizes the difference to 2 or 3 levels (i.e., greater, equal or less). Hampapur et al 9 estimated block motion vectors and quantized them to four orientations to form motion signatures. With a different approach to motion estimation based on tracking points of interest from frame to frame, Law-To et al <ref type="bibr" target="#b20">21</ref> computed temporal trajectories of points of interest and abstracted their motion signatures by labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Color Signatures</head><p>Color signatures are among the first being used in video fingerprinting. Naphade el al <ref type="bibr" target="#b4">5</ref> proposed a technique that was also experimented by Hampapur el al. <ref type="bibr" target="#b8">9</ref> In this approach, a level-quantized histogram is computed for Y, U, and V components for each video frame. To reduce the resulting signature data, a polynomial approximation is used to model the pixel counts in each bin of the histograms along the temporal direction. A special distance metric based on histogram intersection is used as a similarity measure. Li et al <ref type="bibr" target="#b28">29</ref> applied ordinal ranking to the bins of histogram based on the frequency counts of each bin. Hu <ref type="bibr" target="#b29">30</ref> computed Alpha-trimmed average histogram on a group of subdivided frames and quantized each color component to eight bins. In the latter two designs, the color signatures can be compared with common distance measures such as the Manhattan distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Transform-Domain Signatures</head><p>There are some designs that compute video signatures in a transform domain. In many cases, the choice of an image (2D) or video (3D) transform is motivated by some invariance properties of the transform. For example, Swaminathan et al <ref type="bibr" target="#b15">16</ref> used polar Fourier Transform to compute an image hash that is resilient to rotation and translation. By a similar motivation, Radon Transform <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b21">22</ref> and Singular Value Decomposition <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31</ref> were explored to generate transform-domain signatures that are robust to geometric distortions.</p><p>In another study, Coskun and Sankur 32 used 3D Discrete Cosine Transform (DCT) on a group of 64 frames and hashed the resulting coefficients into binary signatures with median-based thresholding. Their work was later extended <ref type="bibr" target="#b32">33</ref> to also use a 3D Random Bases Transform (RBT) followed by hashing. So far, the DCT and RBT based signatures have been tested with a small set of short video clips. <ref type="bibr" target="#b32">33</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fingerprint Matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Exhaustive Search</head><p>As introduced in Section 2.1, fingerprint matching is a similarity search problem where the degree of similarity is quantified by certain distance metric. More specifically, for a given video fingerprint of a target video clip, fingerprint matching amounts to finding in a reference video fingerprint database the closest match or matches to the target video fingerprint. In some applications, it suffices to return a list of match candidates ranked by their similarity scores or distance values; in many other applications, however, an explicit judgment of match or no-match has to be made. In either case, the heavy load of computation lies in computing the distance values or similarity scores between the target video fingerprint and reference video fingerprints in the database. In a brute-force approach, this comes down to computing and comparing the distance from the target video fingerprint to each and every reference video fingerprint in the reference database, and finding the one(s) that have the shortest distance. Because a target video usually has different length than the ones in the reference database, and can match to any part of a reference video, comparing the target video fingerprint with each reference video fingerprint involves evaluating every offset position for the best alignment between the two video fingerprints that gives the smallest distance. To sum up, the time complexity of a brute-force fingerprint matching by exhaustive search can be expressed as follows:</p><formula xml:id="formula_5">Time complexity = O(k*N) (5) = O(k*l*M) (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where k is the length of the target video fingerprint, N is the total length of video fingerprints in the reference database, l is the average length of the video fingerprints in the reference database, and M is the total number of video fingerprints in the reference database; N = l*M.</p><p>Considering that the length of a given video is finite and not growing, the dominating factor of complexity in fingerprint matching is clearly the reference database size which can be represented either by the total length of reference video fingerprints, N, or the total number of reference video fingerprints, M. More specifically, the time complexity of fingerprint matching by exhaustive search is linear of the reference database size. Since in practice the reference database size can be very large and continues to grow, fingerprint matching by exhaustive search is clearly not scalable for practical applications. Fortunately, exhaustive search is rarely necessary in practice. In most cases, a well-designed approximate and fast search can find the same best match as an exhaustive search does in a tiny fraction of time required for the exhaustive search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Approximate and Fast Search</head><p>A well-known algorithm for approximate similarity search is called Locality Sensitive Hashing (LSH). It was first introduced by Indyk and Motwani <ref type="bibr" target="#b33">34</ref> and refined by Gionis el al. <ref type="bibr" target="#b34">35</ref> Although LSH has been widely used in many applications that involve similarity search, video fingerprint matching was among the first applications in which LSH was used. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref> Since then, other researchers have explored LSH in fingerprint matching along with their video fingerprinting algorithms. <ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36</ref> The LSH algorithm was conceived for solving the approximate Nearest Neighbor Search (NNS) problem in high dimensions; fingerprint matching can be formulated as an NNS problem. Consider a d-dimensional vector space P where a distance metric D(x,y) is defined. For a query point q, if the nearest point p exists in P such that D(p,q) = r, the socalled -Nearest Neighbor Search ( -NNS) seeks to find a point p´ in P such that D(p´,q) r (1+ ) for any &gt; 0. For P containing N points, it was shown <ref type="bibr" target="#b34">35</ref> that with LSH the approximate nearest neighbor p´ can be found with high probability in sublinear time of N, more specifically,</p><formula xml:id="formula_7">Time complexity = O( d*N 1/(1+ ) )<label>(7)</label></formula><p>Behind the mathematical rigor of LSH is an intuitive geometric reasoning: if two points in a d-dimensional space are close to each other, then, their projections onto lower dimensional spaces are very likely to be close as well. An important part of LSH design is to devise hash functions such that the points that are close to the query point will be hashed into the same bucket with high probability. Then, a linear, exhaustive search may be used to find the closest point(s) to the query point in the bucket that often contains a much smaller number of candidates than the original search space. In Gionis et al <ref type="bibr" target="#b34">35</ref> , the hash functions being chosen are random projections from high dimensions to a lower dimensional space. Recently, Baluja and Covell 37 took a different approach to hashing for reducing the search space. Instead of designing deterministic hash functions, they used machine learning techniques and training data to devise a hashing system that adapts to the identification task and data. This results in a more compact hash bucket that contains significantly fewer candidates that may need to be compared with a linear search, thus boosting the speed of fingerprint matching. So far, this "learning to hash" technique has been applied to audio fingerprint matching with excellent results; <ref type="bibr" target="#b36">37</ref> it would be interesting to see how it works for video fingerprint matching.</p><p>One of the benefits of using training data for machines to learn to hash is to forgo an explicit definition of similarity that can be hard to define precisely for video content and its fingerprints. This is in contrast to LSH in metric space where a distance metric measuring similarity needs to be defined explicitly. In another approach that does without using distance-based similarity measures and deterministic hash functions, Joly et al <ref type="bibr" target="#b37">38</ref> proposed to use a statistical similarity search based on probabilistic models of common distortion vectors. Similar to hashing, a statistical similarity search maps the full search space into a small bucket of candidates by probabilistic filtering. In Joly et al, <ref type="bibr" target="#b37">38</ref> probabilistic models for common distortion vectors associated with Harris spatial signatures <ref type="bibr" target="#b19">20</ref> were used and a substantial speed-up in fingerprint query was achieved over exhaustive search with little loss in accuracy.</p><p>From optimization point of view, a system for fingerprint matching can be divided into two parts; each part can employ some kind of approximation that trades possibly a little loss in accuracy for speed in a fingerprint query. The first part of approximation is to reduce the search space. More specifically, for a fingerprint query to a reference database containing M fingerprint records, we seek to map the reference database to a bucket of size B that is smaller than M. A refined search including possibly exhaustive search may be performed in the resulting bucket for the query fingerprint. Ideally, we would like the bucket to contain only the most likely candidates for match, and the mapping from the full search space to the bucket to be super fast. Besides the hashing and probabilistic mapping methods that have been reviewed above, some heuristic techniques can be also very effective for reducing search space. For example, Oostveen el al <ref type="bibr" target="#b11">12</ref> attempted to reduce search space by selecting only reference candidates that contain identical anchor fingerprint blocks that are present in the query fingerprint.</p><p>It is possible that after search space reduction, the resulting bucket size B remains large, though it is smaller than M. In this situation, a refined search in the bucket of candidates can be quite costly by itself. Thus, the second part of approximation in fingerprint matching aims to reduce the cost of a linear search in the bucket. For systems that seek to match a sequence of frame fingerprints based on a distance measure, several approximation techniques are often used.</p><p>One of these techniques is greedy search: if a portion of the query fingerprint is matched to some references, subsequent search for match is directed at the part that immediately follows the matched portion in both the query fingerprint and the references. Another technique is "early exit" that aborts the comparison with a reference if an intermediate value of distance measure is already above the threshold for no-match. Yet another technique is to use downsampled frames in distance calculation.</p><p>All of the approximation techniques except "early exit" can incur a loss in search accuracy. Nonetheless, real-world data suggests that with a good video signature design, a speedup in several orders of magnitude can be achieved using these approximation techniques with a negligible loss in search accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Remarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Which One to Use?</head><p>With so many designs of video signatures and associated video fingerprinting algorithms, a natural question is: which one is the best? The answer is that there is no absolute best. Some video signatures are robust against certain types of distortions in video content but vulnerable to other types of distortions; other video signatures may be the other way around. Nonetheless, judging by the criteria outlined in Section 2.1, namely, robustness, discriminability, compactness, low complexity, and efficiency for search, the overall category winner appears to be spatial signatures, particularly block-based spatial signatures. Temporal and color signatures, while useful in improving discriminability, tend to fall short in robustness in comparison to spatial signatures. This observation is supported by experimental results reported in research literature, industry evaluation tests, as well as the success of some commercial systems deployed in the real world.</p><p>Despite the motivation of using some special transforms for their resilience to geometric transformations, transformdomain signatures are not widely adopted in video fingerprinting in practice due to their computational complexity. On the other hand, by using some adaptive techniques in fingerprint matching, block-based spatial signatures that are known to be prone to geometric transformations can achieve good robustness against moderate geometric transformations, e.g., frame rotation by 10 degrees. One of commonly used adaptive techniques is to apply weighting in distance calculations in fingerprint matching. Generally speaking, simply weighting down the block differences towards the edges of a video frame is often helpful because content around the center of the video frame is better preserved in geometric transformations and less affected by logo and subtitle overlays. See, e.g., Figure <ref type="figure" target="#fig_2">2</ref> for a visual comparison of distortions on the center and edges of a video frame. Iwamoto et al <ref type="bibr" target="#b14">15</ref> used a more sophisticated method in determining the weights in distance calculations.</p><p>Block-based spatial signatures are also compact and have low computational complexity. For many designs using spatial ordinal or differential signatures, the data size of a frame fingerprint is on the order of a few hundred bits, or less than 10 Kbps in data rate for video with frame rate at 30 fps. These fingerprints can be computed from a standard-definition video source in 1/10 of video playback time (or 10 times faster than real-time) on an off-the-shelf consumer-grade PC.</p><p>Because of their many advantages, spatial signatures particularly block-based spatial signatures are most widely used and studied in video fingerprinting. For designs that employ spatial signatures as the primary component of video fingerprint, temporal and color signatures are sometimes used as a secondary component to complement spatial signatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Temporal Structure from Spatial Signatures vs. Temporal Signatures</head><p>Many block-based spatial signatures are computed on each frame of the source video at certain frame rate, generating a sequence of time-stamped frame fingerprints. These frame fingerprints characterize not only spatial patterns in their corresponding video frames, but also temporal structure of the video. They provide a strong temporal constraint for a video being compared for match, increasing both discriminability and robustness of fingerprint matching. This assertion comes from an intuitive reasoning: if a single frame fingerprint is matched to a video, it may be by accident; if a number of consecutive frames are matched to a video in high degree, the chance of an accidental match decreases quickly as the number of consecutive frames increases. Indeed, many block-based spatial signatures correspond to an ultra-low resolution grid downsampled from the original frame resolution (e.g., 4x4 grid of blocks downsampled from a frame of 720x480); video sequence matching based on these low-resolution spatial signatures relies on a multitude of consecutive frame fingerprint matches to increase discriminability. On the other hand, robustness can also be enhanced with a multitude of consecutive frame fingerprints to smooth out a small number of mismatches due to distortions (e.g., frame drops) or local content changes (e.g., a fade or dissolve introduced by video editing).</p><p>The temporal structure that is imposed by a sequence of frame fingerprints with spatial signatures can be such a strong constraint in video identification that gives a nonessential role to separate temporal signatures such as the ones that characterizes the differential patterns in adjacent frames. Indeed, many proposed designs 8,10, 13-15 of video fingerprints do not include separate temporal signatures; they rely solely on spatial signatures to form a sequence of frame fingerprints. Like an I-frame only video sequence, a sequence of video fingerprints without inter-frame temporal signatures have some benefits in content editing and management; for example, it can be cut, split, and merged at any point without a need of re-computing or modifying temporal signatures on the boundaries. Nonetheless, temporal signatures can be a good complement to spatial signatures in video identification. For example, video sequences containing many still frames (e.g., a slide show) can be characterized more effectively when temporal signatures are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Algorithms vs. Systems</head><p>The various approximation techniques used in fingerprint matching creates a complex pipeline where each stage can contribute to an increase in FN. Therefore, in an end-to-end video identification system including both fingerprinting and fingerprint matching, robustness depends on not only video signature and fingerprinting algorithms, but also a number of other factors in the system design. Specifically, reducing search space may introduce FNs. For example, a matching reference that does not fall into the bucket by hashing or mapping results in a FN. Additionally, the use of greedy search in computing alignment and frame downsampling in distance calculation can also introduce FNs.</p><p>To separate algorithms and systems, BER and normalized L1 or L2 distance can be used for pure video fingerprint evaluation and comparison, assuming a query fingerprint is perfectly aligned to the matching reference fingerprint, and no approximation is made in computing the distance or error rate. When recall or FN rate is reported, however, it should be understood that one is evaluating an end-to-end video identification system; the superiority of a video fingerprint measured by BER or other distance-based error rates may not translate into a superior video identification system. It is highly desirable that a video fingerprint design can facilitate and work well with various approximation techniques, because in the end, all practical systems must use some approximation techniques in fingerprint matching and what matters is the accuracy and speed of such systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Topics of Continuing Research</head><p>There is active research in new video signatures and fingerprinting algorithms as well as faster fingerprint matching algorithms and techniques. Designing an ultimate, versatile video signature is the Holy Grail. The author of this paper believes that it is more achievable and beneficial to develop a set of video signatures that are complementary to each other in enhancing robustness and discriminability. In fingerprint matching, there is a real need for continued advance in search algorithms because of the rapid growth in the size of video fingerprint database that is already in the order of tens of millions for UGC videos. Additionally, it would be highly useful to quantify the relationship between the loss in accuracy and gain in speed by various approximation techniques used in fingerprint matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPLICATIONS OF VIDEO FINGERPRINTING</head><p>Video fingerprinting technology that can identify video content accurately, efficiently, and automatically has many practical applications. As was introduced in Section 1, the development of video fingerprinting technology has been driven largely by needs for finding copyrighted video content on the Internet. As the technology matures, other applications are also emerging. In this Section, we review a few industry applications that have been commercially deployed for copyright management in video distribution on the Internet. We also provide a brief overview for a few emerging applications that are being developed and experimented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Content Registration</head><p>Before a video can be identified by its fingerprint, the video fingerprint must be extracted from at least one version of the same video content and ingested into a reference database. Typically, a master reference fingerprint database is centrally located while fingerprints are often collected and ingested from distributed locations. This is similar to the process of populating a human fingerprint database. Like a human fingerprint database, a video fingerprint database contains not only fingerprint data, but also information about or associated with the fingerprint data. Such information is of critical importance to applications that query a video fingerprint database.</p><p>One type of information that binds with video fingerprint is the so-called metadata that describes the video content and/or the particular instance of the video content from which the fingerprint is extracted. Currently, there is not yet a standardized metadata schema for video fingerprints, but commonly used content metadata includes title, ownership information, production and release dates, genres, etc. Instance metadata includes length, resolution, and frame rate of the video, codec and file format, etc. Because a wealth of video metadata already exists elsewhere, e.g., in an existing video asset management system, it is unnecessary to replicate a full set of metadata in a video fingerprint database. Most video fingerprint databases stores only a limited set of metadata for content identification purposes, and reference other unique video asset IDs (e.g., ISAN <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> ) that link to sources of information outside the video fingerprinting system. Another type of information that is associated with video fingerprint is business rules. They are specified by content owners to determine what actions should be taken when an unauthorized copy of reference content is identified. Recently, MovieLabs published Content Recognition Rules (CRR) <ref type="bibr" target="#b40">41</ref> that defines standard XML interfaces to communicate about business rules for identified unauthorized content. Putting a content identification system in the center, the CRR defines two interfaces. One interface is from content owners to a content identification system for specifying business rules; the other interface is from the content identification system to a caller of content identification service to communicate match results as well as the rules and actions specified by the content owner. The CRR provides a flexible framework for existing and anticipated business and application scenarios. For example, the content owner could specify if the UGC site should take it down or allow it to post, or could advertise on it when a copy of unauthorized content is identified on the site. Furthermore, these actions could be determined on different conditions and circumstances, such as how long the identified content is, and the geography of the site. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Content Filtering</head><p>Content filtering has long been proposed as a solution to the piracy problem on the Internet. In 2001, Napster implemented a music content filter based on audio fingerprinting in its P2P network. Because of its availability and maturity, audio fingerprinting has also been used in video content filtering by identifying the associated audio tracks.</p><p>Only in the last few years content filtering systems based on video fingerprinting have become available and been deployed. Today, video content filtering systems can be based on video fingerprinting, audio fingerprinting, or both, but they share basically the same architecture and workflow.</p><p>Figure <ref type="figure" target="#fig_4">3</ref> is a diagram illustrating a content filtering system based on video fingerprinting in the service backend of a UGC site. The filter is integrated in an automatic workflow for converting and publishing user uploaded video clips. Typically, a user uploaded video clip is processed and transcoded into a common format in site-specified settings (e.g., FLV in 320x240, 30 fps, 200 Kbps). The video is also indexed by its metadata for search and retrieval. Video fingerprinting and identification can be done before or after the transcoding. In the diagram of Figure <ref type="figure" target="#fig_4">3</ref>, fingerprinting is done on transcoded video content. The resulting fingerprint is queried against a pre-populated reference fingerprint database for identification. The identification results are fed back to the publishing workflow. If the query fingerprint is matched to a copyrighted asset in the reference database, the corresponding video clip is taken down or handled differently according to the rules and actions specified by the copyright owner; otherwise, the video is replicated and published to the Web frontend.</p><p>The point of integration of content filters is an important design consideration for effective and efficient content filtering. Today, a top online video site such as YouTube has hundreds of thousands of video uploads each day; a second-tier site Match result processing &amp; reporling also has tens of thousands. While these numbers are non-trivial for a content filtering system, they are a small fraction compared with the number of downloads that are in the order of a hundred millions each day for YouTube alone. So, in the UGC filtering above, the optimum point of integration is in the path of upload processing on the UGC service backend. In the P2P networks, however, the best point of integration of a content filter is less obvious. Existing designs and implementations put content filters in the P2P client software or on servers that host a directory or search index of P2P video content.</p><p>There are also content filtering systems that are designed to work in the packet network to identify and filter out unauthorized copyrighted video content in P2P file swaps. These systems are typically deployed at the gateway of an intranet, such as a college campus network, or a traffic aggregation point in a broadband access network operated by an ISP. Content identification in the packet network is more challenging because of the stricter requirement on low latency and high scalability. For this consideration, practical systems often adopt a hybrid design that combines content fingerprints with packet-level signatures for increased efficiency in content identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Video Content Tracking</head><p>Owners of video content often want to know where their content is distributed on the Internet and how many people have watched it. Video content tracking is an application and service that serve this purpose. Figure <ref type="figure" target="#fig_6">4</ref> is a diagram of a video tracking system that consists of a web crawler, a video fingerprinting system, and a web interface. The web crawler serves to discover the "suspects". Crawling may be targeted to specific sites or specific content categories on a site, and may be guided by keywords. The video fingerprinting system serves to check and verify the "suspects". The reference fingerprint database can be very targeted; it may contain only the fingerprints of the video content being tracked. The web interface is used to report and update the tracking results. As the crawler continues to discover new "suspects", they will be verified and reported (if matched) in a continuous, 24x7, and non-stopping workflow. Figure <ref type="figure" target="#fig_7">5</ref> shows a screenshot of the VideoTracker™ system developed by Vobile, Inc.</p><p>To date, video tracking has been successfully deployed to track high-valued copyrighted video content, from Hollywood blockbuster releases to the 2008 Beijing Summer Olympic programs. <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> Besides copyright enforcement, video tracking has also been used to track various types of video content on the Internet, such as commercials and political campaign videos. One may observe that some of these tasks used to be done by humans before video fingerprinting technology became available. Indeed, the key value that video fingerprinting brings in these applications is enabling an automatic, low cost, and more accurate and efficient workflow. This has changed the way business is done. For example, automatic video tracking systems based on video fingerprinting can now track tens of thousands of titles simultaneously, comparing to no more than tens of titles previously by human-based tracking services. Better yet, the tracking results can now be updated instantly and continuously instead of daily or weekly reports. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Applications</head><p>Broadcast monitoring is among the first applications of video fingerprinting. It monitors broadcast programs in various local markets to find out where and when a program is broadcast and for how many times. The gathered information is useful for rights owners to collect royalties for their content or for advertisers to audit the airing of their commercials in the paid time slots in a broadcast network. <ref type="bibr" target="#b28">29</ref> Contextual advertising based on video identification works in a way similar to Google's AdSense. While AdSense pairs ads with keywords indexed from Web pages or text derived from other media (e.g., audio tracks), fingerprint-based video identification can tell exactly what content is being consumed, thus providing a good context for serving relevant ads. When a content owner allows advertising on its content on UGC sites (e.g., specified in a CRR-compliant rule), it creates a new model for monetizing its content. Currently, the industry is experimenting with this approach that will hopefully lead to a new way for solving the piracy problem.</p><p>Video asset management using video fingerprints recognizes the fundamental role of video fingerprints: they are contentbased IDs. There are many benefits of using video fingerprints as content-based IDs in an asset management system. Because a video fingerprint is computed from video content, it is a permanent ID that can always be regenerated.</p><p>Copies, segments, and edits of the same video content can be easily identified and related to each other by their fingerprints. For example, Kasutani et al <ref type="bibr" target="#b43">44</ref> developed an video archiving system that automatically detects and links video edits to the source video footages based on video identification.</p><p>Content-based video search is an area of active research. The notion of "query by video clip" was coined by Jain et al <ref type="bibr" target="#b44">45</ref> a decade ago. The techniques described in their paper -using color, texture, and motion signatures in a query -are essentially a video fingerprint query. To date, video fingerprint queries are primarily for finding copies of the same content, partial or whole, transformed or unaltered. There have been attempts to explore video fingerprinting in broader video search for discovering similar but different video content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>Research in video fingerprinting has come a long way since it began a decade ago and developed into a technology that is adopted by the industry. Key areas of research include designs of video signatures, fingerprinting and fingerprint matching algorithms. Among the large number of designs, video signatures can be classified into spatial, temporal, color, and transform-domain signatures. Although none is perfect, spatial signatures are found to be the overall winner in terms of robustness, discriminability, compactness, and computational complexity. Temporal and color signatures can provide enhanced discriminability when used together with spatial signatures. Fingerprint matching by exhaustive search has a linear time complexity with regard to the size of reference database. Fortunately, effective approximation techniques have been developed that provide a dramatic reduction in computational complexity, speeding up fingerprint queries by several orders of magnitude over an exhaustive search with a negligible loss in accuracy. This made it possible to build practical fingerprint matching systems that are scalable.</p><p>The adoption of video fingerprinting technology was accelerated in the last few years as the content industry responded to the increasing cases of copyright violations in the rapidly growing P2P and UGC networks. As such, major commercial applications of video fingerprinting to date are for identifying unauthorized distribution of copyrighted video content on the Internet, including video content filtering and tracking. Moving forward, researchers and practitioners also exploring and experimenting other applications of video fingerprinting, including contextual advertising, video asset management, and content-based video search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ordinal ranking -an example of spatial signatures. (a) Color-converted and subdivided luminance frame; (b) average pixel values of blocks; (c) ordinal ranks of blocks.</figDesc><graphic coords="5,126.72,184.26,162.00,92.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>estimated luminance edges in 8 quantized directions for each block and kept the direction having maximum strength as an edge signature. It is worth noting that like ordinal ranking for block luminance patterns, differential block luminance patterns are quantized or abstracted to form differential signatures. Abstraction further increases robustness and reduces data size; it is key to all fingerprinting algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mismatch of content in blocks in a fixed-sized grid after rotation and cropping. (a) Color-converted and subdivided luminance frame; (b) rotation by 10 degrees; (c) rotation followed by cropping and expanding to full screen. Blocks in the center of the frame are less distorted than those on the edge.</figDesc><graphic coords="6,127.68,184.50,160.08,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Integration of video content filtering in a UGC service backend.</figDesc><graphic coords="11,63.00,204.00,486.00,233.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Workflow of a video content tracking system.</figDesc><graphic coords="12,101.00,221.00,410.00,212.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: VideoTracker™ -screenshot of a real-world video tracking system.</figDesc><graphic coords="13,149.00,71.00,314.00,243.00" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>SPIE-IS&amp;T/ Vol. 7254 725402-2 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>SPIE-IS&amp;T/ Vol. 7254 725402-3 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>SPIE-IS&amp;T/ Vol. 7254 725402-5 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>SPIE-IS&amp;T/ Vol. 7254 725402-6 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>SPIE-IS&amp;T/ Vol. 7254 725402-8 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>SPIE-IS&amp;T/ Vol. 7254 725402-9 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>SPIE-IS&amp;T/ Vol. 7254 725402-10 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>SPIE-IS&amp;T/ Vol. 7254 725402-12 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 08/05/2013 Terms of Use: http://spiedl.org/terms</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Finding pirated video sequences on the Internet</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shivakumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford InfoLab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting digital copyright violations on the Internet</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shivakumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-08">Aug. 1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-dimensional computational geometry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimation of web video multiplicity</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Internet Imaging</title>
		<meeting>SPIE, Internet Imaging</meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="volume">3964</biblScope>
			<biblScope unit="page" from="34" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel scheme for fast and efficient video sequence matching using compact signatures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Yeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Storage and Retrieval for Media Databases</title>
		<meeting>SPIE, Storage and Retrieval for Media Databases</meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="volume">3972</biblScope>
			<biblScope unit="page" from="564" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison of distance measures for video copy detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2001-08">Aug. 2001</date>
			<biblScope unit="page" from="188" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ordinal measures for image correspondence</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Ana. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="423" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video sequence matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3697" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comparison of sequence matching techniques for video copy detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Storage and Retrieval for Media Databases</title>
		<meeting>SPIE, Storage and Retrieval for Media Databases</meeting>
		<imprint>
			<date type="published" when="2002-01">Jan. 2002</date>
			<biblScope unit="volume">4676</biblScope>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust video signature based on ordinal measure</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Proc. (ICIP)</title>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatiotemporal sequence matching for efficient video copy detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vasudev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature extraction and a database strategy for video fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oostveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5 th Int. Conf. Recent Advance in Visual Information Systems</title>
		<meeting>5 th Int. Conf. Recent Advance in Visual Information Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video fingerprinting based on centroids of gradient orientations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="401" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust video fingerprinting for content-based video identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="983" to="988" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image signature robust to caption superimposition for video sequence identification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kasutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Proc. (ICIP)</title>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="3185" to="3188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image hashing resilient to geometric and filtering operations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Multimedia Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="355" to="358" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Affine transform resilient image fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2003-04">Apr. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="61" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust video hashing based on radial projections of key frames</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Roover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lefèbvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4020" to="4037" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust video fingerprints based on subspace embedding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2008-04">Apr. 2008</date>
			<biblScope unit="page" from="2245" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust content-based video copy identification in a large reference database</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
		<author>
			<persName><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Image and Video Retrieval (CIVR)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="414" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust voting algorithms based on labels of behavior for video copy detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Law-To</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gouet-Brunetand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. on Multimedia</title>
		<meeting>ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="835" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local feature extraction for video copy detection in a database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Proc. (ICIP)</title>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="1716" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video fingerprinting: features for duplicate and similar video detection and query-based video retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moxley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Multimedia Content Access: Algorithms and Systems</title>
		<meeting>SPIE, Multimedia Content Access: Algorithms and Systems</meeting>
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
			<biblScope unit="volume">6820</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A video fingerprint based on visual digest and local fingerprints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Massoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chupeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Proc. (ICIP)</title>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="2297" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust video fingerprinting based on affine covariant regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2008-04">Apr. 2008</date>
			<biblScope unit="page" from="1237" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient video similarity measurement with video signature</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="74" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video sequence matching based on temporal ordinal measurement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W M</forename><surname>Stentiford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1824" to="1831" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Content-based video signatures based on projections of difference images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9 th Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="341" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching commercial clips from TV streams using a unique, robust, and compact signature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Digital Imaging Computing: Techniques and Applications</title>
		<meeting>Digital Imaging Computing: Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
			<biblScope unit="page" from="266" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient video retrieval by locality sensitive hashing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech and Signal Processing (ICASSP)</title>
		<meeting>Int. Conf. Acoust., Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="449" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video sequence matching using singular value decomposition</title>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3 rd Int. Conf. Image Analysis and Recognition (ICIAR)</title>
		<meeting>3 rd Int. Conf. Image Analysis and Recognition (ICIAR)</meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
			<biblScope unit="page" from="426" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust video hash extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Signal Processing</title>
		<meeting>European Conf. on Signal essing</meeting>
		<imprint>
			<date type="published" when="2004-09">Sep. 2004</date>
			<biblScope unit="page" from="2295" to="2298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal transform based video hashing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor -towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30 th Symposium on Theory of Computing</title>
		<meeting>30 th Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25 th Int. Conf. Very Large Data Bases (VLDB)</title>
		<meeting>25 th Int. Conf. Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical, non-uniform locality sensitive hashing and its application to video identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="743" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to hash: forgiving hash functions and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="402" to="430" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Content-based copy retrieval using distortion-based probabilistic similarity search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">International Standard Audiovisual Number (ISAN) -Part 1: Audiovisual work identifier, ISO 15706-1</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m">International Standard Audiovisual Number (ISAN) -Part 2: Version identifier</title>
		<imprint>
			<date type="published" when="2007">15706-2, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Content Recognition Rules</title>
		<ptr target="http://www.movielabs.com/CRR/" />
	</analytic>
	<monogr>
		<title level="j">MovieLabs TR-CRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video piracy&apos;s Olympic showdown</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burrows</surname></persName>
		</author>
		<ptr target="http://www.businessweek.com/magazine/content/08_23/b4087073685542.htm" />
	</analytic>
	<monogr>
		<title level="j">BusinessWeek</title>
		<imprint>
			<date type="published" when="2008-09">Jun. 9, 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Let the Games begin, online</title>
		<author>
			<persName><forename type="first">K</forename><surname>Voigt</surname></persName>
		</author>
		<ptr target="http://edition.cnn.com/2008/TECH/08/06/db.olympicdigitalrights/" />
	</analytic>
	<monogr>
		<title level="j">CNN.com International</title>
		<imprint>
			<date type="published" when="2008-06">Aug. 6, 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video material archive system for efficient video editing based on media identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kasutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="727" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Query by video clip</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="369" to="384" />
			<date type="published" when="1999-09">Sep. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
