<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From the perspective of expressive power, this work compares multi-layer Graph Neural Networks (GNNs) with a simplified alternative that we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first augments node features with certain multi-hop operators on the graph and then applies an MLP in a node-wise fashion. From the perspective of graph isomorphism testing, we show both theoretically and numerically that GA-MLPs with suitable operators can distinguish almost all non-isomorphic graphs, just like the Weifeiler-Lehman (WL) test. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, we prove a separation in expressive power between GA-MLPs and GNNs that grows exponentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the number of attributed walks. We also demonstrate via community detection experiments that GA-MLPs can be limited by their choice of operator family, as compared to GNNs with higher flexibility in learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While multi-layer Graph Neural Networks (GNNs) have gained popularity for their applications in various fields, recently authors have started to investigate what their true advantages over baselines are, and whether they can be simplified. On one hand, GNNs based on neighborhood-aggregation allows the combination of information present at different nodes, and by increasing the depth of such GNNs, we increase the size of the receptive field. On the other hand, it has been pointed out that deep GNNs can suffer from issues including over-smoothing, exploding or vanishing gradients in training as well as bottleneck effects <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b27">Li et al., 2018;</ref><ref type="bibr" target="#b29">Luan et al., 2019;</ref><ref type="bibr" target="#b37">Oono &amp; Suzuki, 2020;</ref><ref type="bibr" target="#b39">Rossi et al., 2020;</ref><ref type="bibr" target="#b1">Alon &amp; Yahav, 2020)</ref>.</p><p>Recently, a series of models have attempted at relieving these issues of deep GNNs while retaining their benefit of combining information across nodes, using the approach of firstly augmenting the node features by propagating the original node features through powers of graph operators such as the (normalized) adjacency matrix, and secondly applying a node-wise function to the augmented node features, usually realized by a Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b47">(Wu et al., 2019;</ref><ref type="bibr" target="#b36">NT &amp; Maehara, 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2019a;</ref><ref type="bibr" target="#b39">Rossi et al., 2020)</ref>. Because of the usage of graph operators for augmenting the node features, we will refer to such models as Graph-Augmented MLPs (GA-MLPs). These models have achieved competitive performances on various tasks, and moreover enjoy better scalability since the augmented node features can be computed during preprocessing <ref type="bibr" target="#b39">(Rossi et al., 2020)</ref>. Thus, it becomes natural to ask what advantages GNNs have over GA-MLPs.</p><p>In this work, we ask whether GA-MLPs sacrifice expressive power compared to GNNs while gaining these advantages. A popular measure of the expressive power of GNNs is their ability to distinguish non-isomorphic graphs <ref type="bibr" target="#b14">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b49">Xu et al., 2019;</ref><ref type="bibr">Morris et al., 2019)</ref>. In our work, besides studying the expressive power of GA-MLPs from the viewpoint of graph isomorphism tests, we propose a new perspective that better suits the setting of node-prediction tasks: we analyze the expressive power of models including GNNs and GA-MLPs as node-level functions, or equivalently, as functions on rooted graphs. Under this perspective, we prove an exponential-in-depth gap between the expressive powers of GNNs and GA-MLPs. We illustrate this gap by finding a broad family of user-friendly functions that can be provably approximated by GNNs but not GA-MLPs, based on counting attributed walks on the graph. Moreover, via the task of community detection, we show a lack of flexibility of GA-MLPs compared to GNNs learn the best operators to use.</p><p>In summary, our main contributions are:</p><p>• Finding graph pairs that several GA-MLPs cannot distinguish while GNNs can, but also proving there exist simple GA-MLPs that distinguish almost all non-isomorphic graphs. • From the perspective of approximating node-level functions, proving an exponential gap between the expressive power of GNNs and GA-MLPs in terms of the equivalence classes on rooted graphs that they induce. • Showing that the functions that count a particular type of attributed walk among nodes can be approximated by GNNs but not GA-MLPs both in theory and numerically. • Through community detection tasks, demonstrate the limitations of GA-MLPs due to the choice of the operator family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Depth in GNNs <ref type="bibr" target="#b20">Kipf &amp; Welling (2016)</ref> observe that the performance of Graph Convolutional Networks (GCNs) degrade as the depth grows too large, and the best performance is achieved with 2 or 3 layers. Along the spectral perspective on GNNs <ref type="bibr" target="#b5">(Bruna et al., 2013;</ref><ref type="bibr" target="#b11">Defferrard et al., 2016;</ref><ref type="bibr" target="#b4">Bronstein et al., 2017;</ref><ref type="bibr" target="#b36">NT &amp; Maehara, 2019)</ref>, <ref type="bibr" target="#b27">Li et al. (2018)</ref> and <ref type="bibr" target="#b47">Wu et al. (2019)</ref> explain the failure of deep GCNs by the over-smoothing of the node features. <ref type="bibr" target="#b37">Oono &amp; Suzuki (2020)</ref> show an exponential loss of expressive power as the depth in GCNs increases in the sense that the hidden node states tend to converge to Laplacian sub-eigenspaces as the depth increases to infinity. <ref type="bibr" target="#b1">Alon &amp; Yahav (2020)</ref> show an over-squashing effect of deep GNNs, in the sense that the width of the hidden states needs to grow exponentially in the depth in order to retain all information about long-range interactions. In comparison, our work focuses on more general GNNs based on neighborhoodaggregation that are not limited in the hidden state widths, and demonstrates the their advantage in expressive power compared to GA-MLP models at finite depth, in terms of distinguishing rooted graphs for node-prediction tasks. On the other hand, there exist examples of useful deep GNNs. <ref type="bibr" target="#b8">Chen et al. (2019b)</ref> apply 30-layer GNNs for community detection problems, which uses a family of multi-scale operators as well as normalization steps <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b44">Ulyanov et al., 2016)</ref>. Recently, <ref type="bibr" target="#b24">Li et al. (2019;</ref><ref type="bibr">2020a)</ref> and <ref type="bibr" target="#b6">Chen et al. (2020a)</ref> build deeper GCN architectures with the help of various residual connections <ref type="bibr" target="#b15">(He et al., 2016)</ref> and normalization steps to achieve impressive results in standard datasets, which further highlights the need to study the role of depth in GNNs.</p><p>Existing GA-MLP-type models Motivated by better understanding GNNs as well as enhancing computational efficiency, several models of the GA-MLP type have been proposed and they achieve competitive performances on various datasets. <ref type="bibr" target="#b47">Wu et al. (2019)</ref> propose the Simple Graph Convolution (SGC), which removes the intermediary weights and nonlinearities in GCNs. <ref type="bibr" target="#b7">Chen et al. (2019a)</ref> propose the Graph Feature Network (GFN), which further adds intermediary powers of the normalized adjacency matrix to the operator family and is applied to graph-prediction tasks. <ref type="bibr" target="#b36">NT &amp; Maehara (2019)</ref> propose the Graph Filter Neural Networks (gfNN), which enhances the SGC in the final MLP step. <ref type="bibr" target="#b39">Rossi et al. (2020)</ref> propose Scalable Inception Graph Neural Networks (SIGNs), which augments the operator family with Personalized-PageRank-based <ref type="bibr" target="#b21">(Klicpera et al., 2018;</ref><ref type="bibr">2019)</ref> and triangle-based <ref type="bibr" target="#b33">(Monti et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019b)</ref> adjacency matrices.</p><p>Expressive Power of GNNs <ref type="bibr" target="#b49">Xu et al. (2019)</ref> and <ref type="bibr">Morris et al. (2019)</ref> show that GNNs based on neighborhood-aggregation are no more powerful than the Weisfeiler-Lehman (WL) test for graph isomorphism <ref type="bibr" target="#b46">(Weisfeiler &amp; Leman, 1968)</ref>, in the sense that these GNNs cannot distinguish between any pair of non-isomorphic graphs that the WL test cannot distinguish. They also propose models that match the expressive power of the WL test. Since then, many attempts have been made to build GNN models whose expressive power are not limited by WL <ref type="bibr">(Morris et al., 2019;</ref><ref type="bibr" target="#b30">Maron et al., 2019a;</ref><ref type="bibr" target="#b9">Chen et al., 2019c;</ref><ref type="bibr" target="#b34">Morris &amp; Mutzel, 2019;</ref><ref type="bibr" target="#b50">You et al., 2019;</ref><ref type="bibr" target="#b3">Bouritsas et al., 2020;</ref><ref type="bibr" target="#b26">Li et al., 2020b;</ref><ref type="bibr" target="#b12">Flam-Shepherd et al., 2020;</ref><ref type="bibr" target="#b42">Sato et al., 2019;</ref><ref type="bibr">2020)</ref>. Other perspectives for understanding the expressive power of GNNs include function approximation <ref type="bibr" target="#b31">(Maron et al., 2019b;</ref><ref type="bibr" target="#b9">Chen et al., 2019c;</ref><ref type="bibr" target="#b18">Keriven &amp; Peyré, 2019)</ref>, substructure counting <ref type="bibr" target="#b10">(Chen et al., 2020b)</ref>, Turing universality <ref type="bibr" target="#b28">(Loukas, 2020)</ref> and the determination of graph properties <ref type="bibr" target="#b13">(Garg et al., 2020)</ref>. <ref type="bibr" target="#b41">Sato (2020)</ref> provides a survey on these topics. In this paper, besides studying the expressive power of GA-MLPs along the line of graph isomorphism tests, we propose a new perspective of approximating functions on rooted graphs, which is motivated by node-prediction tasks, and show a gap between GA-MLPs and GNNs that grows exponentially in the size of the receptive field in terms of the equivalence classes that they induce on rooted graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NOTATIONS</head><p>Let G = (V, E) denote a graph, with V being the vertex set and E being the edge set. Let n denote the number of nodes in G, A ∈ R n×n denote the adjacency matrix, D ∈ R n×n denote the diagonal degree matrix with D ii = d i being the degree of node i. We call D −<ref type="foot" target="#foot_0">1</ref> 2 AD − 1 2 the (symmetrically) normalized adjacency matrix, and D −α AD −β a generalized normalized adjacency matrix for any α, β ∈ R. Let X ∈ R n×d denote the matrix of node features, where X i denotes the d-dimensional feature that node i possesses. For a node i ∈ V , let N (i) denote the set of neighbors of i. We assume that the edges do not possess features. In a node prediction task, the labels are given by Y ∈ R n .</p><p>For a positive integer K, we let [K] = {1, ..., K}. We use {...} m to denote a multiset, which allows repeated elements. We say a function f</p><formula xml:id="formula_0">(K) is doubly-exponential in K if log log f (K) is polynomial in K, and poly-exponential in K if log f (K) is polynomial in K, as K tends to infinity.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GRAPH NEURAL NETWORKS (GNNS)</head><p>Following the notations in <ref type="bibr" target="#b49">Xu et al. (2019)</ref>, we consider K-layer GNNs defined generically as follows. For k ∈ [K], we compute the hidden node states</p><formula xml:id="formula_1">H ∈ R n×d (k)</formula><p>iteratively as</p><formula xml:id="formula_2">M (k) i = AGGREGATE (k) ({H (k−1) j : j ∈ N (i)}) , H (k) i = COMBINE (k) (H (k−1) i , M<label>(k)</label></formula><p>i ) , (1) where we set H (0) = X to be the node features. If a graph-level output is desired, we finally let</p><formula xml:id="formula_3">Z G = READOUT({H (K) i : i ∈ V }) ,<label>(2)</label></formula><p>Different choices of the trainable COMBINE, AGGREGATE and READOUT functions result in different GNN models, though usually AGGREGATE and READOUT are chosen to be permutationinvariant. As graph-level functions, it is shown in <ref type="bibr" target="#b49">Xu et al. (2019)</ref> and <ref type="bibr">Morris et al. (2019)</ref> that the maximal expressive power of models of this type coincides with running K iterations of the WL test for graph isomorphism, in the sense that any two non-isomorphic graphs that cannot be distinguished by the latter cannot be distinguished by the K-layer GNNs, either. For this reason, we will not distinguish between GNN and WL in discussions on expressive powers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GRAPH-AUGMENTED MULTI-LAYER PECEPTRONS (GA-MLPS)</head><p>GA-MLPs are models that consist of two steps -first augmenting the node features with some operators based on the graph topology, and then applying a node-wise learnable function 1 . Let Ω = {ω 1 (A), ..., ω K (A)} be a set of (usually multi-hop) linear operators that are functions of the adjacency matrix, A. Common choices of the operators are powers of the (normalized) adjacency matrix, and several particular choices of Ω that give rise to existing GA-MLP models are listed in Appendix B. In its general form, a GA-MLP first computes a series of augmented features via</p><formula xml:id="formula_4">Xk = ω k (A) • ϕ(X) ,<label>(3)</label></formula><p>with ϕ : R d → R d being a learnable function acting as a feature transformation applied to each node separately. It can be realized by an MLP, e.g. ϕ(X) = σ(XW 1 )W 2 , where σ is a nonlinear activation function and W 1 , W 2 are trainable weight matrices of suitable dimensions. Next, the model concatenates X1 , ..., XK into X = [ X1 , ..., XK ] ∈ R n× (K d) , and computes where ρ : R T d → R d is also a learnable node-wise function, again usually realized by an MLP. If a graph-level output is desired, we can also add a READOUT function as in (2).</p><formula xml:id="formula_5">Z = ρ( X) ,<label>(4)</label></formula><p>A simplified version of the model sets ϕ to be the identity function, in which case (3) and ( <ref type="formula" target="#formula_5">4</ref>) can be written together as</p><formula xml:id="formula_6">X = ρ([ω 1 (A) • X, ..., ω K (A) • X])<label>(5)</label></formula><p>Such a simplification improves computational efficiency since the matrix products ω k (A) • X can be pre-computed before training <ref type="bibr" target="#b39">(Rossi et al., 2020)</ref>. Since we are mostly interested in an upper bounds on the expressive power of GA-MLPs, we will work with the more general update rule (3) in this paper, but the lower-bound result in Proposition 2 remains valid even when we restrict to the subset of models where ϕ is taken to be the identity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPRESSIVE POWER AS GRAPH ISOMORPHISM TESTS</head><p>We first study the expressive power of GA-MLPs via their ability to distinguish non-isomorphic graphs. It is not hard to see that when Ω = {I, Ã, ..., ÃK }, where Ã = D −α AD −β for any α, β ∈ R generalizes the normalized adjacency matrix, this is upper-bounded by the power of K + 1 iterations of WL. We next ask whether it can fall strictly below. Indeed, for two common choices of Ω, we can find concrete examples: 1) If Ω consists of integer powers of any normalized adjacency matrix of the form D −α AD −(1−α) for some α ∈ [0, 1], then it is apparent that the GA-MLP cannot distinguish any pair of regular graphs with the same size but different node degrees; 2) If Ω consists of integer powers of the adjacency matrix, A, then the model cannot distinguish between the pair of graphs shown in Figure <ref type="figure" target="#fig_0">1</ref>, which can be distinguished by 2 iterations of the WL test. The proof of the latter result is given in Appendix K. Together, we summarize the results as: α) for some α ∈ [0, 1], there exists a pair of graphs which can be distinguished by GNNs but not this GA-MLP.</p><formula xml:id="formula_7">Proposition 1. If Ω ⊆ { Ãk : k ∈ N}, with either Ã = A or Ã = D −α AD −(1−</formula><p>Nonetheless, if we focus on not particular counterexamples but rather the average performance in distinguishing random graphs, it is not hard for GA-MLPs to reach the same level as WL, which is known to distinguish almost all pairs of random graphs under a uniform distribution <ref type="bibr" target="#b2">(Babai et al., 1980)</ref>. Specifically, building on the results in <ref type="bibr" target="#b2">Babai et al. (1980)</ref>, we prove in Appendix F that:</p><p>Proposition 2. For all n ∈ N + , ∃α n &gt; 0 such that any GA-MLP that has {D, AD −αn } ⊆ Ω can distinguish almost all pairs of non-isomorphic graphs of at most n nodes, in the sense that the fraction of graphs on which such a GA-MLP fails to test isomorphism is 1 − o(1) as n → ∞.</p><p>The hypothesis that distinguishing non-isomorphic graphs is not difficult on average for either GNNs or GA-MLPs is further supported by the numerical results provided in Appendix C, in which we count the number of equivalence classes that either of them induce on graphs that occur in realworld datasets. This further raises the question of whether graph isomorphism tests along suffice as a criterion for comparing the expressive power of models on graphs, which leads us to the explorations in the next section.</p><p>Lastly, we remark that with suitable choices of operators in Ω, it is possible for GA-MLPs to go beyond the power of WL. For example, if Ω contains the power graph adjacency matrix introduced in Chen et al. (2019b), min(A 2 , 1), then the GA-MLP can distinguish between a hexagon and a pair of triangles, which WL cannot distinguish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPRESSIVE POWER AS FUNCTIONS ON ROOTED GRAPHS</head><p>To study the expressive power beyond graph isomorphism tests, we consider the setting of nodewise prediction tasks, for which the final readout step (2) is dropped in both GNNs and GA-MLPs.</p><p>Whether the learning setup is transductive or inductive, we can consider the models as functions on rooted graphs, or egonets (Preciado &amp; Jadbabaie, 2010), which are graphs with one node designated as the root {i 1 , ..., i n } is a set of nodes in the graphs {G 1 , ..., G n } (not necessarily distinct) and with node-level labels {Y i1 , ..., Y in } known during training, respectively, then the goal is to fit a function to the input-output pairs (G</p><formula xml:id="formula_8">[in]</formula><p>n , Y in ), where we use G [i] to denote the rooted graph with G being the graph and the node i in G being the root. Thus, we can evaluate the expressive power of GNNs and GA-MLPs by their ability to approximate functions on the space of rooted graphs, which we call E.</p><p>To do so, we introduce a notion of induced equivalence relations on E, analogous to the equivalence relations on G introduced in Appendix C. Given a family of functions F on E, we can define an equivalence relation E;F among all rooted graphs such that ∀G [i] , G</p><formula xml:id="formula_9">[i ] ∈ E, G [i] E;F G [i ] if and only if ∀f ∈ F, f (G [i] ) = f (G [i ]</formula><p>). By examining the number and sizes of the induced equivalence classes of rooted graphs, we can evaluate the relative expressive power of different families of functions on E in a quantitative way.</p><p>In the rest of this section, we assume that the node features belong to a finite alphabet X ⊆ N and all nodes have degree at most m ∈ N + . Firstly, GNNs are known to distinguish neighborhoods up to the rooted aggregation tree, which can be obtained by unrolling the neighborhood aggregation steps in the GNNs as well as the WL test <ref type="bibr" target="#b49">(Xu et al., 2019;</ref><ref type="bibr">Morris et al., 2019;</ref><ref type="bibr" target="#b13">Garg et al., 2020)</ref>. The depth-K rooted aggregation tree of a rooted graph G [i] is a depth-K rooted tree with a (possibly many-to-one) mapping from every node in the tree to some node in G [i] , where (i) the root of the tree is mapped to node i, and (ii) the children of every node j in the tree are mapped to the neighbors of the node in G [i] to which j is mapped. An illustration of rooted graphs and rooted aggregation trees is given in Figure <ref type="figure" target="#fig_5">4</ref>. Hence, each equivalence class in E induced by the family of all depth-K GNNs consists of all rooted graphs that share the same rooted aggregation tree of depth-K. Thus, to estimate the number of equivalence classes on E induced by GNNs, we need to estimate the number of possible rooted aggregation trees, which is given by Lemma 3 in Appendix G. Thus, we derive the following lower bound on the number of equivalence classes in E that depth-K GNNs induce: Proposition 3. Assume that |X | ≥ 2 and m ≥ 3. The total number of equivalence classes of rooted graphs induced by GNNs of depth K grows at least doubly-exponentially in K.</p><p>In comparison, we next demonstrate that the equivalence classes induced by GA-MLPs are more coarsened. To see this, let's first consider the example where we take Ω = {I, Ã, Ã2 , ..., ÃK }, in which Ã = D −α AD −β with any α, β ∈ R is a generalization of the normalized adjacency matrix. From formula (3), by expanding the matrix product, we have</p><formula xml:id="formula_10">( Ãk ϕ(X)) i = (i1,...,i k )∈W k (G [i] ) d −α i d −(α+β) i1 ...d −(α+β) i k−1 d −β i k ϕ(X i k ) ,<label>(6)</label></formula><p>where we define</p><formula xml:id="formula_11">W k (G [i] ) = {(i 1 , ..., i k ) ⊆ V : A i,i1 , A i1,i2 , ..., A i k−1 ,i k &gt; 0}</formula><p>to be set of all walks of length k in the rooted graph G [i] starting from node i (an illustration is given in Figure <ref type="figure">2</ref>). Thus, the kth augmented feature of node i, ( Ãk ϕ(X)) i , is completely determined by the number of each "type" of walks in G [i] of length k, where the type of a walk, (i 1 , ..., i k ), is determined jointly by the degree multiset, {d i1 , ..., d i k−1 } as well as the degree and the node feature of the end node, d i k and X i k . Hence, to prove an upper bound on the total number of equivalence classes on E induced by such a GA-MLP, it is sufficient to upper-bound the total number of possibilities of assigning the counts of all types of walks in a rooted graph. This allows us to derive the following result, which we prove in Appendix H.</p><p>Proposition 4. Fix Ω = {I, Ã, Ã2 , ..., ÃK }, where Ã = D −α AD −β for some α, β ∈ R. Then the total number of equivalence classes in E induced by such GA-MLPs is poly-exponential in K.</p><p>Compared with Proposition 3, this shows that the number of equivalence classes on E induced by such GA-MLPs is exponentially smaller than that by GNNs. In addition, as the other side of the same coin, these results also indicate the complexity of these hypothesis classes. Building on the results Figure <ref type="figure">2</ref>: A pair of rooted graphs, G [1] (left) and G [1] (right), in which blue nodes have node feature 0 and green nodes have node feature 1. They belong to the same equivalence class induced by any GA-MLP with operators that only depend on the graph structure, but different equivalence classes induced by GNNs.</p><p>In particular, G [1] and</p><formula xml:id="formula_12">G [1] ∈ T 2,2,(1,1,3) (defined in Appendix I), and |W2(G [1] ; (1, 1))| = 1 whereas |W2(G [1] ; (1, 1))| = 0.</formula><p>in <ref type="bibr" target="#b9">Chen et al. (2019c;</ref><ref type="bibr">2020b)</ref> on the equivalence between distinguishing non-isomorphic graphs and approximating arbitrary permutation-invariant functions on graphs by neural networks, and by the definition of VC dimension <ref type="bibr" target="#b45">(Vapnik &amp; Chervonenkis, 1971;</ref><ref type="bibr" target="#b32">Mohri et al., 2018)</ref>, we conclude that Corollary 1. The VC dimension of all GNNs of K layers as functions on rooted graphs grows at least doubly-exponentially in K; Fixing α, β ∈ R, the VC dimension of all GA-MLPs with Ω = {I, Ã, Ã2 , ..., ÃK } as functions on rooted graphs is at most poly-exponential in K.</p><p>Meanwhile, for more general operators, we can show that the equivalence classes induced by GA-MLPs are coarser than those induced by GNNs at least under some measurements. For instance, the pair of rooted graphs in Figure <ref type="figure">2</ref> belong to the same equivalence class induced by any GA-MLP (as we prove in Appendix I) but different equivalence classes induced by GNNs. Rigorously, we characterize such a gap in expressive power by finding certain equivalence classes in E induced by GA-MLPs that intersect with many equivalence classes induced by GNNs. In particular, we have the following general result, which we prove in Appendix I: In essence, this result establishes that GA-MLP circuits can express fewer (exponentially fewer) functions than GNNs with equivalent receptive field. Taking a step further, we can find explicit functions on rooted graphs that can be approximated by GNNs but not GA-MLPs. In the framework that we have developed so far, this occurs when the image of each equivalence class in E induced by GNNs under this function contains a single value, whereas the image of some equivalence class in E induced by GA-MLPs contains multiple values. Inspired by the proofs of the results above, a natural candidate is the family of functions that count the number of walks of a particular type in the rooted graph. We can establish the following result, which we prove in Appendix J: Proposition 6. For any sequence of node features {x k } k∈N+ ⊆ X , consider the sequence of func-</p><formula xml:id="formula_13">tions f k (G [i] ) := |W k (G [i] ; (x 1 , ..., x k ))| on E.</formula><p>For all k ∈ N + , the image under f k of every equivalence class in E induced by depth-k GNNs contains a single value, while for any GA-MLP using equivariant linear operators that only depend on the graph topology, there exist exponentiallyin-k many equivalence classes in E induced by this GA-MLP whose image under f k contains exponentially-in-k many values.</p><p>In other words, there exist graph instances where the attributed-walk-counting-function f k takes different values, yet no GA-MLP model can predict them apart -and there are exponentially many of these instances as the number of hops increases. This suggests the possibility of lower-bounding the average approximation error for certain functions by GA-MLPs under various random graph families, which we leave for future work. </p><formula xml:id="formula_14">-MLP-A 1.23E-1 1.56E-1 1.75E-2 2.13E-2 GA-MLP-A+ 1.87E-2 6.44E-2 1.69E-2 2.13E-2 GA-MLP-Ã(1) 4.22E-1 5.79E-1 1.02E-1 1.58E-1 GA-MLP-Ã(1) + 4.00E-1 5.79E-1 1.12E-1 1.52E-1</formula><p>Table <ref type="table">2</ref>: MSE loss divided by label variance for counting attributed walks on the Cora graph and RRG. The models denoted as "+" contain twice as many powers of the operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>The baseline GA-MLP models we consider has operator family Ω = {I, A, ..., A K } for a certain K, and we call it GA-MLP-A. In Section 6.2 and 6.3, we also consider GA-MLPs with Ω = {I, Ã(1) , ..., ÃK</p><p>(1) } ( Ã( ) is defined in Appendix B), denoted as GA-MLP-Ã(1) . For the experiments in Section 6.3, due to the large K as well as the analogy with spectral methods <ref type="bibr" target="#b8">(Chen et al., 2019b)</ref>, we use instance normalization <ref type="bibr" target="#b44">(Ulyanov et al., 2016)</ref>. Further details are described in Appendix L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">NUMBER OF EQUIVALENCE CLASSES OF ROOTED GRAPHS</head><p>Motivated by Propositions 3 and 4, we numerically count the number of equivalence classes induced by GNNs and GA-MLPs among the rooted graphs found in actual graphs with node features removed. For depth-K GNNs, we implement a WL-like process with hash functions to map the depth-K egonet associated with each node to a string before comparing across nodes. For GA-MLP-A, we compare the augmented features of each egonet computed via (3). From the results in Table <ref type="table" target="#tab_0">1</ref>, we see that indeed, the number of equivalence classes induced by GA-MLP-A is smaller than that by GNNs, with the highest relative difference occurring at K = 2. The contrast is much more visible than their difference in the number of graph equivalence classes given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">COUNTING ATTRIBUTED WALKS</head><p>Motivated by Proposition 6, we test the ability of GNNs and GA-MLPs to count the number of walks of a particular type in synthetic data. We take graphs from the Cora dataset (with node features removed) as well as generate a random regular graph (RRG) with 1000 nodes and the node degree being 6. We assign node feature blue to all nodes with even index and node feature red to all nodes with odd index. The node feature is given by 2-dimensional one-hot encoding. On the Cora graph, a node i's label is given by the number of walks of the type blue− →blue− →blue starting from i. On the RRG, the label is given by the number of walks of the type blue− →blue− →blue− →blue starting from i. The number of nodes for training and testing is split as 1000/1708 for the Cora graph and 300/700 for the random regular graph. We test four GA-MLP models, two with as many powers of the operator as the walk length and the other two with twice as many operators, and compare their performances against that of the Graph Isomorphism Network (GIN), a GNN model that achieves the expressive power of the WL test <ref type="bibr" target="#b49">(Xu et al., 2019)</ref>. From Table <ref type="table">2</ref>, we see that GIN significantly outperforms GA-MLPs in both training and testing on both graphs, consistent with the theoretical result in Proposition 6 that GNNs can count attributed walks while GA-MLPs cannot. Thus, this points out an intuitive task that lies in the gap of expressive power between GNNs and GA-MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">COMMUNITY DETECTION ON STOCHASTIC BLOCK MODELS (SBM)</head><p>We use the task of community detection to illustrate another limitation of GA-MLP models: a lack of flexibility to learn the family of operators. SBM is a random graph model in which nodes are partitioned into underlying communities and each edge is drawn independently with a probability that only depends on whether the pair of nodes belong to the same community or not. The task of community detection is then to recover the community assignments from the connectivity pattern. We focus on binary (that is, having two underlying communities) SBM in the sparse regime, where it is known that the hardness of detecting communities is characterized by a signal-to-noise ratio (SNR) that is a function of the in-group and out-group connectivity <ref type="bibr" target="#b0">(Abbe, 2017)</ref>. We select 5 pairs of in-group and out-group connectivity, resulting in 5 different hardness levels of the task. Among all different approaches to community detection, spectral methods are particularly worth mentioning here, which usually aim at finding a certain eigenvector of a certain operator that is correlated with the community assignment, such as the second largest eigenvector of the adjacency matrix or the second smallest eigenvector of the Laplacian matrix or the Bethe Hessian matrix <ref type="bibr" target="#b23">(Krzakala et al., 2013)</ref>. In particular, the Bethe Hessian matrix is known to be asymptotically optimal in the hard regime, provided that a data-dependent parameter is known. Note that spectral methods bear close resemblance to GA-MLPs and GNNs. In particular, <ref type="bibr" target="#b8">Chen et al. (2019b)</ref> propose a spectral GNN (sGNN) model for community detection that can be viewed as a learnable generalization of power iterations on a collection of operators. Further details on Bethe Hessian and sGNN are provided in Appendix L. We first compare two variants of GA-MLP models: GA-MLP-A with K = 120, and GA-MLP-H with Ω generated from the Bethe Hessian matrix with the oracle data-dependent parameter also up to K = 120. From Figure <ref type="figure" target="#fig_3">3</ref>, we see that the latter consistently outperforms the former, indicating the importance of the choice of the operators for GA-MLPs. As reported in Appendix L, replacing A by Ã(1) yields no improvement in performance. Meanwhile, we also test a variant of sGNN that is only based on powers of the A and has the same receptive field as GA-MLP-A (further details given in Appendix L). We see that its performance is comparable to that of GA-MLP-H. Thus, this demonstrates a scenario in which GA-MLP with common choices of Ω do not work well, but there exists some choice of Ω that is a priori unknown, with which GA-MLP can achieve good performance. In contrast, a GNN model does not need to rely on the knowledge of such an oracle set of operators, demonstrating its superior capability of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We have studied the separation in terms of representation power between GNNs and a popular alternative that we coined GA-MLPs. This latter family is appealing due to its computational scalability and its conceptual simplicity, whereby the role of topology is reduced to creating 'augmented' node features then fed into a generic MLP. Our results show that while GA-MLPs can distinguish almost all non-isomorphic graphs, in terms of approximating node-level functions, there exists a gap growing exponentially-in-depth between GA-MLPs and GNNs in terms of the number of equivalence classes of nodes (or rooted graphs) they induce. Furthermore, we find a concrete class of functions that lie in this gap given by the counting of attributed walks. Moreover, through community detection, we demonstrate the lack of GA-MLP's ability to go beyond the fixed family of operators as compared to GNNs. In other words, GNNs possess an inherent ability to discover topological features through learnt diffusion operators, while GA-MLPs are limited to a fixed family of diffusions.</p><p>While we do not attempt to provide a decisive answer of whether GNNs or GA-MLPs should be preferred in practice, our theoretical framework and concrete examples help to understand their differences in expressive power and indicate the types of tasks in which a gap is more likely to be seen -those exploiting stronger structures among nodes like counting attributed walks, or those involving the learning of graph operators. That said, our results are purely on the representation side, and disregard optimization considerations; integrating the possible optimization counterparts is an important direction of future improvement. Finally, another open question is to better understand the links between GA-MLPs and spectral methods, and how this can help learning diffusion operators.</p><p>Below, we define a more general form of GA-MLP models that extend the use of equivariant linear operators for node feature propagation to that of general equivariant graph operators. We first define a map ω : R n×n × R n×d → R n×d , whose first input argument is always the adjacency matrix of a graph, A, and second input argument is a node feature matrix. We say the map satisfies equivariance to node permutations if ∀π ∈ S n , ∀Z ∈ R n×d , there is ω(π A, π Z) = π ω(A, Z). With a slight abuse of notations, we also use ω[A](Z) to denote ω(A, Z), thereby considering ω[A] : R n×d → R n×d as an operator on node features. If ω satisfies equivariance to node permutations as defined above, we then call ω[A] an equivariant graph operator. We can then define a general (nonlinear) GA-MLP model as</p><formula xml:id="formula_15">X = ω[A](X) Z = ρ( X)<label>(7)</label></formula><p>where ω is an equivariant graph operator, and ρ is a node-wise function.</p><p>It is easy to see that Proof : Similar to the proof of Proposition 5 given in Appendix I, we consider the set of full m-ary rooted trees of depth K, T m,K,X , that is all rooted trees of depth K in which the nodes have features belonging to the discrete set X ⊆ N and all non-leaf nodes have m children. T m,K,X is a subset of E, the space of all rooted graphs. Suppose f is a function represented by a general GA-MLP defined in (7) with an equivariant graph operator ω[A]. Let V k denote the set of nodes at depth k of T . Notice the following symmetry among nodes in each V k : if π is the permutation of a pair of nodes in some V k for 1 ≤ k ≤ K, then π A = A. By the equivariance property of ω, this implies that ω</p><formula xml:id="formula_16">Proposition 7. If ω[A](X) = m(A) • X, where m(•) = R n×n → R n×n is</formula><formula xml:id="formula_17">[A](π Z) =ω[π A](π Z) =π ω[A](Z)<label>(8)</label></formula><p>Let X denote the node feature matrix associated with T , and π T denote the rooted tree in T m,K,X with the same topology (i.e., also a full m-ary rooted tree) but node feature matrix π X. Then, since the root node is not permuted under π, we know that</p><formula xml:id="formula_18">f (T ) =ρ (ω[A](X) 1,: ) =ρ (π ω[A](X)) 1,: =ρ (ω[A](π X) 1,: ) =f (π T )<label>(9)</label></formula><p>This implies that for two trees T and</p><formula xml:id="formula_19">T ∈ T m,K,X , if ∀0 ≤ k ≤ K, ∀x ∈ X , they satisfy |W k (T ; x)| = |W k (T ; x)|, then f (T ) = f (T )</formula><p>for all such f 's, and hence T and T belong to the same equivalence class in E induced by GA-MLPs. Therefore, by the rest of the argument given in Proposition 5, Proposition 8 can be proven analogously for GA-MLPs with general equivariant graph operators.</p><p>Similarly, Proposition 6 can also be extended to Proposition 9. For any sequence of node features {x k } k∈N+ ⊆ X , consider the sequence of functions f k (G [i] ) := |W k (G [i] ; (x 1 , ..., x k ))| on E. For all k ∈ N + , the image under f k of every equivalence class in E induced by depth-k GNNs contains a single value, while for any GA-MLP using equivariant graph operators, there exist exponentially-in-k many equivalence classes in E induced by this GA-MLP whose image under f k contains exponentially-in-k many values.</p><p>The proof replies on the same extension as described above in the proof of Proposition 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXAMPLES OF EXISTING GA-MLP MODELS</head><p>For ∈ R, let Ā( ) = A + I, D( ) be the diagonal matrix with D( ),ii = j A ij + , and</p><formula xml:id="formula_20">Ã( ) = D−1/2 ( ) Ā( ) D−1/2 ( ) .</formula><p>• Simple Graph Convolution <ref type="bibr" target="#b47">(Wu et al., 2019)</ref>:</p><formula xml:id="formula_21">Ω(A) = {( Ã(1) ) K } for some K &gt; 0.</formula><p>In addition, ϕ is the identity function and ρ(H) = sof tmax(HW ) for some trainable weight matrix W .</p><p>• Graph Feature Network <ref type="bibr" target="#b7">(Chen et al., 2019a)</ref>:</p><formula xml:id="formula_22">Ω(A) = {I, D, Ã( ) , .</formula><p>.., ( Ã( ) ) K } for some K &gt; 0 and &gt; 0. In addition, ϕ is the identity function and ρ is an MLP.</p><p>• Scalable Inception Graph Networks <ref type="bibr" target="#b39">(Rossi et al., 2020)</ref>:</p><formula xml:id="formula_23">Ω(A) = {I} ∪ Ω 1 (A) ∪ Ω 2 (A) ∪ Ω 3 (A),</formula><p>where Ω 1 (A) is a family of simple / normalized adjacency matrices, Ω 2 (A) is a family of Personalized-PageRank-based adjacency matrices, and Ω 3 (A) is a family of triangle-based adjacency matrices. In addition, writing  Given a space of graphs, G, and a family F of functions mapping G to R, F induces an equivalence relation that we denote by G;F among graphs in G such that for</p><formula xml:id="formula_24">X = [ X1 , ..., XK ], there is Z = ρ( X) = σ 1 (σ 2 ([ X1 W 1 , ..., XK W K ])W out ),</formula><formula xml:id="formula_25">G 1 , G 2 ∈ G, G 1 G;F G 2 if and only if ∀f ∈ F, f (G 1 ) = f (G 2 ).</formula><p>For example, if F is powerful enough to distinguish all pairs of non-isomorphic graphs, then each equivalence class under G,F contains exactly one graph. Thus, by examining the number or sizes of the equivalence classes induced by different families of functions on G, we can evaluate their relative expressive power in a quantitative way.</p><p>Hence, we supplement the theoretical result of Proposition 2 with the following numerical results on five real-world datasets for graph-predictions. For graphs in each of the two real datasets, we remove their node features and count the total number of equivalence classes among them induced by depth-K GNNs (equivalent to K-iterations of the WL test, as discussed in Section 3.2) as well as GA-MLPs with Ω = {I, A, ..., A K } for different K's. We see from the results in Table <ref type="table" target="#tab_2">3</ref> that as soon as K ≥ 2, the number of equivalence classes induced by GNNs and the GA-MLPs are both close to the total number of graphs up to isomorphism, implying that they are indeed both able to distinguish almost all pairs of non-isomorphic graphs among the ones occurring in these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL NOTATIONS</head><p>For any k ∈ N + and any rooted graph</p><formula xml:id="formula_26">G [i] = (V, E, i) ∈ E, define W k (G [i] ) = {(i 1 , ..., i k ) ⊆ V : A i,i1 , A i1,i2 , ..., A i k−1 ,i k &gt; 0} (10) W k (G [i] ) = {(i 1 , ..., i k ) ∈ W k (G [i] ) : i = i 2 , i 1 = i 3 , ..., i k−3 = i k−1 , i k−2 = i k } (11)</formula><p>as the sets of walks and non-backtracking walks of length k in G [i] starting from the root node, respectively. Note that when G [i] is a rooted tree, a non-backtracking walk of length k is a path from the root node to a node at depth k. In addition, for 0 ≤ d 1 , ..., d k ≤ m and x 1 , ..., x k ∈ X , define the following subsets of W k (G [i] ):</p><formula xml:id="formula_27">W k G [i] ; (d 1 , ..., d k ), x k = {(i 1 , ..., i k ) ∈ W k (G [i] ) : {d i1 , ..., d i k } m = {d 1 , ..., d k } m , X i k = x k } (12) W k G [i] ; (x 1 , ..., x k ) = {(i 1 , ..., i k ) ∈ W k (G [i] ) : (X i1 , ..., X i k ) = (x 1 , ..., x k )} (13) W k (G [i] ; x k ) = {(i 1 , ..., i k ) ∈ W k (G [i] ) : X i k = x k } (14)</formula><p>We also define  [1] and G [1] . Right: the rooted aggregation tree that both G [1] and G [1] correspond to.</p><formula xml:id="formula_28">W k G [i] ; (d 1 , ..., d k ), x k , W k G [i] ; (x 1 , ..., x k ) and W k (G [i] ; x k ) similarly.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ILLUSTRATION OF ROOTED GRAPHS AND ROOTED AGGREGATION TREES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F PROOF OF PROPOSITION 2</head><p>With node features being identical in the random graphs, we take X ∈ R n×1 to be the all-1 vector. Thus,</p><formula xml:id="formula_29">(DX) i = d i ,<label>(15)</label></formula><p>and</p><formula xml:id="formula_30">(AD −α X) i = j∈N (i) d −α j . (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>Since ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_3">2</ref>) together can approximate arbitrary permutation-invariant functions on multisets <ref type="bibr" target="#b51">(Zaheer et al., 2017)</ref>, if two graphs G = (V, E) and G = (V , E ) cannot be distinguished by the GA-MLP with an operator family Ω that includes {D, AD −α } under any choice of its parameters, it means that the two multisets</p><formula xml:id="formula_32">{(d i , j∈N (i) d −α j ) : i ∈ V } m = {(d i , j ∈N (i ) d −α j ) : i ∈ V } m</formula><p>, and therefore both of the following hold:</p><formula xml:id="formula_33">{d i : i ∈ V } m ={d i : i ∈ V } m (17) { j∈N (i) d −α j : i ∈ V } m ={ j ∈N (i ) d −α j : i ∈ V } m (18)</formula><p>To see what this means, we need the two following lemmas. Lemma 1. Let S n be the set of all multisets consisting of at most n elements, all of which are integers between 0 and n. Consider the function h α (S) := u∈S u −α defined for multisets S.</p><formula xml:id="formula_34">If α &gt; log n log n−log(n−1) , h α is an injective function on S n .</formula><p>Proof of Lemma 1: For h α to be injective on S n , it suffices to require that ∀l ≤ n − 1, there is l −α &gt; n(l + 1) −α , for which it is sufficient to require that (n − 1)</p><formula xml:id="formula_35">−α &gt; n −α+1 , or α &gt; log n log n−log(n−1) .</formula><p>Lemma 2 <ref type="bibr" target="#b2">(Babai et al. (1980)</ref>, Theorem 1). Consider the space of graphs with n vertices, G n . There is a subset K n ⊆ G n that contains almost all such graphs (i.e. the fraction converges to 1 as n → ∞) such that the following algorithm yields a unique identifier for every graph G = (V, E) ∈ K n :</p><p>Algorithm 1: Set r = [3 log n/ log 2], and let d(G) be the degree of the node in V with the rth largest degree; For each node i in G, define the multiset</p><formula xml:id="formula_36">γ i = {d j : j ∈ N (i), d j &gt; d(G)} m ;</formula><p>Finally define a multiset associated with G, F (G) = {γ i : i ∈ V } m , which is the output of the algorithm.</p><p>In other words, ∀G, G ∈ K n , G and G are isomorphic if and only if F (G) = F (G ) as multisets.</p><p>In particular, we can choose K n such that the top r node degrees of every graph in K n are distinct.</p><p>Based on these lemmas, we will show that when α &gt; log n log n−log(n−1) and for G, G ∈ K n , ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>) together imply that G is isomorphic to G . To see this, suppose that ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>) hold. Because of (17), we know that G and G share the same degree sequence, and hence d(G) = d(G ). Because of (18), we know that there is a bijective map σ from V to V such that ∀i ∈ V , j∈N (i)</p><formula xml:id="formula_37">d −α j = j ∈N (i ) d −α j ,<label>(19)</label></formula><p>which, by Lemma 1, implies that {d j : j ∈ N (i)} m = {d j : j ∈ N (i )} m . We then have</p><formula xml:id="formula_38">γ i = {d j : j ∈ N (i)} m = {d j : j ∈ N (i)} m ∩ ( d(G), ∞) = {d j : j ∈ N (i )} m ∩ ( d(G ), ∞) = γ i ,</formula><p>and therefore F (G) = F (G ), which implies that G and G are isomorphic by Lemma 2. This shows a contradiction. Therefore, if G, G ∈ K n are not isomorphic, then it cannot be the case that both ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>) hold, and hence there exists a choice of parameters for the GA-MLP with {D, AD −α } ⊆ Ω that makes it return different outputs when applied to G and G . This proves Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PROOF OF PROPOSITION 3</head><p>As argued in the main text, to estimate the number of equivalence classes on E induced by GNNs, we need to estimate the number of possible rooted aggregation trees. In particular, to lower-bound the number of equivalence classes on E induced by GNNs, we only need to focus on a subset of all possible rooted aggregation trees, namely those in which every node has exactly m children. Letting T A m,K,X denote the set of all rooted aggregation trees of depth K in which each non-leaf node has degree exactly m and the node features belong to X , we will first prove the following lemma:</p><formula xml:id="formula_39">Lemma 3. If |X | ≥ 2, then |T A m,K,X | ≥ (m − 1) (2 K −1) .</formula><p>Note that a rooted aggregation tree needs to satisfy the constraint that each of its node must have its parent's feature equal to one of its children's feature, and so this lower bound is not as straightforward to prove as lower-bounding the total number of rooted subtrees. As argued above, this will allow us to derive Proposition 3.</p><p>Proof of Lemma 3: Define B := {0, 1}. Since |X | ≥ 2, we assume without loss of generality that B ⊆ X . To prove a lower-bound on the cardinality of T A m,K,X , it suffices to restrict our attention to its subset, T A m,K := T A m,K,B , where all nodes have feature either 0 or 1. Furthermore, it is sufficient to restrict our attention to the subset of T A m,K which contain all 2 K possible types of paths of length K from the root to the leaves. Formally, with W k defined as in Appendix D, we let</p><formula xml:id="formula_40">T A m,K = {T ∈ T A m,K : ∀x 1 , ..., x K ∈ B, W K (T ; (x 1 , ..., x K )) ≥ 1} ,<label>(20)</label></formula><p>and it is sufficient to prove a lower bound on the cardinality of T A m,K . Define P k = {(x 1 , ..., x k ) : x 1 , ..., x k ∈ B} to be the set of all binary k-tuples. By the definition of (20), we know that ∀τ ∈ P K , |W K (T ; τ )| ≥ 1. This means that ∀τ ∈ P K , there exists at least one leaf node in T such that the path from the root node to this node consists of a sequence of nodes with features exactly as given by τ . We call any such node a node under τ .</p><p>We show such a lower bound on the cardinality of T A m,K inductively. For the base case, we know that T A m,1 consists of all binary-featured depth-1 rooted trees with at least 1 leaf node of feature 0 and 1 leaf node of feature 1, and hence T A m,1 = 2(m − 1). Next, we consider the inductive step. For every K ≥ 1 and every T ∈ T A m,K , we can generate rooted aggregation trees belonging to T ∈ T A m,K+1 by assigning children of feature 0 or 1 to the leaf nodes of T . First note that, from two non-isomorphic rooted aggregation trees T and T ∈ T A m,K , we obtain non-isomorphic rooted aggregation trees in T A m,K+1 in this way. Moreover, as we will show next, for every T ∈ T A m,K , we can lower-bound the number of distinct rooted aggregation trees belonging to T A m,K+1 obtained from T in this way.</p><p>There are many choices to assign the children. To get a lower-bound on the cardinality of T A m,K+1 , we only need to consider a subset of these choices of assignments, namely, those that assign the same number of children with feature 0 to every node under the same τ ∈ P K . Thus, we let qK+1,τ denote the number of children of feature 0 assigned to every node in τ . Due to the constraint that each node in the rooted aggregation tree must have its parent's feature equal to one of its children's feature, not all choices of {q K+1,τ } τ ∈P K lead to legitimate rooted aggregation trees. Nonetheless, when restricting to the choices where ∀τ ∈ P K , 1 ≤ qK+1,τ ≤ m − 1, we see that every leaf node of T gets assigned at least one child of feature 0 and another child of feature 1, thereby satisfying the constraint above whether its parent has feature 0 or 1. Moreover, for such choices, the rooted aggregation tree of depth K+1 obtained in this way contains all 2 K+1 possible paths of length K+1, and therefore belongs to T A m,K+1 . Hence, it remains to show a lower bound on how many distinct trees in T A m,K+1 can be obtained in this way from each T . Since for τ, τ ∈ P K such that τ = τ , a node under τ is distinguishable from a node under τ , we see that every legitimate choice of the tuple of 2 K integers, (q K+1,τ ) τ ∈P K , leads to a distinct rooted aggregation tree of depth K + 1, and there are (m − 1) 2 K of these choices. Hence, we have derived that</p><formula xml:id="formula_41">| T A m,K+1 | ≥ (m − 1) 2 K | T A m,K |, and therefore | T A m,K | ≥ (m − 1) K k=1 2 k = (m − 1) 2 K −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H PROOF OF PROPOSITION 4</head><p>According to the formula (3), by expanding the matrix product, we have</p><formula xml:id="formula_42">( Ãk ϕ(X)) i = (i1,...,i k )∈W k (G [i] ) d −α i d −(α+β) i1 ...d −(α+β) i k−1 d −β i k ϕ(X i k ) =d −α i { d1,..., dt−1}m, dk ,x (i,i1,...,i k )∈ W k (G [i] ;{ d1,..., dk−1 }m, dk ,x) ( d1 ... dk−1 ) −(α+β) dk ϕ(x) =d −α i { d1,..., dk−1 }m, dk ,x ( d1 ... dk−1 ) −(α+β) dk ϕ(x) W k (G [i] ; { d1 , ..., dt−1 } m , dk , x) ,<label>(21)</label></formula><p>with W k (G [i] ; { d1 , ..., dt−1 } m , dk , x) defined in Appendix D. Hence, for two different nodes i in G and i in G (G and G can be the same graph), the node-wise outputs of the GA-MLP at i and i will be identical if the rooted graphs G i and G</p><formula xml:id="formula_43">[i ] satisfy W k (G [i] ; { d1 , ..., dk−1 } m , dk , x) = W k (G [i ] ; { d1 , ..., dk−1 } m , dk , x)</formula><p>for every combination of choices on the multiset { d1 , ..., dk−1 } m , the integer dk and the node feature x, under the constraints of d1 , ..., dk ≤ m and x ∈ X . Note that there are at most k+m−2 m−1</p><formula xml:id="formula_44">≤ (k + m − 2) m−1 possible choices of the multiset { d1 , ..., dk−1 } m , m choices of dk and |X | choices of x, thereby allowing at most |X |m(k + m − 2) m−1 possible choices. Because of the constraint { d1,..., dk−1 }m, dk ,x W k (G [i] K ; { d1 , ..., dt−1 } m , dk , x) = |W k (G [i] )| ≤ m k ,<label>(22)</label></formula><p>We see that the total number of equivalence classes on E induced by such a GA-MLP is upperbounded by</p><formula xml:id="formula_45">m k +|X |m(k+m−2) m−1 −1 |X |m(k+m−2) m−1</formula><p>, which is on the order of O(m k m ) with k growing and m bounded. Finally, since the total number of equivalence classes induced by multiple operators can be upper-bounded by the product of the number of equivalence classes induced by each operator separately, we derive the proposition as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I PROOF OF PROPOSITION 5</head><p>Consider the set of full m-ary rooted trees of depth K, T m,K,X , that is all rooted trees of depth K in which the nodes have features belonging to the discrete set X ⊆ N and all non-leaf nodes have m children. T m,K,X is a subset of E, the space of all rooted graphs. If f is a function represented by a GA-MLP using operators of at most K-hop, then for T ∈ T m,K,X , we can write</p><formula xml:id="formula_46">f (T ) = ρ( j∈V a j X j ) ,<label>(23)</label></formula><p>where we denote the node set of T by V and the vectors a j 's depend only on the topological relationship between j and the root node. Let V k denote the set of nodes at depth k of T . By the assumption that the operators depend only on the graph topology, and thanks to the topological symmetry of such full m-ary trees among all nodes on the same depth, we have that ∀1 ≤ k ≤ K and ∀j, j ∈ V k , there is a j = a j =: a [k] . Thus, we can write</p><formula xml:id="formula_47">f (T ) =ρ( 0≤k≤K j∈V k a [k] φ(X j )) =ρ( 0≤k≤K x∈X ā[k],x |W k (T ; x)|)<label>(24)</label></formula><p>for some other set of coefficients āV k ,x 's, and where W k (T ; x) is defined in Appendix D. In other words, for two trees T and</p><formula xml:id="formula_48">T ∈ T m,K,X , if ∀0 ≤ k ≤ K, ∀x ∈ X , they satisfy |W k (T ; x)| = |W k (T ; x)|, then f (T ) = f (T )</formula><p>for all such f 's, and hence T and T belong to the same equivalence class in E induced by GA-MLPs. Thus, for a certain subset of these equivalence classes, we can lower-bound the number of equivalence classes in E induced by GNNs that they intersect by lower-bounding the number of distinct trees in T m,K,X that they contain, because GNNs are able to distinguish non-isomorphic rooted subtrees. In particular, as a lower-bound is sufficient, we restrict attention to the subset of those trees with node features either 0 or 1, that is, trees belonging to T m,K := T m,K,B , with B := {0, 1}.</p><p>In a rooted tree T , W k (T ; x) gives the total number of nodes with feature x at depth k. For integers q 0 , q 1 , ..., q K such that 0 ≤ q k ≤ m k , ∀k ≤ K, define</p><formula xml:id="formula_49">T m,K,(q0,q1,...,q K ) = {T ∈ T m,K : ∀k ≤ K, |W k (T ; 0)| = q k } ,<label>(25)</label></formula><p>that is, the subset of trees whose per-level-node-counts, {|W k (T ; 0)|} k≤K (and therefore {|W k (T ; x)|} k≤K,x∈B ) are given by the tuple (q 0 , q 1 , ..., q k ). From the argument above, all trees in the same T m,K,(q0,q1,...,q K ) belong to the same equivalence class in E induced by GA-MLPs. On the other hand, every pair of non-isomorphic trees belong to different equivalence class in E induced by GNNs. Thus, to show Proposition 5, it is sufficient to find sufficiently many choices of (q 0 , q 1 , ..., q K ) such that T m,K,(q0,q1,...,q K ) contains sufficiently many non-isomorphic trees. Specifically, we will show the following: Lemma 4. For all integers q 0 , q 1 , ..., q K such that ∀2 ≤ k ≤ K,</p><formula xml:id="formula_50">2 k − 2 k−2 ≤ q k ≤ 1 2 m k ,<label>(26)</label></formula><p>there is |T m,K,(q0,q1,...,q</p><formula xml:id="formula_51">K ) | ≥ 2 2 K−1 −1 (27)</formula><p>Proof of Lemma 4: To prove such a lower bound on the cardinality of T m,K,(q0,q1,...,q K ) , it is sufficient to prove a lower bound on the cardinality of its subset, Tm,K,(q0,q1,...,q K ) = {T ∈ T m,K,(q0,q1,...,q K ) : ∀x 1 , ..., x K ∈ B, W K (T ; (x 1 , ..., x K )) ≥ 1} .</p><p>(28) A similar construction is involved in the proof of Lemma 3 in Appendix G. Then, we will prove this lemma by induction on K. For the base cases, it is obvious that | Tm,0,(0) | = | Tm,0,(1) | = 1, and | Tm,1,(0,0)</p><formula xml:id="formula_52">| = | Tm,1,(0,1) | = | Tm,1,(0,2) | = | Tm,1,(1,0) | = | Tm,1,(1,1) | = | Tm,1,(1,2) | = 1.</formula><p>We next prove the inductive hypothesis that, for K ≥ 2 and when q 0 , q 1 , ..., q K satisfying (26), there is</p><formula xml:id="formula_53">| Tm,K,(q0,q1,...,q K ) | ≥ 2 2 K−2 • | Tm,K−1,(q0,q1,...,q K−1 ) | . (<label>29</label></formula><formula xml:id="formula_54">)</formula><p>To see this, we will next show that ∀T ∈ Tm,K−1,(q0,q1,...,q K−1 ) , we can generate enough number of depth-K trees in Tm,K,(q0,q1,...,q K ) by appending children to the leaf nodes of T . Since any two depth-K trees generated from two non-isomorphic depth-(K − 1) trees in this way are nonisomorphic, this will allow us to lower-bound the total number of trees in Tm,K,(q0,q1,...,q K ) .</p><p>Consider the set of binary k-tuples, P k = {(x 1 , ..., x k ) : x 1 , ..., x k ∈ B}, of cardinality 2 k . As T ∈ Tm,K−1,(q0,q1,...,q K−1 ) , we know that ∀τ ∈ P K−1 , |W K−1 (T ; τ )| ≥ 1. This means that ∀τ ∈ P k , there exists at least one leaf node in T such that the path from the root node to this node consists of a sequence of nodes with features given by τ . We call any such node a node under τ . The total number of the children of all nodes under τ is thus m•|W K−1 (T ; τ )| ≥ m. Thus, the total number of children with feature 0 of all nodes under τ is bounded between 0 and m•|W K−1 (T ; τ )|. Conversely, for any 2 K−1 -tuple of non-negative integers, (q K,τ ) τ ∈P K−1 , which satisfy ∀τ ∈ P K−1 , 1 ≤ qK,τ ≤ m • |W K−1 (T ; τ )| − 1 can be "realized" by at least some depth-K tree T obtained by appending children to the leaf nodes of T , in the sense that ∀τ = (x 1 , ..., x K−1 ) ∈ P K−1 , there is W K (T ; (x 1 , ..., x K−1 , 0)) = qK,τ and W K (T ; (x 1 , ..., x K−1 , 1)) = m • |W K−1 (T ; τ )| − qK,τ , and hence T ∈ T m,K,(q0,...,q K−1 ,q K ) , with qK = τ ∈P K−1 qK,τ . Because of the requirement that 1 ≤ qK,τ ≤ m • |W K−1 (T ; τ )| − 1, we further have that ∀τ ∈ P K , W K (T , τ ) ≥ 1, which implies that T ∈ Tm,K,(q0,...,q K−1 ,q K ) . Therefore, for some fixed q K , in order to lower-bound the cardinality of Tm,K,(q0,...,q K ) by that of Tm,K−1,(q0,...,q K−1 ) , it is sufficient to show a lower bound (which is uniform for all T ∈ Tm,K−1,(q0,q1,...,q K−1 ) ) on the number of 2 K−1 -tuples, (q K,τ ) τ ∈P K−1 , which satisfy</p><formula xml:id="formula_55">q K = τ ∈P K−1 q K,τ ∀τ ∈ P K−1 , 1 ≤q K,τ ≤ m • |W K−1 (T ; τ )| − 1<label>(30)</label></formula><p>A simple bound can be obtained in the following way. For every such T , we sort the 2 K−1 -tuples in P K−1 in ascending order of |W K−1 (T ; •)|, and define P K−1,T to be the subset of the first 2 K−2 of these elements according to this order. Thus, for example,</p><formula xml:id="formula_56">∀τ ∈ P K−1,T , ∀τ ∈ P K−1 \ P K−1,T , there is |W K−1 (T ; τ )| ≤ |W K−1 (T ; τ )|.</formula><p>As a consequence, we have</p><formula xml:id="formula_57">τ ∈P K−1,T |W K−1 (T ; τ )| ≤ τ ∈P K−1 \P K−1,T |W K−1 (T ; τ )|,</formula><p>and so</p><formula xml:id="formula_58">τ ∈P K−1,T |W K−1 (T ; τ )| ≤ 1 2 τ ∈P K−1 |W K−1 (T ; τ )| = 1 2 m K−1 ≤ τ ∈P K−1 \P K−1,T |W K−1 (T ; τ )|.</formula><p>Lemma 5. Let K ≥ 2 and q K satisfy (26). Then for all choices of the 2 K−2 -tuple of integers, (q K,τ ) τ ∈P K−1,T , such that ∀τ ∈ P K−1,T , qK,τ = 1 or 2, we can complete it into at least one 2 K−1 -tuple of integers, (q K,τ ) τ ∈P K−1 , which satisfy (30).</p><p>Proof of Lemma 5: For any such 2 K−2 -tuple, (q K,τ ) τ ∈P K−1,T , in order to satisfy the constraints of (30), it is sufficient to find another 2 K−2 integers, (q K,τ ) τ ∈P K−1 \P K−1,T , which satisfy</p><formula xml:id="formula_59">τ ∈P K−1 \P K−1,T qK,τ =q K − τ ∈P K−1,T qK,τ<label>(31)</label></formula><formula xml:id="formula_60">∀τ ∈ P K−1 \ P K−1,T , 1 ≤q K,τ ≤ m • |W K−1 (T ; τ )| − 1<label>(32)</label></formula><p>On one hand, since qK,τ = 1 or 2, ∀τ ∈ P K−1,T , there is q</p><formula xml:id="formula_61">K − 2 K−1 ≤ q K − τ ∈P K−1,T qK,τ ≤ q K − 2 K−2 .</formula><p>On the other hand, with the only other constraint being (32), it is possible to find</p><formula xml:id="formula_62">(q K,τ ) τ ∈P K−1 \P K−1,T such that τ ∈P K−1 \P K−1,T equals any integer between 2 K−2 and m • τ ∈P K−1 \P K−1,T |W K−1 (T ; τ )| − 2 K−2</formula><p>, and hence any integer between 2 K−2 and</p><formula xml:id="formula_63">m • 1 2 m K−1 − 2 K−2 = 1 2 m K − 2 K−2 . Hence, as long as 2 K − 2 K−2 ≤ q K ≤ 1 2 m K ,</formula><p>which is the assumption of (26), Lemma 5 holds true.</p><p>Lemma 5 implies that ∀T ∈ Tm,K−1,(q0,q1,...,q K−1 ) , there are at least 2 2 K−2 distinct choices of 2 K−1 -tuples (q ) τ ∈P K−1 that satisfy the constraint of (30), and hence at least 2 2 K−2 nonisomorphic trees in Tm,K,(q0,q1,...,q K−1 ,q K ) obtained by appending children to the leaf nodes of T . This proves the inductive hypothesis. Hence, we have</p><formula xml:id="formula_64">| Tm,K,(q0,q1,...,q K ) | ≥ K k=2 2 2 k−2 = 2 2 K−1 −1 ,<label>(33)</label></formula><p>which implies Lemma 4.</p><p>Since m ≥ 2 by assumption, 1 2 m K − (2 K − 2 K−2 ) grows exponentially in K. This proves Proposition 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J PROOF OF PROPOSITION 6</head><p>Since the number of walks of a particular type that has length at most k is completely determined by the rooted aggregation tree structure of depth k, it is straightforward to see that all egonets in the same equivalence class induced by k iterations of WL (and therefore GNNs of depth k), which yield the same rooted aggregation tree, will get mapped to the same value by f k .</p><p>For the second part of the claim pertaining to GA-MLPs, we assume for simplicity that X = B = {0, 1}, as the extension to the general case is straightforward but demanding heavier notations. Following the strategy in the proof of Proposition 5, it is sufficient to find exponentially-in-k many choices of the tuple (q 0 , q 1 , ..., q k ), with 0 ≤ q k ≤ m k , such that the image of T m,k,(q0,q1,...,q k ) (as defined in (25)) under f k contains exponentially-in-k many values.</p><p>To make it simpler to refer to different nodes in the tree, we index each node in a rooted tree by a tuple of natural numbers: for example, the index-tuple [1, 3, 2] refers to the node at depth 3 that is the second children of the third children of the first children of the root. Since there is no intrinsic ordering to different children of the same node, there exist multiple ways of consistently indexing the nodes in a rooted tree. However, to specify a tree, it suffices to specify the node features of all nodes under one such way of indexing.</p><p>Given x 1 , ..., x k ∈ B, we consider a set of depth-k full m-ary trees that satisfy the following: x</p><formula xml:id="formula_65">∀k ≤ k − 1 and l 1 , ..., l k ∈ [m], x [l1,l2,...,l k ] = x k if l 1 = 1 and ¬x k if l 1 &gt; 1. Note that these trees satisfy, for k ≤ k − 1, q k = m k −1 if x k = 0 and q k = (m − 1)m k −1 if x k = 1. Thus, ∀l 2 , ..., l k ∈ [m], the node [1, l 2 , ..., l k ] is under the path τ = (x 1 , ..., x k ) if and only if</formula><formula xml:id="formula_66">[1,l2,...,l k ] = x k , whereas for l 1 &gt; 1, the [l 1 , l 2 , ..., l k ] is not under the path τ regardless of the feature of [1, l 2 , ..., l k ]. Therefore, f k (G [i] ) = |W k (G [i] ; (x 1 , ..., x k ))| equals the number of node of feature x k among the set of m k−1 nodes, {[1, l 2 , ..., l k ]} l2,...,l k ∈[m] . Hence, if for k ≤ k − 1, we set q k = m k −1 if x k = 0 and q k = (m − 1)m k −1 if x k = 1</formula><p>, then choosing any q k between m k−1 and (m − 1)m k−1 , we have that for every integer between 0 and m k−1 , there exists a tree T in T m,k,(q0,...,q k ) such that f k (T ) equals this integer. Since there are (m − 2)m k−1 choices of q k (and therefore the tuple (q 0 , ..., q k )) and m k−1 + 1 values in the image of T m,k,(q0,...,q k ) under f k , this proves the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K PROOF OF PROPOSITION 1</head><p>We will first prove that the pair of graphs cannot be distinguished by any GA-MLP with Ω ⊆ {A k } k∈N . Let X and A, X and A be the node feature vector and adjacency matrix of the two graphs, G and G , respectively. As these two graphs both contain 14 nodes that have identical features, we have X, X ∈ R 14×1 both being the all-1 vector. Moreover, ∀i ∈ [14],</p><formula xml:id="formula_67">(A k X) i = w k (i) , ((A ) k (X )) i = w k (i)<label>(34)</label></formula><p>where we use w k (i) and w k (i) to denote the numbers of walks (allowing backtracking) of length k starting from node i in graphs G and G , respectively. Thus, to show that any GA-MLP with Ω ⊆ {A k } k∈N necessarily returns the same output on G and G , it is sufficient to show that ∀k ∈ N, A k X = (A ) k (X ), and therefore sufficient to show that ∀k ∈ N and ∀i ∈ [14], there is w k (i) = w k (i). In fact, we will prove the following lemma: Lemma 6. ∀k ∈ N,</p><formula xml:id="formula_68">w k (i) =w k (i), ∀i ∈ [14] (35) w k (1) =w k (2) (36) w k (3) + w k (9) =w k (6) + w k (8) (37) w k (5) + w k (7) =w k (4) + w k (10)<label>(38)</label></formula><p>Proof of Lemma 6: We prove this lemma by induction. For the base case, we have that w 0 (i) = w 0 (i), ∀i ∈ [14]. Next, we assume that ( <ref type="formula">35</ref>) -( <ref type="formula" target="#formula_68">38</ref>) hold for some k ∈ N and prove it for k + 1. A first property to note is that ∀k ∈ N, w k+1 (i) = j∈N (i) w k (j) and w k+1 (i) = j∈N (i) w k (j), where we use N (i) and N (i) to denote the neighborhood of i in G and G , respectively.</p><p>To show (35) for k + 1, we look at each node separately: </p><formula xml:id="formula_69">• i = 1 w k+1<label>(</label></formula><p>This proves the inductive hypothethis for k + 1.</p><p>We next argue that these two graphs can be distinguished by WL in 2 iterations. This is because 2 iterations of WL distinguish neighborhoods up to the depth-2 rooted aggregation trees (as will be defined in Section 5), and it is not hard to see that the multiset of depth-2 rooted aggregation trees are different for the two graphs. Note that a depth-2 rooted subtree can be represented by the multiset of the degrees of the depth-1 children. Then for example, the depth-2 rooted aggregation trees of 1 and 2 in G are both {3, 2, 2, 1} m , while their rooted aggregation trees in G are {2, 2, 2, 2} m and {3, 3, 1, 1} m , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L EXPERIMENT DETAILS</head><p>L.1 SPECIFIC ARCHITECTURES</p><p>In Section 6, we show experiments on several tasks to confirm our theoretical results with several related architectures. Here are some explanations for them:</p><p>• GIN: Graph Isomorphism Networks proposed by <ref type="bibr" target="#b49">Xu et al. (2019)</ref>. In our experiment of counting attributed walks, we take the depth of GIN as same as the depth of target walks. The number of hidden dimensions is searched in {8, 16, 32, 64, 256}. The model is trained with the Adam optimizer (Kingma &amp; Ba, 2014) with learning rate selected from {0.1, 0.02, 0.01, 0.005, 0.001}. We also train a variant with Jumping Knowledge <ref type="bibr" target="#b48">(Xu et al., 2018)</ref>.</p><p>• sGNN: Spectral GNN proposed by <ref type="bibr" target="#b8">Chen et al. (2019b)</ref>, which can be viewed as a learnable generalization of power iterations on a collection of operators. While the best performing variant utilizes the non-backtracking operator on the line graph, for a fairer comparison with GA-MLPs, we choose a variant with the base collection of operators being {I, A, A 2 } on each layer and depth 60, which then has the same receptive field as the chosen GA-MLP models. The model is trained with the Adam optimizer with learning rate selected from {0.001, 0.002, 0.004}.</p><p>• GA-MLP: a multilayer perceptron following graph augmented features. For counting attributed walks, we choose the operators from {I, Āk { } }. The number of hidden dimensions is searched in {8, 32, 64, 256}. We take the highest order of operators as the twice depth of target walks at most. For comminity detection, we choose the operators from {I, Āk { } , Hk } where H is induced from the Bethe Hessian matrix H. The highest order of operators is searched in {30, 60, 120}. The number of hidden dimensions is searched in {10, 20}. On both tasks, the model is trained with the Adam optimizer (Kingma &amp; Ba, 2014) with learning rate selected from {0.1, 0.02, 0.01, 0.005, 0.001, 0.0001}. Additionally, we use Batch Normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref> in community detection after propagating through each operator, following the normalization strategy from <ref type="bibr" target="#b8">Chen et al. (2019b)</ref>. We choose ϕ to be the identity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.2 BETHE HESSIAN</head><p>The Bethe Hessian matrix is defined as H(r) := (r 2 − 1)I − rA + D.</p><p>with r being a flexible parameter. In SBM, an optimal choice is r c = √ c, where c is the average degree. Spectral clustering <ref type="bibr" target="#b40">(Saade et al., 2014)</ref> can be performed by computing the eigenvectors associated with the negative eigenvalues of H(r c ) to get clustering information in assortative binary stochastic block model, which is the scenario we consider. In order to utilize power iterations for eigenvector extraction, we induce a new matrix H as H := κI − H(r c ), so that the smallest eigenvalues of H become the largest eigenvalues of H. We choose κ = 8 in our experiments. For GA-MLP-H, we then let Ω = {I, H, ..., HK }.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A pair of graphs that can be distinguished by 2 iterations of the WL test but not by GA-MLPs with Ω ⊆ {A k : k ∈ N}, as proved in Appendix K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 5 .</head><label>5</label><figDesc>If Ω is any family of equivariant linear operators on the graph that only depend on the graph topology of at most K hops, then there exist exponentially-in-K many equivalence classes in E induced by the GA-MLPs with Ω, each of which intersects with doubly-exponentiallyin-K many equivalence classes in E induced by depth-K GNNs, assuming that |X | ≥ 2 and m ≥ 3. Conversely, in constrast, if Ω = {I, Ã, Ã2 , ..., ÃK }, in which Ã = D −α AD −β with any α, β ∈ R, then each equivalence class in E induced by depth-(K + 1) GNNs is contained in one equivalence class induced by the GA-MLPs with Ω.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Community detection on binary SBM with 5 choices of in-and out-group connectivities, each yielding to a different SNR. Higher overlap means better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>an entry-wise function or matrix product or compositions thereof, then ω[A] is an equivariant graph operator. A.1 EXTENDING THE PROOF OF PROPOSITION 5 AND 6 TO GENERAL GA-MLPS An extension of the first half of Proposition 5 is Proposition 8. If ω[A] is an equivariant graph operator, then there exist exponentially-in-K many equivalence classes in E induced by the general GA-MLPs with ω[A], each of which intersects with doubly-exponentially-in-K many equivalence classes in E induced by depth-K GNNs, assuming that |X | ≥ 2 and m ≥ 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: An illustration of rooted graphs and rooted aggregation trees. Left: a pair of graphs, and G . Center: the rooted graphs of 1 in G and G , G[1] and G[1] . Right: the rooted aggregation tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A pair of graphs with identical node features, G (left) and G (right), which can be distinguished by 2 iterations of the WL test but not by the GA-MLP with Ω ⊆ {A k } k∈N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>www</head><label></label><figDesc>1) =w k (3) + w k (5) + w k (7) + w k (9) =w k (5) + w k (6) + w k (7) + w k (8)=w k (5) + w k (6) + w k (7) + w k (8) =w k+1 (1) (39) • i = 2 w k+1 (2) =w k (4) + w k (6) + w k (8) + w k (10) =w k (3) + w k (4) + w k (9) + w k (10) =w k (3) + w k (4) + w k (9) + w k k+1 (3) =w k (1) + w k (11) + w k (12) =w k (2) + w k (11) + w k (12) =w k (2) + w k (11) + w k k+1 (4) =w k (2) + w k (13) + w k (14) =w k (2) + w k (13) + w k (14) =w k+1 (4)(42)• i = 5w k+1 (5) =w k (1) + w k (13) =w k (1) + w k k+1 (7) =w k (1) + w k (13) =w k (1) + w k (13) =w k+1 (7) w k+1 (10) =w k (2) =w k (2) =w k+1 (10)(48)• i ∈ {11, ..., 14} For each of these i's, N (i) = N (i). Therefore, (36) -(38) at k + 1,w k+1 (1) =w k (3) + w k (5) + w k (7) + w k (9) =w k (4) + w k (6) + w k (8) + w k (10) =w k+1 (2)(50)w k+1 (3) + w k+1 (9) =2w k (1) + w k (11) + w k (12) =2w k (2) + w k (11) + w k (12) =w k+1 (6) + w k+1 (8)(51)w k+1 (5) + w k+1 (7) =2w k (1) + w k (13) + w k (14) =2w k (2) + w k (13) + w k (14) =w k+1 (4) + w k+1 (10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>L. 3</head><label>3</label><figDesc>RESULTS FOR GA-MLP-Ã(1)IN COMMUNITY DETECTION   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The number of equivalence classes of rooted graphs induced by GNN and GA-MLP on node classification datasets with node features removed.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell><cell>Cora</cell><cell>RRG</cell></row><row><cell># Nodes K 1 2</cell><cell cols="6">2708 GNN GA-MLP GNN GA-MLP GNN GA-MLP 3327 19717 37 37 31 31 82 82 1589 756 984 506 8059 3762</cell><cell>Model GIN GA</cell><cell>Train 3.98E-6 9.72E-7 3.39E-5 2.61E-4 Test Train Test</cell></row><row><cell>3</cell><cell>2301</cell><cell>2158</cell><cell>1855</cell><cell>1550</cell><cell>12814</cell><cell>12014</cell></row><row><cell>4</cell><cell>2363</cell><cell>2359</cell><cell>2074</cell><cell>2019</cell><cell>12990</cell><cell>12979</cell></row><row><cell>5</cell><cell>2365</cell><cell>2365</cell><cell>2122</cell><cell>2115</cell><cell>12998</cell><cell>12998</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The number of equivalence classes of graphs induced by GNN and GA-MLP on real datasets with node features removed. The last row gives the ground-truth number of isomorphism classes of graphs computed from the implementation of<ref type="bibr" target="#b17">Ivanov et al. (2019)</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results for community detection on binary SBM by GA-MLP-Ã(1)</figDesc><table><row><cell>Rank of hardness</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Overlap</cell><cell cols="5">0.128 0.164 0.262 0.707 0.563</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We focus on linear graph operators here, while an extension of the definition as well as some of our results to general graph operators is given Appendix A.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random graph isomorphism</title>
		<author>
			<persName><forename type="first">László</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIaM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02133</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Are powerful graph neural nets necessary? a dissection on graph classification</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04579</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019c</date>
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10413</idno>
		<title level="m">Neural message passing on high order paths</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding isomorphism bias in graph data sets</title>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Sviridov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7092" to="7101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13354" to="13366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral redemption in clustering sparse networks</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Sly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="20935" to="20940" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distance encoding-design provably more powerful gnns for structural representation learning</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00142</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1l2bp4YwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multiscale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10945" to="10955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/maron19a.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2019-06">Jun 2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="225" to="228" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards a practical k-dimensional weisfeiler-leman algorithm</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From local measurements to network spectral properties: Beyond degree distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th IEEE Conference on Decision and Control (CDC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2686" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spectral clustering of graphs with the bethe hessian</title>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName><surname>Chervonenkis</surname></persName>
		</author>
		<idno type="DOI">10.1137/1116025</idno>
		<ptr target="https://doi.org/10.1137/1116025" />
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
				<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/wu19e.html" />
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
				<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ga-</surname></persName>
		</author>
		<title level="m">MLP WITH GENERAL EQUIVARIANT GRAPH OPERATORS FOR NODE FEATURE AUGMENTATION For a graph G = (V, E) with n nodes, assume without loss of generality that V</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Let S n denote the set of permutations of n, and ∀π ∈ S n</title>
		<imprint/>
	</monogr>
	<note>it maps a node i ∈ [n] to π(i) ∈ [n</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">For π ∈ S n and a matrix Z ∈ R n×d , we use π Z ∈ R n×d to denote the π-permuted version of M</title>
		<author>
			<persName><surname>For Π ∈ S N And A Matrix M ∈ R N×n</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">p</biblScope>
		</imprint>
	</monogr>
	<note>we use π M ∈ R n×n to denote the π-permuted version of M , that is, (π M ) i,j = M π(i),π(j). that is, (π Z) i,p = Z π(i)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
