<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Animat Vision: Active Vision in Artificial Animals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of Toronto</orgName>
								<address>
									<addrLine>10 King&apos;s College Road</addrLine>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tamer</forename><forename type="middle">F</forename><surname>Rabie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of Toronto</orgName>
								<address>
									<addrLine>10 King&apos;s College Road</addrLine>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Five Cambridge Center</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Five Cambridge Center</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Animat Vision: Active Vision in Artificial Animals</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1089-2788</idno>
					</monogr>
					<idno type="MD5">D51A7E90E4FF444CD5F7EEECA03D8F0C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>active vision</term>
					<term>artificial life</term>
					<term>artificial animals</term>
					<term>autonomous agents</term>
					<term>physics-based modeling</term>
					<term>biomimetic vision systems</term>
					<term>foveation</term>
					<term>visual stabilization</term>
					<term>color object detection</term>
					<term>visionguided navigation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose and demonstrate a new paradigm for active vision research that draws upon recent advances in the fields of artificial life and computer graphics. A software alternative to the prevailing hardware vision mindset, animat vision prescribes artificial animals, or animats, situated in physics-based virtual worlds as autonomous virtual robots with active perception systems. To be operative in its world, an animat must autonomously control its eyes and muscle-actuated body. Computer vision algorithms continuously analyze the retinal image streams acquired by the animat's eyes, enabling it to locomote purposefully through its world. We describe an initial animat vision implementation within lifelike artificial fishes inhabiting a physics-based, virtual marine world. Emulating the appearance, motion, and behavior of real fishes in their natural habitats, these animats are capable of spatially nonuniform retinal imaging, foveation, retinal image stabilization, color object recognition, and perceptually-guided navigation. These capabilities allow them to pursue moving targets such as other artificial fishes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Advances in the emerging field of artificial life (ALife) make possible a fresh approach to computational vision. 1 A major theme in ALife research is the synthesis of artificial animals, or "animats" <ref type="bibr" target="#b0">[1]</ref>. Animats, a term coined by Wilson <ref type="bibr" target="#b1">[2]</ref>, are computational models of real animals situated in their natural habitats. A recent breakthrough in animat research has produced situated virtual agents that realistically emulate animals of nontrivial evolutionary complexity <ref type="bibr" target="#b2">[3]</ref>. This advance prompts us to propose animat vision, a paradigm which prescribes the use of artificial animals as autonomous virtual robots for active vision research. 2  Our zoomimetic approach to vision is made possible by the confluence of three recent trends:</p><p>1. Advanced physics-based artificial life modeling of natural animals. 2. Photorealistic computer graphics rendering and its efficient implementation in modern 3D graphics workstations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Active computer vision algorithms.</head><p>The basic idea in a nutshell is to implement, entirely in software, realistic artificial animals and to give them the ability to locomote, perceive, and in some sense understand the realistic virtual worlds in which they are situated so that they may achieve individual and social functionality within these worlds. To this end, each animat is an autonomous agent possessing a muscle-actuated body capable of locomotion, and a mind with perception, motor, and behavior centers. The animat is endowed with functional eyes that can image the dynamic 3D virtual world onto 2D virtual retinas. The perceptual center of the animat's brain exploits active vision algorithms to continually process the incoming stream of dynamic retinal images in order to make sense of what it sees and, hence, to purposefully navigate its world.</p><p>The uninitiated reader may doubt the possibility of implementing artificial animals rich enough to support serious active vision research. Fortunately, this hurdle has already been cleared. Recent animat theory encompasses the physics of the animal and its world, its ability to locomote using internal muscles, its adaptive, sensorimotor behavior, and its ability to learn. In particular, an animat with these essential capabilities has been implemented that emulates animals as complex as teleost fishes in their marine habitats <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Imagine a virtual marine world inhabited by a variety of realistic fishes (Fig. <ref type="figure" target="#fig_0">1</ref>). In the presence of underwater currents, the fishes employ Figure <ref type="figure" target="#fig_0">1</ref>. Artificial fishes in their physics-based virtual world as it appears to an underwater observer. The 3 reddish fish (center) are engaged in mating behavior, the greenish fish (upper right) is a predator hunting for small prey, the remaining 3 fishes are feeding on plankton (white dots). Seaweeds grow from the ocean bed and sway in the current. their muscles and fins to swim gracefully around immobile obstacles and among moving aquatic plants and other fishes. They autonomously explore their dynamic world in search of food. Large, hungry predator fishes stalk smaller prey fishes in the deceptively peaceful habitat. The sight of predators compels prey fishes to take evasive action. When a dangerous predator appears, similar species of prey form schools to improve their chances of survival. As the predator nears a school, the fishes scatter in terror. A chase ensues in which the predator selects victims and consumes them until satiated. Some species of fishes seem untroubled by predators. They find comfortable niches and feed on floating plankton when they get hungry. Driven by healthy libidos, they perform elaborate courtship rituals to attract mates.</p><p>The challenge undertaken in this paper is to synthesize an active vision system for the fish animat which is based solely on retinal image analysis. The vision system should be extensible so that it will eventually support the broad repertoire of individual and group behaviors described above. It is important to realize that we need not restrict ourselves to modeling the perceptual mechanisms of natural fishes. In fact, the animat vision paradigm applies to animats that model any animal-even a human being-to the level of fidelity that the artificial fish models a real fish. Indeed, the animat vision system that we develop in this paper makes no attempt to model fish perception <ref type="bibr" target="#b6">[7]</ref>. Instead, we have found it an interesting and challenging problem to endow the piscine animat with a biomimetic vision system that enables it to be a functional, active observer of its world.</p><p>The basic functionality of the animat vision system starts with binocular perspective projection of the 3D world onto the animat's 2D retinas. Retinal imaging is accomplished by photorealistic, color graphics renderings of the world from the animat's viewpoint. This projection respects occlusion relationships among objects. It forms spatially variant visual fields, with high resolution foveas and low resolution peripheries. Based on an analysis of the incoming color retinal image stream, the visual center of the animat's brain supplies saccade control signals to the eyes in order to stabilize the visual fields during locomotion through compensatory eye movements (an optokinetic reflex), to attend to interesting colored targets, and to keep these dynamic targets fixated. The artificial fish is thus able to approach and track other artificial fishes under visual guidance. Eventually, its arsenal of active vision algorithms will enable it to forage, evade predators, find mates, etc. The remainder of the paper is organized as follows: the Motivation and Background section briefly motivates our approach vis-a-vis conventional active vision based on robot hardware, and discusses the background of our work. The Review of the Fish Animat section reviews the relevant details of the artificial fish model. The Animat Vision System section describes the active vision system that we have implemented in the animat, including the retinal imaging, foveation, color object detection, and retinal field stabilization algorithms. The Vision-Guided Navigation section presents results in vision-guided navigation and the pursuit of moving nonrigid targets-other artificial fish. The final section presents conclusions and discusses future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation and Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problems with the Hardware Vision Mindset</head><p>Active vision research in most labs today is, in reality, the technologicallydriven pursuit of "hardware vision." To be sure, applications-minded researchers have legitimate reasons for building robot vision systems, but the necessary hardware paraphernalia-CCD cameras, pan-tilt mounts, ocular heads, frame-rate image processors, mobile platforms, manipulators, controllers, interfaces, etc.-can be expensive to fabricate or acquire commercially, and a burden to maintain in working order.</p><p>The animat vision methodology that we advocate in this paper can potentially liberate a significant segment of the computer vision research community from the tyranny of hardware. It addresses the needs of scientists who are motivated to understand and ultimately reverse engineer the powerful vision systems found in higher animals. These researchers realize that readily available hardware systems are woefully inadequate models of natural animals-clearly, animals do not have CCD chip eyes, electric motor muscles, and wheel legs. Moreover, their mobile robots typically lack the compute power necessary to achieve real-time response within a fully dynamic world while running active vision algorithms of much sophistication. Yet in their ambition to understand the complex sensorimotor functions of real animals, active vision researchers have been forced to struggle with whatever hardware is available to them, for lack of a better research strategy; that is, until now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Advantages of the Animat Vision Approach</head><p>Animat vision offers an alternative research strategy for developing biologically-inspired active vision systems. It circumvents the aforementioned problems of hardware vision. The animat vision concept is realized with realistic artificial animals and active vision algorithms implemented entirely in software on readily available 3D graphics workstations. Animat vision offers several additional advantages:</p><p>One can arbitrarily slow down the "cosmic clock" of the virtual world relative to the cycle time of the CPU on which it is being simulated. This increases the amount of computation that each agent can consume between clock ticks without retarding the agent's responses relative to the temporal evolution of its virtual world. This, in turn, permits the development and evaluation of sophisticated new active vision algorithms that are not presently implementable in real-time hardware. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animat Vision 4</head><p>The quantitative photometric, geometric, and dynamic information that is needed to render the virtual world is available explicitly. Generally, the animats are privy to no environmental ground truth data, but must glean visual information "the hard way"-from retinal image streams. However, the readily available ground truth can be extremely useful in assaying the effective accuracy of the vision algorithms or modules under development. <ref type="foot" target="#foot_0">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>It should by now be clear to the reader that our goal in this paper is not to model the vision systems of real fishes, but instead to employ the artificial fish as a virtual robot for designing general-purpose active vision systems. <ref type="foot" target="#foot_1">4</ref>J. J. Gibson <ref type="bibr" target="#b8">[9]</ref>, in a sense the grandfather of active vision, stressed in precomputational terms the importance of modeling the active observer situated in the dynamic environment. Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy <ref type="bibr" target="#b9">[10]</ref> and Ballard <ref type="bibr" target="#b10">[11]</ref> under the names of active perception and animate vision, respectively. <ref type="foot" target="#foot_2">5</ref> The active vision approach was further developed by Aloimonos et al. <ref type="bibr" target="#b11">[12]</ref> and many others (see, e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>) into the prevalent paradigm that it is today.</p><p>The artificial animals that we advocate in this paper are active "vehicles" in the sense of Braitenberg <ref type="bibr" target="#b15">[16]</ref>. We believe that they are as appropriate for grounding active vision systems as are the hardware "mobots" that have come out of the situated robotics work of Brooks and his group <ref type="bibr" target="#b16">[17]</ref> which have been an inspiration to numerous other robotics groups (see, e.g., the compilation <ref type="bibr" target="#b17">[18]</ref>). Undeniably, however, efforts to equip real-time mobile robots with general-purpose active vision systems have been hampered by the hardware and the relatively modest abilities of on-board processors.</p><p>Artificial fishes are animats of unprecedented sophistication. They are autonomous virtual robots situated in a 3D continuous virtual world governed by physical dynamics. This makes them suitable for grounding active vision systems. By contrast, Wilson's original animat <ref type="bibr" target="#b1">[2]</ref>, which was proposed for exploring the acquisition of simple behavior rules, is a point marker on a non-physical 2D grid world that can move between squares containing food or obstacles. Other simple animats include the 2D cockroaches of Beer <ref type="bibr" target="#b18">[19]</ref>. A more sophisticated animat is the kinematic dog described by Blumberg and Galyean <ref type="bibr" target="#b19">[20]</ref>. Prior animats make use of "perceptual oracles"-schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat. One can also find several instances of "oracle vision" in the behavioral animation literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review of the Fish Animat</head><p>The artificial fish model is developed elsewhere <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. This section reviews the animat to a level of detail that suffices to comprehend the remainder of the paper.</p><p>Each artificial fish is an autonomous agent with a deformable body comprising a graphical display model and a biomechanical model actuated by internal muscles. As Figure <ref type="figure" target="#fig_2">2</ref> illustrates, the body includes eyes (among other on-board sensors) and a brain with motor, perception, behavior, and learning centers. Through controlled muscle actions, artificial fishes are able to swim in simulated water in accordance with simplified hydrodynamics. Their functional fins enable them to locomote, maintain balance, and maneuver in the water. Thus the artificial fish model captures not just 3D form and appearance, but also the basic physics of the animal and its environment. Though rudimentary when compared to real animals, the minds of artificial fishes are nonetheless able to learn some basic motor functions and carry out perceptually guided motor tasks within a repertoire of piscine relevant behaviors, including collision avoidance, foraging, preying, schooling, and mating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motor System</head><p>The motor system (see Figure <ref type="figure" target="#fig_2">2</ref>) comprises the fish biomechanical model, including muscle actuators and a set of motor controllers (MCs). Fig. <ref type="figure">3</ref>(a) illustrates the mechanical body model which produces realistic piscine locomotion, using only 23 lumped masses and 91 uniaxial viscoelastic elements, 12 of which are actively contractile muscle elements. These mechanical components are interconnected so as to maintain the structural integrity of the body as it flexes due to the muscle actions.</p><p>Artificial fishes locomote like real fishes, by autonomously contracting their muscles in a coordinated fashion. As the body flexes it displaces virtual fluid which induces local reaction forces normal to the body. These hydrodynamic forces generate thrust that propels the fish forward.  The model mechanics are governed by Lagrange equations of motion driven by the hydrodynamic forces. The system of coupled second-order ordinary differential equations is continually integrated through time by a numerical simulator. 6  The model is sufficiently rich to enable the design of motor controllers by gleaning information from the fish biomechanics literature. The motor controllers coordinate muscle actions to carry out specific motor functions, such as swimming forward (swim-MC), turning left (leftturn-MC), and turning right (right-turn-MC). They translate natural control parameters, such as the forward speed or angle of the turn, into detailed muscle actions that execute the function. The artificial fish is neutrally buoyant in the virtual water and has a pair of pectoral fins which enable it to navigate freely in its 3D world by pitching, rolling, and yawing its body. Additional motor controllers coordinate the fin actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception System</head><p>Artificial fishes gain awareness of their world through sensory perception. As Figure <ref type="figure">3</ref>(b) suggests, it is necessary to model not only the abilities but also the limitations of animal perception systems in order to achieve natural sensorimotor behaviors. Hence, the artificial fish has a limited field of view extending frontally and laterally to an effective radius consistent with visibility in the translucent water (Fig. <ref type="figure">3(b)</ref>). An object may be detected only if some visible portion of it (i.e., not occluded behind some other opaque object) enters the fish's field of view (Fig. <ref type="figure">3(b)</ref>). The perception center of the artificial fish's brain (see Fig. <ref type="figure" target="#fig_2">2</ref>) includes a perceptual attention mechanism which allows the animat to attend to the world in a task-specific way, hence filtering out sensory information superfluous to its immediate behavioral needs. For example, the artificial fish attends to sensory information about nearby food 6. The artificial fish model achieves a good compromise between realism and computational efficiency. To give an example simulation rate, the implementation can simulate a scenario with 10 fishes, 15 food particles, and 5 static obstacles at about 4 frames/sec (with wireframe rendering) on a Silicon Graphics R4400 Indigo 2 Extreme workstation. More complex scenarios with large schools of fish, dynamic plants, and full color texture mapped GL rendering at video resolution can take 5 seconds or more per frame.</p><p>VIDERE 1:1 sources when foraging. The animats in our previous ALife simulations (described in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>) employ a perceptual oracle scheme according to which the artificial fish may satisfy its perceptual needs via direct interrogation of the 3D world model. In particular, subject to the appropriate perceptual limitations, the animat's on-board sensors query the geometric and photometric information that is available to the graphics rendering engine, as well as object identity and dynamic state information within the physics-based virtual world. We emphasize that our goal in the present paper is to replace the perceptual oracle with an artificial fish active vision system that elicits visual information from retinal images, as is described in the Animal Vision System section of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavior System</head><p>The behavior center of the artificial fish's brain mediates between its perception system and its motor system (Fig. <ref type="figure" target="#fig_2">2</ref>). A set of innate characteristics determines the (static) genetic legacy, which dictates whether the fish is male or female, predator or prey, etc. A (dynamic) mental state comprises variables representing hunger, fear, and libido, whose values depend on sensory inputs. The fish's cognitive faculty resides in the action selection component of its behavior center. At each simulation time step, action selection entails combining the innate characteristics, the mental state, and the incoming stream of sensory information to generate sensible, survival sustaining goals for the fish, such as to avoid an obstacle, to avoid predators, to hunt and feed on prey, or to court a potential mate. The action selector ensures that goals have some persistence by exploiting a single-item memory. The behavior memory reduces dithering, thereby improving the robustness of prolonged behaviors such as foraging, schooling, and mating. The action selector also controls the perceptual attention mechanism. At every simulation time step, the action selector activates behavior routines that attend to sensory information and compute the appropriate motor control parameters to carry the fish a step closer to fulfilling its immediate goals. The behavioral repertoire of the artificial fish includes primitive, reflexive behavior routines, such as obstacle avoidance, as well as more sophisticated motivational behavior routines, such as schooling and mating, whose activation is dependent on the mental state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Form and Appearance</head><p>Animat vision requires that artificial animals capture the form and appearance of natural animals with considerable visual fidelity. To this end, photographs of real fishes, such as the one shown in Figure <ref type="figure" target="#fig_6">4</ref>(a), are converted into 3D spline (nurbs) surface body models (Fig. <ref type="figure" target="#fig_6">4(b)</ref>). The digitized photographs are analyzed semi-automatically using deformable models <ref type="bibr" target="#b23">[24]</ref>, in particular, a "snake-mesh" tool which is demonstrated in Fig. <ref type="figure" target="#fig_6">4(d-e</ref>) on a different fish image. The snake mesh slides freely over the image and can be manipulated using the mouse. The border snakes adhere to intensity edges demarcating the fish from the background, and the remaining snakes relax elastically to cover the imaged fish body with a smooth, nonuniform coordinate system (Fig. <ref type="figure" target="#fig_6">4(e)</ref>). The coordinate system serves to map the appropriate image texture onto the spline surface to produce the final texture-mapped fish body model (Fig. <ref type="figure" target="#fig_6">4(c))</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Animat Vision System</head><p>In this section we present the animat vision system that we have developed for the artificial fish, which makes exclusive use of color retinal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eyes and Retinal Imaging</head><p>The artificial fish has binocular vision. The movements of each eye are controlled through two gaze angles (θ , φ) which specify the horizontal and vertical rotation of the eyeball, respectively. The angles are measured with respect to the head coordinate frame, such that the eye is looking straight ahead when θ = φ = 0 • . Each eye is implemented as four coaxial virtual cameras to approximate the spatially nonuniform, foveal/peripheral imaging capabilities typical of biological eyes. The level l = 0 camera has the widest field of view (about 120 • ) and the horizontal and vertical fields of view for the level l camera are related by</p><formula xml:id="formula_0">f l x = 2 tan -1 d x /2 2 l f 0 c f l y = 2 tan -1 d y /2 2 l f 0 c (1)</formula><p>where d x and d y are the horizontal and vertical image dimensions and f 0 c is the focal length of the wide field of view camera (l = 0). 7  Figure <ref type="figure" target="#fig_8">5</ref>(a) shows an example of the 64 × 64 images that are rendered by the four coaxial cameras (using the GL library and SGI graphics pipeline) of the left and right eye. Since the field of view decreases with increasing l, image l is a zoomed version of the central part of image l -1. We refer to the image at level l = 3 as the fovea and the others as peripheral images. We magnify the level l image by a factor of 2 3-l and overlay, in sequence, the four images coincident on their centers starting with the l = 0 image at the bottom (to form an (incomplete) pyramid), thus compositing a 512 × 512 retinal image with a 64 × 64 fovea at the center of a periphery with radially decreasing resolution (and increasing smoothing) in 3 steps. Figure <ref type="figure" target="#fig_8">5</ref>(b) shows the binocular retinal images 7. If f 0 c is unknown, but the l = 0 field of view is known, then f 0 c is first computed using (1) with l = 0 and this value is used to specify the field of view at the other levels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active Vision System Overview</head><p>Figure <ref type="figure">6</ref> shows a block diagram of one ocular channel of the binocular animat vision system. The system currently consists of two main modules-a foveation module and stabilization module. Together they implement a gaze control capability that enables the artificial fish to stabilize the visual world as it locomotes, as well as to detect a visual target in its field of view, foveate the target, and visually navigate towards the target. If the target is in motion, the artificial fish can track it visually and swim in pursuit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foveation using Color Object Detection</head><p>The mind of the fish stores a set of color models of objects that are of interest to it. For instance, if the fish is a predator, it would possess models of prey fish. The models are stored as a list of 64 × 64 color images in the fish's memory.</p><p>We have adopted into the active vision system the color histogram methods of Swain <ref type="bibr" target="#b24">[25]</ref>. The fish employs these methods to detect and Figure <ref type="figure">6</ref>. Gaze control for one eye in the animat vision system. The flow of the algorithm is from right to left. A: Update gaze angles (θ , φ) and saccade using these angles. B: Search current level for model target and, if found, localize it, else search lower level. C: Select level to be processed (see text). F: Reduce field of view for next level and render. M: Compute a general translational displacement vector (u, v) between images I (t -1) and I (t). S: Scale the model's color histogram for use by the current level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified Color Histogram Intersection Method</head><p>Swain <ref type="bibr" target="#b24">[25]</ref> developed a technique called color indexing that efficiently identifies objects in a database in the presence of occlusion and over changes in viewpoint. He demonstrated that object color distributions without geometric information can provide a powerful cue for recognition.</p><p>The effectiveness of the algorithm degrades badly if the area of the object in the model image differs substantially from the area of the target object appearing in the image. Swain suggests scaling the initial model histogram by d 2 M /d 2 where d M is the known range of the model initially and d is the computed range of the target object at the time of backprojection. We will show later that estimating range is straightforward with the eyes verged on a target. Unfortunately, this scaling technique did not work well for the artificial fish, apparently because of noisy depth measurements and the perspective nonlinearity associated with the wide field of view cameras.</p><p>We developed a more robust intersection measure that is invariant to scale changes. This new method iteratively scales down an initially large model color histogram to the approximate size of the target object appearing in the image. Following Swain's notation in <ref type="bibr" target="#b24">[25]</ref>, his original histogram intersection measure is</p><formula xml:id="formula_1">H = n j =1 min(I j , M j ) n j =1 M j (2)</formula><p>where I is the image color histogram, M is the model color histogram, and n is the number of histogram bins used. This measure is effective only if the model histogram is properly scaled. To overcome this limitation, note that the match value H gives the percentage of pixels from the model that have corresponding pixels of the same color in the image (hence, H = 0.9 means that there is a 90% chance the model appeared in the image; however, the value of H can drop significantly; e.g., H = 0.1 if there is a scale difference between target and model). This suggests that we can use H itself to scale the model histogram M. Our experiments revealed that this is not always effective, but that it can be improved by scaling the histogram iteratively; i.e., recomputing H after every scaling of M until the value of H increases above a set threshold, say 0.9. This technique may be expressed as</p><formula xml:id="formula_2">M i+1 k = M i k H i = M i k n j =1 min(I j , M i j ) n j =1 M i j ; k = 1, . . . , n<label>(3)</label></formula><p>where i is the iteration number. Equation 3 is iterated until the value of H i either exceeds the threshold, indicating the presence of the model in the image, or remains constant below threshold (or decreases), indicating that the model matches nothing in the image. The equation converges after a few iterations (usually 2 to 4 if the target size is not too much smaller than the model).</p><p>The iterative technique may degenerate in cases when the model is not present in the image, while a similar color combination is. The problem is that the model histogram gets scaled to the size of the false target to yield a large intersection match value, hence a false alarm.</p><p>To overcome this problem, we employ a new intersection measure after scaling the model histogram using (3). Our measure makes use of a weighted histogram intersection method inspired by the local histogram method proposed by Ennesser and Medioni <ref type="bibr" target="#b25">[26]</ref>. Our measure is</p><formula xml:id="formula_3">H N = n j =1 W j min(I j , M j ) n j =1 M j (4)</formula><p>where the weighting histogram W is given by W j = M j /2 l P j . Here P is the color histogram of the peripheral image (at level l = 0). As is noted by Ennesser and Medioni, the weighted intersection technique increases the selectivity of the method by placing more importance on colors that are specific to the model. In our experiments, H N provided very good separation between intersection match values for false targets (H N &lt; 0.2) and true targets (H N &gt; 0.8).</p><p>An alternative method, which also gives good results, is to incorporate the weighting histogram inside the iteration of equation (3) as follows:</p><formula xml:id="formula_4">M i+1 k = M i k n j =1 (M i j /2 l P j ) min(I j , M i j ) n j =1 M i j (5)</formula><p>The scaled M is then used iteratively to compute the intersection match value H N as before.</p><p>VIDERE 1:1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localization using Color Histogram Backprojection</head><p>Once the model histogram has been properly scaled as described above, Swain's backprojection algorithm works well in localizing the pixel position of the center of the detected target in the foveal image. A thorough description of this algorithm is available in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saccadic Eye Movements</head><p>When a target is detected in the visual periphery, the eyes will saccade to the angular offset of the target to bring it within the fovea. With the object in the high resolution fovea, a more accurate foveation is obtained by a second pass of histogram backprojection. A second saccade typically centers the object accurately in both left and right foveas, thus achieving vergence.</p><p>When the fish is executing a rapid turn, however, the target could partially exit the fovea (l = 3). Part of it will appear in the next coarser image (l = 2). Three saccades are then typically used to regain a foveal fix on the target. The first saccade detects the portion of the target still in the fovea and makes an initial attempt to foveate the target, on the second saccade the target is brought closer to the center of the fovea, and finally the third saccade accurately centers the target in both foveas to verge the eyes.</p><p>Module A in Figure <ref type="figure">6</ref> performs the saccades by incrementing the gaze angles (θ , φ) with differential angles ( θ , φ) in order to rotate the eyes to the required gaze direction. When the pixel location of the target, computed from the left or right images at level l, is (x c , y c ), the correction gaze angles for the eye are given by</p><formula xml:id="formula_5">θ = tan -1 x c 2 l f c φ = tan -1 y c 2 l f c (6)</formula><p>If the target object comes too near the eyes and fills the entire fovea, the algorithm foveates the target at the next coarser level (e.g., l = 2), where the field of view is broader and the target has a more reasonable size for detection and localization. Note that (6) computes the correction angles at level l, but the same corrected (θ, φ) are used to render all the other levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Field Stabilization using Optical Flow</head><p>It is necessary to stabilize the visual field of the artificial fish because its body undulates as it swims. The optokinetic reflex in animals stabilizes vision by measuring image motion and producing compensatory eye movements. Once a target is verged in both foveas, the stabilization process assumes the task of keeping the target foveated as the fish locomotes.</p><p>Stabilization is achieved by computing the displacement (u, v) between the current and previous foveal images and updating the gaze angles to (θ + θ , φ + φ). The displacement is computed as a translational offset in the retinotopic coordinate system using a least squares minimization of the optical flow constraint equation between image frames at times t and t -1 <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. The flow constraint equation is given by <ref type="bibr" target="#b28">[29]</ref> uI x + vI y + I t = 0 (7) </p><p>is minimized by simultaneously solving the two equations ∂E/∂u = 0 and ∂E/∂v = 0 for the image displacement (u, v).</p><p>The correction angles ( θ , φ) for a displacement of (u, v) between the images at level l are computed using (6) by replacing (x c , y c ) with (u, v). If the displacement computed from the foveal images (at level l = 3) is too small (indicating the target is close enough to fill the fovea), the algorithm stabilizes at the next lower level where the target does not fill the entire image area.</p><p>The flow constraint displacement estimation method is accurate only for small displacements between frames. Consequently, when the displacement of the target between frames is large enough that the method is likely to produce bad estimates, the fix is regained by invoking the foveation module to re-detect and re-foveate the target, as described earlier.</p><p>Each eye is controlled independently during foveation and stabilization of a target. Hence, the two eyes must be correlated to keep them verged accurately on the target and not drifting in different directions. The correlation is performed by computing the displacement (u, v) between the left and right foveal images (at l = 3), and correcting the gaze angles of the right eye to (θ R + θ R , φ R + φ R ) using <ref type="bibr" target="#b5">(6)</ref>.</p><p>Once the eyes are verged on a target, it is straightforward for the active vision system to estimate the range to the target from the gaze angles. Referring to Figure <ref type="figure" target="#fig_11">7</ref>, the range is</p><formula xml:id="formula_7">d = b cos(θ R ) cos(θ L ) sin(θ L -θ R ) cos(θ P ) (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>where b is the baseline between the two eyes, and θ P = 1 2 (θ R + θ L ) is the left/right turn angle <ref type="bibr" target="#b28">[29]</ref>. When the eyes are verged on a target the vergence angle is θ V = (θ Rθ L ) and its magnitude increases as the fish comes closer to the target <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Guided Navigation</head><p>The artificial fish can employ the direction of gaze of its eyes to effectively navigate its world. In particular, it is natural to use the gaze angles as the eyes are fixated on a target to navigate towards the target. The θ angles are used to compute the left/right turn angle θ P shown in Figure <ref type="figure" target="#fig_11">7</ref>, and the φ angles are similarly used to compute an up/down turn angle φ P = 1 2 (φ R + φ L ). The fish's turn motor controllers are invoked to execute a left/right turn-left-turn-MC for negative θ P and rightturn-MC for positive θ P (see the Motor System subsection)-with |θ P | as parameter when |θ P | &gt; 30 • . An up/down turn command is issued to the fish's pectoral fins if |φ P | &gt; 5 • , with a positive φ P interpreted as up and negative as down. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pursuit of Nonrigid Targets in Motion</head><p>The problem of pursuing a moving target that has been fixated in the foveas of the artificial fish's eyes is simplified by the gaze control mechanism described above. The fish can robustly foveate a moving target and chase it by using the turn angles (θ P , φ P ) computed from the gaze angles that are continuously updated by the foveation/stabilization algorithms.</p><p>We have carried out numerous experiments in which the moving target is a reddish fish whose color histogram model is stored in the memory of a predator fish equipped with the active vision system. Figure <ref type="figure" target="#fig_12">8</ref> shows plots of the gaze angles and the turn angles obtained over the course of 100 frames in a typical experiment as the predator fixates on and actively pursues a prey target. Figure <ref type="figure" target="#fig_14">9</ref> shows a sequence of image frames acquired by the observer fish during its navigation (only the left retinal images are shown). Frame 0 shows the target visible in the low resolution periphery of the fish's eyes (middle right). Frame 1 shows the view after the target has been detected and the eyes have saccaded to foveate the target (note that the size decrease of the target after foveation is a perspective effect). The subsequent frames show the target remaining fixated in the fovea, despite the side-to-side motion of the fish's body as it swims towards the target. Fixation is achieved by stabilizing the eyes with compensating saccade signals. The signals are indicated in Figure <ref type="figure" target="#fig_12">8</ref> by the undulatory responses of the θ angles.</p><p>Figure <ref type="figure" target="#fig_12">8</ref> shows that the vergence angle tends to increase in magnitude as the fish moves closer to the target (around frame 100). In comparison to the θ angles, the φ angles show little variation, because the fish does not undulate vertically very much as it swims forward. It is apparent from the graphs that the gaze directions of the two eyes are nicely correlated; that is, (θ R , φ R ) follows (θ L , φ L ), with θ Rθ L indicating the reciprocal of range to the target.</p><p>Notice that in frames 87-117 of Figure <ref type="figure" target="#fig_14">9</ref>, a yellow fish whose size is similar to the target fish passes behind the target. In this experiment the fish with active vision was instructed to treat all non-reddish objects as totally uninteresting and not worth foveating. Because of the color difference, the yellow object does not distract the fish's gaze from its reddish target. This demonstrates the robustness of the color-based fixation algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>This paper has presented work that spans the fields of computer vision, artificial life, and computer graphics. Our research was motivated in part by the realization that many active vision researchers would rather not have their progress impeded by the limitations and complications of currently available hardware. Animat vision offers a viable, purely software alternative to the prevailing hardware vision mindset.</p><p>To demonstrate the animat vision approach, we have employed a physics-based, virtual marine world inhabited by lifelike artificial fishes that emulate the appearance, motion, and behavior of natural fishes in their physical habitats. Artificial fishes are virtual zoological robots ("zoobots") that offer active vision researchers greater mobility and maneuverability, lower cost, and higher reliability/repeatability than can be expected from present-day physical robots. Moreover, these physicsbased virtual robots are governed in their virtual world by the same principles that physical robots are subject to in the physical world, hence they share the attraction of situated physical robots for the purposes of active vision research.</p><p>In a relatively short period of time we have successfully implemented within the framework of the artificial fish animat a set of active vision algorithms for foveation and vergence of interesting targets, for retinal image stabilization, and for pursuit of moving targets through visuallyguided navigation. Note, however, that the automated analysis of the class of retinal images that confront our vision algorithms is by no means easy.</p><p>In future work we will endeavor to develop a more extensive arsenal of active vision algorithms to support the complete behavioral repertoire of artificial fishes. The animat vision approach allows us to do this in stages without ever compromising the complete functionality of the artificial fish. We anticipate that the active vision suite that we are developing will be relevant in whole or in part to physical robotics (e.g., autonomous underwater vehicles). In conclusion, it appears that artificial animals in their virtual worlds can serve as a proving ground for  Retinal image sequence from the left eye of the active vision fish as it detects and foveates on a reddish fish target and swims in pursuit of the target. The target appears in the periphery (middle right) in frame 0 and is foveated in frame 1. The target remains fixated in the center of the fovea as the fish uses the gaze direction to swim towards it (frames 7-117). The target fish turns and swims away with the observer fish in visually guided pursuit (frames 135-152). theories that profess sensorimotor competence in animal or robotic situated agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The body of an artificial fish comprises a muscle-actuated biomechanical model, perceptual sensors, and a brain with motor, perception, behavior, and learning centers. To the lower left is an artificial fish graphical display model.</figDesc><graphic coords="6,229.17,45.27,311.04,199.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>VIDERE 1: 1 Figure 3 .</head><label>13</label><figDesc>Figure 3. Biomechanical fish model (a) Nodes denote lumped masses. Lines indicate springs (shown at their natural lengths). Bold lines indicate muscle springs. Artificial fishes perceive objects (b) within a limited field view if objects are close enough and not occluded by other opaque objects (only the fish towards the left is visible to the animat at the center).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Digitized color image of a fish photo. (b) 3D NURBS surface fish body. (c) Color texture mapped 3D fish model. Initial (d) and final (e) snake-mesh on an image of a different fish.</figDesc><graphic coords="9,227.42,45.89,97.92,58.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Binocular retinal imaging. (a) 4 component images; l = 0, 1, 2, are peripheral images; l = 3 is foveal image. (b) Composited retinal images (borders of composited component images are shown in white).l = 0 l = 1 l = 2 l = 3 l = 0 l = 1 l = 2 l = 3 (a)</figDesc><graphic coords="10,393.55,146.84,144.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>that may be imaged in the low resolution periphery of its retinas. Since each model object has a unique color histogram (when the background is subtracted from the object) it can be detected in the periphery by histogram intersection and localized by histogram backprojection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Gaze angles and range to target geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Gaze angles (saccade signals) vs. time (frames) of the observer fish while pursuing the reddish target fish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>VIDERE 1: 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. Retinal image sequence from the left eye of the active vision fish as it detects and foveates on a reddish fish target and swims in pursuit of the target. The target appears in the periphery (middle right) in frame 0 and is foveated in frame 1. The target remains fixated in the center of the fovea as the fish uses the gaze direction to swim towards it (frames 7-117). The target fish turns and swims away with the observer fish in visually guided pursuit (frames 135-152).</figDesc><graphic coords="17,228.42,52.80,74.88,74.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where I x , I y , and I t are the derivatives of the image intensity, such that</figDesc><table><row><cell>I (x, y, t) =</cell><cell>1 3</cell><cell cols="2">[R(x, y, t) + G(x, y, t) + B(x, y, t)]</cell><cell>(8)</cell></row><row><cell cols="5">where R, G, and B denote the color component channels. The error</cell></row><row><cell>function</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">E(u, v) =</cell><cell>(uI x + vI y + I t ) 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x,y∈ fovea</cell></row><row><cell>VIDERE 1:1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>It is often convenient to represent ground truth data iconically in the form of retinocentric intrinsic images, including intensity, range, illumination, reflectance, and object identity images, and these can be computed easily and quickly by the rendering pipelines of 3D graphics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>workstations. 4. However, it is interesting that from the point of view of vision science, fishes, which account for some 22,000 species inhabiting almost every conceivable photic environment from the clear waters of tropical lagoons to the darkest deep seas, provide ideal models for the vertebrate visual system. Fish visual systems have been open to a wider variety of investigations, from biochemistry to ecology, than any other visual system, and many characteristics of vertebrate vision were initially demonstrated using</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>fish (see<ref type="bibr" target="#b7">[8]</ref>).5. Animat vision should not be confused with Ballard's animate vision; the latter does not involve artificial animals. VIDERE 1:1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>VIDERE 1:1</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was made possible by the creativity, dedication, and cooperation of Xiaoyuan Tu, who developed the artificial fish animat. We thank her and Radek Grzeszczuk for their many important contributions to the artificial fishes project. We also thank the many persons who have discussed and debated the animat vision idea with us, especially John Tsotsos, Geoffrey Hinton, and Allan Jepson. The work was supported by a research grant from the Natural Sciences and Engineering Research Council of Canada and by the ARK (Autonomous Robot for a Known environment) Project, which receives its funding from PRECARN Associates Inc., Industry Canada, the National Research Council of Canada, Technology Ontario, Ontario Hydro Technologies, and Atomic Energy of Canada Limited. DT is a fellow of the Canadian Institute for Advanced Research. VIDERE 1:1 Animat Vision 17</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">From Animals to Animats: The 3rd International Conf. on Simulation of Adaptive Behavior</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Husbands</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The animat path to AI</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats</title>
		<editor>
			<persName><forename type="first">J.-A</forename><surname>Meyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="351" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Animat vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Rabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth International Conference on Computer Vision</title>
		<meeting>Fifth International Conference on Computer Vision<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion and color analysis for animat perception</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Rabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th National Conference on Artificial Intelligence</title>
		<meeting>13th National Conference on Artificial Intelligence<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="1090" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial fishes: Physics, locomotion, perception, behavior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series, Proc. SIGGRAPH &apos;94</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Physiology of Fishes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Fernald</surname></persName>
		</author>
		<editor>D. H. Evans</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page" from="161" to="189" />
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
	<note>Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Loew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Mcfarland</surname></persName>
		</author>
		<title level="m">Fish Vision</title>
		<imprint>
			<publisher>of Vision Research</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Ecological Approach to Visual Perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Houghton Mifflin; Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Animate vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="57" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="333" to="356" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principles of animate vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m">Active Vision</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Promising directions in active vision</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Stricker</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="126" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Experiments in Synthetic Psychology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Braitenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Vehicles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intelligence without representation. Artificial Intelligence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="139" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m">Designing Autonomous Agents</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Beer</surname></persName>
		</author>
		<title level="m">Intelligence as Adaptive Behavior</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-level direction of autonomous creatures for real-time virtual environments</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Blumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Galyean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series, Proc. SIGGRAPH &apos;95</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flocks, herds, and schools: A distributed behavioral model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Reynolds</surname></persName>
		</author>
		<idno>VIDERE 1:1 Animat Vision 18</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="34" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A vision-based approach to behavioral animation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Renault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual modeling for the behavioral animation of fishes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Pacific Conf. on Computer Graphics</title>
		<meeting>2nd Pacific Conf. on Computer Graphics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">M</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constraints on deformable models: Recovering 3D shape and nonrigid motion</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="91" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding waldo, or focus of attention using local color information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ennesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conf. (CVPR&apos;93)</title>
		<meeting>Computer Vision and Pattern Recognition Conf. (CVPR&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="711" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object tracking with a moving camera: An application of dynamic motion analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hingorani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kolczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shvaytser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Visual Motion</title>
		<meeting>IEEE Workshop on Visual Motion<address><addrLine>Irvine, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-03">March 1989</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovery of ego-motion using image stabilization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rousso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Visual Motion</title>
		<meeting>IEEE Workshop on Visual Motion</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="454" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Robot Vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time binocular smooth pursuit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coombs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
		<idno>VIDERE 1:1 Animat Vision 19</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="164" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">One Microsoft Way Demetri Terzopoulos</title>
		<meeting><address><addrLine>Sweden Olivier Faugeras, INRIA Sophia-Antipolis, France Avi Kak</addrLine></address></meeting>
		<imprint>
			<publisher>Microsoft Corp</publisher>
		</imprint>
		<respStmt>
			<orgName>Christopher Brown, University of Rochester Giulio Sandini, Università di Genova, Italy Editorial Board Yiannis Aloimonos, University of Maryland Nicholas Ayache, INRIA, France Ruzena Bajcsy, University of Pennsylvania Dana H. Ballard, University of Rochester Andrew Blake, University of Oxford, United Kingdom Jan-Olof Eklundh, The Royal Institute of Technology (KTH ; Purdue University Takeo Kanade, Carnegie Mellon University Joe Mundy, General Electric Research Labs Tomaso Poggio, Massachusetts Institute of Technology Steven A. Shafer ; University of Toronto, Canada Saburo Tsuji, Osaka University, Japan Andrew Zisserman, University of Oxford, United Kingdom Action Editors Minoru Asada, Osaka University, Japan Terry Caelli, Curtin University of Technology, Australia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">F</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">France</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LIFIA -IMAG</title>
		<imprint/>
		<respStmt>
			<orgName>University of Essex, United Kingdom Patrick Courtney</orgName>
		</respStmt>
	</monogr>
	<note>Z.I.R.S.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Huttenlocher</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Cornell University Yasuo Kuniyoshi, Electrotechnical Laboratory, Japan Shree K. Nayar, Columbia University Alex P. Pentland, Massachusetts Institute of Technology Lawrence B. Wolff, Johns Hopkins University Steven W. Zucker, Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
