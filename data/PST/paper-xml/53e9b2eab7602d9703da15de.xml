<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chapter 9. Face Recognition Across Pose and Illumination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ralph</forename><surname>Gross</surname></persName>
							<email>rgross@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
							<email>simonb@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
							<email>iainm@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chapter 9. Face Recognition Across Pose and Illumination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">383CCCE24E896257A365D2DAE1F16E6C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The last decade has seen automatic face recognition evolve from small-scale research systems to a wide range of commercial products. Driven by the FERET face database and evaluation protocol, the currently best commercial systems achieve verification accuracies comparable to those of fingerprint recognizers. In these experiments, only frontal face images taken under controlled lighting conditions were used. As the use of face recognition systems expands toward less restricted environments, the development of algorithms for view and illumination invariant face recognition becomes important. However, the performance of current algorithms degrades significantly when tested across pose and illumination, as documented in a number of evaluations. In this chapter we review previously proposed algorithms for pose and illumination invariant face recognition. We then describe in detail two successful appearance-based algorithms for face recognition across pose, eigen light-fields, and Bayesian face subregions. We furthermore show how both of these algorithms can be extended toward face recognition across pose and illumination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The most recent evaluation of commercial face recognition systems shows the level of performance for face verification of the best systems to be on par with fingerprint recognizers for frontal, uniformly illuminated faces <ref type="bibr" target="#b37">[38]</ref>. Recognizing faces reliably across changes in pose and illumination has proved to be a much more difficult problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. Although most research has so far focused on frontal face recognition, there is a sizable body of work on pose invariant face recognition and illumination invariant face recognition. However, face recognition across pose and illumination has received little attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Multiview Face Recognition and Face Recognition Across Pose</head><p>Approaches addressing pose variation can be classified into two categories depending on the type of gallery images they use. Multiview face recognition is a direct extension of frontal face recognition in which the algorithms require gallery images of every subject at every pose. In face recognition across pose we are concerned with the problem of building algorithms to recognize a face from a novel viewpoint (i.e., a viewpoint from which it has not previously been seen). In both categories we furthermore distinguish between model-based and appearancebased algorithms. Model-based algorithms use an explicit two-dimensional (2D) <ref type="bibr" target="#b11">[12]</ref> or 3D <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> model of the face, whereas appearance-based methods directly use image pixels or features derived from image pixels <ref type="bibr" target="#b35">[36]</ref>.</p><p>One of the earliest appearance-based multiview algorithms was described by Beymer <ref type="bibr" target="#b5">[6]</ref>. After a pose estimation step, the algorithm geometrically aligns the probe images to candidate poses of the gallery subjects using the automatically determined locations of three feature points. This alignment is then refined using optical flow. Recognition is performed by computing normalized correlation scores. Good recognition results are reported on a database of 62 subjects imaged in a number of poses ranging from -30 • to +30 • (yaw) and from -20 • to +20 • (pitch). However, the probe and gallery poses are similar. Pentland et al. <ref type="bibr" target="#b36">[37]</ref> extended the popular eigenface approach of Turk and Pentland <ref type="bibr" target="#b46">[47]</ref> is extended to handle multiple views. The authors compare the performance of a parametric eigenspace (computed using all views from all subjects) with view-based eigenspaces (separate eigenspaces for each view). In experiments on a database of 21 people recorded in nine evenly spaced views from minus 90 • to +90 • , view-based eigenspaces outperformed the parametric eigenspace by a small margin.</p><p>A number of 2D model-based algorithms have been proposed for face tracking through large pose changes. In one study <ref type="bibr" target="#b12">[13]</ref> separate active appearance models were trained for profile, half-profile, and frontal views, with models for opposing views created by simple reflection. Using a heuristic for switching between models, the system was able to track faces through wide angle changes. It has been shown that linear models are able to deal with considerable pose variation so long as all the modeled features remained visible <ref type="bibr" target="#b31">[32]</ref>. A different way of dealing with larger pose variations is then to introduce nonlinearities into the model. Romdhani et al. extended active shape models <ref type="bibr" target="#b40">[41]</ref> and active appearance models <ref type="bibr" target="#b41">[42]</ref> using a kernel PCA to model shape and texture nonlinearities across views. In both cases models were successfully fit to face images across a full 180 • rotation. However, no face recognition experiments were performed.</p><p>In many face recognition scenarios the pose of the probe and gallery images are different. For example, the gallery image might be a frontal "mug shot," and the probe image might be a three-quarter view captured from a camera in the corner of a room. The number of gallery and probe images can also vary. For example, the gallery might consist of a pair of images for each subject, a frontal mug shot and full profile view (like the images typically captured by police departments). The probe might be a similar pair of images, a single three-quarter view, or even a collection of views from random poses. In these scenarios multiview face recognition algorithms cannot be used. Early work on face recognition across pose was based on the idea of linear object classes <ref type="bibr" target="#b47">[48]</ref>. The underlying assumption is that the 3D shape of an object (and 2D projections of 3D objects) can be represented by a linear combination of prototypical objects. It follows that a rotated view of the object is a linear combination of the rotated views of the prototype objects. Using this idea the authors were able to synthesize rotated views of face images from a single-example view. This algorithm has been used to create virtual views from a single input image for use in a multiview face recognition system <ref type="bibr" target="#b6">[7]</ref>. Lando and Edelman used a comparable example-based technique to generalize to new poses from a single view <ref type="bibr" target="#b30">[31]</ref>.</p><p>A completely different approach to face recognition across pose is based on the work of Murase and Nayar <ref type="bibr" target="#b35">[36]</ref>. They showed that different views of a rigid object projected into an eigenspace fall on a 2D manifold. Using a model of the manifold they could recognize objects from arbitrary views. In a similar manner Graham and Allison observed that a densely sampled image sequence of a rotating head forms a characteristic eigensignature when projected into an eigenspace <ref type="bibr" target="#b18">[19]</ref>. They use radial basis function networks to generate eigensignatures based on a single view input. Recognition is then performed by distance computation between the projection of a probe image into eigenspace and the eigensignatures created from gallery views. Good generalization is observed from half-profile training views. However, recognition rates for tests across wide pose variations (e.g., frontal gallery and profile probe) are weak.</p><p>One of the early model-based approaches for face recognition is based on elastic bunch graph matching <ref type="bibr" target="#b48">[49]</ref>. Facial landmarks are encoded with sets of complex Gabor wavelet coefficients called jets. A face is then represented with a graph where the various jets form the nodes. Based on a small number of hand-labeled examples, graphs for new images are generated automatically. The similarity between a probe graph and the gallery graphs is determined as average over the similarities between pairs of corresponding jets. Correspondences between nodes in different poses is established manually. Good recognition results are reported on frontal faces in the FERET evaluation <ref type="bibr" target="#b38">[39]</ref>. Recognition accuracies decrease drastically, though, for matching half profile images with either frontal or full profile views. For the same framework a method for transforming jets across pose has been introduced <ref type="bibr" target="#b34">[35]</ref>. In limited experiments the authors show improved recognition rates over the original representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Illumination Invariant Face Recognition</head><p>In addition to face pose, illumination is the next most significant factor affecting the appearance of faces. Ambient lighting changes greatly within and between days and among indoor and outdoor environments. Due to the 3D structure of the face, a direct lighting source can cast strong shadows that accentuate or diminish certain facial features. It has been shown experimentally <ref type="bibr" target="#b1">[2]</ref> and theoretically for systems based on principal component analysis (PCA) <ref type="bibr" target="#b49">[50]</ref> that differences in appearance induced by illumination are larger than differences between individuals. Because dealing with illumination variation is a central topic in computer vision, numerous approaches for illumination invariant face recognition have been proposed.</p><p>Early work in illumination invariant face recognition focused on image representations that are mostly insensitive to changes in illumination. In one study <ref type="bibr" target="#b1">[2]</ref> various image representations and distance measures were evaluated on a tightly controlled face database that varied the face pose, illumination, and expression. The image representations include edge maps, 2D Gabor-like filters, first and second derivatives of the gray-level image, and the logarithmic transformations of the intensity image along with these representations. However, none of the image representations was found to be sufficient by itself to overcome variations due to illumination changes. In more recent work it was shown that the ratio of two images from the same object is simpler than the ratio of images from different objects <ref type="bibr" target="#b26">[27]</ref>. In limited experiments this method outperformed both correlation and PCA but did not perform as well as the illumination cone method described below. A related line of work attempted to extract the object's surface reflectance as an illumination invariant description of the object <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>. We discuss the most recent algorithm in this area in more detail in Section 4.2. Sashua and Riklin-Raviv <ref type="bibr" target="#b43">[44]</ref> proposed a different illumination invariant image representation, the quotient image. Computed from a small set of example images, the quotient image can be used to re-render an object of the same class under a different illumination condition. In limited recognition experiments the method outperforms PCA.</p><p>A different approach to the problem is based on the observation that the images of a Lambertian surface, taken from a fixed viewpoint but under varying illumination, lie in a 3D linear subspace of the image space <ref type="bibr" target="#b42">[43]</ref>. A number of appearance-based methods exploit this fact to model the variability of faces under changing illumination. Belhumeur et al. <ref type="bibr" target="#b3">[4]</ref> extended the eigenface algorithm of Turk and Pentland <ref type="bibr" target="#b46">[47]</ref> to fisherfaces by employing a classifier based on Fisher's linear discriminant analysis. In experiments on a face database with strong variations in illumination fisherfaces outperform eigenfaces by a wide margin. Further work in the area by Belhumeur and Kriegman showed that the set of images of an object in fixed pose but under varying illumination forms a convex cone in the space of images <ref type="bibr" target="#b4">[5]</ref>. The illumination cones of human faces can be approximated well by low-dimensional linear subspaces <ref type="bibr" target="#b15">[16]</ref>. An algorithm based on this method outperforms both eigenfaces and fisherfaces. More recently Basri and Jacobs showed that the illumination cone of a convex Lambertian surface can be approximated by a nine-dimensional linear subspace <ref type="bibr" target="#b2">[3]</ref>. In limited experiments good recognition rates across illumination conditions are reported.</p><p>Common to all these appearance-based methods is the need for training images of database subjects under a number of different illumination conditions. An algorithm proposed by Sim and Kanade overcomes this restriction <ref type="bibr" target="#b44">[45]</ref>. They used a statistical shape-from-shading model to recover the face shape from a single image and synthesize the face under a new illumination. Using this method they generated images of the gallery subjects under many different illumination conditions to serve as gallery images in a recognizer based on PCA. High recognition rates are reported on the illumination subset of the CMU PIE database <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Algorithms for Face Recognition across Pose and Illumination</head><p>A number of appearance and model-based algorithms have been proposed to address the problems of face recognition across pose and illumination simultaneously. In one study <ref type="bibr" target="#b16">[17]</ref> a variant of photometric stereo was used to recover the shape and albedo of a face based on seven images of the subject seen in a fixed pose. In combination with the illumination cone representation introduced in <ref type="bibr" target="#b4">[5]</ref> the authors can synthesize faces in novel pose and illumination conditions. In tests on 4050 images from the Yale Face Database B, the method performed almost without error. In another study <ref type="bibr" target="#b9">[10]</ref> a morphable model of 3D faces was introduced. The model was created using a database of Cyberware laser scans of 200 subjects. Following an analysis-by-synthesis paradigm, the algorithm automatically recovers face pose and illumination from a single image. For initialization, the algorithm requires the manual localization of seven facial feature points. After fitting the model to a new image, the extracted model parameters describing the face shape and texture are used for recognition. The authors reported excellent recognition rates on both the FERET <ref type="bibr" target="#b38">[39]</ref> and CMU PIE <ref type="bibr" target="#b45">[46]</ref> databases. Once fit, the model could also be used to synthesize an image of the subject under new conditions. This method was used in the most recent face recognition vendor test to create frontal view images from rotated views <ref type="bibr" target="#b37">[38]</ref>. For 9 of 10 face recognition systems tested, accuracies on the synthesized frontal views were significantly higher than on the original images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Eigen Light-Fields</head><p>We propose an appearance-based algorithm for face recognition across pose. Our algorithm can use any number of gallery images captured at arbitrary poses and any number of probe images also captured with arbitrary poses. A minimum of one gallery and one probe image are needed, but if more images are available the performance of our algorithm generally improves.</p><p>Our algorithm operates by estimating (a representation of) the light-field <ref type="bibr" target="#b33">[34]</ref> of the subject's head. First, generic training data are used to compute an eigenspace of head light-fields, similar to the construction of eigenfaces <ref type="bibr" target="#b46">[47]</ref>. Light-fields are simply used rather than images. Given a collection of gallery or probe images, the projection into the eigenspace is performed by setting up a least-squares problem and solving for the projection coefficients similar to approaches used to deal with occlusions in the eigenspace approach <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33]</ref>. This simple linear algorithm can be applied to any number of images captured from any poses. Finally, matching is performed by comparing the probe and gallery eigen light-fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Light-Fields Theory Object Light-Fields</head><p>The plenoptic function <ref type="bibr" target="#b0">[1]</ref> or light-field <ref type="bibr" target="#b33">[34]</ref> is a function that specifies the radiance of light in free space. It is a 5D function of position (3D) and orientation (2D). In addition, it is also sometimes modeled as a function of time, wavelength, and polarization, depending on the application in mind. In 2D, the light-field of a 2D object is actually 2D rather than the 3D that might be expected. See Figure <ref type="figure" target="#fig_6">9</ref>.1 for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigen Light-Fields</head><p>Suppose we are given a collection of light-fields L i (θ, φ) of objects O i (here faces of different subjects) where i = 1, . . . , N. See Figure <ref type="figure" target="#fig_6">9</ref>.1 for the definition of this notation. If we perform an eigendecomposition of these vectors using PCA, we obtain d ≤ N eigen light-fields E i (θ, φ) where i = 1, . . . , d. Then, assuming that the eigenspace of light-fields is a good representation of the set of light-fields under consideration, we can approximate any light-field L(θ, φ) as</p><formula xml:id="formula_0">L(θ, φ) ≈ d i=1 λ i E i (θ, φ)<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">λ i = L(θ, φ), E i (θ, φ) is the inner (or dot) product between L(θ, φ) and E i (θ, φ).</formula><p>This decomposition is analogous to that used for face and object recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref>. The mean light-field could also be estimated and subtracted from all of the light-fields. Capturing the complete light-field of an object is a difficult task, primarily because it requires a huge number of images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>. In most object recognition scenarios it is unreasonable to expect more than a few images of the object (often just one). However, any image of the object corresponds to a curve (for 3D objects, a surface) in the light-field. One way to look at this curve is as a highly occluded light-field; only a small part of the light-field is visible. Can the eigen coefficients λ i be estimated from this highly occluded view? Although this may seem</p><formula xml:id="formula_2">L( , ) φ θ L( , ) φ θ O φ θ O O 0 360 Viewpoint θ v Object Light-Field Radiance φ 90 O -90</formula><p>Fig. <ref type="figure" target="#fig_6">9</ref>.1. The object is conceptually placed within a circle. The angle to the viewpoint v around the circle is measured by the angle θ, and the direction the viewing ray makes with the radius of the circle is denoted φ. For each pair of angles θ and φ, the radiance of light reaching the viewpoint from the object is then denoted by L(θ, φ), the light-field. Although the light-field of a 3D object is actually 4D, we continue to use the 2D notation of this figure in this chapter for ease of explanation.</p><p>hopeless, consider that light-fields are highly redundant, especially for objects with simple reflectance properties such as Lambertian. An algorithm has been presented <ref type="bibr" target="#b32">[33]</ref> to solve for the unknown λ i for eigen images. A similar algorithm was implicitly used by Black and Jepson <ref type="bibr" target="#b7">[8]</ref>. Rather than using the inner product λ i = L(θ, φ), E i (θ, φ) , Leonardis and Bischof <ref type="bibr" target="#b32">[33]</ref> solved for λ i as the least-squares solution of</p><formula xml:id="formula_3">L(θ, φ) - d i=1 λ i E i (θ, φ) = 0 (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where there is one such equation for each pair of θ and φ that are unoccluded in L(θ, φ). Assuming that L(θ, φ) lies completely within the eigenspace and that enough pixels are unoccluded, the solution of Eq. ( <ref type="formula" target="#formula_3">2</ref>) is exactly the same as that obtained using the inner product <ref type="bibr" target="#b20">[21]</ref>. Because there are d unknowns (λ 1 . . . λ d ) in Eq. ( <ref type="formula" target="#formula_3">2</ref>), at least d unoccluded light-field pixels are needed to overconstrain the problem, but more may be required owing to linear dependencies between the equations. In practice, two to three times as many equations as unknowns are typically required to get a reasonable solution <ref type="bibr" target="#b32">[33]</ref>. Given an image I(m, n), the following is then an algorithm for estimating the eigen light-field coefficients λ i .</p><p>1. For each pixel (m, n) in I(m, n), compute the corresponding light-field angles θ m,n and φ m,n . (This step assumes that the camera intrinsics are known, as well as the relative orientation of the camera to the object.) 2. Find the least-squares solution (for λ 1 . . . λ d ) to the set of equations where m and n range over their allowed values. (In general, the eigen light-fields E i need to be interpolated to estimate E i (θ m,n , φ m,n ). Also, all of the equations for which the pixel I(m, n) does not image the object should be excluded from the computation.)</p><formula xml:id="formula_5">I(m, n) - d i=1 λ i E i (θ m,n , φ m,n ) = 0 (3) Input Rerendered Original</formula><p>Although we have described this algorithm for a single image I(m, n), any number of images can obviously be used (so long as the camera intrinsics and relative orientation to the object are known for each image). The extra pixels from the other images are simply added in as additional constraints on the unknown coefficients λ i in Eq. ( <ref type="formula">3</ref>). The algorithm can be used to estimate a light-field from a collection of images. Once the light-field has been estimated, it can then be used to render new images of the same object under different poses. (See Vetter and Poggio <ref type="bibr" target="#b47">[48]</ref> for a related algorithm.) We have shown <ref type="bibr" target="#b20">[21]</ref> that the algorithm correctly re-renders a given object assuming a Lambertian reflectance model. The extent to which these assumptions are valid are illustrated in Figure <ref type="figure" target="#fig_6">9</ref>.2, where we present the results of using our algorithm to re-render faces across pose. In each case the algorithm received the left-most (frontal) image as input and created the rotated view in the middle. For comparison, the original rotated view is included as the right-most image. The re-rendered image for the first subject is similar to the original. Although the image created for the second subject still shows a face in the correct pose, the identity of the subject is not as accurately recreated. We conclude that overall our algorithm works fairly well but that more training data are needed so the eigen light-field of faces can more accurately represent any given face light-field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Application to Face Recognition Across Pose</head><p>The eigen light-field estimation algorithm described above is somewhat abstract. To be able to use it for face recognition across pose we need to do the following things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectorization:</head><p>The input to a face recognition algorithm consists of a collection of images (possibly just one) captured from a variety of poses. The eigen light-field estimation Algorithm operates on light-field vectors (light-fields represented as vectors). Vectorization consists of converting the input images into a light-field vector (with missing elements, as appropriate.) Classification: Given the eigen coefficients a 1 . . . a d for a collection of gallery faces and for a probe face, we need to classify which gallery face is the most likely match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting training and testing sets:</head><p>To evaluate our algorithm we have to divide the database used into (disjoint) subsets for training and testing.</p><p>We now describe each of these tasks in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectorization by Normalization</head><p>Vectorization is the process of converting a collection of images of a face into a light-field vector. Before we can do this we first have to decide how to discretize the light-field into pixels.</p><p>Perhaps the most natural way to do this is to uniformly sample the light-field angles (θ and φ in the 2D case of Figure <ref type="figure" target="#fig_6">9</ref>.1). This is not the only way to discretize the light-field. Any sampling, uniform or nonuniform, could be used. All that is needed is a way to specify what is the allowed set of light-field pixels. For each such pixel, there is a corresponding index in the light-field vector; that is, if the light-field is sampled at K pixels, the light-field vectors are K dimensional vectors. We specify the set of light-field pixels in the following manner. We assume that there are only a finite set of poses 1, 2, . . . , P in which the face can occur. Each face image is first classified into the nearest pose. (Although this assumption is clearly an approximation, its validity is demonstrated by the empirical results in Section 2.3. In both the FERET <ref type="bibr" target="#b38">[39]</ref> and PIE <ref type="bibr" target="#b45">[46]</ref> databases, there is considerable variation in the pose of the faces. Although the subjects are asked to place their face in a fixed pose, they rarely do this perfectly. Both databases therefore contain considerable variation away from the finite set of poses. Our algorithm performs well on both databases, so the approximation of classifying faces into a finite set of poses is validated.)</p><p>Each pose i = 1, . . . , P is then allocated a fixed number of pixels K i . The total number of pixels in a light-field vector is therefore K = P i=1 K i . If we have images from poses 3 and 7, for example, we know K 3 + K 7 of the K pixels in the light-field vector. The remaining K -K 3 -K 7 are unknown, missing data. This vectorization process is illustrated in Figure <ref type="figure" target="#fig_6">9</ref>.3.</p><p>We still need to specify how to sample the K i pixels of a face in pose i. This process is analogous to that needed in appearance-based object recognition and is usually performed by "normalization." In eigenfaces <ref type="bibr" target="#b46">[47]</ref>, the standard approach is to find the positions of several canonical points, typically the eyes and the nose, and to warp the input image onto a coordinate frame where these points are in fixed locations. The resulting image is then masked. To generalize eigenface normalization to eigen light-fields, we just need to define such a normalization for each pose.</p><p>We report results using two different normalizations. The first is a simple one based on the location of the eyes and the nose. Just as in eigenfaces, we assume that the eye and nose locations are known, warp the face into a coordinate frame in which these canonical points are in a fixed location, and finally crop the image with a (pose-dependent) mask to yield the K i pixels. For this simple three-point normalization, the resulting masked images vary in size between 7200 and 12,600 pixels, depending on the pose.</p><p>The second normalization is more complex and is motivated by the success of active appearance models (AAMs) <ref type="bibr" target="#b11">[12]</ref>. This normalization is based on the location of a large number (39-54 depending on the pose) of points on the face. These canonical points are triangulated and the image warped with a piecewise affine warp onto a coordinate frame in which the canonical points are in fixed locations. The resulting masked images for this multipoint normalization vary in size between 20,800 and 36,000 pixels. Although currently the multipoint normalization is performed using hand-marked points, it could be performed by fitting an AAM <ref type="bibr" target="#b11">[12]</ref> and then using the implied canonical point locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Using Nearest Neighbor</head><p>The eigen light-field estimation algorithm outputs a vector of eigen coefficients (a 1 , . . . , a d ). Given a set of gallery faces, we obtain a corresponding set of vectors (a id 1 , . . . , a id d ), where id is an index over the set of gallery faces. Similarly, given a probe face, we obtain a vector (a 1 , . . . , a d ) of eigen coefficients for that face. To complete the face recognition algorithm we need an algorithm that classifies (a 1 , . . . , a d ) with the index id, which is the most likely match. Many classification algorithms could be used for this task. For simplicity, we use the nearestneighbor algorithm, that classifies the vector (a 1 , . . . , a d ) with the index. </p><p>All of the results reported in this chapter use the Euclidean distance in Eq. ( <ref type="formula" target="#formula_6">4</ref>). Alternative distance functions, such as the Mahalanobis distance, could be used instead if so desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting the Gallery, Probe, and Generic Training Data</head><p>In each of our experiments we divided the database into three disjoint subsets:</p><p>Generic training data: Many face recognition algorithms such as eigenfaces, and including our algorithm, require "generic training data" to build a generic face model. In eigenfaces, for example, generic training data are needed to compute the eigenspace. Similarly, in our algorithm, generic data are needed to construct the eigen light-field. Gallery: The gallery is the set of reference images of the people to be recognized (i.e., the images given to the algorithm as examples of each person who might need to be recognized). Probe: The probe set contains the "test" images (i.e., the images to be presented to the system to be classified with the identity of the person in the image).</p><p>The division into these three subsets is performed as follows. First we randomly select half of the subjects as the generic training data. The images of the remaining subjects are used for the gallery and probe. There is therefore never any overlap between the generic training data and the gallery and probe. After the generic training data have been removed, the remainder of the databases are divided into probe and gallery sets based on the pose of the images. For example, we might set the gallery to be the frontal images and the probe set to be the left profiles. In this case, we evaluate how well our algorithm is able to recognize people from their profiles given that the algorithm has seen them only from the front. In the experiments described below we choose the gallery and probe poses in various ways. The gallery and probe are always disjoint unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Databases</head><p>We used two databases in our face recognition across pose experiments, the CMU Pose, Illumination, and Expression (PIE) database <ref type="bibr" target="#b45">[46]</ref> and the FERET database <ref type="bibr" target="#b38">[39]</ref>. Each of these databases contains substantial pose variation. In the pose subset of the CMU PIE database (Fig. <ref type="figure" target="#fig_6">9</ref>.4), the 68 subjects are imaged simultaneously under 13 poses totaling 884 images. In the FERET database, the subjects are imaged nonsimultaneously in nine poses. We used 200 subjects from the FERET pose subset, giving 1800 images in total. If not stated otherwise, we used half of the available subjects for training of the generic eigenspace (34 subjects for PIE, 100 subjects for FERET) and the remaining subjects for testing. In all experiments (if not stated otherwise) we retain a number of eigenvectors sufficient to explain 95% of the variance in the input data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Other Algorithms</head><p>We compared our algorithm with eigenfaces <ref type="bibr" target="#b46">[47]</ref> and FaceIt, the commercial face recognition system from Identix (formerly Visionics). <ref type="foot" target="#foot_0">1</ref>We first performed a comparison using the PIE database. After randomly selecting the generic training data, we selected the gallery pose as one of the 13 PIE poses and the probe pose as any other of the remaining 12 PIE poses. For each disjoint pair of gallery and probe poses, we computed the average recognition rate over all subjects in the probe and gallery sets. The details of the results are shown in Figure <ref type="figure" target="#fig_6">9</ref>.5 and are summarized in Table <ref type="table">9</ref>.1.</p><p>In Figure <ref type="figure" target="#fig_6">9</ref>.5 we plotted color-coded 13 × 13 "confusion matrices" of the results. The row denotes the pose of the gallery, the column the pose of the probe, and the displayed intensity the average recognition rate. A lighter color denotes a higher recognition rate. (On the diagonals the gallery and probe images are the same so all three algorithms obtain a 100% recognition rate.)</p><p>Eigen light-fields performed far better than the other algorithms, as witnessed by the lighter color of Figure <ref type="figure" target="#fig_6">9</ref>.5a,b compared to Figures <ref type="figure" target="#fig_9">9.5c,</ref><ref type="figure">d</ref>. Note how eigen light-fields was far better able to generalize across wide variations in pose, and in particular to and from near-profile views.</p><p>Table <ref type="table">9</ref>.1 includes the average recognition rate computed over all disjoint gallery-probe poses. As can be seen, eigen light-fields outperformed both the standard eigenfaces algorithm and the commercial FaceIt system.</p><p>We next performed a similar comparison using the FERET database <ref type="bibr" target="#b38">[39]</ref>. Just as with the PIE database, we selected the gallery pose as one of the nine FERET poses and the probe pose as any other of the remaining eight FERET poses. For each disjoint pair of gallery and probe poses, we computed the average recognition rate over all subjects in the probe and gallery sets, and then averaged the results. The results are similar to those for the PIE database and are summarized in Table <ref type="table">9</ref>.2. Again, eigen light-fields performed significantly better than either FaceIt or eigenfaces.</p><p>Overall, the performance improvement of eigen light-fields over the other two algorithms is more significant on the PIE database than on the FERET database. This is because the PIE database contains more variation in pose than the FERET database. For more evaluation results see Gross et al. <ref type="bibr" target="#b22">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bayesian Face Subregions</head><p>Owing to the complicated 3D nature of the face, differences exist in how the appearance of various face regions change for different face poses. If, for example, a head rotates from a frontal to a right profile position, the appearance of the mostly featureless cheek region only changes little (if we ignore the influence of illumination), while other regions such as the left eye disappear, and the nose looks vastly different. Our algorithm models the appearance changes of the different face regions in a probabilistic framework <ref type="bibr" target="#b27">[28]</ref>. Using probability distributions for similarity values of face subregions; we compute the likelihood of probe and gallery images coming from the same subject. For training and testing of our algorithm we use the CMU PIE database <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Face Subregions and Feature Representation</head><p>Using the hand-marked locations of both eyes and the midpoint of the mouth, we warp the input face images into a common coordinate frame in which the landmark points are in a fixed location and crop the face region to a standard 128 × 128 pixel size. Each image I in the database is labeled with the identity i and pose φ of the face in the image: I = (i, φ), i ∈ {1, . . . , 68}, φ ∈ {1, . . . , 13}. As shown in Figure <ref type="figure" target="#fig_6">9</ref>.6, a 7 × 3 lattice is placed on the normalized faces, and 9 × 15 pixel subregions are extracted around every lattice point. The intensity values in each of the 21 subregions are normalized to have zero mean and unit variance.</p><p>As the similarity measure between subregions we use SSD (sum of squared difference) values s j between corresponding regions j for all image pairs. Because we compute the SSD after image normalization, it effectively contains the same information as normalized correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Local Appearance Change Across Pose</head><p>For probe image I i,p = (i, φ p ) with unknown identity i we compute the probability that I i,p is coming from the same subject k as gallery image I k,g for each face subregion j, j ∈ {1, . . . , 21}. Using Bayes' rule we write: We assume the conditional probabilities P (s j |i = k, φ p , φ g ) and P (s j |i = k, φ p , φ g ) to be Gaussian distributed and learn the parameters from data. It is reasonable to assume that the pose of each gallery image is known. However, because the pose φ p of the probe images is in general not known, we marginalize over it. We can then compute the conditional densities for similarity value s j as</p><formula xml:id="formula_7">P (i = k|s j , φ p , φ g ) = P (s j |i = k, φ p , φ g )P (i = k) P (s j |i = k, φ p , φ g )P (i = k) + P (s j |i = k, φ p , φ g )P (i = k)<label>(5)</label></formula><formula xml:id="formula_8">P (s j |i = k, φ g ) = p P (φ p )P (s j |i = k, φ p , φ g ) and P (s j |i = k, φ g ) = p P (φ p )P (s j |i = k, φ p , φ g )</formula><p>If no other knowledge about the probe pose is given, the pose prior P (φ p ) is assumed to be uniformly distributed. Similar to the posterior probability defined in Eq. ( <ref type="formula" target="#formula_7">5</ref>) we compute the probability of the unknown probe image coming from the same subject (given similarity value s j and gallery pose φ g ) as</p><formula xml:id="formula_9">P (i = k|s j , φ g ) = P (s j |i = k, φ g )P (i = k) P (s j |i = k, φ g )P (i = k) + P (s j |i = k, φ g )P (i = k) (6)</formula><p>To decide on the most likely identity of an unknown probe image I i,p = (i, φ p ) we compute match probabilities between I i,p and all gallery images for all face subregions using Eq. ( <ref type="formula" target="#formula_7">5</ref>) or <ref type="bibr" target="#b5">(6)</ref>. We currently do not model dependencies between subregions, so we simply combine the different probabilities using the sum rule <ref type="bibr" target="#b28">[29]</ref> and choose the identity of the gallery image with the highest score as the recognition result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>We used half of the 68 subjects in the CMU PIE database for training of the models described in Section 3.2. The remaining 34 subjects are used for testing. The images of all 68 subjects are used in the gallery. We compare our algorithm to eigenfaces <ref type="bibr" target="#b46">[47]</ref> and the commercial FaceIt system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Unknown Probe Pose</head><p>For the first experiment we assume the pose of the probe images to be unknown. We therefore must use Eq. ( <ref type="formula">6</ref>) to compute the posterior probability that probe and gallery images come from the same subject. We assume P (φ p ) to be uniformly distributed, that is, P (φ p ) = 1 13 . Figure <ref type="figure" target="#fig_6">9</ref>.8 compares the recognition accuracies of our algorithm with eigenfaces and FaceIt for frontal gallery images. Our system clearly outperforms both eigenfaces and FaceIt. Our algorithm shows good performance up until 45 • head rotation between probe and gallery image (poses 02 and 31). The performance of eigenfaces and FaceIt already drops at 15 • and 30 • rotation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Known Probe Pose</head><p>In the case of known probe pose we can use Eq. ( <ref type="formula" target="#formula_7">5</ref>) to compute the probability that probe and gallery images come from the same subject. Figure <ref type="figure" target="#fig_6">9</ref>.9 compares the recognition accuracies of our algorithm for frontal gallery images for known and unknown probe poses. Only small differences in performances are visible.  .10 shows recognition accuracies for all three algorithms for all possible combinations of gallery and probe poses. The area around the diagonal in which good performance is achieved is much wider for our algorithm than for either eigenfaces or FaceIt. We therefore conclude that our algorithm generalizes much better across pose than either eigenfaces or FaceIt. Here lighter pixel values correspond to higher recognition accuracies. The area around the diagonal in which good performance is achieved is much wider for our algorithm than for either eigenfaces or FaceIt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Face Recognition Across Pose and Illumination</head><p>Because appearance-based methods use image intensities directly, they are inherently sensitive to variations in illumination. Drastic changes in illumination such as between indoor and outdoor scenes therefore cause significant problems for appearance-based face recognition algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>. In this section we describe two ways to handle illumination variations in facial imagery. The first algorithm extracts illumination invariant subspaces by extending the previously introduced eigen light-fields to Fisher light-fields <ref type="bibr" target="#b21">[22]</ref>, mirroring the step from eigenfaces <ref type="bibr" target="#b46">[47]</ref> to fisherfaces <ref type="bibr" target="#b3">[4]</ref>. The second approach combines Bayesian face subregions with an image preprocessing algorithm that removes illumination variation prior to recognition <ref type="bibr" target="#b19">[20]</ref>. In both cases we demonstrate results for face recognition across pose and illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fisher Light-Fields</head><p>Suppose we are given a set of light-fields L i,j (θ, φ), i = 1, . . . , N, j = 1, . . . , M where each of N objects O i is imaged under M different illumination conditions. We could proceed as described in Section 2.1 and perform PCA on the whole set of N ×M light-fields. An alternative approach is Fisher's linear discriminant (FLD) <ref type="bibr" target="#b13">[14]</ref>, also known as linear discriminant analysis (LDA) <ref type="bibr" target="#b50">[51]</ref>, which uses the available class information to compute a projection better suited for discrimination tasks. Analogous to the algorithm described in Section 2.1, we now find the least-squares solution to the set of equations</p><formula xml:id="formula_10">L(θ, φ) - m i=1 λ i W i (θ, φ) = 0<label>(7)</label></formula><p>where W i , i = 1, . . . , m are the generalized eigenvectors computed by LDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>For our face recognition across pose and illumination experiments we used the pose and illumination subset of the CMU PIE database <ref type="bibr" target="#b45">[46]</ref>. In this subset, 68 subjects are imaged under 13 poses and 21 illumination conditions. Many of the illumination directions introduce fairly subtle variations in appearance, so we selected 12 of the 21 illumination conditions that span the set widely. In total we used 68 × 13 × 12 = 10, 608 images in the experiments. We randomly selected 34 subjects of the PIE database for the generic training data and then removed the data from the experiments (see Section 2.2). There were then a variety of ways to select the gallery and probe images from the remaining data. Same pose, different illumination: The gallery and probe poses are the same. The gallery and probe illuminations are different. This scenario is like traditional face recognition across illumination but is performed separately for each pose. Different pose, same illumination: The gallery and probe poses are different. The gallery and probe illuminations are the same. This scenario is like traditional face recognition across pose but is performed separately for each possible illumination. Different pose, different illumination: Both the pose and illumination of the probe and gallery are different. This is the most difficult and most general scenario.</p><p>We compared our algorithms with FaceIt under these three scenarios. In all cases we generated every possible test scenario and then averaged the results. For "same pose, different illumination," for example, we consider every possible pose. We generated every pair of disjoint probe and gallery illumination conditions. We then computed the average recognition rate for each such case. We averaged over every pose and every pair of distinct illumination conditions. The results are included in Table <ref type="table">9</ref>.3. For "same-pose, different illumination," the task is essentially face recognition across illumination separately for each pose. In this case, it makes little sense to try eigen light-fields because we know how poorly eigenfaces performs with illumination variation. Fisher light-fields becomes fisherfaces for each pose, which empirically we found outperforms FaceIt. Example illumination "confusion matrices" are included for two poses in Figure <ref type="figure" target="#fig_6">9</ref>.11.</p><p>For "different pose, same illumination," the task reduces to face recognition across pose but for a variety of illumination conditions. In this case there is no intraclass variation, so it makes little sense to apply Fisher light-fields. This experiment is the same as Experiment 1 in Section 2.3 but the results are averaged over every possible illumination condition. As we found for Experiment 1, eigen light-fields outperforms FaceIt by a large amount. Finally, in the "different pose, different illumination" task both algorithms perform fairly poorly. However, the task is difficult. If the pose and illumination are both extreme, almost none of the face is visible. Because this case might occur in either the probe or the gallery, the chance that such a difficult case occurs is large. Although more work is needed on this task, note that Fisher light-fields still outperforms FaceIt by a large amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Illumination Invariant Bayesian Face Subregions</head><p>In general, an image I(x, y) is regarded as product I(x, y) = R(x, y)L(x, y), where R(x, y) is the reflectance and L(x, y) is the illuminance at each point (x, y) <ref type="bibr" target="#b25">[26]</ref>. Computing the reflectance and the illuminance fields from real images is, in general, an ill-posed problem. Our approach uses two widely accepted assumptions about human vision to solve the problem: (1) human vision is mostly sensitive to scene reflectance and mostly insensitive to the illumination conditions; and (2) human vision responds to local changes in contrast rather than to global brightness levels. Our algorithm computes an estimate of L(x, y) such that when it divides I(x, y) it produces R(x, y) in which the local contrast is appropriately enhanced. We find a </p><p>Here Ω refers to the image. The parameter λ controls the relative importance of the two terms. The space varying permeability weight ρ(x, y) controls the anisotropic nature of the smoothing constraint. See Gross and Brajovic <ref type="bibr" target="#b19">[20]</ref> for details. Figure <ref type="figure" target="#fig_6">9</ref>.12 shows examples from the CMU PIE database before and after processing with our algorithm. We used this algorithm to normalize the images of the combined pose and illumination subset of the PIE database. Figure <ref type="figure" target="#fig_6">9</ref>.13 compares the recognition accuracies of the Bayesian face subregions algorithm for original and normalized images using gallery images with frontal pose and illumination. The algorithm achieved better performance on normalized images across all probe poses. Overall the average recognition accuracy improved from 37.3% to 44%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>One of the most successful and well studied approaches to object recognition is the appearancebased approach. The defining characteristic of appearance-based algorithms is that they directly use the pixel intensity values in an image of the object as the features on which to base the recognition decision. In this chapter we described an appearance-based method for face recognition across pose based on an algorithm to estimate the eigen light-field from a collection of images. Unlike previous appearance-based methods, our algorithm can use any number of gallery images captured from arbitrary poses and any number of probe images also captured from arbitrary poses. The gallery and probe poses do not need to overlap. We showed that our Original Normalized Fig. <ref type="figure" target="#fig_6">9</ref>.13. Recognition accuracies of the Bayesian face subregions algorithm on original and normalized images using gallery images with frontal pose and illumination. For each probe pose the accuracy is determined by averaging the results for all 21 illumination conditions. The algorithm achieves better performance on normalized images across all probe poses. The probe pose is assumed to be known. algorithm can reliably recognize faces across pose and also take advantage of the additional information contained in widely separated views to improve recognition performance if more than one gallery or probe image is available.</p><p>In eigen light-fields all face pixels are treated equally. However, differences exist in how the appearance of various face regions change across face poses. We described a second algorithm, Bayesian face subregions, which derives a model for these differences and successfully employes it for face recognition across pose. Finally, we demonstrated how to extend both algorithms toward face recognition across both pose and illumination. Note, however, that for this task recognition accuracies are significantly lower, suggesting that there still is room for improvement. For example, the model-based approach of Romdhani et al. <ref type="bibr" target="#b39">[40]</ref> achieved better results across pose on the PIE database than the appearance-based algorithms described here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 9 . 2 .</head><label>92</label><figDesc>Fig. 9.2. Our eigen light-field estimation algorithm for re-rendering a face across pose. The algorithm is given the left-most (frontal) image as input from which it estimates the eigen light-field and then creates the rotated view shown in the middle. For comparison, the original rotated view is shown in the right-most column. In the figure we show one of the better results (top) and one of the worst (bottom.) Although in both cases the output looks like a face, the identity is altered in the second case.</figDesc><graphic coords="7,213.84,155.92,76.53,73.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 9 . 3 .</head><label>93</label><figDesc>Fig. 9.3. Vectorization by normalization. Vectorization is the process of converting a set of images of a face into a light-field vector. Vectorization is performed by first classifying each input image into one of a finite number of poses. For each pose, normalization is then applied to convert the image into a subvector of the light-field vector. If poses are missing, the corresponding part of the light-field vector is missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>a i -a id i 2</head><label>2</label><figDesc>1 , . . . , a d ), (a id 1 , . . . , a id d ) = arg min id d i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 . 4 .</head><label>94</label><figDesc>Fig. 9.4. Pose variation in the PIE database. The pose varies from full left profile (c34) to full frontal (c27) and to full right profile (c22). Approximate pose angles are shown below the camera numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 . 5 .</head><label>95</label><figDesc>Fig. 9.5. Comparison with FaceIt and eigenfaces for face recognition across pose on the CMU PIE [46] database. For each pair of gallery and probe poses, we plotted the color-coded average recognition rate. The row denotes the pose of the gallery and the column the pose of the probe. The fact that the images in a and b are lighter in color than those in c and d implies that our algorithm performs better.</figDesc><graphic coords="12,105.30,204.87,143.45,113.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 . 6 .</head><label>96</label><figDesc>Fig. 9.6. Face subregions for two poses of the CMU PIE database. Each face in the database is warped into a normalized coordinate frame using the hand-labeled locations of both eyes and the midpoint of the mouth. A 7 × 3 lattice is placed on the normalized face, and 9 × 15 pixel subregions are extracted around every lattice point, resulting in a total of 21 subregions.</figDesc><graphic coords="14,190.56,78.85,60.54,79.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>7 shows histograms of similarity values for the right eye region. The examples in Figure 9.7 show that the discriminative power of the right eye region diminishes as the probe pose changes from almost frontal (Figure 9.7a) to right profile (Figure 9.7c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 . 7 .</head><label>97</label><figDesc>Fig. 9.7. Histograms of similarity values sj for the right eye region across multiple poses. The distribution of similarity values for identical gallery and probe subjects are shown with solid curves, the distributions for different gallery and probe subjects are shown with broken curves.</figDesc><graphic coords="15,107.59,78.28,94.68,166.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 . 8 .Fig. 9 . 9 .</head><label>9899</label><figDesc>Fig. 9.8. Recognition accuracies for our algorithm (labeled BFS), eigenfaces, and FaceIt for frontal gallery images and unknown probe poses. Our algorithm clearly outperforms both eigenfaces and FaceIt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9</head><label>9</label><figDesc>Figure 9.10 shows recognition accuracies for all three algorithms for all possible combinations of gallery and probe poses. The area around the diagonal in which good performance is achieved is much wider for our algorithm than for either eigenfaces or FaceIt. We therefore conclude that our algorithm generalizes much better across pose than either eigenfaces or FaceIt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 . 10 .</head><label>910</label><figDesc>Fig. 9.10. Recognition accuracies for our algorithm, eigenfaces, and FaceIt for all possible combinations of gallery and probe poses.Here lighter pixel values correspond to higher recognition accuracies. The area around the diagonal in which good performance is achieved is much wider for our algorithm than for either eigenfaces or FaceIt.</figDesc><graphic coords="17,51.14,79.47,148.95,111.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 . 11 .</head><label>911</label><figDesc>Fig. 9.11. Example "confusion matrices" for the "same-pose, different illumination" task. For a given pose, and a pair of distinct probe and gallery illumination conditions, we color-code the average recognition rate. The superior performance of Fisher light-fields is witnessed by the lighter color of (a-b) over (c-d).</figDesc><graphic coords="19,113.91,207.59,139.28,109.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 . 12 .</head><label>912</label><figDesc>Fig. 9.12. Result of removing illumination variations with our algorithm for a set of images from the PIE database.</figDesc><graphic coords="20,106.53,163.19,53.46,70.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 9 . 1 .</head><label>91</label><figDesc>Comparison of eigen light-fields with FaceIt and eigenfaces for face recognition across pose on the CMU PIE database. The table contains the average recognition rate computed across all disjoint pairs of gallery and probe poses; it summarizes the average performance in Figure9.5.</figDesc><table><row><cell>Algorithm</cell><cell>Average recognition accuracy (%)</cell></row><row><cell>Eigenfaces</cell><cell>16.6</cell></row><row><cell>FaceIt</cell><cell>24.3</cell></row><row><cell>Eigen light-fields</cell><cell></cell></row><row><cell>Three-point norm</cell><cell>52.5</cell></row><row><cell>Multipoint norm</cell><cell>66.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 9 . 2 .</head><label>92</label><figDesc>Comparison of eigen light-fields with FaceIt and eigenfaces for face recognition across pose on the FERET database. The table contains the average recognition rate computed across all disjoint pairs of gallery and probe poses. Again, eigen light-fields outperforms both eigenfaces and FaceIt.</figDesc><table><row><cell>Algorithm</cell><cell>Average recognition accuracy (%)</cell></row><row><cell>Eigenfaces</cell><cell>39.4</cell></row><row><cell>FaceIt</cell><cell>59.3</cell></row><row><cell>Eigen light-fields three-point normalization</cell><cell>75.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 9 . 3 .</head><label>93</label><figDesc>Performance of eigen light-fields and Fisher light-fields with FaceIt on three face recognition across pose and illumination scenarios. In all three cases, eigen light-fields and Fisher light-fields outperformed FaceIt by a large margin.</figDesc><table><row><cell>Conditions</cell><cell cols="3">Eigen light-fields Fisher light-fields FaceIt</cell></row><row><cell>Same pose, different illumination</cell><cell>-</cell><cell>81.1%</cell><cell>41.6%</cell></row><row><cell>Different pose, same illumination</cell><cell>72.9%</cell><cell>-</cell><cell>25.8%</cell></row><row><cell>Different pose, different illumination</cell><cell>-</cell><cell>36.0%</cell><cell>18.1%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Version</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2.5.0.17 of the FaceIt recognition engine was used in the experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research described here was supported by U.S. Office of Naval Research contract N00014-00-1-0915 and in part by U.S. Department of Defense contract N41756-03-C4024. Portions of the research in this paper used the FERET database of facial images collected under the FERET program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The plenoptic function and elements of early vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>Landy and Movshon</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition: The problem of compensating for changes in illumination direction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Adini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="721" to="732" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is the set of images of an object under all possible lighting conditions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Face recognition under varying pose</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<idno>1461</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT AI Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face recognition from one example view. A.I. Memo No. 1536</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT AI Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eigen-tracking: robust matching and tracking of articulated objects using a view-based representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="130" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<title level="m">Facial recognition vendor test 2000: evaluation report</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face identification across different poses and illumination with a 3D morphable model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Face and Gesture Recognition</title>
		<meeting>the Fifth International Conference on Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="657" to="664" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Pattern Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From few to many: Generative models for recognition under variable pose and illumination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Face and Gesture Recognition</title>
		<meeting>the Fourth International Conference on Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Illumination cones for recognition under variable lighting: faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From few to many: generative models for recognition under variable pose and illumination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition from unfamiliar views: subspace methods and pose dependency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image pre-processing algorithm for illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brajovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Audio-and Video Based Biometric Person Authentication (AVBPA)</title>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eigen light-fields and face recognition across pose</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Face and Gesture Recognition</title>
		<meeting>the Fifth International Conference on Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fisher light-fields for face recognition across pose and illumination</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Symposium on Pattern Recognition (DAGM)</title>
		<meeting>the German Symposium on Pattern Recognition (DAGM)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="481" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Appearance-based face recognition and light-fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="465" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quo vadis face recognition?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Empirical Evaluation Methods in Computer Vision</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Determining lightness from an image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="277" to="299" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Robot Vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing images under variable illumination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="610" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-subregion based probabilistic approach toward pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalization from a single view in face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Automatic Face-and Gesture-Recognition</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic interpretation and coding of face images using flexible models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust recognition using eigenimages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single-view based recognition of faces rotated in depth</title>
		<author>
			<persName><forename type="first">T</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Automatic Face and Gesture Recogition</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="248" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3-D objects from appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">View-based and modular eigenspaces for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bone</surname></persName>
		</author>
		<title level="m">Face recognition vendor test 2002: evaluation report</title>
		<imprint>
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face identification by matching a 3D morphable model using linear shape and texture error functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view nonlinear active shape model using kernel PCA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Psarrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On utilising template and feature-based correspondence in multi-view appearance models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Psarrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="799" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Geometry and Photometry in 3D visual recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Quotient image: class-based re-rendering and recognition with varying illumination conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Riklin-Raviv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combining models and exemplars for face recognition: an illuminating example</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Models versus Exemplars in Computer Vision</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Linear object classes and image synthesis from a single example image</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="733" to="741" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Robust face recognition using symmetric shape-from-shading</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Center for Automation Research, University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discriminant analysis of principal components for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition: From Theory to Applications</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
