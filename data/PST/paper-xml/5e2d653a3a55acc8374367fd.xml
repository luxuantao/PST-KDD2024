<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Recognition of Names and Publications in Academic Homepages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yimeng</forename><surname>Dai</surname></persName>
							<email>yimengd@student.unimelb.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
							<email>jianzhong.qi@unimelb.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>rui.zhang@unimelb.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Recognition of Names and Publications in Academic Homepages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3336191.3371771</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Academic homepages are an important source for learning researchers' profiles. Recognising person names and publications in academic homepages are two fundamental tasks for understanding the identities of the homepages and collaboration networks of the researchers. Existing studies have tackled person name recognition and publication recognition separately. We observe that these two tasks are correlated since person names and publications often co-occur. Further, there are strong position patterns for the occurrence of person names and publications. With these observations, we propose a novel deep learning model consisting of two main modules, an alternatingly updated memory module which exploits the knowledge and correlation from both tasks, and a positionaware memory module which captures the patterns of where in a homepage names and publications appear. Empirical results show that our proposed model outperforms the state-of-the-art publication recognition model by 3.64% in F1 score and outperforms the state-of-the-art person name recognition model by 2.06% in F1 score. Ablation studies and visualisation confirm the effectiveness of the proposed modules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recognition of person names and publications from academic homepages are two essential tasks for analysing researchers' profiles. There have been extensive research interests in the extraction and mining of such information from academic homepages <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. The recognition process has become a necessary part of many online systems, such as AMiner <ref type="bibr" target="#b23">[24]</ref> and CiteSeerX <ref type="bibr" target="#b15">[16]</ref>, and the extracted person names and publications can bring interesting applications. For example, person names can provide valuable insights for analysing researchers' collaboration networks. Publications can be used to mine the evolution of a researcher's research interests and predict the development directions of the researcher. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of the two tasks. Given the plain text of an academic homepage, the aim is to recognise every person name and every publication as a text string shown in the example.</p><p>Recently, deep learning based methods have been developed to address these problems. The state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and line-level structure. The state-of-the-art for person name in academic homepages <ref type="bibr" target="#b0">[1]</ref> uses a co-guided neural network to learn from fine-grained annotation of names. Despite their success, these studies have tackled the two tasks separately. We observe that there is a strong correlation between person names and publications. For example, a string is more likely to refer to a publication if it contains multiple names consecutively. Also, strings that appear in multiple publications are likely to be a person's name, e.g., the page owner or a frequent coauthor. Such an observation motivates us to design joint learning models for publication and person names simultaneously. A straightforward method to learn a model for the two tasks jointly is to train them together by minimising the total loss of the two tasks or simply concatenating the representation of publication and person name when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. However, our experimental study shows that such a straightforward approach performs poorly. The issue is that they cannot capture the correlation between the two tasks well since the learned signals from the Technical Presentation WSDM '20, February 3-7, 2020, Houston, TX, USA two tasks do not have enough intermediate interaction with each other at each iteration of training. Further, we observe that there are strong patterns of where in a homepage names and publications appear. Academic homepages usually present information in separate blocks, e.g., one for biography and another for publications (cf. Figure <ref type="figure" target="#fig_0">1</ref>). These blocks may use different formatting styles, such as paraghraphs, lists, and tables. The grouping of similar contents into separate blocks and the similar formatting styles within the same block lead to strong position patterns in the plain text of academic homepages. Specifically, the contents of the same block may run across multiple consecutive lines, while the contents of different blocks may be separated. Further, each line or several consecutive lines in a block may describe one piece of information. For example, the block for publications in Figure <ref type="figure" target="#fig_0">1</ref> consists of six consecutive lines and each publication consists of three lines. The position of lines and blocks provides valuable signals for the recognition tasks.</p><p>To address the issues in straightforward joint models, and to better utilise the correlation and position patterns of person names and publications, we propose a novel Position-aware Alternating Memory (PAM) network. PAM consists of two main modules, an alternatingly updated memory (AM) module which exploits the knowledge and correlation from both tasks, and a position-aware memory (PM) module which captures the patterns of where in a homepage names and publications appear. In the AM module, an attention-based memory updating controller is used to activate hidden representation from a name encoder and a publication encoder alternatingly, and update the memory representation alternatingly to enhance the intermediate interaction in each iteration. The correlation representation between person names and publications is captured in the alternating updates of memories. In the PM module, position representations are integrated into the correlation representation between person names and publications. The position representations consist of local and global positions. The local position representations capture the difference in line numbers between tokens. The global position representations capture the attention distribution of all the lines in a homepage with respect to the publication block.</p><p>In summary, this paper makes the following contributions:</p><p>• We address the tasks of person name recognition and publication recognition in academic homepages simultaneously by modeling their correlation and the position patterns.</p><p>• We propose a deep learning model named PAM, which consists of two main modules, an alternatingly updated memory module and a position-aware memory module.</p><p>• We conduct a thorough experimental study using real datasets. The empirical results show that our model PAM outperforms the state-of-the-art publication recognition model by 3.64% in F1 score and outperforms the state-of-the-art person name recognition model by 2.06% in F1 score. Ablation studies and visualisation confirm the effectiveness of the proposed modules in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Previous studies on academic homepages usually use rule-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref> or a hybrid of machine learning and rule-based methods <ref type="bibr" target="#b2">[3]</ref> on the HTML DOM trees of webpages. Yang and Ho <ref type="bibr" target="#b29">[30]</ref> use heuristic rules to locate the publications in a DOM tree. They assume that publications are listed as nodes at the same level in the DOM tree. Chung et al. <ref type="bibr" target="#b2">[3]</ref> uses a linear chain CRF model to analyse the content in a DOM tree and then refines the publication boundaries by rules.</p><p>Recent studies on academic homepages usually treat the plain text of a homepage as a document and recognise information from the plain text using deep learning based natural language processing methods. For example, state-of-the-art techniques for publication recognition <ref type="bibr" target="#b30">[31]</ref> and for person names recognition <ref type="bibr" target="#b0">[1]</ref> use Bi-LSTM-CRF based models to recognise information from the plain text of the homepages. However, they solve the two tasks separately. To the best of our knowledge, no existing work has taken a joint learning approach to recognising person names and publications simultaneously from the plain text of academic homepages.</p><p>A few other studies recognise person names and publications from research papers and digital libraries <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Such a recognition problem is simpler since the text in research papers and digital libraries is usually well-formatted with few format variations. After recognition, these studies may need to solve the name disambiguation problem (i.e., different people with identical names) <ref type="bibr" target="#b22">[23]</ref> before mining the collaboration networks or research interests of a researcher. Such a problem can be alleviated by recognising information from academic homepages.</p><p>Models based on memory networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> are proposed for question answering in recent years. Dynamic Memory Network (DMN) <ref type="bibr" target="#b11">[12]</ref> uses a gated recurrent unit <ref type="bibr" target="#b1">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type="bibr" target="#b16">[17]</ref> uses a multi-head attention <ref type="bibr" target="#b25">[26]</ref> based controller. All these networks use a memory module for a single task and update the memory repeatly, while our model updates the memory alternatingly using the knowledge from two correlated tasks.</p><p>Moreover, we use different methods to capture the position patterns. The state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> trains webpage-level and line-level models together to capture the position information of academic homepage, whereas our model captures position information by integrating them into the memory updating process. Studies have exploited relative token position and importance in a sentence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, whereas our algorithm focuses on relative line position and importance in a page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JOINT LEARNING FOR BOTH TASKS</head><p>Usually, the plain text of an academic homepage is saved first, then the recognition tasks are conducted on text <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. Given the plain text of an academic homepage, we aim to recognise all the person names and publications from the plain text simultaneously. To accomplish this, a straightforward method is to train a model for the two tasks together by solving a joint optimization problem, i.e., minimising the total loss of the two tasks, or using simple concatenation procedures when training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. However, our experimental study (Section 4.3.2) shows that such a naive way of joint learning does not yield good performance since the correlation between the two tasks cannot be captured well.</p><p>To address the problem, we propose a Position-aware Alternating Memory (PAM) network. Figure <ref type="figure" target="#fig_1">2</ref> illustrates our proposed PAM network. Our model consists of four modules including input processor, alternatingly updated memory (AM), position-aware memory (PM) and joint recognition. AM an PM are the two main modules for joint recognition of names and publications. In input processor (Section 3.1), we tokenise the plain text, use word embeddings to represent the tokenised text, and encode them through two encoders to get two sequential hidden representations, one for person names and the other for publications. Then, the hidden representations are passed to AM module (Section 3.2) to capture the correlation between person names and publications. Specifically, we use a memory updating controller to activate hidden representation and update the memory alternatingly. In position-aware memory (Section 3.3), to take advantage of the position patterns in academic homepages, we expand the memory updating controller and integrate local and global position representations into the memory updating process. In the joint recognition module (Section 3.4), we produce recognition reuslts based on the updated memory and jointly learn the two tasks. Table <ref type="table">1</ref> summarises the symbols frequently used in the following discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Processor</head><p>Given the plain text of an academic homepage, we first tokenise it and get a sequence S of n tokens and each token is represented as a d e -dimension word embedding, i.e., S ∈ R n×d e . Following state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>, we use GloVe <ref type="bibr" target="#b18">[19]</ref> to learn word embeddings on an academic homepage dataset (detailed in Section 4.1.1), although other pre-training methods may be used here without loss of generality. Then, we encode the input sequence S via two recurrent neural networks (RNNs), one for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type="bibr" target="#b6">[7]</ref> as the RNN unit:</p><formula xml:id="formula_0">N = LST M (S ) and P = LST M (S )<label>(1)</label></formula><p>Here, N ∈ R n×d h is the hidden representation from the person name encoder (i.e., an LSTM), P ∈ R n×d h is the hidden representation from the publication encoder (another LSTM), and d h is the dimensionality of each token in hidden representations. Next, S, N , and P are passed to the AM module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Alternatingly Updated Memory</head><p>Different from the traditional Memory Networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, which updates a memory representation repeatedly for a single task, we propose to update the memory representation alternatingly using the knowledge from two correlated tasks. Our intuition is to improve the learned representations for one task by taking into account the knowledge from the other task. Specifically, AM initialises the memory representation M with S, i.e., M 0 = S, and updates it using an alternating hidden representation A, which is obtained from the person name encoder and publication encoder by:</p><formula xml:id="formula_1">A = f (i + 1)N + f (i)P<label>(2)</label></formula><p>where A ∈ R n×d h , i is the hop number and function f activate N and P alternatingly in two consecutive hops by providing alternating boolean values for even and odd values of i:</p><formula xml:id="formula_2">f (i) = 1 2 [(−1) i + 1]<label>(3)</label></formula><p>When updating the memory representation M, we use a memory updating controller based on multi-head attention <ref type="bibr" target="#b25">[26]</ref>, which is similar to that used in Working Memory Network <ref type="bibr" target="#b16">[17]</ref>. Multi-head attention allows the model to jointly attend to different representation subspaces using projection matrices.</p><p>Let Z j denote the memory representation in head j. The memory representation in hop i, denoted by M i , is the concatenated memory representations of all the heads:</p><formula xml:id="formula_3">M i = (Z 1 ⊕ Z 2 ⊕ ... ⊕ Z h )W Z (4)</formula><p>where</p><formula xml:id="formula_4">M i ∈ R n×d m , W Z ∈ R d m ×d m is a projection matrix, d m</formula><p>is the dimensionality of each token in M i , ⊕ is the concatenation procedure, and h is the number of heads. Let R j denote the encoding of the specific relationship between the most recent memory representation M i−1 and A in head j. Then Z j is:</p><formula xml:id="formula_5">Z j = so f tmax ( R j √ d z )AW A j<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">j ∈ [1,h], Z j ∈ R n×d z , W A j ∈ R d h ×d z is a projection matrix, 1 √ d z</formula><p>is the scaling factor (cf. Figure <ref type="figure" target="#fig_1">2</ref>), and d z = d m h . R j is the dot-product between the projection of M i−1 in head j and the transpose of the projection of A in head j, which is a key step for capturing the correlation in the alternating updates, given by the following equation:</p><formula xml:id="formula_7">R j = M i−1 W M j (AW A ′ j ) ⊤<label>(6)</label></formula><p>where R j ∈ R n×n , W M j ∈ R d m ×d z and W A ′ j ∈ R d h ×d z are projection matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Position-aware Memory</head><p>To exploit the position patterns in academic homepages, we further learn a position-aware memory. Specifically, we integrate the global position representation U д and the local position representation U l into the memory updating process by extending Equation ( <ref type="formula" target="#formula_7">6</ref>) :</p><formula xml:id="formula_8">R j = M i−1 W M j (AW A j + U l ) ⊤ + f (i)U д<label>(7)</label></formula><p>3.3.1 Global Position. We observe that the contents of a block run across consecutive lines. We utilise such a position pattern and model the attention distribution of all the lines in a homepage in U д when recognising publications, i.e., the lines around the median line of a publication block should have more attention, while the lines far from the median line should have less. The attention distribution is assumed to follow a normal distribution. Let G t denote the attention distribution for token t. Then U д is the concatenation of all such distributions:</p><formula xml:id="formula_9">U д = G 0 ⊕ G 1 ⊕ ... ⊕ G n<label>(8)</label></formula><p>where U д ∈ R n×n . Let l µ denote the median line of the predicted publication block Q and σ denote half of the total number of lines in Q (cf. Figure <ref type="figure" target="#fig_3">3</ref>). Then we have:</p><formula xml:id="formula_10">G t = [− (l 1 − l µ ) 2 2σ 2 , ..., − (l n − l µ ) 2 2σ 2 ] (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where G t ∈ R n and details for optimizing l µ are in Section 3.4.</p><p>Let k denote the number of predicted publication tokens in a line, k denote the mean of all the k values in a homepage, then the predicted publication block Q is the set with the most consecutive lines that contains more predicted publication tokens than k.</p><p>Let V denote the number of tokens in line l and T l v denote the prediction for token v in l, then the number of predicted publication tokens in l, denoted by k l , is:</p><formula xml:id="formula_12">k l = V v=1 T l v (10)</formula><p>where T l v is computed by:</p><formula xml:id="formula_13">T l v = 0, top(B l v ) pub 1, top(B l v ) ∈ pub<label>(11)</label></formula><formula xml:id="formula_14">B = so f tmax (PW B + b B )<label>(12)</label></formula><p>Here, W P ∈ R n×d P , b P ∈ R d P , d P is the number of token labels (e.g., BIO) in the publication recognition task, and B is the probability distribution of the possible token labels for all the tokens based on P. Function top() finds the label with the largest posibillity for each token: top(B l v ) ∈ pub means that the found label is a publication label while top(B l v ) pub means otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Local</head><p>Position. U l captures the difference in line numbers between tokens. Let L t denote the embedding of the relative line distances between token t and every other token in the homepage, then U l is the concatenation of all such embeddings:</p><formula xml:id="formula_15">U l = L 0 ⊕ L 1 ⊕ ... ⊕ L n (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where U l ∈ R n×n×d z . Let l t denote the line number of token t, ε denote the total number of lines in the homepage (cf. Figure <ref type="figure" target="#fig_3">3</ref>), emb denote a function that yields a d z -dimension embedding, then L t is:</p><formula xml:id="formula_17">L t = emb ([ l 1 − l t ε , ..., l n − l t ε ])<label>(14)</label></formula><p>where t ∈ [1,n], L t ∈ R n×d z and emb is applied to reduce the space complexity when computing U l in multi-head attention <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Recognition</head><p>The improved memory representations from PAM is used to produce our final output by:</p><formula xml:id="formula_18">N = so f tmax (M c W N + b N )<label>(15)</label></formula><formula xml:id="formula_19">P = so f tmax (M c−1 W P + b P )<label>(16)</label></formula><p>where</p><formula xml:id="formula_20">W N ∈ R d m ×d N , b N ∈ R d N , W P ∈ R d m ×d P , b P ∈ R d P , d N</formula><p>is the number of token labels (e.g., BIO) in person name recognition task and c is the final hop. N and P contain the learned probability distributions of the labels for all the tokens. For each token in each task, the label with the highest posibillity is our final output.</p><p>Our model is trained by minimising the following loss function:</p><formula xml:id="formula_21">L = L N + L P + λL D (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>where L N and L P are the loss for the person name recognition task and the publication recognition task, respectively, and L D is added to optimise l µ with λ as the weight. Specifically:</p><formula xml:id="formula_23">L N = − 1 n n i=1 Ñi log( Ni )<label>(18)</label></formula><formula xml:id="formula_24">L P = − 1 n n i=1 Pi log( Pi )<label>(19)</label></formula><formula xml:id="formula_25">L D = ∥ l − l µ ∥ ε<label>(20)</label></formula><p>where Ñ and P are the ground turth for the two tasks and L N and L P are the average of cross-entropies for the two tasks, respectively. ∥ l −l µ ∥ is the distance between the ground truth median line l in the publication block and the predicted median line l µ . l is computed in the same way as l µ , except that k l is based on the ground truth P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We report experimental results in this section. We start with the experimental setup (Section 4.1). To evaluate the effectiveness of our proposed model, we compare it with state-of-the-art models that solve the two task separately (Section 4.2). We also compare our model with other naive joint learning models (Section 4.3.1). To evaluate the effectiveness of the architectural choices of our model, we perform an ablation study and compare our model with variants of our models (Section 4.3.2). To better understand how the model works, we also show and analyse the visualisation result (Section 4.4) and conduct an error analysis (Section 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>4.1.1 Dataset and Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type="bibr" target="#b30">[31]</ref> and person name recognition <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_0">2</ref> summarises the dataset statistics.</p><p>• HomePub dataset <ref type="bibr" target="#b30">[31]</ref> contains the plain text of 2,087 homepages from different universities and research institutes with 12,796 publications annotated. • HomeName dataset <ref type="bibr" target="#b0">[1]</ref> is constructed from the HomePub dataset by further labeling the person names. All the 70,864 person names are annotated with fine-grained forms such as whether a name is a first or last name. We keep only the annotation for names. We focus on English webpages and convert any text in Unicode to ASCII using Unidecode <ref type="foot" target="#foot_0">1</ref> . The text is tokenised on whitespace, newline characters, and punctuations. Every punctuation and newline character is considered as a single token. Standard BIO tagging scheme is adopted. Word embeddings are initialised with 100 dimensional GloVe <ref type="bibr" target="#b18">[19]</ref> vectors trained on the tokenised dataset. 4.1.2 Evaluation Metric. We measure precision (P), recall (R), and F1-score (F1). For person name recognition, we report the Token Level performance, which reflects the model capability to recognise each person name token. We also report the Name Level performance, in which the model needs to recognise a whole person name without missing any token. For example, for the name 'John Doe', recognising either 'John' or 'Doe' is a true positive at token level, while only recognising 'John Doe' in full is a true positive at name level. For publication recognition, we report the String Level performance, in which the model needs to recognise a whole publication without missing any token. 4.1.3 Model Implementation . We implement our PAM model in TensorFlow <ref type="foot" target="#foot_1">2</ref> and train it on an NVIDIA GTX1080 GPU. The model is trained with the Adam optimiser <ref type="bibr" target="#b10">[11]</ref> with the learning rates tuned among {0.01, 0.005, 0.001}. The batch size is tuned among {32, 64, 128} and the maximum sequence length is tuned among {100, 200, 500}. The dimensions of the encoders are tuned among {100, 200} and the dropout rate is set to 0.5. The number of hops is tuned among {2, 4, 6, 8}, the number of attention heads is tuned among {2, 5, 8} and λ is tuned among {1, 10, 15}. We use a development set to select the best hyperparameters through grid search and the optimal hyperparameters are highlighted above in bold. The model is trained for a maximum of 20 epochs with early stopping if the performance on the development set does not improve after 3 epochs. The parameters are selected using the same development set with the optimisers and early stopping mechanisms reported in the corresponding papers. If these are not reported, we use the Adam optimiser <ref type="bibr" target="#b10">[11]</ref> to train the model for a maximum of 20 epochs with early stopping if the performance on the development set does not improve after 3 epochs. 4.1.5 Variants. Since there are no existing models that jointly recognise person names and publications, we compare with the following variants of our proposed model:</p><p>• Joint-Naive is resulted from removing the AM and PM modules, in which the outputs of the encoders are fed into the network's final output layers directly, and training the two tasks directly by minimising the total loss. • Joint-Concat is resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. <ref type="bibr" target="#b12">[13]</ref> and Hashimoto et al. <ref type="bibr" target="#b5">[6]</ref>. This model has a pipeline architecture for two jointly trained tasks. In the N → P direction, the output of the name encoder and the publication encoder is concatenated to be the input of the the publication predictor. In the P → N direction, the output of the publication encoder and the name encoder are concatenated to be the input of the name predictor. • Joint-Gate is resulted from replacing the AM and PM modules with the gating function in DMN <ref type="bibr" target="#b11">[12]</ref>. This model has a pipeline architecture for two jointly trained tasks. In the N → P direction, the output of the name encoder and the publication encoder are summed up by the gates for several hops to be the input of the publication predictor. And the gates are learned based on the output of the name encoder and the publication encoder. In the P → N direction, the output of the publication encoder and the name encoder are summed up by the gates for several hops to be the input of the name predictor. And the gates are learned based on the output of the publication encoder and the name encoder. • Joint-Att is resulted from replacing the AM and PM modules with multi-layer multi-head attention <ref type="bibr" target="#b25">[26]</ref>. This model has a pipeline architecture for two jointly trained tasks. In the N → P direction, the attention is computed using the output of the name encoder and the publication encoder, then the output of the publication encoder is weighted by the attention for several hops before feeding into the publication predictor.</p><p>In the P → N direction, the attention is computed using the output of the publication encoder and the name encoder, then the output of the name encoder is weighted by the attention for several hops before feeding into the name predictor. • Joint-Stack is resulted from replacing the AM and PM modules with stacked two groups of multi-layer multi-head attention, each for one task. The attention is computed using the output of the name encoder and the initial word representation, then the output of the name encoder is weighted by the attention for several hops before feeding into the name predictor. After that, the attention is computed using the output of the publication encoder and the updated output from name encoder, then the output of the publication encoder is weighted by the attention for several hops before feeding into the publication predictor. • AM is resulted from removing the PM module from our proposed model. Technical Presentation WSDM '20, February 3-7, 2020, Houston, TX, USA  <ref type="figure" target="#fig_6">4</ref> illustrates the architectures of different joint models. All the above models are trained jointly by minimising the total loss of the two tasks on the NVIDIA GTX1080 GPU with a batch size of 32, a dropout rate of 0.5, a maximum sequence length of 200, encoder dimensions of 100 and a learning rate of 0.01. Joint-Gate and Joint-Att have hops of 2 and Joint-Stack has hops of 2 in each group. Joint-Att and Joint-Stack have attention heads of 5. All the models are trained with the Adam optimiser <ref type="bibr" target="#b10">[11]</ref> and the hyperparameters are selected using the same development set reported in Section 4.1.1. The models are trained for a maximum of 20 epochs with early stopping if the performance on the development set does not improve after 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the State-of-the-Art</head><p>Table <ref type="table" target="#tab_1">3</ref> reports the performance comparison result with the singletask models. Overall, our PAM model outperforms the state-of-theart models on publication recognition and person name recognition by considerable margins and the improvements mainly lie in the recall. The improvements are statistically significant, with p &lt;0.05 based on McNemar's test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Publication</head><p>Recognition. The advantage of PAM over neural baselines such as CNN-sentence <ref type="bibr" target="#b9">[10]</ref> and Bi-LSTM-CNN-CRF <ref type="bibr" target="#b13">[14]</ref> is over 15.47% in terms of F1 score since CNN-sentence and Bi-LSTM-CNN-CRF can hardly handle complex homepages without the extra information about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type="bibr" target="#b30">[31]</ref> model, which can capture the positional diversity, by 3.64% in F1 score. The advantage of our model is more significant in recall than in precision. This may be explained by our use of the global position representation, which helps yield higher attention to the publications on a publication block and helps capture more publications. PubSE may miss some publications since the block information are not captured well in their models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Person Name Recognition.</head><p>Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type="bibr" target="#b4">[5]</ref> and Bi-LSTM-CRF <ref type="bibr" target="#b8">[9]</ref>, by at least 5.56% on token level and 7.09% on name level in F1 score. Our improvements mainly lie in the recall, which is consistent with the observation on the publication recognition task. This indicates that our model has better capability to cover more person names with the knowledge from the publication recognition task. PAM also outperforms CogNN <ref type="bibr" target="#b0">[1]</ref> by 1.40% on token level and 2.06% on name level in F1 score. Note that CogNN relies on extra labelling information such as whether the tokens are first names or family names, while our model does not have this requirement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Model Performance without AM.</head><p>The models with a name prefix of 'Joint-' (i.e., Joint-Naive, Joint-Concat, Joint-Gate, Joint-Att, Joint-Stack) do not contain AM module. We can see from Table <ref type="table" target="#tab_2">4</ref> that they perform worse than AM, with an up to 46.3% drop in F1 score on the string level for publication recognition and an up to 6.7% drop in F1 score on the name level for person name recognition. Models having more architecture similarity with AM achieve better result than others, i.e., Joint-Att and Joint-Stack perform better than others. Joint-Concat tends to introduce noise from one task into the other task, which leads to worser results than Joint-Naive. Joint-Gate tends to use the original information in the corresponding task and discard the new information from the other task, which leads to similar results as Joint-Naive. We also observe that with higher frequency we alternatingly updated the representations, we achieve better results, i.e., AM performs better than Joint-Stack. The reason is that the correlation between person names and publications can be better captured in alternating updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Model Performance without PM. The AM, Local-AM and</head><p>Global-AM models do not contain complete PM module. We can see from Table <ref type="table" target="#tab_2">4</ref> that they perform much worse than the full PAM model, with an up to 13.4% drop in F1 score on the string level for publication recognition and an up to 5.4% drop in F1 score on the name level for person name recognition. This indicates that both global and local position representations are critical to the performance of PAM, i.e., removing either or both would result in a drop in performance. Global position is more important than local position, i.e., performance drops more when the global position representation is removed. We also note that PM is more important for the publication recognition task than the person name recognition task, i.e., performance for publication recognition drops more when PM is removed. This is expected as publications have stronger position patterns than person names. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualisation</head><p>We visualise the attention weights on an academic homepage to better examine and understand how the memory module works.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> shows the attention heatmaps with corresponding tokens in different hops of the memory; tokens with higher attention are in darker colour.</p><p>The attention is generally more focused in fewer tokens as more updating hops have been run. For example, Hop 3 and Hop 4 have higher attention weights on the intended recognition targets (names and publications) than Hop 1 and Hop 2; meanwhile, Hop 3 and Hop 4 have much lower weights on other tokens than Hop 1 and Hop 2 have. We also note that the alternatingly updating mechanism can shift and correct attention weights to the intended level. For example, in Hop 1 (the first name round), we observe that the attention focuses on Mag . Wu Shengqian, while Mag is an academic degree and should not be recognised as a name. In Hop 3 (the second name round), Wu Shengqian gains more attention while Mag gets less attention, so they can be recognised correctly. Similarly, in Hop 2 (the first publication round), Evaluation of the chondrocyte phenotype in health, disease and therapy has high attention,but actually it relates to the researcher's research interest and should not be recognised as a publication. In Hop 4 (the second publication round), this string gains lower attention and can be discarded correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis</head><p>We perform a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type="bibr" target="#b30">[31]</ref> and CogNN <ref type="bibr" target="#b0">[1]</ref>) and our proposed PAM model on 50 randomly selected homepages. We focus on string level performance for the publication recognition task and on name level performance for the person name recognition task. In total, we inspect 1,137 publications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type="bibr" target="#b30">[31]</ref> misrecognises strings about patents, grants, and research projects as publications. PAM avoids these errors since it can capture publication block information and these strings are usually listed on other blocks. Both PubSE and PAM make mistakes when publications are listed together with invited talks or presentations. These strings have high similarity to the publications and are difficult to distinguish when they are listed together. For example, the string Kelly Schrum. "Teaching Hidden History: Creating An Effective Hybrid Graduate Course" Conference on Higher Education Pedagogy, Virginia Tech (Feb 2016) is a talk given by the page owenr but not a publication.</p><p>For person name recognition, we observe that CogNN <ref type="bibr" target="#b0">[1]</ref> tends to produce false negative predictions in groups, i.e., a series of person names in a publication string cannot be recognised. PAM does not make such mistakes since it captures the correlation between names and publications. However, PAM may misrecognise person names with complex name format while CogNN is better on those cases since CogNN uses many fine-grained name form annotations. For example, the string Tyler, L.M.K. Mellor, D. Hauser, KD. contain three person names, which are marked underlined, while PAM cannot distinguish them. Both CogNN and PAM may not recognise some person names hidden in a long paraghraphs, such as a long biography section. We conjecture that this may be caused by the LSTM-based encoder. We aim to solve this problem in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Based on the observations that there is correlation between person names and publications and that there are position patterns in academic homepages, we propose to jointly recognise person names and publications while taking into account the important feature of position patterns. We proposed a Position-aware Alternating Memory network. The network has an alternatingly updated memory module to exploit the correlation of two tasks, and a position-aware memory module to exploit global and local position information. Empirical results show that our model outperforms the state-ofthe-art publication recognition model by 3.64% in F1 score and outperforms the state-of-the-art person name recognition model by 2.06% in F1 score. Our model also outperforms naive joint models by up to 59.80% and 12.10% in F1 score for publication and person name recognition, respectively. Ablation studies and visualisation confirm the effectiveness of the proposed modules in our model.</p><p>Our proposed way of modelling interdependency may be applied to other tasks which are inherently correlated, such as entity recognition and relation extraction. Our framework may also be applied to IE tasks on other datasets which have strong position patterns, such as resumes and other webpages. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of recognising person names and publications in academic homepages. Names are marked in bold italic and publications are marked with grey background.</figDesc><graphic url="image-1.png" coords="1,317.96,176.07,240.23,195.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the PAM network. The Alternatingly updated memory (AM) module exploits the knowledge from both tasks. The Position-aware memory (PM) module integrates local and global position into the memory updating process.</figDesc><graphic url="image-2.png" coords="3,53.80,83.68,504.38,198.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 1 :</head><label>1</label><figDesc>Frequently used symbols Symbols Description n the number of tokens d e the dimension of word embeddings d h the dimension of hidden representations d m the dimension of memeory representations d z the dimension of each head i the hop number A the alternating hidden representation M the memory representation W the projection matrix R the relationship representation U д the global position representation U l the local position representation Q the predicted publication block l µ the median line of Q σ half of the total number of lines in Q k the number of predicted publication tokens in a line l t the line number of token t ε the total number of lines in the homepage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of the computation of U д and U l .</figDesc><graphic url="image-3.png" coords="4,317.96,83.69,240.22,170.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 . 4</head><label>14</label><figDesc>Baselines. We compare our proposed model with the following single-task models for publication recognition:• ParsCit<ref type="bibr" target="#b3">[4]</ref> is an open-source package3 for parsing publications based on feature engineering and CRF. • CNN-Sentence [10] is used to classify whether each line in a webpage is a publication. It has filter windows with sizes of 3, 4, 5 and 100 feature maps for each, a dropout rate of 0.5, a batch size of 40 and a learning rate of 0.01. • Bi-LSTM-CNN-CRF [14] has a filter window size of 3 with 30 feature maps, a hidden dimension of 100, a dropout rate of 0.5, a batch size of 40 and a learning rate of 0.01. • PubSE [31] is the state-of-the-art for publication recognition based on Bi-LSTM-CNN-CRF. It has a filter window size of 3 with 30 feature maps, a hidden dimension of 100, a learning rate of 0.01 and a dropout rate of 0.5 for both the line-level model and the webpage-level model. The batch size is 40 for the line-level model and is 1 for the webpage-level model. The coefficients in the loss function are 1, 0.05, 1, 0.3. We compare with the following single-task models for person name recognition:• Stanford-NER [5] is a named entity recognisor based on CRF provided by the Stanford NLP Group 4 . • Bi-LSTM-CRF [9] has a hidden dimension of 100, dropout rate of 0.5, batch size of 32 and an initial learning rate of 0.01 with a decay rate of 0.05. • CogNN [1] uses fine-grained name form annotations through co-attention. This is the state-of-the-art model for person name recognition. It has a hidden dimension of 100, dropout rate of 0.5, batch size of 32 and an initial learning rate of 0.01 with a decay rate of 0.05. All the above models are trained on the same training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3 http://parscit.comp.nus.edu.sg/ 4 https://nlp.stanford.edu/software/CRF-NER.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of Different Joint Models.</figDesc><graphic url="image-4.png" coords="6,317.96,515.44,240.22,175.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualisation of attention weights along with corresponding tokens in Alternatingly Updated Memory. The colour scale represents the strength of the attention weights. Each box represents the attention weights in a memory updating hop.</figDesc><graphic url="image-5.png" coords="9,53.80,83.67,240.24,601.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the dataset used in experiments.</figDesc><table><row><cell>Summary of Dataset</cell><cell></cell></row><row><cell># homepages</cell><cell>2,087</cell></row><row><cell># homepages for training</cell><cell>1342</cell></row><row><cell># homepages for development</cell><cell>335</cell></row><row><cell># homepages for testing</cell><cell>410</cell></row><row><cell># publications</cell><cell>12,796</cell></row><row><cell># homepages containing publications</cell><cell>702</cell></row><row><cell>Avg. # publications per page</cell><cell>18.23</cell></row><row><cell>Std. # publications per page</cell><cell>36.15</cell></row><row><cell># person names</cell><cell>70,864</cell></row><row><cell># homepages containing names</cell><cell>2087</cell></row><row><cell>Avg. # names per page</cell><cell>34</cell></row><row><cell>Std. # names per page</cell><cell>133</cell></row><row><cell># pages with names only</cell><cell>1385</cell></row><row><cell># pages with names and pubs</cell><cell>702</cell></row><row><cell># names in pages with names only</cell><cell>9,490</cell></row><row><cell cols="2"># names in pages with names and pubs 61,372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on single-task models for publication and person name recognition.</figDesc><table><row><cell>Model</cell><cell>Pub (String Level) R P F1</cell><cell>Model</cell><cell cols="3">Name (Token Level) R P F1</cell><cell cols="3">Name (Name Level) R P F1</cell></row><row><cell>ParsCit [4]</cell><cell cols="2">70.34 18.22 28.94 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN-sentence [10]</cell><cell cols="8">73.39 76.69 75.00 Stanford-NER [5] 64.94 94.68 77.04 41.31 40.98 41.15</cell></row><row><cell cols="9">Bi-LSTM-CNN-CRF [14] 74.15 77.22 75.65 Bi-LSTM-CRF [9] 87.97 89.64 88.79 79.48 82.34 80.89</cell></row><row><cell>PubSE [31]</cell><cell cols="2">84.12 91.12 87.48 CogNN [1]</cell><cell cols="6">93.06 92.85 92.95 86.40 85.32 85.85</cell></row><row><cell>PAM (proposed)</cell><cell cols="2">89.02 93.34 91.12 PAM (proposed)</cell><cell cols="6">95.51 93.21 94.35 88.40 87.42 87.91</cell></row><row><cell cols="3">• Local-AM is resulted from removing the global position</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">representation from our proposed model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">• Global-AM is resulted from removing the local position</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">representation from our proposed model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>reports the results where we compare our PAM model with other joint models and variants of PAM. Overall, PAM outperforms other joint models. AM makes effective improvements to the overall model performance, and both global and local position representations in PM contribute substantially to the model performance, especially for the publication recognition task. The improvements achieved by both PM and AM modules are statistically significant, with p &lt;0.05 based on McNemar's test.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Experimental results of joint models and variants of PAM for publication and person name recognition. F1-score is reported. Models with * have pipeline architectures for two jointly trained tasks and the reported results for each task are from the corresponding pipeline directions, i.e., the result for publication is from N → P and vice versa.</figDesc><table><row><cell>Model</cell><cell cols="4">Pub Token String Token Name Name</cell></row><row><cell>Joint-Naive</cell><cell>88.5</cell><cell>33.0</cell><cell>88.9</cell><cell>77.5</cell></row><row><cell>Joint-Concat  *</cell><cell>89.5</cell><cell>31.3</cell><cell>89.4</cell><cell>75.8</cell></row><row><cell>Joint-Gate  *</cell><cell>88.8</cell><cell>33.8</cell><cell>89.1</cell><cell>77.7</cell></row><row><cell>Joint-Att  *</cell><cell>93.6</cell><cell>65.4</cell><cell>90.9</cell><cell>81.5</cell></row><row><cell>Joint-Stack</cell><cell>94.8</cell><cell>73.1</cell><cell>90.6</cell><cell>82.2</cell></row><row><cell>AM</cell><cell>95.2</cell><cell>77.4</cell><cell>91.4</cell><cell>82.5</cell></row><row><cell>Local-AM</cell><cell>96.3</cell><cell>84.8</cell><cell>92.1</cell><cell>84.1</cell></row><row><cell>Global-AM</cell><cell>96.7</cell><cell>88.7</cell><cell>92.6</cell><cell>85.7</cell></row><row><cell>PAM</cell><cell>97.2</cell><cell>91.1</cell><cell>94.3</cell><cell>87.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://pypi.org/project/Unidecode/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.tensorflow.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by Australian Research Council Discovery Project DP180102050 and by the China Scholarship Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Co-guided Neural Network for Person Name Recognition in Academic Homepages</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=H1eBBGbdnN" />
	</analytic>
	<monogr>
		<title level="j">OpenReview Preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining publication records on personal publication web pages based on conditional random fields</title>
		<author>
			<persName><forename type="first">Jen-Ming</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya-Huei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahn-Ming</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Ming</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI-IAT</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ParsCit: an Open-source CRF Reference String Parsing Package</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Isaac G Councill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="661" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1923" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FireCite: Lightweight real-time reference string extraction from webpages</title>
		<author>
			<persName><forename type="first">Ching</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><forename type="middle">Prabawa</forename><surname>Gozali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>ICML. 1378-1387</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint Learning for Targeted Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4737" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bidirectional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metadata extraction from PDF papers for digital library ingest</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Marinai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="251" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Big scholarly data in CiteSeerX: Information extraction from the web</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Alexander G Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clyde</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<idno>WWW. 597-602</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Allende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Allende-Cid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1000" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate Information Extraction from Research Papers using Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Named entity recognition and identification for finding the owner of a home page</title>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="554" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Alvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CERMINE: automatic extraction of structured metadata from scientific literature</title>
		<author>
			<persName><forename type="first">Dominika</forename><surname>Tkaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Szostek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Fedoryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Dendek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Bolikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="317" to="335" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards building a scholarly big data platform: Challenges, lessons and opportunities</title>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Hsuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suppawong</forename><surname>Tuarob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Sagnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling Localness for Self-Attention Networks</title>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4449" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parsing publication lists on the web</title>
		<author>
			<persName><forename type="first">Kai-Hsiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Ming</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/WIC/ACM WI-IAT</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="444" to="447" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages</title>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuandong</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Integrating local context and global cohesiveness for open information extraction</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>WSDM. 42-50</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
