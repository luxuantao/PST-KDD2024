<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Text Detection via Connected Component Clustering and Nontext Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hyung</forename><forename type="middle">Il</forename><surname>Koo</surname></persName>
							<email>hikoo@ajou.ac.kr</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Duck</forename><forename type="middle">Hoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Mary</forename><forename type="middle">H I</forename><surname>Comer</surname></persName>
						</author>
						<author>
							<persName><surname>Koo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ajou University</orgName>
								<address>
									<postCode>443-749</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Research Korea</orgName>
								<address>
									<postCode>135-010</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scene Text Detection via Connected Component Clustering and Nontext Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E18B055282EF56952147C0FC04287429</idno>
					<idno type="DOI">10.1109/TIP.2013.2249082</idno>
					<note type="submission">received August 22, 2012; revised November 29, 2012; accepted February 4, 2013. Date of publication February 26, 2013; date of current version April 12, 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Connected component (CC)-based approach</term>
					<term>CC clustering</term>
					<term>machine learning classifier</term>
					<term>nontext filtering</term>
					<term>scene text detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new scene text detection algorithm based on two machine learning classifiers: one allows us to generate candidate word regions and the other filters out nontext ones. To be precise, we extract connected components (CCs) in images by using the maximally stable extremal region algorithm. These extracted CCs are partitioned into clusters so that we can generate candidate regions. Unlike conventional methods relying on heuristic rules in clustering, we train an AdaBoost classifier that determines the adjacency relationship and cluster CCs by using their pairwise relations. Then we normalize candidate word regions and determine whether each region contains text or not. Since the scale, skew, and color of each candidate can be estimated from CCs, we develop a text/nontext classifier for normalized images. This classifier is based on multilayer perceptrons and we can control recall and precision rates with a single free parameter. Finally, we extend our approach to exploit multichannel information. Experimental results on ICDAR 2005 and 2011 robust reading competition datasets show that our method yields the state-of-the-art performance both in speed and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INCE mobile devices equipped with high-resolution digital cameras are widely available, research activities using these devices in the field of human computer interaction (HCI) have received much attention for the last decades. Among them, text detection and recognition in cameracaptured images have been considered as very important problems in computer vision community <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It is because text information is easily recognized by machines and can be used in a variety of applications. Some examples are aids for visually impaired people, translators for tourists, information retrieval systems in indoor and outdoor environments, and automatic robot navigation. Although there exist a lot of research activities in this field, scene text detection is still remained as a challenging problem. This is because scene text images usually suffer from photometric degradations as well as geometrical distortions so that many algorithms faced the accuracy and/or speed (complexity) issues <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Most of the scene text detection algorithms in the literature can be classified into region-based and connected component (CC)-based approaches <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Region-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> adopted a sliding window scheme, which is basically a bruteforce approach which requires a lot of local decisions. Therefore, the region-based methods have focused on an efficient binary classification (text versus nontext) of a small image patch. In other words, they have focused on the following problem:</p><p>1) Problem (A): to determine whether a given patch is a part of a text region. For efficient classification, researchers addressed this problem by adopting cascade structures. In their approaches, simple features such as horizontal and vertical derivatives were used at the early stages of the cascade and complex features were incrementally employed <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Even though this structure enables efficient text detection, Problem-(A) is still challenging. It is not straightforward even for human to determine the class of a small image patch when we do not have knowledge of text properties such as scale, skew, and color. Multi-scale scheme using different window sizes can alleviate the scale issues, however, it makes boxes in different scales overlap. Experimental results on ICDAR 2005 dataset have shown that this region based approach is efficient, however, it yields worse performance compared with CC-based approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>CC-based methods begin with CC extraction and localize text regions by processing only CC-level information. Therefore, they have focused on the following problems:</p><p>1) Problem (B): to extract text-like CCs.</p><p>2) Problem (C): to filter out nontext CCs.</p><p>3) Problem (D): to infer text blocks from CCs. In the literature, many CC extraction methods were developed to address Problem (B). For example, some methods assumed that the boundaries of text components should show strong discontinuities and they extracted CCs from edge maps. Others were inspired by the observation that text is written in the same color and they applied color segmentation (or reduction) techniques <ref type="bibr" target="#b10">[11]</ref>. On the other hand, some researchers developed their own CC extraction methods from the scratch: the curvilinearity of text was exploited in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> and local binarization by using estimated scales was adopted in <ref type="bibr" target="#b8">[9]</ref>.</p><p>After the CC extraction, CC-based approaches filter out nontext CCs. In the end, features such as "aspect ratio," "the number of holes in a CC," and "the variance of the stroke width within each CC" were employed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, conditional random fields (CRFs) were adopted in order to consider binary (relational) features as well as unary features. In <ref type="bibr" target="#b13">[14]</ref>, a neural network was used to filter out nontext components.</p><p>Finally, CC-based approaches infer text blocks from the remaining CCs. This step is also known as text line aggregation, text line formation, or text line grouping. Interestingly, many methods were based on similar rules. For example, the height ratio between two letters and color difference have been used in a number of methods <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Although CC-based approaches have shown better performance than region-based ones, they usually suffer from the computational complexity. It is because their performances depend on the quality of CCs and they adopted sophisticated CC extraction and filtering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Approach</head><p>We have illustrated the block diagram of our system in Fig. <ref type="figure" target="#fig_0">1</ref>. As shown in the figure, our method consists of three steps: candidate generation, candidate normalization, and nontext filtering. Our candidate generation step is based on popular CC-based approaches, however, we have focussed on Problem-(D) rather than Problem-(B) and (C). That is, we use an efficient CC extraction method <ref type="bibr" target="#b15">[16]</ref> and we do not adopt CC filtering ideas which are usually time-consuming. Rather, we address the problems caused by the absence of low level CC processing with the ideas in region-based approaches, i.e., we address Problem-(A). We can normalize candidate regions with CC-level information and this normalization allows us to build a simple but reliable text/nontext classifier.</p><p>In our approach, both problems ((D) and (A)) are addressed based on machine learning techniques, so that our method is largely free from heuristics. We have trained a classifier that determines adjacency relationship between CCs for Problem-(D) and we generate candidates by identifying adjacent pairs. In training, we have selected efficient features and trained the classifier with the AdaBoost algorithm <ref type="bibr" target="#b16">[17]</ref>. As mentioned, we apply the above CC clustering method to raw CC sets (that usually contains many nontext CCs) and some candidates may correspond to "nontext clusters." Therefore, nontext rejection scheme should be followed, which is the main problem of region-based methods. However, our situation is different from conventional ones because we can use normalized inputs in classification. We estimate the skew and scale of a candidate from the distribution of CCs, and estimate text and background colors from the CCs. Examples of our normalized results are shown in Fig. <ref type="figure" target="#fig_9">8(f)</ref>. Finally, we reject nontexts among normalized candidates with a multilayer perceptron that is trained with the back-propagation algorithm <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>Our CC extraction algorithm is the maximally stable extremal region (MSER) algorithm that is invariant to scales and affine intensity changes, and other blocks in our method are also designed to be invariant to these changes. These invariance allows us to exploit multi-channel information: we can apply our method to multiple channels at the same time and treat their outputs as if they are from a single source. In this way, we are able to detect text that are not salient in luminance channel images.</p><p>We submitted our preliminary results to 2011 ICDAR robust reading competition <ref type="bibr" target="#b5">[6]</ref> and won the first prize. Experimental results on 2005 competition results have also shown that our method yields better performance with small computational complexity. The rest of this paper is organized as follows.</p><p>In Section II, we explain the way of adding more information to ground truth and explain frequently used notations in this paper. We present our candidate generation method in Section III, which consists of a MSER-based CC extraction block and an AdaBoost-based CC clustering block. In Section IV and V, we present our normalization method and nontext rejection algorithm respectively. Note that normalization in our paper means not only geometric normalization but also binarization (color normalization). Finally, we show experimental results in Section VI, and conclude the paper in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. AUGMENTED GROUND TRUTH AND FREQUENTLY USED NOTATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Construction of New Ground Truth</head><p>The ICDAR dataset consists of natural images annotated with bounding boxes around each instance of a word <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Although it contains sufficient information in performance evaluation, we need more information for the training of our classifiers. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we use a classifier that tells us the adjacency relation between CCs in the candidate generation step, and CC-level information is essential for the training of such a classifier (which will be clarified in  We have pixel-level annotations (binarization results) and text-line numbers. (c) MSER results of (a). We assigned random colors to CCs for better visibility. Note that many CCs are nested due to the properties of stable regions.</p><p>Section III-B). Therefore, we augmented ground truth that was released as 2011 ICDAR training set by adding pixel-level annotations (binarization results) and text-line information. For example, in case of Fig. <ref type="figure" target="#fig_2">2(a)</ref>, there was bounding box information of four words in original ground truth. In addition to them, we build binarization results as shown in Fig. <ref type="figure" target="#fig_2">2(b</ref>) and assign a text-line number for each CC. That is, we assign "1" to CCs in a box containing "SUMMER," assign "2" to CCs in a box containing "WHATEVER" and "THE," and assign "3" to CCs in a box containing "WEATHER."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Frequently Used Notations</head><p>We say that two CCs are adjacent when both are text components in the same word and the number of characters between them is less than 2. For a word "WHATEVER," we say that "W is adjacent to H" and "W is adjacent to A," however, we say that "W is not adjacent to T" because there are two characters (i.e., "H" and "A") between them. We also use a notation</p><formula xml:id="formula_0">c i ∼ c j (1)</formula><p>when c i and c j are adjacent, and</p><formula xml:id="formula_1">c i c j (2) otherwise. Let t (c i ) denote the text-line number of c i . Note that c i ∼ c j means that t (c i ) = t (c j ), however, t (c i ) = t (c j )</formula><p>does not necessarily mean c i ∼ c j . In addition, μ i ∈ 3 indicates the mean color of pixels in c i , and</p><formula xml:id="formula_2">s i = |c i |<label>(3)</label></formula><p>where | • | indicates the number pixels in a given CC. Let us assume that we apply a CC extraction method (the MSER algorithm in our case) to an image and get a set of CCs as illustrated in Fig. <ref type="figure" target="#fig_2">2(c</ref>). Fig. <ref type="figure" target="#fig_3">3</ref> shows some CCs excerpted from Fig. <ref type="figure" target="#fig_2">2(c</ref>) for clarity. We denote the set of CCs as C, and partition this set into a text component set T and a nontext component set N :</p><formula xml:id="formula_3">T ∪ N = C (4) T ∩ N = ∅.<label>(5)</label></formula><p>Here we consider an element in C as a text component when there is a corresponding CC in the ground truth binary image.</p><p>In other words, a CC in Fig. <ref type="figure" target="#fig_2">2(c</ref>) is considered as an element of T if there exists a corresponding CC in Fig. <ref type="figure" target="#fig_2">2(b</ref>). However, it is very unlikely that c ∈ C is identical to a certain CC in the ground truth binary image, and we relax this condition. That is, c ∈ T means that there exists a CC, i.e., e, in the ground truth binary image satisfying</p><formula xml:id="formula_4">|c ∩ e| ≥ 0.8 × |c| (6) |c ∩ e| ≥ 0.8 × |e|.<label>(7)</label></formula><p>For example, a CC "S" in Fig. <ref type="figure" target="#fig_3">3</ref> is considered as a text component because there is a similar CC in Fig. <ref type="figure" target="#fig_2">2(b</ref>). On the other hand, a CC consisting of "M" and "E" in Fig. <ref type="figure" target="#fig_3">3</ref> is different from any CCs in Fig. <ref type="figure" target="#fig_2">2</ref>(b) and it is considered as a nontext component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CANDIDATE GENERATION</head><p>For the generation of candidates, we extract CCs in images and partition the extracted CCs into clusters, where our clustering algorithm is based on an adjacency relation classifier. In this section, we first explain our CC extraction method. Then, we will explain our approaches (i) to build training samples, (ii) to train the classifier, and (iii) to use that classifier in our CC clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CC Extraction</head><p>Among a number of CC extraction methods, we have adopted the MSER algorithm because it shows good performance with a small computation cost <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This algorithm can be considered as a process to find local binarization results that are stable over a range of thresholds, and this property allows us to find most of the text components <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The MSER algorithm yields CCs that are either darker or brighter than their surroundings. In Fig. <ref type="figure" target="#fig_2">2</ref>(c), we have illustrated brighter CCs by assigning random colors to them. Note that many CCs are overlapping due to the properties of stable regions <ref type="bibr" target="#b15">[16]</ref>. More MSER extraction examples can be found in Fig. <ref type="figure" target="#fig_9">8</ref>(a) and (b), which show brighter and darker CCs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Building Training Sets</head><p>Our classifier is based on pairwise relations between CCs, and let us first consider cases that can happen for a CC pair</p><formula xml:id="formula_5">(c i , c j ) ∈ C × C (i = j ): 1) c i ∈ T , c j ∈ T , c i ∼ c j 2) c i ∈ T , c j ∈ T , c i c j , t (c i ) = t (c j ) 3) c i ∈ T , c j ∈ T , c i c j , t (c i ) = t (c j ) 4) c i ∈ T , c j ∈ N 5) c i ∈ N , c j ∈ N .</formula><p>We have illustrated them in Fig. <ref type="figure" target="#fig_3">3</ref>. In a strict sense, we may have to classify the case (1) from the (2) ∼ ( <ref type="formula" target="#formula_3">5</ref>) cases (of course, this classification will allow us to find all words having more than one character). However, it is not straightforward to train such a classifier. For example, let us consider "R" and "T" in the second line in Fig. <ref type="figure" target="#fig_2">2(b</ref>). It is not straightforward to determine whether they are in the same word without considering other characters. Therefore, rather than focusing on this difficult problem, we address a relatively simple problem by adopting an idea in region-based approaches. That is, we adopt a nontext filtering block as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, and we are no longer required to care about the case (5). If we have c i ∼ c j for some c i , c j ∈ N , it will yield a candidate consisting of nontext CCs and this candidate will be rejected at the nontext rejection step. Also, we will perform word segmentation as a postprocessing step and the case (2) does not mean negative samples. Based on these observations, we build training sets. Specifically, we first obtained sets of CCs by applying the MSER algorithm to a training set released by <ref type="bibr" target="#b5">[6]</ref>. Then, for every pair (c i , c j ) ∈ C × C (i = j ), we identify its category among 5 cases. A positive set is built by gathering samples corresponding to the case (1) and a negative set by gathering samples corresponding to the case (3) or (4). Samples from other cases were discarded. Note that this process can be automated by using our augmented ground truth in Section II-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. AdaBoost Learning</head><p>With the collected samples, we train an AdaBoost classifier that tells us whether (c i , c j ) ∈ C ×C (i = j ) is adjacent or not. For the presentation of our method, let us define some local properties of CCs. Given c i ∈ C, we find a bounding box of c i and denote its width and height as w i and h i respectively. Given a pair (c i , c j ) ∈ C×C, the horizontal distance, horizontal overlap, and vertical overlap between two boxes are denoted as d i j , ho i j , and vo i j respectively. We have illustrated some of them in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>We have used 6-dimensional feature vectors consisting of five geometrical features and one color-based feature. All of geometric features are designed to be invariant to the scale of an input image:</p><formula xml:id="formula_6">max(h i , h j ) min(h i , h j ) , max(s i , s j ) min(s i , s j ) (<label>8</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">ho i j min(w i , w j ) , vo i j min(h i , h j ) (<label>9</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">|h i -h j | min(h i , h j ) × d i j min(w i , w j ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>and the color feature is given by the color distance between two CCs in RGB space:</p><formula xml:id="formula_12">μ i -μ j 2 . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>Features in (8) reflect the relative scales (refer to (3) for the definition of s i and s j ) between two CCs and features in <ref type="bibr" target="#b8">(9)</ref> encode their relative locations. The scalar in <ref type="bibr" target="#b9">(10)</ref> is the product of normalized height difference and normalized distance, which will be large when c i and c j are not adjacent.</p><p>All of these features are informative and we consider each feature as a weak classifier. For example, the heights of adjacent English characters are similar, and 1 2 ≤ h i h j ≤ 2 means that it is likely that c i and c j are adjacent. From these weak classifiers, we build a strong classifier with the Gentle AdaBoost learning algorithm <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The Gentle AdaBoost is a variant of AdaBoost learning, and it is easy to implement and known to show good performance in many applications <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CC Clustering</head><p>The AdaBoost algorithm yields a function</p><formula xml:id="formula_14">φ : C × C →<label>(12)</label></formula><p>and we use this function in binary decisions:</p><formula xml:id="formula_15">φ(c i , c j ) &gt; τ 1 ⇐⇒ c i ∼ c j (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>with a threshold τ 1 . Given φ(•, •) and τ 1 , we can find all adjacent pairs by evaluating that function for all possible pairs in C. Based on these adjacency relations, C is partitioned into a set of clusters</p><formula xml:id="formula_17">W = {w k }<label>(14)</label></formula><p>where w k ⊂ C. Formally speaking, c i , c j ∈ w k (i.e., c i and c j are in the same cluster) means that there exists  We build W by using the union-find algorithm <ref type="bibr" target="#b22">[23]</ref>. After clustering, we have discarded clusters having only one CC.</p><formula xml:id="formula_18">{e i } m i=1 ⊂ C such that c i ∼ e 1 ∼ e 2 ∼ • • • e m ∼ c j . (<label>15</label></formula><formula xml:id="formula_19">) (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to Other MSER-Based Methods</head><p>The MSER algorithm has desirable properties for text detection: (i) detection results are invariant to affine transformation of image intensities and (ii) no smoothing operation is involved so that both very fine and very large structures can be detected at the same time <ref type="bibr" target="#b15">[16]</ref>. Therefore, the algorithm has been adopted in many methods <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. However, unlike our approach, they focused on the retrieval of CCs corresponding to individual characters: the authors in <ref type="bibr" target="#b12">[13]</ref> developed a variant of MSER in order to prevent the merging of individual characters, and a Support Vector Machine (SVM) based classifier was developed for the character and non-character classification in <ref type="bibr" target="#b14">[15]</ref>. That is, they tried to develop MSER-based CCextractors yielding individual characters (i.e., high precision and high recall).</p><p>On the other hand, we mainly focus on retrieving the text components as much as possible. As a result, redundant and noisy CCs could be involved in finding clusters. As shown in Figs. <ref type="figure" target="#fig_5">5(c</ref>) and 6(b), due to the characteristics of the MSER algorithm, some characters are detected more than once and there are lots of nontext components. Moreover, some of them do not correspond to individual characters (e.g., "ST" and "RS" in Fig. <ref type="figure" target="#fig_6">6</ref>). The advantages of our approach are its efficiency and robustness. Our method can be efficiently implemented because CC-level feature extraction and classification are not involved. We can also deal with the variations of characters (caused by the font variations and blurs) because we do not exploit the features of individual characters (our algorithm successfully detects texts in Fig. <ref type="figure" target="#fig_6">6(a)</ref>). This approach has drawbacks that text regions could be overlapping and nontext regions are sometimes detected, which will be addressed in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CANDIDATE NORMALIZATION</head><p>After CC clustering, we have a set of clusters. In this section, we normalize corresponding regions for the reliable text/nontext classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Geometric Normalization</head><p>Given w k ∈ W, we first localize its corresponding region.</p><p>Even though text boxes can experience perspective distortions, we approximate the shape of text boxes with parallelograms whose left and right sides are parallel to y-axis. This approximation alleviates difficulties in estimating text boxes having a high degree of freedom (DOF): we only have to find a skew and four boundary supporting points. To estimate the skew of a given word candidate w k , we build two sets:</p><formula xml:id="formula_20">T k = {t (c i )|c i ∈ w k } (16) B k = {b(c i )|c i ∈ w k } (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>where t (c i ) and b(c i ) are the top-center point and the bottomcenter point of a bounding box of c i , respectively. We illustrate B k in Fig. <ref type="figure" target="#fig_7">7</ref>. For every pair in B k and T k , the slope of a line connecting the pair is discretized into one of 32 levels in [-π 8 , π 8 ], and each pair votes for the skew angle. After voting, the most common angle is considered as a skew. Localized blocks are shown in Fig. <ref type="figure" target="#fig_9">8(c</ref>). Then, we perform geometric normalization by applying an affine mapping that transforms the corresponding region to a rectangle. During the transformation, we use a constant target height (48 pixels in experiments) and preserve the aspect ratio of the box. Geometrically normalized results of localized blocks in Fig. <ref type="figure" target="#fig_9">8(c</ref>) are shown in Fig. <ref type="figure" target="#fig_9">8(e)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Binarization</head><p>Given geometrically normalized images, we build binary images. In many cases, MSER results can be considered as binarization results as shown in Fig. <ref type="figure" target="#fig_5">5(c</ref>). However, we perform the binarization separately by estimating text and background colors. It is because (i) the MSER results may miss some character components and/or yield noisy regions (mainly due to the blur) and (ii) we have to store the point information of all CCs for the MSER-based binarization. We consider the average color of CCs as the text color:</p><formula xml:id="formula_22">c i ∈w k s i μ i c i ∈w k s i ∈ 3 (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>and consider the average color of an entire block as the background color. Then, we obtain a binary value of each pixel by comparing the distances to the estimated text color and the estimated background color. We have used l 2 norm in RGB space. The binarization results of Fig. <ref type="figure" target="#fig_9">8</ref>(e) are shown in Fig. <ref type="figure" target="#fig_9">8(f</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TEXT/NONTEXT CLASSIFICATION</head><p>In order to get final results like Fig. <ref type="figure" target="#fig_9">8(d</ref>) from Fig. <ref type="figure" target="#fig_9">8(c</ref>) and (f), we develop a text/nontext classifier that rejects nontext blocks among normalized images. In our classification, we do not adopt sophisticated techniques such as cascade structures, since the number of samples to be classified is usually small. However, one challenge for our approach is the variable aspect ratio as shown in Fig. <ref type="figure" target="#fig_9">8(f</ref>). One possible approach to this problem is to split the normalized images into patches covering one of the letters and develop a character/non-character classifier as <ref type="bibr" target="#b14">[15]</ref>. However, character segmentation is not an easy problem <ref type="bibr" target="#b23">[24]</ref> and there are examples where this approach may fail (See Fig. <ref type="figure" target="#fig_6">6</ref>). Rather, we split a normalized block into overlapping squares as illustrated in Fig. <ref type="figure" target="#fig_11">9</ref>(a), and develop a classifier that assigns a textness value to each square block. Finally, decision results for all square blocks (in the right hand side of Fig. <ref type="figure" target="#fig_11">9(a)</ref>) are integrated so that the original block (in the left hand side) is classified. In this section, we first present our training method that allows us to have a textness value for each square. Then, we explain our text/nontext classification method for normalized images such as Fig. <ref type="figure" target="#fig_9">8(f)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction from a Square Block</head><p>Our feature vector is based on mesh and gradient features as adopted in <ref type="bibr" target="#b24">[25]</ref>. We divide each square into 4 horizontal  and vertical ones as shown in Fig. <ref type="figure" target="#fig_11">9</ref>(b) and extract features. For a horizontal block H i (i = 1, 2, 3, 4), we consider 1) the number of white pixels, 2) the number of vertical white-black transitions, 3) the number of vertical black-white transitions as features, and features for a vertical block is similarly defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multilayer Perceptron Learning</head><p>For the training, we need normalized images such as Fig. <ref type="figure" target="#fig_9">8</ref>(f). For this goal, we applied our algorithm presented in the previous sections (i.e., candidate generation and normalization algorithms) to the training images in <ref type="bibr" target="#b5">[6]</ref>. Then, we manually classified them into text and nontext. We discarded some images showing poor binarization results, and collected 676 text block images and 863 nontext block images. However, we have found that more negative samples are needed for the reliable rejection of nontext components and collected more negative samples by applying the same procedure to images that do not contain any text. Finally, we have 3, 568 nontext images. These text/nontext images are divided into squares as illustrated in Fig. <ref type="figure" target="#fig_11">9</ref>(a) and we have trained a multi-layer perceptron for the classification of square patches <ref type="bibr" target="#b17">[18]</ref>. We use one hidden layer consisting of 20 nodes and set the output value to +1 for text samples and 0 otherwise. To help the learning, input features are normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Integration of Decision Results</head><p>For the integration of square classification results, we accumulate the outputs of the classifier:</p><formula xml:id="formula_24">ψ(w k ) = i∈P F i (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>where P is the square patch set (e.g., the right hand side of Fig. <ref type="figure" target="#fig_11">9</ref>(a)) and F i is the continuous output of the classifier for the i -th square block in P. We consider ψ(w k ) as a textness measure and classify w k as a text region when</p><formula xml:id="formula_26">1 |P| ψ(w k ) &gt; τ 2 (<label>20</label></formula><formula xml:id="formula_27">)</formula><p>where |P| is the number of patches. Textness measure ( <ref type="formula" target="#formula_24">19</ref>) is also useful for imposing a non-overlap constraint, i.e., two text blocks should not be overlapping. When two localized regions are significantly overlapping, we simply choose a block showing a higher ψ(•) value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>We tried to build a text/nontext classifier based on normalized gray-scale images (without binarizaton), because gray scale images seem to be more informative. To be precise, we adopted the AdaBoost learning method and gradient features <ref type="bibr" target="#b25">[26]</ref>. However, experiments have shown that this approach yielded almost the same performance, with a considerable amount of overhead in training. We also tried to use both classifiers in a row (neural network with binary images and Adaboost with gray images), however, we could not find noticeable gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>In experiments, we have used ground truth and evaluation tools released by <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Since our method provides detection results considering skews as shown in Fig. <ref type="figure" target="#fig_12">10(a)</ref>, we have transformed these parallelograms into rectangles as shown in Fig. <ref type="figure" target="#fig_12">10(b</ref>) for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Word Segmentation Heuristics</head><p>Although word segmentation is not the main issue of the scene text detection problem, it is essential in the evaluation. Hence, we have developed a heuristic rule that partitions detected text boxes into words. Given a cluster w k , we sort distances between adjacent CCs in a normalized image in descending order and estimate a (minimum) word spacing distance T . If the distance between adjacent CCs is over T , a separation between them occurs. To be precise,</p><formula xml:id="formula_28">T = α × Q + β (<label>21</label></formula><formula xml:id="formula_29">)</formula><p>where Q is the top 65% distance value. We empirically set α = 1.5 and β = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exploitation of Multichannel Information</head><p>The MSER algorithm extracts CCs in a single-channel (scalar valued) image. Therefore, our method sometimes suffers from the loss of information during color reduction as illustrated in Fig. <ref type="figure" target="#fig_13">11</ref>(a) and (b). However, such a text may become clear in another channel as shown in Fig. <ref type="figure" target="#fig_13">11(c</ref>), and we can alleviate this problem by applying our method to multiple channels. Note that our method is invariant to affine transform of intensity and we can apply our method to the chrominance channel without any modification. In case that similar (significantly overlapping) text blocks are detected in more than one channel, we have selected one text block having the largest textness value.</p><p>One may think that CC extraction methods in multi-channel images can alleviate the same problem, even if they are much slower than single channel methods (for example, the maximally stable color region algorithm is several times slower than the original MSER algorithm <ref type="bibr" target="#b26">[27]</ref>). However, they may yield over-segmented results. Note that characters in color (vector valued) images are not homogeneous (See Fig. <ref type="figure" target="#fig_13">11(a)</ref>) but they consist of several clusters in color spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results on ICDAR 2011 Dataset</head><p>We submitted our preliminary results to ICDAR 2011 robust reading competition and we won the first prize. The results are summarized in Table <ref type="table" target="#tab_0">I</ref>, where color spaces, such as L-a-b, H-S-V, and Y-Cb-Cr, were also evaluated to verify the effectiveness of exploiting multi-channel information. Since the Y-Cb-Cr space shows the best performance, it is selected among the color spaces for further comparison. We have also plotted our precision-recall curves in Fig. <ref type="figure" target="#fig_14">12</ref>. A solid blue line stands for a precision-recall curve when Y, Cb, and Cr channels are adopted, and a dotted red line means a precisionrecall curve when the luminance channel is only considered. We have drawn these graphs by changing a free parameter τ 2 in <ref type="bibr" target="#b19">(20)</ref>.</p><p>We have also conducted experiments showing the effects of multi-channel processing. Five points on the green dotted line are operating points when we add a new channel image sequentially. Starting from a luminance channel image, we have added Cb, Cr, R, and B channels sequentially. The performance of our method is improved when we add two chrominance channels, however, its precision drops when other channels (such as R and B channels) are added. The effect of our multi-channel scheme can be found in Fig. <ref type="figure" target="#fig_13">11(d)</ref>.</p><p>Our method takes about 50 ms for 640 × 480 luminance inputs for a standard PC with 2.8 GHz Intel processor. Also, it takes about 120 ms when three channels are used. Specifically, the complexity of our method is not linear to the number of channels since chrominance channel images usually have less CCs than luminance one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results on ICDAR 2005 Dataset</head><p>We have evaluated the performance on ICDAR 2005 dataset with the traditional measure <ref type="bibr" target="#b3">[4]</ref>. As in Table <ref type="table" target="#tab_0">II</ref>, color spaces, such as L-a-b, H-S-V, and Y-Cb-Cr, were evaluated and the   Y-Cb-Cr space also shows the best performance. In Fig. <ref type="figure" target="#fig_15">13</ref>, we have plotted our precision-recall curves. Note that we have also illustrated operating points of conventional methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Although our recall rate is worse than the method in <ref type="bibr" target="#b8">[9]</ref>, our method shows better precision and f -measure. Especially, our method is (at least) several times faster than that method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Parameters</head><p>Our method has two trained classifiers, i.e., adjacency relation classifier with a parameter τ 1 in ( <ref type="formula" target="#formula_15">13</ref>) and text/nontext classifier with a parameter τ 2 in <ref type="bibr" target="#b19">(20)</ref>. Then, as shown in Fig. <ref type="figure" target="#fig_4">14</ref>, τ 1 and τ 2 can be utilized in controlling the overall precision-recall. Unfortunately, τ 1 seems not to control the precision and recall in an inversely proportional way.  Specifically, Fig. <ref type="figure" target="#fig_4">14</ref> shows that the overall recall could drop in the case of a small τ 1 which definitely increases the recall of adjacency relation classifier. The increased recall in the adjacency relation classifier is likely to decrease its precision so the (3) and (4) cases in Fig. <ref type="figure" target="#fig_3">3</ref> could be classified as positive samples. This may result in the failure in localizing text boxes and drop the overall recall. On the other hand, a large τ 2 increases the overall precision (decreases the overall recall), and vice versa, so we think τ 1 should be fixed while τ 2 can be changed according to the purpose of the application (high recall versus high precision).</p><p>We have also illustrated precision-recall curves for different MSER parameters in Fig. <ref type="figure" target="#fig_5">15</ref>. The OpenCV library provides Our method with different τ 2 (fixed τ 1 ) Fig. <ref type="figure" target="#fig_4">14</ref>. Precision-recall curves with different parameters (τ 1 and τ 2 ). YCbCr channels are used and the measure in <ref type="bibr" target="#b5">[6]</ref> is adopted for the evaluation. Our method with different τ 2 (fixed Δ) Fig. <ref type="figure" target="#fig_5">15</ref>. Precision-recall curves with different parameters ( and τ 2 ). YCbCr channels are used and the measure in <ref type="bibr" target="#b5">[6]</ref> is adopted for the evaluation.</p><p>an efficient implementation of the MSER algorithm <ref type="bibr" target="#b19">[20]</ref> and experiments have shown that = 6 yielded the best f -measure, where represents the range of intensities where the regions are stable (interested readers may refer to <ref type="bibr" target="#b27">[28]</ref> for the definition of ).</p><p>= 6 is a relatively small value compared to the whole range of intensities (0 ∼ 255), and our CC extraction method could generate low-contrast or blurred CCs (high recall). However, as shown in Fig. <ref type="figure" target="#fig_5">15</ref>, the value seems not to control the precision and recall as τ 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Future Work</head><p>Our system is designed to address the scene text detection problem in natural images such as Fig. <ref type="figure" target="#fig_2">2</ref>(a) and 11(a), where English alphabets are placed horizontally <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. We have exploited these properties in our algorithm (especially in feature selection), and our method should be changed in order to detect Asian scripts and/or texts of arbitrary orientations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We think our general framework can be extended by developing new features and merging rules, and this is our future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have presented a novel scene text detection algorithm based on machine learning techniques. To be precise, we developed two classifiers: one classifier was designed to generate candidates and the other classifier was for the filtering of nontext candidates. We have also presented a novel method to exploit multi-channel information. We have conducted experiments on ICDAR 2005 and 2011 datasets which showed that our method yielded the state-of-the-art performance in both new and traditional evaluation protocols.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of our system.</figDesc><graphic coords="2,96.95,53.45,417.62,121.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Input image. (b) Illustration of our ground truth.We have pixel-level annotations (binarization results) and text-line numbers. (c) MSER results of (a). We assigned random colors to CCs for better visibility. Note that many CCs are nested due to the properties of stable regions.</figDesc><graphic coords="3,84.11,154.85,180.14,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of possible relations between CCs. For clarity, we have only illustrated some CCs in C.</figDesc><graphic coords="3,347.03,43.25,180.02,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of local properties between two CCs.</figDesc><graphic coords="4,347.51,57.53,180.02,108.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Input image. (b) Color-coded MSER results. (c) Hierarchical tree of MSER results in the red box in (b).</figDesc><graphic coords="5,56.99,204.53,108.50,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Input image. (b) Hierarchical tree of MSER results in the red box in (a).</figDesc><graphic coords="5,171.35,204.53,120.02,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. We denote points in B k as red. By computing the angles connecting two points in B k , we can estimate the skew of a word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) and (b) Brighter and darker CCs extracted by the MSER algorithm [16]. (c) We localize candidate regions in image domains. (d) Nontext blocks are filtered out by exploiting the statistical properties of the regions (our final results). (e) Geometrically normalized results of localized boxes. (f) Our normalization results.</figDesc><graphic coords="6,49.43,271.25,118.70,74.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) In order to handle variable aspect ratios, we split a block into squares. (b) For the feature extraction, we divide a square block into four horizontal and four vertical blocks.</figDesc><graphic coords="6,180.71,165.77,118.82,89.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. (a) Our detection results. (b) Transformed results of (a) for the comparison with the rectangle-based ground truth.</figDesc><graphic coords="7,66.47,57.77,102.02,136.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Some text is not salient in a luminance channel image and we can detect such a text by exploiting multichannel information. (a) Color input. (b) Luminance channel image. (c) Chrominance channel image. (d) Our detection result using Y and Cr channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Precision-recall curves on ICDAR 2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Precision-recall curves on ICDAR 2005 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>different τ 1 (fixed τ 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>ON ICDAR 2011 DATASET (WITH A NEW MEASURE)<ref type="bibr" target="#b5">[6]</ref> </figDesc><table><row><cell></cell><cell></cell><cell>Algorithm</cell><cell></cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell><cell cols="2">f -Measure</cell></row><row><cell></cell><cell></cell><cell>Yi's</cell><cell></cell><cell></cell><cell>0.6722</cell><cell cols="2">0.5809</cell><cell>0.6232</cell></row><row><cell></cell><cell cols="2">TH-TextLoc</cell><cell></cell><cell></cell><cell>0.6697</cell><cell cols="2">0.5768</cell><cell>0.6198</cell></row><row><cell></cell><cell></cell><cell>Neumann's</cell><cell></cell><cell></cell><cell>0.6893</cell><cell cols="2">0.5254</cell><cell>0.5963</cell></row><row><cell></cell><cell></cell><cell>TDMIACS</cell><cell></cell><cell></cell><cell>0.6352</cell><cell cols="2">0.5352</cell><cell>0.5809</cell></row><row><cell></cell><cell cols="3">Our method (Y)</cell><cell></cell><cell>0.8123</cell><cell cols="2">0.6441</cell><cell>0.7185</cell></row><row><cell></cell><cell cols="3">Our method (L-a-b)</cell><cell></cell><cell>0.8150</cell><cell cols="2">0.6469</cell><cell>0.7213</cell></row><row><cell></cell><cell cols="3">Our method (H-S-V)</cell><cell></cell><cell>0.7070</cell><cell cols="2">0.6749</cell><cell>0.6905</cell></row><row><cell cols="4">Our method (Y-Cb-Cr)</cell><cell></cell><cell>0.8144</cell><cell cols="2">0.6868</cell><cell>0.7452</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.65 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Our method (YCbCr) Our method (Y)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Our method (Y-Cb-Cr-R-B)</cell></row><row><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Yi</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TH-TextLoc</cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Neumann</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TDM IACS</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>0.45</cell><cell>0.5</cell><cell>0.55</cell><cell>0.6</cell><cell>0.65</cell><cell>0.7</cell><cell>0.75</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text information extraction in images and video: A survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="977" to="997" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Camera-based analysis of text and documents: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="84" to="104" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extraction of text objects in video documents: Recent progress</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IAPR Int</title>
		<meeting>8th IAPR Int</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="5" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ICDAR 2003 robust reading competitions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal. Recognit</title>
		<meeting>Int. Conf. Document Anal. Recognit</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Icdar 2005 text locating competition results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal. Recognit</title>
		<meeting>Int. Conf. Document Anal. Recognit</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Anal. Recognit</title>
		<meeting>Int. Conf. Document Anal. Recognit</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A time-efficient cascade for real-time object detection: With applications for the visually impaired</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., Workshops</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text string detection from natural scenes by structure-based partition and grouping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2594" to="2605" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust text detection in natural images with edge-enhanced maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="2609" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new class of learnable detectors for categorisation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scandinavian Conf. Image Anal</title>
		<meeting>Scandinavian Conf. Image Anal</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Additive logistic regression: A statistical view of boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural Networks: A Comprehensive Foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linear time maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning OpenCV: Computer Vision with the OpenCV Library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Cambridge, MA, USA: O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sharing features: Efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun.-Jul. 2004</date>
			<biblScope unit="page" from="762" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<title level="m">Data Structures and Algorithm Analysis in C++</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of methods and strategies in character segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lecolinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1996-07">Jul. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance features for neural network-based recognition of handwritten characters</title>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Document Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="88" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximally stable colour regions for recognition and matching</title>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gyeonggi-do, Korea, in 2012, where he is currently an Assistant Professor. His current research interests include computer vision and machine learning</title>
		<ptr target="http://docs.opencv.org/HyungIlKoo" />
	</analytic>
	<monogr>
		<title level="m">Duck Hoon Kim (S&apos;03-M&apos;06) received the B.S., M.S., and Ph.D. degrees in electrical engineering and computer</title>
		<meeting><address><addrLine>Seoul, Korea; Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">2008. 2002. 2004, and 2010. 2010 to 2012. 1998, 2000, and 2005. March 2005 to October 2006. 2006 to March 2011</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Seoul National University (SNU), Seoul, Korea ; Division of Electrical and Computer Engineering, Ajou University ; science from Seoul National University ; National University and the University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include computer vision, computer graphics, and multimedia application</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
