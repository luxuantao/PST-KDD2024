<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A System for Query-Specific Document Summarization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vagelis</forename><surname>Hristidis</surname></persName>
							<email>vagelis@cis.fiu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University Miami</orgName>
								<address>
									<postCode>33199</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramakrishna</forename><surname>Varadarajan</surname></persName>
							<email>ramakrishna@cis.fiu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University Miami</orgName>
								<address>
									<postCode>33199</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A System for Query-Specific Document Summarization *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E0EC7B4E0CCBEAC4601B990D7D0FE9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Search process Algorithms</term>
					<term>Performance</term>
					<term>Experimentation query-specific summarization</term>
					<term>keyword search</term>
					<term>Steiner tree problem</term>
					<term>user survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a great amount of work on query-independent summarization of documents. However, due to the success of Web search engines query-specific document summarization (query result snippets) has become an important problem, which has received little attention. We present a method to create queryspecific summaries by identifying the most query-relevant fragments and combining them using the semantic associations within the document. In particular, we first add structure to the documents in the preprocessing stage and convert them to document graphs. Then, the best summaries are computed by calculating the top spanning trees on the document graphs. We present and experimentally evaluate efficient algorithms that support computing summaries in interactive time. Furthermore, the quality of our summarization method is compared to current approaches using a user survey.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As the number of documents available on users' desktops and the Internet increases, so does the need to provide high-quality summaries in order to allow the user to quickly locate the desired information. A compelling application of document summarization is the snippets generated by Web search engines for each query result, which assist users in further exploring individual results. The Information Retrieval (IR) community has largely viewed text documents as linear sequences of words for the purpose of summarization (with some exceptions as explained in Section 2). Although this model has proven quite successful in efficiently answering keyword queries, it is clearly not optimal since it ignores the inherent structure in documents.</p><p>Furthermore, most summarization techniques are queryindependent and follow one of the following two extreme approaches: Either they simply extract relevant passages viewing the document as an unstructured set of passages, or they employ Natural Language Processing techniques. The former approach ignores the structural information of documents while the latter is too expensive for large datasets (e.g., the Web) and sensitive to the writing style of the documents.</p><p>In this paper, we propose a method to add structure, in form of a graph, to text documents in order to allow effective queryspecific summarization. That is, we view a document as a set of interconnected text fragments (passages). We focus on keyword queries since keyword search is the most popular information discovery method on documents, because of its power and ease of use. Our technique has the following key steps: First, at the preprocessing stage, we add structure to every document (explained later), which can then be viewed as a labeled, weighted graph, called the document graph. Then, at query time, given a set of keywords, we perform keyword proximity search on the document graphs to discover how the keywords are associated in the document graphs. For each document its summary is the minimum spanning tree on the corresponding document graph that contains all the keywords (or equivalent based on a thesaurus).</p><p>The document graph is constructed as follows. First we parse the document and split it into text fragments using a delimiter (e.g., the new line character). Each text fragments becomes a node in the document graph. A weighted edge is added to the document graph between two nodes if they either correspond to adjacent text fragments in the text or if they are semantically related, and the weight of an edge denotes the degree of the relationship. There are many possible ways to define the degree of the relationship between two text fragments.</p><p>In this work we consider two fragments to be related if they share common words (not stop words) and the degree of relationship is calculated by an adaptation of traditional IR term weighting formulas. We also consider a thesaurus to enhance the word matching capability of the system. To avoid dealing with a highly interconnected graph, which would lead to slower execution times and higher maintenance cost, we only add edges with weight above a threshold. Also notice that the edge weights are query-independent, so they can be precomputed. Example 1. Figure <ref type="figure" target="#fig_1">2</ref> shows the document graph for the document of Figure .1. The document is first split to text fragments v0…v16, which correspond to its paragraphs (other delimiters are possible as we explain below). Notice that the edge between nodes v8 and v7 has the highest weight because there are many infrequent (and hence with higher idf value) words that are common between them like "Donoghue" and "BrainGate". At query time, the precomputed document graph of a document is processed as follows to create the best query-specific summary. First, each node of the document graph is assigned a score according to the relevance of the corresponding text fragment to the query. To do so we employ traditional IR ranking functions. Notice that a full-text index is used to accelerate this step. Then, we execute our keyword proximity algorithms, which are inspired by the techniques developed in previous work on proximity search on graphs <ref type="bibr" target="#b7">[7]</ref>, where approximation algorithms are presented for the Group Steiner Tree problem (which is equivalent <ref type="foot" target="#foot_0">1</ref> to the proximity search problem). The best summary is the top-ranked spanning tree that contains all the keywords. The ranking considers both the node and the edge weights (which are query-dependent and independent respectively). Notice that the problem can be easily modified to allow summaries that do not contain all keywords, although this case is not further discussed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 (cont'd).</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the top-ranked spanning trees for the document graph of Figure <ref type="figure" target="#fig_1">2</ref> for the query "Brain chip research". The values shown above the nodes in Table <ref type="table" target="#tab_0">1</ref> indicate the node scores with respect to the query. The scores of the spanning trees are a function of their node and edge scores, as explained in Section 4. Notice that all results contain all query keywords. The top result is the best summary of the document of Figure <ref type="figure" target="#fig_0">1</ref> (the keywords of the query are shown in bold) for this query. Intuitively, this result is the best because it contains the minimum possible number of nodes and the edge that connects the two nodes is strong.</p><p>Also observe that Result #4 is ranked lower than Result #3 even though it has fewer nodes. The reason is that the nodes of Result #4 are connected through very commonly occurring words like "computer" and "brain" whereas in Result #3 they are connected through infrequent words like "Friehs". Notice that to compute the frequency of a keyword we consider all documents of the corpus. The contributions of this paper are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We present a framework to add structure to text documents, which is used for summarization purposes in this work, although it can be leveraged for other problems as well, like ranking of query results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We show how we can use the generated document graphs to create high-quality query-specific summaries. We performed two user surveys to compare the quality of our approach to other current approaches-Desktop search engines and DUC peers<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We present and experimentally evaluate execution algorithms that prove the feasibility of our approach.</p><p>• We built a prototype of the system, which is available on the Web at http://dbir.cs.fiu.edu/summarization.</p><p>The paper is organized as follows. Section 2 presents the related work. Section 3 formally defines the problem, while Section 4 explains how we add structure to documents. Section 5 presents the various algorithms for efficient summary computation using the document graphs. Sections 6 and 7 present the quality and performance experiments respectively. Section 8 describes the developed prototype. Finally Section 9 discusses our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK 2.1 Document Summarization.</head><p>A large corpus of work has focused on generating queryindependent summaries <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b5">5</ref>]. The OCELOT system <ref type="bibr" target="#b6">[6]</ref> provides the summary of a web page by selecting and arranging the most (query-independent) "important" words of the page. OCELOT uses probabilistic models to guide the selection and ordering of words into a summary. Amitay and Paris <ref type="bibr" target="#b3">[3]</ref> propose a new fully automatic pseudo-summarization technique for Web pages, where the anchor text of hyperlinked pages is used to construct summaries. This approach is unique since it ignores the actual content of a document. <ref type="bibr" target="#b5">[5]</ref> uses lexical chains for text summarization. In particular, they use Wordnet to create all lexical chains and choose the strongest ones as a summary of the document.</p><p>The majority of systems participating in the past Document Understanding Conference <ref type="bibr" target="#b11">[11]</ref> (a large scale summarization evaluation effort sponsored by the United States government), and the Text Summarization Challenge <ref type="bibr" target="#b14">[14]</ref> are extraction based. Extraction-based automatic text summarization systems extract parts of original documents and output the results as summaries <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b25">25]</ref>. Other systems based on information extraction <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref> and discourse analysis <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b41">42]</ref> also exist but they are not yet usable for general-domain summarization. However these works do not exploit the inherent structure of the document and mostly focus on query-independent summaries. In this work (a preliminary version appears in <ref type="bibr" target="#b43">[44]</ref>) we also show the semantic connections between the extracted fragments, which improve the quality as shown in Section 6. <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">31]</ref> use natural language processing techniques to create summaries for documents, which cannot scale to large corpora like the Web and are limited to the writing style of the page authors.</p><p>White et al. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, Tombros and Sanderson <ref type="bibr" target="#b42">[43]</ref>, Zechner <ref type="bibr" target="#b46">[47]</ref> and Goldstein et al. <ref type="bibr" target="#b16">[16]</ref> create query-dependent summaries using a sentence extraction model in which the documents (web pages) are broken up into their component sentences and scored according to factors such as their position, the words they contain, and the proportion of query terms they contain. A number of the highest-scoring sentences are then chosen as the summary. Lin <ref type="bibr" target="#b27">[27]</ref> compresses the sentences to achieve better summaries. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> select the best passage of a document as its summary. However, these works ignore possible semantic connections between the sentences or the possibility that linking a relevant set of text fragments will provide a better summary. Radev et al. <ref type="bibr" target="#b35">[36]</ref> provide a technique for multi-document summarization used to cluster the results of a web keyword query. However, their clustering and summarization techniques are query-independent in contrast to our work. <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b32">32]</ref> provide a technique to rank sentences based on their similarity with other sentences across multiple documents and then provide a summary with the top ranked sentences. However, their methods are query-independent in contrast to our work.</p><p>The idea of splitting a Web page to fragments has been used by Cai et al. <ref type="bibr">[9]</ref>, Lee et al. <ref type="bibr" target="#b26">[26]</ref> and Song et al. <ref type="bibr" target="#b40">[41]</ref>, where they extract query-independent rankings for the fragments, for the purpose of improving the performance of web search and also to facilitate web mining and accessibility. Cai et al. <ref type="bibr">[9]</ref> partition a web page into blocks using the vision-based page segmentation algorithm. Based on block-level link analysis, they proposed two new algorithms, Block Level PageRank and Block Level HITS to extract authoritative parts of a page. Lee et al. <ref type="bibr" target="#b26">[26]</ref> discuss a Web block classification algorithm after Web page division into semantic blocks, while Song et al. <ref type="bibr" target="#b40">[41]</ref> provide learning algorithms for block importance.</p><p>Finally, all major Web search engines (Google, Yahoo!, MSN Search, and so on) generate query-specific snippets of the returned results. Although their algorithms are not published, we observed that they simply extract some of the query keywords and their surrounding words. Recently, some of these companies made available tools to provide the same search and snippet functionality on a user's desktop <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b33">33]</ref>. We include these snippets in our user study of Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IR Ranking.</head><p>In creating the document graph and computing the node weights, we adopt ranking principles from the Information Retrieval community. Various methods for weighting terms have been developed <ref type="bibr" target="#b39">[40]</ref>. The most widely used are the Okapi (Equation <ref type="formula">1</ref>) and the pivoted normalization weighting, which are based on the tf-idf principle.</p><formula xml:id="formula_0">qtf k qtf k tf avdl dl b b k tf k df df N d Q t + + + + - + + + - ∑ ∈ 3 3 1 1 , ) 1 ( . ) ) 1 ( ( ) 1 ( . 5 . 0 5 . 0 ln</formula><p>tf is the term's frequency in document, qtf is the term's frequency in query, N is the total number of documents in the collection, df is the number of documents that contain the term, dl is the document length (in words), avdl is the average document length and k1 (between 1.0-2.0), b (usually 0.75), and k3 (between 0-1000) are constants (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Keyword search in data graphs.</head><p>In the second stage of our approach, when the document graphs are already created and a query arrives, the system searches the document graphs for sub-trees that contain all query keywords. This problem has been studied by the database and graphalgorithms communities. In particular, recent work <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b18">18]</ref> has addressed the problem of free-form keyword search on structured and semi-structured data. These works follow various techniques to overcome the NP-completeness of the Group Steiner problem, to which the keyword proximity search problems can be reduced.</p><p>Goldman et al. <ref type="bibr" target="#b15">[15]</ref> use precomputation to minimize the runtime cost. BANKS <ref type="bibr" target="#b7">[7]</ref> views the database as a graph and proposes algorithms to approximate the Group Steiner Tree problem. We consider and experimentally evaluate modifications of these algorithms in this work. XRANK <ref type="bibr" target="#b18">[18]</ref> works on XML trees, which simplifies the problem. Li et al. <ref type="bibr" target="#b28">[28]</ref> tackle the problem of proximity search on the Web, which is viewed as a graph of hyperlinked pages. They use of the concept of information unit, which can be viewed as a logical Web document consisting of multiple physical pages. <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b21">21]</ref> perform keyword search on relational databases and exploit the schema properties to achieve efficient execution.</p><p>Finally, notice that Buneman et al. <ref type="bibr" target="#b8">[8]</ref> view the problem of adding structure to unstructured data from a completely different angle: how to define a schema to describe a labeled graph (e.g., an XML document).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITION</head><p>Let D={d 1 ,d 2 ,,…,d n } be a set of documents d 1 ,d 2 ,,…,d n . Also let size(d i ) be the length of d i in number of words. Term frequency tf(d,w) of term (word) w in document d is the number of occurrences of w in d. Inverse document frequency idf(w) is the inverse of the number of documents containing term w in them.</p><p>A keyword query Q is a set of keywords Q={w 1 ,…,w m }. The result of the keyword query, which is not the focus of this work, is a list of documents from D ranked according to their relevance to Q. A key component in our work is the document graph G(V,E) of a document d, which is defined as follows:</p><p>Definition 1 (Document Graph). The document graph G(V,E) of a document d is defined as follows:</p><p>• d is split to a set of non-overlapping text fragments t(v), each corresponding to a node v∈V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>An edge e(u,v)∈E is added between nodes u,v∈V if there is an association (further discussed in Section 4) between t(u) and t(v) in d.</p><p>Hence, we can view G as an equivalent representation of d, where the associations between text fragments of d are depicted. Example 1 explains a possible document graph for the document of Figure <ref type="figure" target="#fig_0">1</ref>. Notice that there are many ways to define the document graph for a document. In this work we follow a semantic approach where a delimiter is chosen to create text fragments, and edges are added when the text fragments contain common (or equivalent) words as we explain in Section 4. Furthermore, notice that the nodes and edges of the document graph may be weighed according to a variety of reasons, both query-dependent and independent. For example, authority flow techniques <ref type="bibr" target="#b4">[4]</ref> on the document graph can be employed to assign both query-dependent and independent scores. In this work (see Section 4) we consider query-dependent (resp. independent) weights for the nodes (resp. edges).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (Minimal Total Spanning Tree).</head><p>Given a document graph G(V,E), a minimal total spanning tree of G with respect to a keyword query Q={w 1 ,…,w m } is a sub-tree T of G that is both:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Total: every keyword w∈Q is contained in at least one node of T.</p><p>• Minimal: we cannot remove any node from T and still have a total sub-tree.</p><p>A summary of a document d with document graph G, with respect to a keyword query Q={w 1 ,…,w m }, is a minimal total spanning tree of G for Q.</p><p>Problem 1 (Summarization). Given a document d∈D and its document graph G, and a keyword query Q, find the top summary, i.e., the minimum score minimal total spanning tree.</p><p>Notice that the totality property implies that we use AND semantics, that is, require all keywords to be in the summary. Another alternative is OR semantics where not all keywords are required to be in the summary. OR semantics are useful in the following scenarios: (a) the keywords are rare and hence no document contains all of them, so to summarize the query result we need OR semantics and (b) in order to have more compact summaries we may choose to not display the less important keywords. In this paper we only present our results on AND semantics due to space limitations. Our techniques and algorithms can be extended to generate summaries for OR semantics, by relaxing the totality constraint on summary spanning trees.</p><p>Furthermore, the fact that the summaries are minimal means that we do not allow any nodes not containing any keyword as leaf nodes in the summary tree. However, nodes with no keywords can be internal nodes. For example, in the second result of Table <ref type="table" target="#tab_0">1</ref>, node v7 has no keywords, but it acts as a connector between nodes v0 and v4 which contain the keywords.</p><p>The score of a (summary) tree T is calculated using a scoring function based on the weights of the nodes and edges of T. The scoring function used in this work is presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 (cont'd).</head><p>For the document of Figure <ref type="figure" target="#fig_0">1</ref> and the keyword query "Brain Chip Research", the top summary is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Notice that by the definition of Problem 1, a summary may contain internal text fragments with no query keywords, which are called Steiner nodes. The reason we include such nodes is to achieve semantic coherence in the generated summaries, which increases user satisfaction as we show in Section 6. If brevity is the top priority then Steiner nodes can be omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ADDING STRUCTURE TO DOCUMENTS</head><p>As we explain below, there are many ways to create and assign weights to a document graph. In this section we present the specific approach we follow to create a document graph. In particular, given a document d∈D, a query Q and a set of input parameters (explained below), we construct a document graph G(V,E). Notice that Q is only used in assigning weights to the nodes of G, which is a desirable property since the rest of G can be computed before queries arrive.</p><p>The following input parameters are required at the precomputation stage to create the document graph:</p><p>1. Threshold for edge weights. Only edges with weight not below threshold will be created in the document graph.</p><p>2. Parsing Delimiter. The parsing delimiter is used to split the document to text fragments. Typical choices are the new-line character (each text fragment corresponds to a paragraph) or the period (each text fragment is a sentence). We found that for the domain of news articles that we experimented with (see Section 6) the new-line is preferable since paragraphs are typically short and leads to more compact document graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 (cont'd)</head><p>The new-line character was used to parse the document of Figure <ref type="figure" target="#fig_0">1</ref> into 17 text fragments v0,…,v16.</p><p>After parsing the document and creating the graph nodes (text fragments), for each pair of nodes u,v we compute the association degree between them, that is, the score (weight) EScore(e) of the edge e(u,v). If EScore(e)≥threshold, then e is added to E. The score of edge e(u,v) where nodes u, v have text fragments t(u), t(v) respectively is:</p><formula xml:id="formula_1">( ) )) ( ( )) ( ( )) ( )) ), ( ( ) ), ( ( ( ) ( )) ( ) ( ( v t size u t size w idf w v t tf w u t f t e EScore v t u t w + ⋅ + = ∑ ∈ I<label>(2)</label></formula><p>where tf(d,w) is the number of occurrences of w in d, idf(w) is the inverse of the number of documents containing w, and size(d) is the size of the document (in words).</p><p>That is, for every word w appearing in both text fragments we add a quantity equal to the tf⋅idf score of w. Notice that stop words are ignored. Furthermore, we use thesaurus and stemmer (we rely on Oracle interMedia as explained in Section 7) to match words that are equivalent. The sum is divided by the sum of the lengths of the text fragments in the same way as the document length (dl) is used in traditional IR formulas. Notice that Equation 2 is an adaptation of traditional IR formulas for a pair of documents.</p><p>Notice that alternative ways to compute the edge weights are possible, like the cosine document distance, which however have similar effect as the tf⋅idf method that we employ. In future versions of our system we plan to also use Wordnet and Latent Semantic Indexing techniques to improve the quality of the edge weights, which is challenging on the performance level since our system is interactive.</p><p>The calculation of the edge weights concludes the queryindependent part of the document graph creation. Next, when a query Q arrives, the nodes in V are assigned query-dependent weights according to their relevance to Q. In particular, we assign to each node v corresponding to a text fragment t(v) node score NScore(v) defined by the Okapi formula (Equation <ref type="formula">1</ref>). In order to accelerate this step of assigning node scores we built a full-text index on the set D of documents that efficiently allows locating the nodes that contain the query keywords and also calculate the query-dependent score. The details of this index are out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Summary Scoring Function</head><p>Given the document graph G and a query Q, a summary (subtree of G) T is assigned a score Score(T) by combining the scores of the nodes v∈T and the edges e∈T. In particular Equation 3 computes the summary score. where a and b are constants (we use a=1 and b=0.5), EScore(e) is the score of edge e using Equation <ref type="formula" target="#formula_1">2</ref>, NScore(v) is the score of node v using Equation <ref type="formula">1</ref>.</p><p>Intuitively, if T is larger (has more edges) then its score should degrade (increase) since larger trees denote looser semantic connections <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b7">7]</ref>. This is the reason we take the sum of the inverse of the edge scores in Equation <ref type="formula">3</ref>. Furthermore, if more nodes of T are relevant to Q, the score should improve (decrease). Hence, we take the inverse of the sum of the node scores.</p><p>Constants a and b are used to calibrate the importance of the size of the summary (in number of edges) versus the amount of relevant information contained. In particular, higher a values boost the score of smaller and tightly connected summaries, whereas higher b benefit summaries with more relevant content (i.e., containing nodes with high score with respect to the query). Notice that a and b can also be viewed as adjusting parameters for the query-independent and dependent parts of the scoring function respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 (cont'd).</head><p>The top summary T for the document of Figure <ref type="figure" target="#fig_0">1</ref> with document graph shown in Figure <ref type="figure" target="#fig_1">2</ref> is shown in Figure <ref type="figure" target="#fig_4">4</ref>. T has a single edge e(v0,v10) with score determined by the common word "brain" between v0 and v10. Also, the scores of nodes v0, v10 are computed using Equation <ref type="formula">1</ref>, for the query "Brain chip research".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EFFICIENT SUMMARY COMPUTATION</head><p>This section tackles the problem of how; given the document graph G of a document d for a query Q, to compute the top summary (or summaries) for d with respect to Q. For clarity, we only present algorithms for AND semantics. Notice that the problem of finding the top summary (total minimal spanning tree) is very similar to the Group Steiner Tree problem <ref type="bibr" target="#b36">[37]</ref>, which is known to be NP-complete. Our problem is slightly more complex since the groups of nodes are not disjoint, in contrast to the Group Steiner Problem, which is defined as follows:</p><p>Given an undirected, connected, and weighted graph G=(V, E, l), where V is the set of all vertices in G, E is the set of edges in G, and l is a weight function which maps each edge e∈E to a non-negative real number; and given a family R={R 1 ,….R k } of disjoint groups of vertices, where R i is a subset of V, the problem is to find a minimum-cost tree T which contains at least one vertex from each group R i . Since the weights of the graph are nonnegative, the solution is a tree-structure.</p><p>In contrast to previous work on proximity search on data graphs (see Section 2) where the top-k <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b7">7]</ref> or all <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b22">22]</ref> total minimal spanning trees are requested, in our summarization problem we typically care for only the single top summary, that is, the top-1 total minimal spanning tree. This allows more efficient algorithms as we explain below. Notice that the presented algorithms, which can be viewed as approximations of the Group Steiner Tree problem, can be divided along two dimensions. First, we have multi-result and top-1 algorithms, which compute a set of summaries or a single summary for a document and query pair respectively. Second, we have enumeration and expanding algorithms, which follow different execution approaches as explained below.</p><p>Precomputation. In order to boost the performance of the algorithms, we precompute and store the following information:</p><p>• A full-text index is built, as discussed above, to efficiently locate the nodes that contain the keywords and calculate their query-specific score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The all-pairs shortest paths between the nodes of the document graph G of every document d. That is, for each pair of nodes u,v∈G, we precompute and store the shortest path u~u 1 ~…~u r ~v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-Result Enumeration Algorithm</head><p>This algorithm returns a ranked list of summaries for a document and a query. In particular, it returns a summary for each possible combination of nodes that contain the keywords.</p><p>The algorithm (Figure <ref type="figure" target="#fig_4">4</ref>) proceeds as follows. First, we find all combinations of nodes in G that are minimal (no node is redundant) and total (contain all keywords in Q). Then, for each combination we create a complete graph G c (called closure graph) that contains all nodes in the combination and all-pairs edges between them with weight equal to their distance (taken by the precomputed all-pairs shortest paths). Then, we calculate all possible spanning trees in G c , and compute their scores using Equation 3. Then, for the top spanning tree we insert the Steiner nodes and trim redundant nodes to make it minimal. Then its accurate score is computed and added to the results list. Finally, the results are ranked and displayed. Example 2. Consider the document graph in Figure <ref type="figure" target="#fig_1">2</ref> and the query "Brain chip research". The nodes that contain the keywords are v0, v1, v3, v4, v10, v11, and v15. We then find all minimal and total node combinations, which are {v0, v10}, {v15, v0}, {v0, v3}, MultiResultEnumeration (document graph G, query Q) } {v4, v0}, and so on. For each combination we create a closure graph. For example, the closure graph for the second combination is v15~v0 with edge weight 0.096 (which is the length of the shortest path from v15 to v0). We then find all possible spanning trees of this graph, which is just v15~v0, for the above closure graph. Then, we replace the edge between v15 and v0 with the shortest path between them, which is v15~v14~v7~v0. This tree is already minimal and hence we output this result along with its score. The Steiner nodes in this result are v14 and v7, which don't have any keywords in them but are used to relate the other 2 nodes, v15 and v0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Top-1 Enumeration Algorithm</head><p>The Top-1 enumeration algorithm returns only one summary per document, for a query. The reason we created top-1 variants for both the enumeration and the expanding search (Sections 5.3, 5.4) is that typically the user only requests a single summary for a document, as in the case of snippets in Web search engine results.</p><p>This algorithm is similar but more efficient than the multiresult enumeration algorithm, because it only adds the Steiner nodes (line 9 in Figure <ref type="figure" target="#fig_4">4</ref>) for a single spanning tree. In particular, this algorithm finds the top spanning tree among all node combinations and then substitutes the Steiner nodes, while the multi-result algorithm finds the top spanning tree and substitutes the Steiner nodes for each node combination. The pseudo-code for this algorithm has the following difference with respect to Figure <ref type="figure" target="#fig_4">4</ref>: Lines 8-11 are moved outside the for-loop, that is, the for-loops ends at line 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2 (cont'd).</head><p>For the document graph in Figure <ref type="figure" target="#fig_1">2</ref> and query "Brain chip research", this algorithm goes through the same steps as in the case of enumeration algorithm. It computes all node combinations as explained in the previous example. The only difference is that this algorithm first finds the minimumscore spanning tree v1~v3 with edge weight 0.03 (which is the length of the shortest path from v1 to v3) among all spanning trees of all node combinations, and then replaces that edge with the shortest path v1~v2~v3, where v2 is the Steiner node and recomputes the score and displays it as the summary of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-Result Expanding Search Algorithm</head><p>The basic idea behind this algorithm (inspired by the algorithm in BANKS <ref type="bibr" target="#b7">[7]</ref>) is that we start from the nodes that contain the query keywords and progressively expand them in parallel until we find all minimal total spanning trees. The advantage of this algorithm compared to the enumeration algorithms is that we do not need to repeat the processing for all combinations of nodes, which may be too many if the document is large and contains many occurrences of the query keywords.</p><p>In particular, the algorithm (Figure <ref type="figure" target="#fig_5">5</ref>) finds (using the fulltext index) all the nodes that match some keywords in the query and starts expanding them incrementally, the best (maximumscore) edge at a time. We call the subgraph created from each keyword node v expanding area of v. Notice that, in contrast to BANKS, we use the precomputed all-pairs shortest paths data to efficiently grow the expanding area. That is, we only consider the edges that are contained in a shortest path from the current node v to any other node u that contains additional query keywords than v. When two or more expanding areas meet we check for possible new summaries. If a summary is found, it is trimmed to become minimal and its score is calculated using Equation <ref type="formula">3</ref>. The parallel expansion of the expanding areas terminated when for each combination of nodes that contains all keywords, their expanding areas have met.</p><p>Example 3. For the document graph in Figure <ref type="figure" target="#fig_1">2</ref> and the query "Brain chip research", the keyword nodes are v0, v1, v3, v4, v10, v11, and v15. We grow the expanding area of v0 to v0~v10, which is the first precomputed single source shortest path of source v0 and check for possible summaries. v0~v10 is total as well as minimal and hence we add it to the set of results. We grow each expanding area using its precomputed shortest paths. Then we grow v1 to v1~v2, v3 to v3~v2, v4 to v4~v3, v10 to v10~v9, v11 to v11~v10 and once we expand v11 we have another summary v11~v10 that is total and minimal. We keep doing this until the expanding areas of all the keywords nodes have been met and hence we can't have any more possible summaries and hence we terminate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Top-1 Expanding Search Algorithm</head><p>This algorithm differs from the multi-result expanding search algorithm in that it stops expanding the expanding areas once the first summary is produced. Intuitively this greedy approach produces a high-quality summary, as the trees produced first have smaller sizes, which implies smaller scores (Equation <ref type="formula">3</ref>). The pseudo code for the Top-1 expanding search algorithm differs from the multi-result variant in that, once it finds a summary in line 6 it trims it, calculates the score and adds it to results and exits the loop. So we have an extra line in Figure <ref type="figure" target="#fig_5">5</ref>: "7a. break;".  <ref type="figure" target="#fig_1">2</ref> and the query "Brain chip research", this algorithm goes through the same steps as its multi-result variant, but stops expanding once it finds the first summary, which is v10~v11 as explained in the previous example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiResultExpandingSearch(document graph G, query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">QUALITY EXPERIMENTS</head><p>To evaluate the quality of the results of our approach, we conducted two surveys. The subjects of the survey are fifteen students (of all levels and various majors) of FIU, who were not involved in the project. In this survey the users were asked to evaluate the summaries based on their quality and size (a longer summary carries more information but is less desirable).Each participant was asked to compare the summaries and rank them, assigning a score of 1 to 5, according to their quality for the corresponding query. A rank of 5 <ref type="bibr" target="#b1">(1)</ref> represents the summary that is most (least) descriptive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with DUC dataset</head><p>The dataset used in this survey consists of ten documents and two queries taken from the DUC 2005 dataset <ref type="foot" target="#foot_2">3</ref> , as shown in Table <ref type="table" target="#tab_3">2</ref>. We compare our summaries with DUC Peer summaries for quality. DUC peers are human and automatic summaries used in quality evaluation. We compared our summaries against the DUC peers with highest linguistic quality. Unfortunately, most of the summaries in the DUC datasets are query-independent and the few query-dependent ones are multi-document. Hence, in order to compare our work to that of DUC we used the following method to extract single-document summaries from query-dependent multi-document summaries for a set of ten documents over two topics. The sentences that have been extracted from a document d to construct the multi-document summary are viewed as d's single-document summary for the query/topic. Notice that the DUC summaries are created by extracting whole sentences from documents. The results of the survey prove the superiority of our approach, as shown in Table <ref type="table" target="#tab_3">2</ref>. Our method of combining extracted sentences using semantic connections in the form of Steiner trees leads to higher user satisfaction than the traditional sentence extraction methods. In particular, the Steiner sentences in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with Google and MSN Desktop</head><p>The dataset used in this survey consists of two news documents taken from the technology section of cnn.com. The participants were asked to evaluate the quality of the summaries of the two documents with respect to five queries. We chose queries where keywords appear both close and far from each other. For each query-document pair, three summaries are displayed corresponding to (a) the result of the Top-1 expanding search algorithm, (b) Google Desktop's summary, and (c) MSN Desktop's summary. Summaries (b) and (c) were created by indexing the two documents in our desktop and then submitting the five queries to the Desktop engines. The summaries are the snippets output for these documents. In order to compare apples to apples, we chose queries for which the length of the summaries produced by all three methods are similar, since clearly it is not fair to compare summaries of different lengths as some people favor conciseness while others the amount of information.</p><p>In this survey we set constant a to 1 and b to 0.5 in Equation <ref type="formula">3</ref>, which we found to produce higher-quality summaries. Notice that by increasing the value of constant a, we favor short results, while by increasing constant b we favor longer and more informative results. Hence, by setting a to 1 and b to 0.5 we favor shorter summaries, which have similar size to the ones produced by Google and MSN Desktop. This makes their comparison fairer. The results of the survey, which prove the superiority of our approach, are shown in Table <ref type="table" target="#tab_4">3</ref>. Notice that Google and MSN Desktop systems do not always include all keywords in the summary when they are more than two and have big distances between them. In contrast, our approach always finds a meaningful way to connect them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">PERFORMANCE EXPERIMENTS</head><p>To evaluate the performance of our approach we used a dataset of 200 news documents taken from the technology section of cnn.com. We used a PC with Pentium 4 2.44GHz processor and 256MB of RAM running Windows XP. The algorithms were implemented in Java. To build the full-text index we used Oracle interMedia <ref type="bibr">[34]</ref> and stored the documents in the database. JDBC was used to connect to the database system. First, we compare the performance of the four algorithms of Section 5 for summarizing keyword queries of various lengths. The execution times consist of two parts: (a) the computation of the scores of the nodes of the document graph (remember that this is query-specific and cannot be precomputed), and (b) the generation of the top summaries (minimal total spanning trees) in the document graph. The first part is handled by Oracle interMedia and the average times for a single document for various-length queries are shown in Table <ref type="table">5</ref>. In particular, Figure <ref type="figure" target="#fig_6">6</ref> (a) compares the performance of the Multi-Result algorithms, whereas Figure <ref type="figure" target="#fig_6">6</ref> (b) the Top-1 algorithms. We observe that the expanding search algorithms are faster than the enumeration ones, especially for long queries. Also, notice that there is only a slight difference in the performance of the Top-1 and the Multi-Result algorithms, because the document graphs are relatively small and hence there is no big difference between computing one or more summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5. Average times to calculate node weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of keywords</head><p>Notice that we do not compare the performance of our algorithms to BANKS, since our Multi-Result algorithms are adaptations of the BANKS algorithms to our problem, which is different as we explain in Section 5.</p><p>Finally, we measure the accuracy of the Top-1 versions of the algorithms. In particular, we measure (Table <ref type="table" target="#tab_6">6</ref>) the average rank of the summary of the Top-1 algorithms in the list of summaries created by the Multi-Result algorithms. For example, if the summary of the Top-1 algorithm appears as the third summary of the Multi-Result algorithm, then the rank is 3. We observe that the Top-1 expanding algorithm better approximates the corresponding Multi-Result algorithm's results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS AND FUTURE WORK</head><p>In this work we presented a structure-based technique to create query-specific summaries for text documents. In particular, we first create the document graph of a document to represent the hidden semantic structure of the document and then perform keyword proximity search on this graph. We show with a user survey that our approach performs better than other state of the art approaches. Furthermore, we show the feasibility of our approach with a performance evaluation.</p><p>In the future, we plan to extend our work to account for links between documents of the dataset. For example, exploit hyperlinks in providing summarization on the Web. Furthermore, we are investigating how the document graph can be used to rank documents with respect to keyword queries. Finally, we plan to work on more elaborate techniques to split a document to text fragments and assign weights on the edges of the document graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sample news document from www.cnn.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Document Graph for the document in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Top summary of the document of Figure 1 for query "Brain Chip Research"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Brain chip offers hope for paralyzed.   Donoghue's initial research published in the science journal Nature in 2002 consisted of attaching an implant to a monkey's brain that enabled it to play a simple pinball computer game remotely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Multi-Result Enumeration Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Multi-Result Expanding Search Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Processing times</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Top-5 summaries for query "Brain Chip Research"</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Rank Score</cell><cell></cell><cell></cell><cell cols="3">Summary</cell></row><row><cell>1</cell><cell>67.74</cell><cell>0.046</cell><cell></cell><cell>0.008</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>v0</cell><cell>0.017</cell><cell>v10</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>84.77</cell><cell>0.046</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.0003</cell></row><row><cell></cell><cell></cell><cell>v0</cell><cell>0.027</cell><cell>v7</cell><cell>0.027</cell><cell>v4</cell></row><row><cell>3</cell><cell>87.64</cell><cell>0.012</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.0005</cell></row><row><cell></cell><cell></cell><cell>v1</cell><cell>0.043</cell><cell>v14</cell><cell>0.037</cell><cell>v15</cell></row><row><cell>4</cell><cell>103.77</cell><cell>0.008</cell><cell></cell><cell>0.005</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>v10</cell><cell>0.015</cell><cell>v11</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>167.41</cell><cell>0.046</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.0</cell><cell>0.0005</cell></row><row><cell></cell><cell></cell><cell>v0</cell><cell>0.027</cell><cell>v7</cell><cell>0.032</cell><cell>v14</cell><cell>0.037</cell><cell>v15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 .</head><label>1</label><figDesc>Results ←∅; /*stores summaries*/ 2. Find all nodes in G that contain some keyword of Q; /*use full-text index*/</figDesc><table /><note><p>3. Find all minimal combinations of nodes that when taken together contain all keywords in Q; 4. For each minimal node combination C do { 5. Create closure graph G c that contains only the nodes in C; 6. Find all possible spanning trees S of G c ; 7. Calculate the score of each spanning tree in S using Eq. 3; 8. Pick the spanning tree T with the minimum score; 9. Replace the edges u~v in T with their precomputed shortest paths u~u 1 ~…~u k ~v; /* i.e., we are adding the Steiner nodes.*/ 10. Trim T to make it a minimal total spanning tree; 11. Calculate the score of T using Equation 3 and add T to Results; 12. Sort and output summaries in Results;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For the document graph in Figure</figDesc><table><row><cell>Example 3 (cont'd).</cell><cell></cell></row><row><cell></cell><cell>Q)</cell></row><row><cell cols="2">1. Results ←∅; /*stores summaries*/</cell></row><row><cell cols="2">2. Find all nodes N={N 1 ,…,N m } that contain the keywords in</cell></row><row><cell></cell><cell>Q; /*N i has the nodes that contain w i */</cell></row><row><cell cols="2">3. Repeat until the expanding areas of all combinations of</cell></row><row><cell></cell><cell>nodes in N 1 ,…,N m meet. {</cell></row><row><cell cols="2">4. For each node v in N do</cell></row><row><cell></cell><cell>{</cell></row><row><cell>5.</cell><cell>Add to the expanding area of v the maximum-score</cell></row><row><cell></cell><cell>adjacent edge from the (precomputed) shortest paths</cell></row><row><cell></cell><cell>starting at v and ending at a node in N not containing the</cell></row><row><cell></cell><cell>same keywords as v;</cell></row><row><cell>6.</cell><cell>Check for new results (summaries) T; /*i.e., trees that</cell></row><row><cell></cell><cell>contain a node from each of N 1 ,…,N m */</cell></row><row><cell>7.</cell><cell>Trim summaries T to become minimal;</cell></row><row><cell>8.</cell><cell>Calculate the score of T using Equation 3 and store in</cell></row><row><cell></cell><cell>Results;</cell></row><row><cell>9.</cell><cell>Sort and output summaries in Results;</cell></row><row><cell></cell><cell>} }</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Average summary ratings for DUC topics</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Query 1 (International Organized</cell><cell cols="3">Query 2 (Women in</cell></row><row><cell></cell><cell>Crime)</cell><cell></cell><cell cols="3">Parliaments)</cell></row><row><cell cols="3">DUC Topic ID: d301i</cell><cell cols="3">DUC Topic ID: d321f</cell></row><row><cell></cell><cell>DUC</cell><cell>Top-1</cell><cell></cell><cell>DUC</cell><cell>Top-1</cell></row><row><cell>Doc. ID</cell><cell>Peer</cell><cell>Expanding</cell><cell>Doc. ID</cell><cell>Peer</cell><cell>Expanding</cell></row><row><cell cols="2">FT941-3237 2.33</cell><cell>4.66</cell><cell cols="2">FT921-7786 4.00</cell><cell>2.50</cell></row><row><cell cols="2">FT944-8297 2.50</cell><cell>3.33</cell><cell cols="2">FT922-190 2.00</cell><cell>4.00</cell></row><row><cell cols="2">FT931-3563 2.83</cell><cell>3.00</cell><cell cols="2">FT921-937 2.00</cell><cell>4.33</cell></row><row><cell cols="2">FT943-16477 4.00</cell><cell>4.17</cell><cell cols="2">FT922-13353 2.83</cell><cell>4.17</cell></row><row><cell cols="2">FT943-16238 3.67</cell><cell>3.67</cell><cell cols="2">FT921-74 2.33</cell><cell>3.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . Average summary ratings for documents D1 and D2</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">Google Desktop MSN Desktop Top-1 Expanding</cell></row><row><cell>Queries</cell><cell>D1</cell><cell>D2</cell><cell>D1</cell><cell>D2</cell><cell>D1</cell><cell>D2</cell></row><row><cell>1</cell><cell>2.33</cell><cell>3.67</cell><cell>2.33</cell><cell>3.67</cell><cell>4.87</cell><cell>3.67</cell></row><row><cell>2</cell><cell>2.00</cell><cell>3.33</cell><cell>2.00</cell><cell>3.00</cell><cell>4.33</cell><cell>3.33</cell></row><row><cell>3</cell><cell>3.00</cell><cell>2.67</cell><cell>0.67</cell><cell>3.00</cell><cell>4.93</cell><cell>4.00</cell></row><row><cell>4</cell><cell>1.67</cell><cell>2.67</cell><cell>1.67</cell><cell>3.00</cell><cell>4.67</cell><cell>4.00</cell></row><row><cell>5</cell><cell>2.00</cell><cell>1.67</cell><cell>3.00</cell><cell>1.00</cell><cell>4.00</cell><cell>3.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . Queries used for documents D1 and D2</head><label>4</label><figDesc></figDesc><table><row><cell>Queries</cell><cell>Document D1</cell><cell>Document D2</cell></row><row><cell>1</cell><cell cols="2">Microsoft worm protection IT Research awards</cell></row><row><cell>2</cell><cell>Anti-virus protection</cell><cell>Algorithms development</cell></row><row><cell></cell><cell></cell><cell>research</cell></row><row><cell>3</cell><cell>Recovering worm deleted</cell><cell>Software projects</cell></row><row><cell></cell><cell>files</cell><cell></cell></row><row><cell>4</cell><cell>Worm affected agencies</cell><cell>Large research grants</cell></row><row><cell>5</cell><cell cols="2">Deleted computer software Computer network</cell></row><row><cell></cell><cell></cell><cell>security project</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 . Average ranks of Top-1 Algorithms with respect to Multi-Result algorithms</head><label>6</label><figDesc></figDesc><table><row><cell>Number of keywords</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Top-1 Enumeration</cell><cell>1.4</cell><cell>1.8</cell><cell>2.1</cell><cell>2.78</cell></row><row><cell>Algorithm.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Expanding Search</cell><cell>1.1</cell><cell>1.3</cell><cell>1.4</cell><cell>1.8</cell></row><row><cell>Algorithm.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The proximity problem is slightly harder since the sets of nodes do not have to be disjoint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://duc.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://duc.nist.gov/ summaries provide coherency in the aggregation of the keywordcontaining-sentences.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Project partly supported by NSF grant IIS-0534530.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical methods for retrieving most significant paragraphs in newspaper articles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abracos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pereira-Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/EACL Workshop on Intelligent Scalable Text Summarization</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DBXplorer: A System For Keyword-Based Search Over Relational Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
		<title level="m">Automatically Summarizing Web Sites -Is there any way around it? CIKM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Authority-Based Keyword Queries in Databases using ObjectRank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using lexical chains for text summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISTS</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OCELOT: A System for summarizing web pages</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keyword Searching and Browsing in Databases using BANKS</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhalotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nakhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hulgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarshan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adding Structure to Unstructured Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Block-level Link Analysis. SIGIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustering and Visualization in a Multi-Lingual Multi-Document Summarization System</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="http://duc.nist.gov" />
		<title level="m">Document Understanding Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edmundson: New Methods in Automatic Abstracting</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal</title>
		<imprint>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text Summarization Challenge Text Summarization Evaluation in Japan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fukusima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WAS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Proximity Search in Databases</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Summarizing text documents: Sentence selection and evaluation metrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kantrowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="http://desktop.google.com/" />
		<title level="m">Google Desktop search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">XRANK: Ranked Keyword Search over XML Documents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shanmugasundaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using categories to provide context for fulltext retrieval results</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the RIAO</title>
		<meeting>the RIAO</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCL</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Papakonstantinou: Efficient IR-Style Keyword Search over Relational Databases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Papakonstantinou: DISCOVER: Keyword Search in Relational Databases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balmin</surname></persName>
		</author>
		<title level="m">Keyword Proximity Search on XML Graphs. ICDE</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Kacholia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sudarshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karambelkar</surname></persName>
		</author>
		<title level="m">Bidirectional Expansion For Keyword Search on Graph Databases. VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pederson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">A Trainable Document Summarizer, SIGIR</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Stylistic and Lexical Cotraining for Web Block Classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>WIDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving Summarization Performance by Sentence Compression -A Pilot Study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRAL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Retrieving and Organizing Web Pages by &quot;Information Unit</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying topics by position</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference on Applied Natural Language Processing</title>
		<meeting>the ACL Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discourse trees are good indicators of importance in text</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Automatic Text Summarization</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of natural language texts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TextRank: Bringing Order into Texts</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="http://www.oracle.com/technology/products/intermedia" />
		<title level="m">MSN Desktop search</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating Natural Language Summaries from Multiple On-line Sources</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">WebInEssence: A Personalized Web-Based Multi-Document Summarization and Recommendation System</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL Workshop on Automatic Summarization</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond Steiner&apos;s Problem: A VLSI Oriented Generalization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W G</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Graph-Theoretic Concepts in Computer Science</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic text decomposition using text segments and text themes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Hypertext</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic text structuring and summarization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Information Retrieval: A Brief Overview, Google, IEEE Data Eng</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning Block Importance Models for Web Pages</title>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Robust Practical Text Summarizer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Automatic Text Summarization</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Advantages of Query Biased Summaries in Information Retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure-Based Query-Specific Document Summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><surname>Hristidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2005</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<title level="m">Finding Relevant Documents using Top Ranking Sentences: An Evaluation of Two Alternative Schemes, SIGIR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Korelsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Multidocument Summarization via Information Extraction. HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast generation of abstracts from general domain text corpora by extracting relevant sentences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
