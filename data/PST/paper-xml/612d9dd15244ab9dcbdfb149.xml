<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing and Mitigating Interference in Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Leng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Analyzing and Mitigating Interference in Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weight sharing is a popular approach to reduce the cost of neural architecture search (NAS) by reusing the weights of shared operators from previously trained child models. However, the rank correlation between the estimated accuracy and ground truth accuracy of those child models is low due to the interference among different child models caused by weight sharing. In this paper, we investigate the interference issue by sampling different child models and calculating the gradient similarity of shared operators, and observe: 1) the interference on a shared operator between two child models is positively correlated with the number of different operators; 2) the interference is smaller when the inputs and outputs of the shared operator are more similar. Inspired by these two observations, we propose two approaches to mitigate the interference: 1) MAGIC-T: rather than randomly sampling child models for optimization, we propose a gradual modification scheme by modifying one operator between adjacent optimization steps to minimize the interference on the shared operators; 2) MAGIC-A: forcing the inputs and outputs of the operator across all child models to be similar to reduce the interference. Experiments on a BERT search space verify that mitigating interference via each of our proposed methods improves the rank correlation of super-net and combining both methods can achieve better results. Our discovered architecture outperforms RoBERTa base by 1.1 and 0.6 points and ELECTRA base by 1.6 and 1.1 points on the dev and test set of GLUE benchmark. Extensive results on the BERT compression, reading comprehension and ImageNet task demonstrate the effectiveness and generality of our proposed methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) <ref type="bibr" target="#b68">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b69">Zoph et al., 2018;</ref><ref type="bibr" target="#b29">Liu et al., 2018)</ref> aims to automatize the process of discovering high-performance architectures. Conventional NAS <ref type="bibr" target="#b68">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b69">Zoph et al., 2018;</ref><ref type="bibr" target="#b40">Real et al., 2017)</ref> obtains the accuracy of an architecture by training it from scratch, which requires huge computational resources (e.g., hundreds of GPU days). To speed up the training process, an important technique, called weight sharing, is proposed by <ref type="bibr" target="#b36">Pham et al. (2018)</ref>. Briefly speaking, weight sharing is to maintain a single copy of weights on a super-net that contains all the architectures in the search space. Rather than stand-alone training from scratch for each architecture, the weight sharing method samples an architecture at each training step as a child model or a sub-graph of the super-net, and continues the training on the weights of shared operators from previously trained architectures in the super-net. Due to low computational cost and competitive performance, weight sharing has drawn wide attention <ref type="bibr" target="#b36">(Pham et al., 2018;</ref><ref type="bibr" target="#b0">Bender et al., 2018;</ref><ref type="bibr" target="#b29">Liu et al., 2018;</ref><ref type="bibr" target="#b55">Xie et al., 2019;</ref><ref type="bibr" target="#b1">Cai et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b27">Li &amp; Talwalkar, 2020;</ref><ref type="bibr" target="#b15">Guo et al., 2020;</ref><ref type="bibr" target="#b35">Peng et al., 2020;</ref><ref type="bibr" target="#b7">Chu et al., 2021;</ref><ref type="bibr" target="#b33">Ning et al., 2021)</ref>.</p><p>Although a number of NAS methods based on weight sharing have been proposed and achieved impressive results, many works <ref type="bibr" target="#b33">(Ning et al., 2021;</ref><ref type="bibr" target="#b15">Guo et al., 2020;</ref><ref type="bibr" target="#b63">Zela et al., 2019;</ref><ref type="bibr" target="#b62">Yu et al., 2019;</ref><ref type="bibr" target="#b66">Zhang et al., 2020a;</ref><ref type="bibr">b)</ref> find that the rank correlation between the estimated accuracy and ground truth accuracy of child models is low due to the interference among different child models on shared weights. The shared operators receive different gradient directions from child models with different architecture topologies for optimization even with the same batch of training data, which seriously affects the rank of the child models since interference can cause insufficient training and inaccurate performance evaluation. This phenomenon is especially severe for the complex search space that contains different types of candidate operators (e.g., convolutions and multi-head attention) and numerous child models.</p><p>While previous research works have noticed the interference issue and empirically found that the interference is correlated with factors such as the size of a search space <ref type="bibr" target="#b43">(Shu et al., 2019;</ref><ref type="bibr" target="#b66">Zhang et al., 2020a)</ref>, little has been discussed about the causes of the interference and how to mitigate it.</p><p>Through our quantitative analyses, we observe the interference on a shared operator is positively correlated with the number of different operators that exist between two child models. The main reason for the interference is that the topology of the architecture varies randomly along with the training process. Then the operator shared by different architecture topologies may receive activations from different inputs during the forward process and receive gradients from different outputs during the backward process. In this way, the shared operator is optimized towards different directions by different child models, which causes interference. Thus, we force the inputs and outputs of the operator to be similar to the average inputs and outputs of all the child models, and find that the interference can be reduced.</p><p>Inspired by the above observations, we propose two methods for MitigAtinG InterferenCe (MAGIC) from different perspectives: 1) reduce the changes of the architecture topologies in the adjacent sampling steps (MAGIC-T) and 2) align the inputs and outputs of the operator shared by different child models (MAGIC-A). As for MAGIC-T, we first analyze the commonly-used random single path one-shot algorithms <ref type="bibr" target="#b15">(Guo et al., 2020)</ref> and find that the number of different operators between adjacent sampling steps is positively correlated with the number of layers of the super-net and number of candidate operators, which leads to serious interference in a large search space. To minimize the interference, we gradually change the topological environment of the shared operators by sampling a child model that differs from the child model sampled at the previous step with only one operator at each training step. As for MAGIC-A, we select the model with the best validation accuracy among the child models as an anchor model to align the inputs and outputs of all child models together respectively. The anchor model can be replaced when the performance of any child model outperforms it. Finally, MAGIC-T and MAGIC-A can be combined to further reduce the interference.</p><p>To verify the effectiveness of our methods, we adopt a challenging hybrid super-net including three types of operators: multi-head attention, feed-forward network and convolution, and conduct experiments on the large scale BERT pretraining task. The experiments verify that MAGIC-T and MAGIC-A can mitigate the interference and improve rank correlation individually. Combining them can achieve better performance. Extensive experiments on 10 natural language processing datasets of BERT pre-training, and ImagetNet classification tasks on MobileNet-V2 search space <ref type="bibr" target="#b41">(Sandler et al., 2018)</ref> show the effectiveness and generality of our proposed methods.</p><p>The contributions of this paper are summarized as follows:</p><p>• We conduct thorough analyses on the interference issue in NAS, and find interference between two models is caused by diverse gradient directions, and is positively correlated with differences of their architecture topologies as well as their inputs and outputs of shared operators. To the best of our knowledge, we are the first to quantitatively analyze the interference with thorough empirical results.</p><p>• Inspired by our analyses, we propose two methods, MAGIC-T and MAGIC-A, to mitigate interference by reducing topological changes during the sampling process and aligning the inputs and outputs of the shared operators among different child models.</p><p>• Experiments on the BERT search space demonstrate that our methods can significantly improve the rank correlation of the super-net by mitigating interference. Our discovered architecture outperforms RoBERTa base by 1.1 and 0.6 points, and ELECTRA base by 1.6 and 1.1 points on the dev and test of GLUE tasks respectively. Extensive experiments on the BERT compression task, reading comprehension and ImageNet classification tasks verify the generality of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural architecture search. Recently, NAS methods <ref type="bibr" target="#b36">(Pham et al., 2018;</ref><ref type="bibr" target="#b58">Xu et al., 2019;</ref><ref type="bibr" target="#b33">Ning et al., 2021)</ref> mainly adopt weight sharing to re-use the weights of previously trained architectures within a super-net (a.k.a one-shot NAS methods) to speed up the training process. Many methods leverage differentiable NAS <ref type="bibr" target="#b29">(Liu et al., 2018;</ref><ref type="bibr">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b50">Wang et al., 2021b)</ref> by relaxing the discrete architectures into continuous space, and jointly learn the super-net weights and architectural weights. A number of other approaches utilize sampled single path one-shot NAS approaches <ref type="bibr" target="#b0">(Bender et al., 2018;</ref><ref type="bibr" target="#b7">Chu et al., 2021;</ref><ref type="bibr" target="#b27">Li &amp; Talwalkar, 2020;</ref><ref type="bibr" target="#b15">Guo et al., 2020;</ref><ref type="bibr" target="#b2">Cai et al., 2019)</ref>. They usually adopt a chain-styled super-net, train it by randomly sampling and optimizing a single path (child model), and then use the trained super-net as a performance estimator to evaluate the accuracy of child models to search for good architectures using progressively shrinking <ref type="bibr" target="#b57">(Xu et al., 2021;</ref><ref type="bibr" target="#b18">Hu et al., 2020)</ref> or evolution algorithms <ref type="bibr" target="#b15">(Guo et al., 2020;</ref><ref type="bibr" target="#b61">Yu et al., 2020)</ref>. They enjoy the flexibility of decoupling the training and searching stages and can support searching multiple efficient networks under different constraints <ref type="bibr" target="#b2">(Cai et al., 2019;</ref><ref type="bibr" target="#b57">Xu et al., 2021)</ref>. However, in the sampling process, different optimization directions, caused by training different child models, interfere with each other, which causes insufficient training and inaccurate performance evaluation <ref type="bibr" target="#b66">(Zhang et al., 2020a)</ref>. Our work focuses on the interference issue of chain-styled search space in sampled single path one-shot NAS approaches.</p><p>Weight sharing and interference. Multiple works have analyzed the effects of weight sharing. <ref type="bibr" target="#b43">Shu et al. (2019)</ref> calculate gradient variance by adding randomly sampled Gaussian noise on the weights of super-net, and find the optimization process is noisier and less efficient in complex search space such as stacking more cells. <ref type="bibr" target="#b37">Pourchot et al. (2020)</ref> observe that, for weight sharing algorithms, architectures with a residual connection or a 3×3 convolution on the first node are preferred. <ref type="bibr" target="#b25">Laube &amp; Zell (2021)</ref> explore various training settings, including regularization, learning rate schedule and gradient clipping, and study their effects on the rank correlation of sampled single path one-shot NAS approaches <ref type="bibr" target="#b15">(Guo et al., 2020)</ref>. These methods study how different training configurations and search spaces affect the results of weight sharing algorithms. Different from these works, we try to understand and mitigate the interference on the shared weights and improve the rank ability of super-net.</p><p>Previous works have noticed the interference issue <ref type="bibr" target="#b0">(Bender et al., 2018;</ref><ref type="bibr" target="#b15">Guo et al., 2020;</ref><ref type="bibr" target="#b25">Laube &amp; Zell, 2021;</ref><ref type="bibr" target="#b54">Xie et al., 2020)</ref>. Among them, <ref type="bibr" target="#b66">Zhang et al. (2020a)</ref> conduct experiments to study the interference of weight sharing using a search space of 64 architectures. By randomly sampling architectures per step for training and plotting the accuracy of previous sampled architectures, they find that current updating with the sampled child model is detrimental to other models and thus causes high variance of the rank. In contrast, our quantitative experiment analyses reveal that the essence of the interference is different gradient directions on shared weights caused by different topologies of child models. Several works alleviate the interference with search space pruning. <ref type="bibr" target="#b66">Zhang et al. (2020a)</ref> propose to divide the search space according to similarities of child models, such as sorting the models lexicographically and evenly slice these models into several groups, then train the supernet on individual groups of models. Moreover, to obtain more accurate performance, they directly shrink the size of search space to one child model and fine-tune it individually. However, for a large search space, identifying similar architectures is difficult and fine-tuning each child model could incur significant computational overhead. Other methods <ref type="bibr" target="#b67">(Zhang et al., 2020b;</ref><ref type="bibr" target="#b18">Hu et al., 2020;</ref><ref type="bibr" target="#b57">Xu et al., 2021)</ref> progressively shrink the search space by removing the unpromising architectures and operators in the training process to reduce interference. <ref type="bibr" target="#b33">Ning et al. (2021)</ref> propose several practical approaches such as operation pruning, progressively search space pruning and removing affine operations in batch normalization to reduce sharing extent. Different from these methods, inspired by our analyses, our work mitigates interference by modifying the sampling procedure and aligning inputs and outputs of shared operators. Thus, progressively shrink (pruning) methods are complementary to our methods. <ref type="bibr" target="#b7">Chu et al. (2021)</ref> also modify the sampling process by accumulating the updates over k samples, chosen such that each of the k operators of the super-net appears exactly once in the super-net to ensure fairness of operators updating, which increase the training costs by over k× and cannot avoid the interference between different training steps. Our work aims at quantitatively analyzing the interference and proposing effective methods to mitigate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analyzing Interference</head><p>In single path one-shot NAS approaches <ref type="bibr" target="#b0">(Bender et al., 2018;</ref><ref type="bibr" target="#b27">Li &amp; Talwalkar, 2020;</ref><ref type="bibr" target="#b15">Guo et al., 2020;</ref><ref type="bibr" target="#b2">Cai et al., 2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis Setup</head><p>The gradients are influenced by various factors including input data, weights of the super-net and selected child model.</p><p>To study the gradient changes solely caused by different child models, we first train a super-net using single path one-shot NAS algorithm <ref type="bibr" target="#b15">(Guo et al., 2020)</ref>. Then, we freeze the weights of super-net and only study the gradients caused by different child models under the same batch of training data. The super-net is organized in chain-style with N layers. Each layer contains all candidate operators in</p><formula xml:id="formula_0">O = {o 1 , • • • , o C },</formula><p>where C is the number of predefined candidate operators. A child model is a single path from the bottom layer to the top layer. Thus, there are C N possible child models in the super-net. To study the interference in more challenging settings, we adopt a hybrid BERT search space including several popular candidate operatorsmulti-head attention (MHA), feed-forward network (FFN) and convolution (CONV), and conduct experiments on the BERT pre-training task <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. Specifically, to study the interference both within the same type operators and between different types of operators, we use candidate operators O ={MHA6, MHA8, FFN, FFN', CONV3, CONV5}, where MHA6 is MHA with 6 heads, CONV3 is convolution with kernel size 3 and FFN' is FFN with a slightly larger inner hidden size. To exclude the influence of parameter size of different operators, we further adjust the inner hidden size of operators to ensure that they have similar parameter sizes (see Appendix A for more details). We train an N = 12 layer super-net using a batch of 1024 sentences on 32 NVIDIA P40 GPUs until 62,500 steps. Then we freeze the super-net, and study the gradients of different child models by feeding the same batch of data (a large batch of 2048 sentences).</p><formula xml:id="formula_1">Operation ! " ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Layer 1 Layer 2 Layer 3 … Forward ! " ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Backward Layer N Figure 1:</formula><p>The illustration of the forward and backward process regarding the operator o g in layer 1 that is shared by child models that differ in layer two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results and Analyses</head><p>We first focus on a simple case: child models that only differ in one operator. As shown in Figure <ref type="figure">1</ref>, we choose C child models and they only differ in layer 2. Then we compare their gradients on o g at the first layer<ref type="foot" target="#foot_3">1</ref> . To measure the gradients similarity on o g between different child models, we flatten the gradients and calculate their cosine similarity<ref type="foot" target="#foot_4">2</ref> . A lower cosine similarity indicates larger difference in gradients and thus more interference on the shared operators. As shown in Figure <ref type="figure">2</ref> (a), we can find 1) although different child models differ by only one operator, the gradient cosine similarity is still not high, indicating that different child models can actually lead to very different gradient directions on their shared operators; 2) the same type of operators have less gradient interference (larger cosine similarity) than different types of operators, demonstrating that the interference in hybrid search space tends to be more serious than those with the same type operators.</p><p>For the hybrid search space, although all child models are trained under the same task, there is no guarantee that the operator should learn the same transformation on different child models. In other words, the inputs and outputs of the operator differ when it locates on different child models. It motivates us to explicitly force the inputs and outputs of the operators to be similar to reduce such interference. To this end, we re-train a super-net. At each training step, we first randomly choose C child models and calculate their average inputs and outputs in each layer. Then we randomly sample a child model for training and add an extra alignment training objective by calculating MSE loss between its inputs and outputs and the average ones in a layer-by-layer manner (see detailed formulation in Appendix A). Finally, we freeze the super-net and re-calculate the gradient cosine similarity as aforementioned. As shown in Figure <ref type="figure">2</ref> (b), we can observe that by aligning the inputs and outputs of the shared operators to be similar to the average inputs and outputs, the gradient interference can be reduced.</p><p>We further extend the analyses to a more general case: child models that differ in m operators. Specifically, we choose C child models that differ from the second layer to the (m + 1)-th layers and obtain the cosine similarity matrix on o g similar to Figure <ref type="figure">2</ref> (a). Then we calculate the average of the cosine similarity matrix to represent the average interference by varying m operators. The results are shown in Figure <ref type="figure">2</ref> (c). As we can see, the interference on a shared operator between two child models is positively correlated with the number of different operators between them. It demonstrates that randomly sampling child models in the training process in a complex search space could cause serious interference since their architecture topologies may differ a lot.</p><p>Although the analyses are performed under conditions of freezing the super-net and feeding the same batch of data, our findings about which factors influence the interference are general, and can help develop new methods to mitigate interference during training, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mitigating Interference</head><p>Inspired by the observations in Sec. 3, we propose two methods: MAGIC-T and MAGIC-A, to mitigate the interference.</p><p>MitigAtinG InTerferenCe from the pespective of Topological environment (MAGIC-T). According to our analyses in Sec. 3.2, more topological differences between two child models can cause more interference on the shared operators. Given a super-net with N layers and C candidate operators, previous single path NAS approaches <ref type="bibr" target="#b0">(Bender et al., 2018;</ref><ref type="bibr" target="#b7">Chu et al., 2021;</ref><ref type="bibr" target="#b27">Li &amp; Talwalkar, 2020;</ref><ref type="bibr" target="#b15">Guo et al., 2020)</ref> change</p><formula xml:id="formula_2">N (C−1)</formula><p>C operators between α t and α t−1 on average, where α t is the sampled child model at step t, because there are N layers (operators) and the probability of each operator in α t being different from the one in</p><formula xml:id="formula_3">α t−1 is C−1 C .</formula><p>Modern algorithms usually employ a huge search space with dozens of layers <ref type="bibr" target="#b36">(Pham et al., 2018;</ref><ref type="bibr" target="#b29">Liu et al., 2018;</ref><ref type="bibr" target="#b2">Cai et al., 2019;</ref><ref type="bibr" target="#b7">Chu et al., 2021)</ref>. For such a search space, two child models sampled between adjacent sampling steps differ a lot and the gradients of shared operators interfere with each other, which is detrimental to making progress in training. To mitigate such interference, we propose MAGIC-T to gradually change the topological environment. Specifically, at each training step, MAGIC-T samples a child model α t by randomly substituting one operator (k = 1) in the child model α t−1 sampled at the last step with another operator, and applies the forward and backward computation using α t for weights updating.</p><p>Although MAGIC-T substitutes only one operator in one iteration, it does not only sample locally around the ini- tial child model α 0 . Indeed, as t increases, MAGIC-T can sample diverse architectures, approaching to the uniform sampling over the entire search space. This can bee shown by viewing MAGIC-T performing a random walk on the architecture graph G, in which each architecture corresponds to a node and two nodes are connected by an edge if and only if they differ in only one operator. It is easy to see that G consists of C N nodes (recall that N is the number of layers and C is the number of operators) and the (shortest) distance between any two nodes is at most N . Suppose the random walk {X t } t starts at node X 0 = α 0 , and denote the node distribution at time t as π t (π t is a C N dimensional vector and π t (v) = Pr[X t = v]). Let π be the uniform distribution over the nodes of G (which is the stationary distribution). By standard convergence theory of random walk in Markov chain <ref type="bibr" target="#b26">(Levin &amp; Peres, 2017)</ref> <ref type="foot" target="#foot_5">3</ref> , one can show that the total variational distance between π t and π</p><formula xml:id="formula_4">d T V (π t , π) ≤ exp(−t/N − ln N ).</formula><p>Hence, if we want d T V (π t , π) ≤ ϵ, we only need t ≥ N ln N + N log 1/ϵ. Since t is sufficiently large (e.g., 250,000 steps for BERT), the variational distance between the node distribution of MAGIC-T and uniform sampling is sufficiently small. Thus, MAGIC-T can sample diverse architectures from the whole search space like uniform sampling.</p><p>MitigAtinG InTerferenCe from the perspective of inputs and outputs Alignment (MAGIC-A). In Sec. 3.2, we find that using the average inputs and outputs of shared operators for alignment can reduce the interference. However, it increases the computational cost by a factor of C, and limits the flexibility of search algorithms. Thus, we directly pick a top-performing anchor child model from the search space to align other child models. The anchor model can be replaced when the performance of another child model outperforms it. Formally, the inputs and outputs alignment loss between sampled child model α t and the anchor model α l is defined as</p><formula xml:id="formula_5">L align (H (α l ) , H (αt) ) = N n=1 MSE(H (α l ) n , H (αt) n ),<label>(1)</label></formula><p>where H n is the outputs of n-th layer (the inputs of (n + 1)th layer), and N is the number of layers. Then the training objective of a sampled child model α t is</p><formula xml:id="formula_6">L = L pred + λL align ,<label>(2)</label></formula><p>where L pred is the supervised loss between predictions and ground truth (e.g., cross entropy loss in masked language modeling), λ is the scaling parameter that controls the weight of alignment loss. In practice, we can adopt block-wise alignment where each block contains a few layers to avoid over-regularization. The training procedure of MAGIC-A at each step is as follows:</p><p>• Obtain a batch of data and an anchor child model α l , and randomly sample a child model α t ,</p><p>• Calculate the loss according to Eq. ( <ref type="formula" target="#formula_12">5</ref>) and update the weights of α t ,</p><p>• Replace</p><formula xml:id="formula_7">α l with α t if Val(α t ) &gt; Val(α l ),</formula><p>where Val(•) is the accuracy obtained from the dev set. Maintaining a top-performing anchor model is similar to prioritized paths introduced by <ref type="bibr" target="#b35">Peng et al. (2020)</ref>. However, our anchor model is used to align inputs and outputs of shared operators and reduce interference rather than distillation on the final layer to boost the training of child models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>Search space and super-net training. We adopt a chainstyled super-net with candidate operators O ={MHA12, FFN, CONV3, CONV5}. Following BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, we train the super-net and discover architectures using BookCorpus plus English Wikipedia (16GB in total). The super-net is trained with the batch size of 1024 sentences for 250,000 steps (same computational resources as BERT base ). We adopt commonly-used SPOS <ref type="bibr" target="#b15">(Guo et al., 2020)</ref> as the baseline method, which randomly samples a child model for training at each step. For MAGIC-T, unless otherwise mentioned, it gradually changes k = 1 operator to reduce the interference. For MAGIC-A, we train the supernet with masked language modeling loss in the first three epochs for a warm-start and then add the alignment loss with λ = 0.5 in every four layers to avoid over-regularization.</p><p>Since MAGIC-T and MAGIC-A are independent methods proposed from two different perspectives, they can also be combined (MAGIC-AT) to collaboratively mitigate the interference. See Appendix C for detailed configurations.</p><p>Architecture search and re-training. Previous single path one-shot methods usually adopt progressively shrinking <ref type="bibr" target="#b57">(Xu et al., 2021;</ref><ref type="bibr" target="#b18">Hu et al., 2020)</ref> or evolution algorithms <ref type="bibr" target="#b15">(Guo et al., 2020;</ref><ref type="bibr" target="#b2">Cai et al., 2019)</ref> to search effective models. Since progressively shrinking can allocate more computational resources to promising architectures and speed up the search process, we adopt it by deleting five unpromising operators <ref type="bibr" target="#b57">(Xu et al., 2021)</ref> at the end of each epoch until only one child model is left in the search space. Note that we do not adopt progressively shrinking when analyzing the rank correlation in Sec. 5.2. To evaluate the standalone performance of child models, we re-train them from scratch for 125,000 steps with batch size 2048 on 32 NVIDIA P40 GPUs and keep other configurations the same as those of super-net. <ref type="foot" target="#foot_6">4</ref> We train RoBERTa base <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> following its original configurations while using the same computational resources as BERT base .</p><p>Evaluation. We evaluate performance by fine-tuning pretrained models on GLUE benchmark <ref type="bibr" target="#b48">(Wang et al., 2019)</ref> and follow hyper-parameters of <ref type="bibr" target="#b30">Liu et al. (2019)</ref> for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analyses on the Rank Correlation</head><p>Comparison of Kendall rank correlation. We first perform correlation analysis to evaluate whether our method MAGIC can improve the rank of child models by mitigating interference. First, we uniformly sample 60 child models, train them on the pre-training tasks and obtain their groundtruth performance on the downstream MNLI task <ref type="bibr" target="#b53">(Williams et al., 2018)</ref>, which is widely used to indicate the performance of the NLP pre-training models <ref type="bibr" target="#b30">(Liu et al., 2019;</ref><ref type="bibr" target="#b45">Song et al., 2020;</ref><ref type="bibr">Dong et al., 2019)</ref>. Then, given a supernet, we can calculate the Kendall rank correlation coefficient (Kendall's Tau) <ref type="bibr" target="#b22">(Kendall, 1938)</ref> between their negative weight sharing validation loss and ground-truth performance. As shown in Table <ref type="table" target="#tab_1">1</ref>, compared with the baseline, MAGIC-T improves the rank correlation from 0.36 to 0.49, which shows the benefits of gradual modification to mitigate the interference. Furthermore, combining MAGIC-T and MAGIC-A achieves an even higher rank correlation of 0.57. It shows that MAGIC-T and MAGIC-A, proposed from two different perspectives, can collaboratively reduce interference to further improve the rank correlation.    In the following sections, we demonstrate the effectiveness and generality of MAGIC-AT on the BERT pre-training, compression task and image classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Searching Effective BERT Models</head><p>Results on GLUE benchmark. To show the generality of the architecture discovered by our MAGIC-AT, we train it with two different settings: 1) masked language modeling (MLM) <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> proposed in BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and 2) replace token detection (RTE) proposed in ELECTRA <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>. The discovered architecture is shown in Figure <ref type="figure">6</ref> and the experimental results are shown in Table <ref type="table" target="#tab_2">2</ref>. Using the MLM object, our model consistently outperforms other models by a large margin on both the dev and the test set. Compared with RoBERTa, our model achieves 1.1 and 0.6 higher points on the dev and the test set respectively. Using replace token detection objective following ELECTRA, our model is superior to ELECTRA by 1.6 and 1.1 points on the dev and the test set respectively.</p><p>Results on SQuAD datasets. We further evaluate the generalizability of our searched architecture by fine-tuning it to reading comprehension tasks SQuAD v1.1 <ref type="bibr" target="#b38">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b39">(Rajpurkar et al., 2018)</ref>. We adopt standard evaluation metrics of Exact-Match (EM) and F1 scores following <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b30">Liu et al., 2019;</ref><ref type="bibr" target="#b8">Clark et al., 2020)</ref>. The results are shown in Table <ref type="table" target="#tab_4">4</ref>.  <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> 82.1 89.3 74.9 78.2 ELECTRA base <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>  Compared with RoBERTa, our model achieves 0.6 and 0.7 higher points on EM and F1 for SQuAD v1.1, 1.7 and 2.2 higher points on EM and F1 for SQuAD v2.0 while using fewer parameters. Furthermore, our model also surpasses the ELECTRA using replace token detection objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Searching Compressed BERT Models</head><p>We further validate the generalizability of our algorithms for BERT model compression. Previous works on BERT compression aim to design novel architectures to explore the potential of different architectures <ref type="bibr" target="#b57">(Xu et al., 2021)</ref> or advanced distillation methods <ref type="bibr" target="#b42">(Sanh et al., 2019;</ref><ref type="bibr" target="#b51">Wang et al., 2020;</ref><ref type="bibr" target="#b47">Turc et al., 2019;</ref><ref type="bibr" target="#b16">Hou et al., 2020)</ref> to efficiently learn knowledge from the teacher model. We build a supernet as described in Sec. 5.1 but reduce the hidden size of candidate operators from 768 to 512. We re-run MAGIC-AT on this search space and evaluate the architecture found by our algorithm for model compression. Following NAS-BERT <ref type="bibr" target="#b57">(Xu et al., 2021)</ref>, we apply knowledge distillation on two stages to train the discovered model (i.e., pre-training and fine-tuning) and do not add extra techniques such as attention matrix distillation <ref type="bibr" target="#b19">(Jiao et al., 2020;</ref><ref type="bibr" target="#b16">Hou et al., 2020)</ref>. We adopt the teacher model used in NAS-BERT <ref type="bibr" target="#b57">(Xu et al., 2021)</ref> to ensure that the improvement is caused by a superior architecture. Other training hyper-parameters are the same as those in Sec. 5.3. The discovered architecture and detailed configurations are presented in Appendix C. As shown in Table <ref type="table" target="#tab_3">3</ref>, our model achieves better performance compared to all previous approaches. This again shows the general effectiveness of our algorithms.  <ref type="bibr" target="#b64">(Zhang et al., 2018b)</ref> 25.1/-∼5M 591M</p><p>DARTS <ref type="bibr" target="#b29">(Liu et al., 2018)</ref> 26.9/9.0 4.9M 595M PC-DARTS <ref type="bibr" target="#b58">(Xu et al., 2019)</ref> 24.2/7.3 5.3M 597M CARS <ref type="bibr" target="#b60">(Yang et al., 2020)</ref> 24.8/7.5 5.1M 591M PC-NAS <ref type="bibr">(Li et al., 2020)</ref> 23.9/-5.1M -EnTranNAS-DST <ref type="bibr" target="#b59">(Yang et al., 2021)</ref> 23.8/7.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2M 594M</head><p>Models searched on the MobileNetV2 search space NAO <ref type="bibr" target="#b31">(Luo et al., 2018)</ref> 24.5/7.8 6.5M 590M LaNAS <ref type="bibr" target="#b49">(Wang et al., 2021a)</ref> 25.0/7.7 5.1M 570M BN-NAS <ref type="bibr" target="#b4">(Chen et al., 2021)</ref> 24.3/-4.4M 470M ProxelessNAS <ref type="bibr" target="#b1">(Cai et al., 2018)</ref> 24.0/7.1 5.8M 595M RLNAS <ref type="bibr" target="#b65">(Zhang et al., 2021)</ref> 24.4/7.4 5.3M 473M SemiNAS <ref type="bibr" target="#b32">(Luo et al., 2020)</ref> 23.5/6.8 6.3M 599M MAGIC-AT 23.2/6.7 6.0M 598M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Searching on ImageNet</head><p>We use a MobileNet-v2 <ref type="bibr" target="#b41">(Sandler et al., 2018)</ref> based search search space following ProxylessNAS <ref type="bibr" target="#b1">(Cai et al., 2018)</ref>, which excludes squeeze-and-excitation block <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>. Candidate operations include inverted bottleneck convolution <ref type="bibr" target="#b41">(Sandler et al., 2018)</ref> with various kernel sizes {3, 5, 7}, expansion ratios {3, 6} and zero-out layer. For super-net training, we use the SGD optimizer with an initial learning rate of 0.4 and a cosine learning rate, and train the super-net on 8 V100 GPUs for 150 epochs with a batch size of 512. For stand-alone model training, to be consistent with the previous works, we follow the same strategy as Proxy-lessNAS 5 and do not employ tricks like cutout <ref type="bibr" target="#b11">(Devries &amp; Taylor, 2017)</ref> or mixup <ref type="bibr" target="#b64">(Zhang et al., 2018a)</ref>. We mainly compare MAGIC-AT to works with the same search space and present the results in Table <ref type="table" target="#tab_6">5</ref>. MAGIC-AT achieves 23.2% top-1 test error rate on ImageNet under the 600M FLOPS constraint, which outperforms baseline NAS works. The discovered architecture is depicted in Figure <ref type="figure" target="#fig_7">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we quantitatively analyze the causes of interference on shared weights in neural architecture search and develop two approaches: MAGIC-T and MAGIC-A, to mitigate it. The proposed methods can improve the rank correlation of the super-net and can search efficient architectures. Experiments on various natural language tasks and ImageNet task demonstrate the effectiveness of our methods. We hope our quantitative analyses and approaches to mitigate interference can help the NAS community to better understand and resolve the interference issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Configurations and More Analyses of Interference</head><p>In this section, we present detailed configurations of results presented in Sec. 3 and more analyses of interference. We first describe the structure of operators in Sec. A.1. Then we present how to align the inputs and outputs of shared operators with the average inputs and outputs in Sec. A.2. Next, to give readers a better understanding, we depict how we extend the analyses to a more general case: child models that differ in m operators in Sect. A.3. Finally, we discuss more results of interference when o g locates in different layers in Sec. A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Candidate Operators</head><p>We adopt a hybrid BERT search space including popular and different types of candidate operators: multi-head attention (MHA), feed-forward network (FFN) and convolution (CONV). The structures of FFN and MHA follow those in BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> as shown in Figure <ref type="figure" target="#fig_3">4</ref> (b) and (c). We adopt separable convolution <ref type="bibr" target="#b6">(Chollet, 2017)</ref>, as shown in Figure <ref type="figure" target="#fig_3">4</ref> (a), since 1) its effectiveness in natural language processing tasks have been demonstrated by previous work <ref type="bibr" target="#b20">(Kaiser et al., 2018;</ref><ref type="bibr" target="#b21">Karatzoglou et al., 2020;</ref><ref type="bibr" target="#b57">Xu et al., 2021)</ref> and 2) its number of parameters does not change much by varying the kernel size, which allows us to conveniently obtain different CONV operators with the similar size of parameters. To exclude the influence of parameter size of different type operators, we further adjust the inner hidden size of operators to ensure that they have a similar number of parameters as shown in Table <ref type="table" target="#tab_7">6</ref>. To explicitly align the inputs and outputs of the shared operators to be similar to the average inputs and outputs, we should first enumerate all possible child models in the search space and obtains their inputs and outputs in every layer. However, it can incur significant computational overhead. Thus, we only randomly choose C child models and obtain their average inputs and outputs as follows:</p><formula xml:id="formula_8">H avg n = 1 C C i=1 H (α i ) n , n = 1, 2, • • • , N<label>(3)</label></formula><p>where</p><formula xml:id="formula_9">H (α i ) n</formula><p>is the outputs of n-th layer (the inputs of (n + 1)-th layer) of child model α i , H avg n is the average outputs of C child models in n-th layer and N is the number of layers. Then, the alignments loss is defined as</p><formula xml:id="formula_10">L align (H avg n , H (α) ) = N n=1 MSE(H avg n , H (α) n ), (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>where α is a sampled child model for optimization and MSE is mean square error loss. The training objective of a child model α is</p><formula xml:id="formula_12">L = L pred + λL align ,<label>(5)</label></formula><p>where L pred is the supervised loss between the predictions and the ground truth (e.g., cross entropy loss in masked language modeling), λ is the scaling parameter that controls the weight of the alignment loss and we adopt λ = 0.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. The Setting about Interference of the Operator Shared by Child Models that Differ in m Operators</head><p>To extend the analyses to a more general case, we study the child models that differ in m operators in Sec. 3.2. In this section, we depict how child models differ in m operators to give readers a better understanding. Figure <ref type="figure">5</ref> (c) shows the case that we randomly choose C child models that differ from the second layer to the fourth layer (m = 2 different operators). Then the C child models can apply the forward and backward computation individually and obtain their gradients on o g . In this way, we can study the gradient interference on o g when child models differ in m operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Study of Interference when o g Locates in Different Layers</head><p>In the above analyses, we select o g in the first layer as an example to study interference. In this section, we further investigate interference when o g locates in different layers.</p><p>As shown in Figure <ref type="figure">6</ref>, we choose C child models which differ in layer j + 1. Then we compare their gradients on o g at the j-th layer. The gradient cosine similarities with different j are presented in Figure <ref type="figure">8</ref>. We can find that our previous observations still hold no matter which layer o g locates in. Specifically, 1) by aligning the inputs and outputs of the shared operators to be similar to the average inputs and outputs, the cosine similarity is improved and the gradient interference can be reduced; 2) the interference on a shared operator between two child models is positively correlated with the number of different operators between them (the larger m, the lower cosine similarity). !</p><formula xml:id="formula_13">" ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Layer 1 Layer 2 Layer 3 … Forward ! " ! # … ! % Layer 4 Layer N ! " ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Layer 1 Layer 2 Layer 3 … Forward ! " ! # … ! % Layer 4 Layer N ! " ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Layer 1 Layer 2 Layer 3 … Forward ! " ! # … ! % Layer 4 Layer N (a) m = 1 (b) m = 2 (c) m = 3</formula><p>Figure <ref type="figure">5</ref>: The illustration of the forward process regarding the operator o g in layer 1 that is shared by child models that differ from the second layer to the m + 1 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MAGIC-AT</head><p>It is straightforward to combine MAGIC-A and MAGIC-T to collaboratively mitigate interference since these two independent methods proposed from two different perspectives are independent. Here we present the complete procedure for MAGIC-AT as follows:</p><p>• Obtain a batch of data, an anchor child model α l and a sampled child model α t−1 at step t − 1.</p><p>• Sample a child model α t by randomly substituting one operator (k = 1) in α t−1 with another operator,</p><p>• Calculate the loss according to Eq. 2 and update the weights of α t ,</p><p>• Replace α l with α t if Val(α t ) &gt; Val(α l ),</p><p>where Val(•) is the accuracy obtained from the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Configurations</head><p>Candidate operators for BERT tasks. To search an effective architecture for practical usage, we adopt candidate operators O ={MHA12, FFN, CONV3, CONV5}, where FFN and MHA12 (head dimension 64) follow the designs in BERT base <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, CONV with kernel size 3 and 5 are used since their effectiveness and efficiency in natural language tasks have been demonstrated by previous work <ref type="bibr" target="#b57">(Xu et al., 2021)</ref>. We adopt the architecture of CONV as shown in Figure <ref type="figure" target="#fig_4">7</ref> since we observe its superior performance and efficiency than that of the separable convolution. The hidden size of operators is 768 following BERT base <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. BERT base has 24 sublayers (each Transformer layer has a MHA12 and FFN).</p><p>Since CONV operators have fewer parameters than FFN, the searched architectures usually have fewer parameters than BERT base with the same layers in the super-net. We adopt a chain-styled super-net with two more layers to enable that searched architecture can have similar parameters and FLOPs as BERT base . The space contains about 4.5×10 15</p><formula xml:id="formula_14">! " ! # … ! % ! " ! # … ! % ! " ! # ! &amp; ! % ! " ! # … ! % Layer j Layer j+1 Layer j+2 … Forward Layer N ! " ! # … ! % ! " ! # … ! % Layer j-1 Layer 1 … Figure 6:</formula><p>The illustration of the forward and backward process regarding the operator o g in layer j that is shared by child models that differ in layer j + 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv GeLU LayerNorm Dense</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super-net Training</head><p>We use Adam <ref type="bibr" target="#b23">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 1e-4, β 1 = 0.9 and β 2 = 0.999. The peak learning rate is 5e-4 with a warmup step of 10,000 followed by linear annealing. The dropout rate is 0.1 and the weight decay is 0.01. We set the max length of sentences as 128 tokens. The super-net is trained with the batch size of 1024 sentences for 250,000 steps (same computational resources as BERT base ). We adopt commonly-used SPOS <ref type="bibr" target="#b15">(Guo et al., 2020)</ref> as the baseline method, which randomly samples a child model for training at each step. For our proposed method MAGIC-T, unless otherwise mentioned, it gradually changes k = 1 operator to reduce the interference. For MAGIC-A, we train the super-net with masked language modeling loss in the first three epochs for a warm-start and then add the alignment loss with λ = 0.5 in every four layers to avoid over-regularization. Our experiments are implemented with fairseq codebase <ref type="bibr" target="#b34">(Ott et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE Benchmark</head><p>We evaluate performance by finetuning the pre-trained model on GLUE benchmark <ref type="bibr" target="#b48">(Wang et al., 2019)</ref>, which includes three inference tasks (MNLI <ref type="bibr" target="#b53">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b38">(Rajpurkar et al., 2016)</ref> and RTE <ref type="bibr" target="#b9">(Dagan et al., 2006)</ref>), three similarity and paraphrase tasks (MRPC <ref type="bibr" target="#b12">(Dolan &amp; Brockett, 2005</ref>) STS-B <ref type="bibr" target="#b3">(Cer et al., 2017)</ref>, QQP <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>) and two single-sentence tasks (CoLA <ref type="bibr" target="#b52">(Warstadt et al., 2019)</ref>, and SST-2 <ref type="bibr" target="#b44">(Socher et al., 2013)</ref>). We follow hyper-parameters of RoBERTa for fine-tuning, where STS-B, MRPC and RTE are started from the model fine-tuned on MNLI <ref type="bibr" target="#b30">(Liu et al., 2019;</ref><ref type="bibr" target="#b8">Clark et al., 2020;</ref><ref type="bibr" target="#b45">Song et al., 2020)</ref>.</p><p>Configurations of the BERT pre-training task. To show the generality of the architecture found by our proposed MAGIC, we train the architecture with two different training objectives: 1) masked language modeling (MLM) <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> proposed in BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and 2) replace token detection (RTE) proposed in ELEC-TRA <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>. For MLM training objective, RoBERTa <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> is a competitive baseline. For a fair comparison with it, we train our pre-training model with a large byte-level BPE vocabulary containing 50K subword units following RoBERTa <ref type="bibr" target="#b30">(Liu et al., 2019)</ref>. However, a large vocabulary increases the size of parameters in the embedding layer. Thus, we factorize the embedding matrix into a multiplication of a small embedding matrix with hidden size 512 and another 512×768 transformation matrix following <ref type="bibr" target="#b24">(Lan et al., 2020)</ref>. For RTE training objective, we used the character-level BPE vocabulary of size 30K following ELECTRA <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>. The searched architecture is shown in Figure <ref type="figure">9</ref>.</p><p>Configurations of the BERT compression task. Previous works usually compress BERT into a model size of 66M or 60M by taking advantage of novel architectures <ref type="bibr" target="#b57">(Xu et al., 2021)</ref> or sophisticated distillation techniques <ref type="bibr" target="#b42">(Sanh et al., 2019;</ref><ref type="bibr" target="#b51">Wang et al., 2020;</ref><ref type="bibr" target="#b47">Turc et al., 2019;</ref><ref type="bibr" target="#b16">Hou et al., 2020)</ref>. Following NAS-BERT <ref type="bibr" target="#b57">(Xu et al., 2021)</ref>, we search a novel architecture of about 60M for comparison. To this end, we reduce the hidden size of candidate operators from 768 to 512 and re-run MAGIC-AT to search for architectures. The searched architecture is shown in Figure <ref type="figure">10</ref>. We adopt the large byte-level BPE vocabulary containing 50K subword units <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> and re-train the searched architecture exactly following NAS-BERT for a fair comparison.</p><p>Configurations of the ImageNet task. We adopt candidate operators O ={MB3 3×3, MB3 6×6, MB5 3×3, MB5 6×6, MB7 3×3, MB7 6×6, Zero-out} following Proxyless-NAS <ref type="bibr" target="#b1">(Cai et al., 2018)</ref>, where MB7 6×6 refers to mobile inverted bottleneck convolution with kernel size 6 and expansion rate 7. We search the operation of each individual layer via MAGIC-AT. The discovered architecture by MAGIC-AT is shown in Figure <ref type="figure" target="#fig_7">11</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Gradient cosine similarity. In Figures (a) and (b), the value represents the cosine similarity between two child models. For example, MHA6-FFN-0.50 in row 1 and column 3 of the similarity matrix in Figure (a) refers to the cosine similarity of the gradients on operator o g between two paths is 0.50 where the operator in layer 2 of one path is MHA6, and that in another path is FFN. For Figure (b), the super-net is obtained by training with an extra alignment training objective, and then is used to calculate gradient cosine similarity. In Figure (c), we choose child models that differ in m operators and calculate the average value of the cosine similarity matrix. In each experiment, we randomly choose an o g and C paths as shown in Figure 1. The presented results are an average of 10 repeated experiments.</figDesc><graphic url="image-1.png" coords="5,84.24,67.18,436.25,101.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Study MAGIC-T by varying k. (b) Study of MAGIC-A. (c) Gradient cosine similarity of MAGIC-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analyses of MAGIC-T and MAGIC-A.</figDesc><graphic url="image-4.png" coords="7,220.72,67.06,145.90,112.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of operators and the super-net for analyzing interference.</figDesc><graphic url="image-5.png" coords="14,83.34,67.06,437.41,150.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Architecture of convolution operator. child models in total.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Gradient cosine similarity comparison when og locates in layer j = 3 Gradient cosine similarity comparison when og locates in layer j = 5 Gradient cosine similarity comparison when og locates in layer j = 7 Gradient cosine similarity comparison when og locates in layer j = 9 Gradient cosine similarity comparison when og locates in layer j = 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Figure 8: Gradient cosine similarity. By varying j, we repeat the experiments and present results as described in Sec. 3.2.</figDesc><graphic url="image-19.png" coords="16,221.88,561.47,160.91,106.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Architecture found by MAGIC-AT for ImageNet.</figDesc><graphic url="image-21.png" coords="17,79.74,502.37,437.39,139.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Kendall's Tau on the BERT search space.</figDesc><table><row><cell>Method</cell><cell>SPOS (Guo et al., 2020)</cell><cell cols="3">MAGIC-T MAGIC-A MAGIC-AT</cell></row><row><cell>Kendall's Tau</cell><cell>0.36</cell><cell>0.49</cell><cell>0.44</cell><cell>0.57</cell></row><row><cell cols="5">Analyses of MAGIC-T. MAGIC-T gradually modifies</cell></row><row><cell cols="5">k = 1 operators at each training step to reduce the inter-</cell></row><row><cell cols="5">ference. We further analyze MAGIC-T by varying k and</cell></row><row><cell cols="5">measuring the rank correlation. According to analyses in</cell></row><row><cell cols="5">Sec. 3.2, more topological changes (larger k) can lead to</cell></row><row><cell cols="5">more serious interference on the shared operators. As shown</cell></row><row><cell cols="5">in Figure 3 (a), it is clear to observe that larger k causes</cell></row><row><cell cols="5">worse rank correlation, which demonstrates that the inter-</cell></row><row><cell cols="5">ference is detrimental to the super-net training, and our pro-</cell></row><row><cell cols="5">posed MAGIC-A can efficiently mitigate the interference</cell></row><row><cell cols="4">and improve the rank ability of the super-net.</cell><cell></cell></row><row><cell cols="5">Analyses of MAGIC-A. MAGIC-A selects a top-</cell></row><row><cell cols="5">performing anchor model to align inputs and outputs of other</cell></row><row><cell cols="5">child models to be similar to reduce the interference. Be-</cell></row><row><cell cols="5">sides a top-performing one, we further analyze the method</cell></row><row><cell cols="5">by choosing different anchor models. Specifically, we eval-</cell></row><row><cell cols="5">uate 10, 000 child models at the end of every epoch and</cell></row><row><cell cols="5">select a top p% child model as the anchor model. However,</cell></row><row><cell cols="5">different from the top-performing model, the exact top p%</cell></row><row><cell cols="5">model varies dramatically, which results in selecting differ-</cell></row></table><note>ent anchor models in different epochs. Using such anchor models to align other models in each epoch can lead to unstable optimization of the super-net. For stable training, we substitute the anchor model when the previously selected anchor model in the last epoch escapes from the range of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of pre-training models on GLUE benchmark. The test set results are obtained from the official GLUE leaderboard. All models are trained under the same computational resources as BERT base for a fair comparison. "E-" refers to training in the ELECTRA style. Following<ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, Spearman correlation is reported for STS-B, Matthews correlation is reported for CoLA and accuracy is reported for other tasks. We can observe that a proper anchor model that ranks within the top 30% is enough to align the inputs and outputs and mitigate interference, and a better anchor model can obtain slightly better results. However, choosing a model that performs poorly as the anchor model has a negative effect on supernet training. Since the optimization goal is L pred + λL align , alignment regularization guided by a bad child model may deviate with the training target L pred .</figDesc><table><row><cell>Model</cell><cell cols="10">Params FLOPs MNLI QQP QNLI CoLA SST-2 STS-B RTE MRPC AVG</cell></row><row><cell>dev set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT base (Devlin et al., 2019)</cell><cell>110M</cell><cell>2.9e10 84.4</cell><cell cols="2">89.9 88.4</cell><cell>54.3</cell><cell>92.7</cell><cell>88.9</cell><cell cols="2">71.1 86.7</cell><cell>82.1</cell></row><row><cell>RoBERTa base (Liu et al., 2019)</cell><cell>125M</cell><cell>3.3e10 85.3</cell><cell cols="2">91.1 91.1</cell><cell>61.0</cell><cell>92.7</cell><cell>90.0</cell><cell cols="2">77.5 87.9</cell><cell>84.6</cell></row><row><cell cols="2">ELECTRA base (Clark et al., 2020) 110M</cell><cell>2.9e10 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.1</cell></row><row><cell>MPNet base (Song et al., 2020)</cell><cell>110M</cell><cell>2.9e10 85.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPOS (Guo et al., 2020)</cell><cell>114M</cell><cell>3.3e10 84.7</cell><cell cols="2">91.4 91.4</cell><cell>59.6</cell><cell>92.1</cell><cell>89.7</cell><cell cols="2">80.9 86.3</cell><cell>84.4</cell></row><row><cell>MAGIC-AT</cell><cell>113M</cell><cell>3.3e10 85.6</cell><cell cols="2">91.3 91.8</cell><cell>61.1</cell><cell>93.5</cell><cell>90.3</cell><cell cols="2">80.9 90.9</cell><cell>85.7</cell></row><row><cell>E-MAGIC-AT</cell><cell>110M</cell><cell>2.9e10 86.3</cell><cell cols="2">91.7 92.5</cell><cell>65.8</cell><cell>92.5</cell><cell>91.0</cell><cell cols="2">84.0 89.7</cell><cell>86.7</cell></row><row><cell>test set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT base (Devlin et al., 2019)</cell><cell>110M</cell><cell>2.9e10 84.6</cell><cell cols="2">89.2 90.5</cell><cell>52.1</cell><cell>93.5</cell><cell>85.8</cell><cell cols="2">66.4 84.8</cell><cell>80.9</cell></row><row><cell>RoBERTa base (Liu et al., 2019)</cell><cell>125M</cell><cell>3.3e10 84.8</cell><cell cols="2">89.0 91.7</cell><cell>57.1</cell><cell>93.3</cell><cell>88.0</cell><cell cols="2">74.1 84.1</cell><cell>82.8</cell></row><row><cell cols="2">ELECTRA base (Clark et al., 2020) 110M</cell><cell>2.9e10 85.8</cell><cell cols="2">89.1 92.7</cell><cell>59.7</cell><cell>93.4</cell><cell>87.7</cell><cell cols="2">73.1 86.7</cell><cell>83.5</cell></row><row><cell>SPOS (Guo et al., 2020)</cell><cell>114M</cell><cell>3.3e10 84.3</cell><cell cols="2">88.6 91.0</cell><cell>56.1</cell><cell>92.8</cell><cell>88.1</cell><cell cols="2">74.9 83.4</cell><cell>82.4</cell></row><row><cell>MAGIC-AT</cell><cell>113M</cell><cell>3.3e10 84.9</cell><cell cols="2">89.1 92.0</cell><cell>57.0</cell><cell>94.1</cell><cell>87.8</cell><cell cols="2">77.4 85.2</cell><cell>83.4</cell></row><row><cell>E-MAGIC-AT</cell><cell>110M</cell><cell>2.9e10 85.9</cell><cell cols="2">89.6 92.4</cell><cell>60.3</cell><cell>93.4</cell><cell>87.3</cell><cell cols="2">80.4 87.4</cell><cell>84.6</cell></row></table><note>top p ± r% models (r = 10) in the current epoch. We train different super-nets with different p and present the ranking correlation in Figure 3 (b). The cosine similarity results, as introduced in Sec. 3.2, are presented in Figure 3 (c).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of compression methods in the commonly-used 60M model size. "*" means using data augmentation.</figDesc><table><row><cell>Model</cell><cell cols="8">Params MNLI QQP QNLI CoLA SST-2 STS-B RTE MRPC AVG</cell></row><row><cell>dev set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DistilBERT (Sanh et al., 2019)</cell><cell>66M</cell><cell>82.2</cell><cell>88.5 89.2</cell><cell>51.3</cell><cell>91.3</cell><cell>86.9</cell><cell>59.9 87.5</cell><cell>79.6</cell></row><row><cell>MiniLM (Wang et al., 2020)</cell><cell>66M</cell><cell>84.0</cell><cell>91.0 91.0</cell><cell>49.2</cell><cell>92.0</cell><cell>-</cell><cell>71.5 88.4</cell><cell>-</cell></row><row><cell cols="2">BERT-of-Theseus (Xu et al., 2020) 66M</cell><cell>82.3</cell><cell>89.6 89.5</cell><cell>51.1</cell><cell>91.5</cell><cell>88.7</cell><cell>68.2 -</cell><cell>-</cell></row><row><cell>PD-BERT (Turc et al., 2019)</cell><cell>66M</cell><cell>82.5</cell><cell>90.7 89.4</cell><cell>-</cell><cell>91.1</cell><cell>-</cell><cell>66.7 84.9</cell><cell>-</cell></row><row><cell>DynaBERT* (Hou et al., 2020)</cell><cell>60M</cell><cell>84.2</cell><cell>91.2 91.5</cell><cell>56.8</cell><cell>92.7</cell><cell>89.2</cell><cell>72.2 84.1</cell><cell>82.7</cell></row><row><cell>NAS-BERT (Xu et al., 2021)</cell><cell>60M</cell><cell>84.1</cell><cell>91.0 91.3</cell><cell>58.1</cell><cell>92.1</cell><cell>89.4</cell><cell>79.2 88.5</cell><cell>84.2</cell></row><row><cell>SPOS (Guo et al., 2020)</cell><cell>60M</cell><cell>84.0</cell><cell>90.7 91.1</cell><cell>57.1</cell><cell>91.6</cell><cell>88.2</cell><cell>75.9 86.5</cell><cell>83.1</cell></row><row><cell>MAGIC-AT</cell><cell>60M</cell><cell>84.5</cell><cell>90.9 91.1</cell><cell>61.8</cell><cell>92.8</cell><cell>89.0</cell><cell>78.9 89.2</cell><cell>84.8</cell></row><row><cell>test set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BERT-of-Theseus (Xu et al., 2020) 66M</cell><cell>82.4</cell><cell>89.3 89.6</cell><cell>47.8</cell><cell>92.2</cell><cell>84.1</cell><cell>66.2 83.2</cell><cell>79.4</cell></row><row><cell>PD-BERT (Turc et al., 2019)</cell><cell>66M</cell><cell>82.8</cell><cell>88.5 88.9</cell><cell>-</cell><cell>91.8</cell><cell>-</cell><cell>65.3 81.7</cell><cell>-</cell></row><row><cell>BERT-PKD (Sun et al., 2019)</cell><cell>66M</cell><cell>81.5</cell><cell>88.9 89.0</cell><cell>-</cell><cell>92.0</cell><cell>-</cell><cell>65.5 79.9</cell><cell>-</cell></row><row><cell>TinyBERT* (Jiao et al., 2020)</cell><cell>66M</cell><cell>84.6</cell><cell>89.1 90.4</cell><cell>51.1</cell><cell>93.1</cell><cell>83.7</cell><cell>70.0 82.6</cell><cell>80.6</cell></row><row><cell>NAS-BERT (Xu et al., 2021)</cell><cell>60M</cell><cell>83.5</cell><cell>88.9 90.9</cell><cell>48.4</cell><cell>92.9</cell><cell>86.1</cell><cell>73.7 84.5</cell><cell>81.1</cell></row><row><cell>SPOS (Guo et al., 2020)</cell><cell>60M</cell><cell>83.5</cell><cell>88.5 90.6</cell><cell>52.4</cell><cell>91.7</cell><cell>86.5</cell><cell>74.2 83.6</cell><cell>81.4</cell></row><row><cell>MAGIC-AT</cell><cell>60M</cell><cell>84.2</cell><cell>88.8 90.6</cell><cell>53.6</cell><cell>92.1</cell><cell>86.8</cell><cell>75.6 84.3</cell><cell>82.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on the dev set of SQuAD datasets.</figDesc><table><row><cell>Model</cell><cell cols="4">SQuAD v1.1 SQuAD v2.0 EM F1 EM F1</cell></row><row><cell>BERT base (Devlin et al., 2019)</cell><cell>80.5</cell><cell>88.5</cell><cell>-</cell><cell>-</cell></row><row><cell>RoBERTa base</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of models on ImageNet.</figDesc><table><row><cell>Model</cell><cell cols="3">Top1/Top5 Err. Params FLOPS</cell></row><row><cell>MobileNetV2 (Sandler et al., 2018)</cell><cell>25.3/-</cell><cell>6.9M</cell><cell>585M</cell></row><row><cell>ShuffleNetV2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Configurations of different operators used for analyzing interference.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Candidate Operators</cell></row><row><cell>O</cell><cell cols="3">{MHA6, MHA8, FFN, FFN', CONV3, CONV5}</cell></row><row><cell cols="3">Operator Hidden Size Parameters</cell><cell>Configurations</cell></row><row><cell>MHA 6</cell><cell>768</cell><cell>1.18M</cell><cell>Heads 6, QKV hidden 384</cell></row><row><cell>MHA 8</cell><cell>768</cell><cell>1.18M</cell><cell>Heads 8, QKV hidden 384</cell></row><row><cell>FFN</cell><cell>768</cell><cell>1.18M</cell><cell>Inner hidden 768</cell></row><row><cell>FFN'</cell><cell>768</cell><cell>1.28M</cell><cell>Inner hidden 832</cell></row><row><cell>CONV3</cell><cell>768</cell><cell>1.18M</cell><cell>Kernel 3</cell></row><row><cell>CONV5</cell><cell>768</cell><cell>1.19M</cell><cell>Kernel 5</cell></row><row><cell cols="4">A.2. Aligning the Inputs and Outputs with the Average</cell></row><row><cell cols="2">Inputs and Outputs</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">University 2 Microsoft</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Research Asia 3 University of Science and Technology of China. Correspondence to: Xu Tan &lt;xuta@microsoft.com&gt;, Jian Li &lt;lijian83@mail.tsinghua.edu.cn&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3">In fact, the gradients on other shared operators are also different. We take og as an example for the analyses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4">  2  The cosine similarity between vector g i and g j is calculated byg i •g j ||g i ||||g j || .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_5">This can be proved using the standard coupling argument. See the example of random walk on the hypercube in Sec. 5.3<ref type="bibr" target="#b26">(Levin &amp; Peres, 2017)</ref>. Although our graph G is slightly more general than hypercube, the same argument applies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6">The total computational resources (125,000*2048 sentences of 16GB corpus) is exactly the same as BERT base<ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, MPNet base<ref type="bibr" target="#b45">(Song et al., 2020)</ref> and ELECTRA base<ref type="bibr" target="#b8">(Clark et al., 2020)</ref> for a fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>Jin Xu and Jian Li are supported in part by the National Natural Science Foundation of China Grant 62161146004, Turing AI Institute of Nanjing and Xi'an Institute for Interdisciplinary Information Core Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oncefor-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="https://github.com/mit-han-lab/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bn-nas: Neural architecture search with batch normalization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="307" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.195" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="12239" to="12248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ELECTRA: pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>CoRR, abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynabert: Dynamic bert with adaptive width and depth</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Angle-based search space shrinking for neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58529-7_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58529-7_8" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="volume">12364</biblScope>
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIX</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depthwise separable convolutions for neural machine translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applying depthwise separable and multi-channel convolutional neural networks of varied kernel size on semantic trajectories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6685" to="6698" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><surname>Albert</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa</title>
				<meeting><address><addrLine>Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring single-path architecture search ranking correlations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Laube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=J40FkbdldTX" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Markov chains and mixing times</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving one-shot nas by suppressing the posterior fading</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13836" to="13845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised neural architecture search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating efficient performance estimators of neural architectures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Fairseq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
				<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cream of the crop: Distilling prioritized paths for one-shot neural architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">To share or not to share: A comprehensive appraisal of weight-sharing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pourchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ducarouge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04289</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Distilbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">a distilled version of bert: smaller, faster, cheaper and lighter</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding architectures learnt by cell-based neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mpnet: Masked and permuted pre-training for language understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4314" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=rJ4km2R5t7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sampleefficient neural architecture search by learning actions for monte carlo tree search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking architecture selection in differ-entiable nas</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep self-attention distillation for taskagnostic compression of pre-trained transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Minilm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1710" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Weight-sharing neural architecture search:a battle to shrink the optimization gap</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01475</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=rylqooRqK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bert-oftheseus: Compressing bert by progressive module replacing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7859" to="7869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467262</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467262" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
				<editor>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">August 14-18, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards improving the consistency, efficiency, and flexibility of differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="6667" to="6676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cars: Continuous evolution for efficient neural architecture search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1829" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big singlestage models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018a. 2018b</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural architecture search with random labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="10907" to="10916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01431</idno>
		<title level="m">Deeper insights into weight sharing in neural architecture search</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">How does supernet help in neural architecture search?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08219</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ue8Hcxg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
