<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Causal Explanations for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-14">14 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Generative Causal Explanations for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-14">14 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.06643v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem 1 , once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to 30% and speeds up the explanation process by up to 110× as compared to its state-of-the-art alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many problems in scientific domains, ranging from social networks <ref type="bibr" target="#b12">(Lin et al., 2020)</ref> to biology <ref type="bibr" target="#b26">(Zitnik et al., 2018)</ref> and chemistry <ref type="bibr" target="#b25">(Zitnik &amp; Leskovec, 2017)</ref>, can be naturally modeled as problems of property learning on graphs. For example, in biology, identifying the functionality of proteins is critical to find the proteins associated with a disease, where proteins are represented by local protein-protein interaction (PPI) graphs. Supervised learning of graphs, especially with graph neural networks (GNNs), has had a significant impact on these domains, mainly owing to their efficiency and capability of inductive learning <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>.</p><p>Despite their practical success, most GNNs are deployed as black boxes, lacking explicit declarative knowledge representations. Therefore, they have difficulty in generating the required underlying explanatory structure. The deficiency of 1 The source code and appendix are in the supplementary.</p><p>explanations for the decisions of GNNs significantly hinders the applicability of these models in decision-critical settings, where both predictive performance and interpretability are of paramount importance. For example, medical decisions are increasingly being assisted by complex predictions that should lend themselves to be verified by human experts easily. Model explanations allow us to argue for model decisions and exhibit the situation when algorithmic decisions might be biased or discriminating. In addition, precise explanations may facilitate model debugging and error analysis, which may help decide which model would better describe the data's underlying semantics.</p><p>While explaining graph neural networks on graphs is still a nascent research topic, a few recent works have emerged <ref type="bibr" target="#b14">(Luo et al., 2020;</ref><ref type="bibr" target="#b18">Vu &amp; Thai, 2020;</ref><ref type="bibr" target="#b22">Ying et al., 2019;</ref><ref type="bibr" target="#b23">Yuan et al., 2020)</ref>, each with its own perspective on this topic. In particular, XGNN <ref type="bibr" target="#b23">(Yuan et al., 2020)</ref> was proposed to investigate graph patterns that lead to a specific class, while GNNExplainer <ref type="bibr" target="#b22">(Ying et al., 2019)</ref> provided the local explanation for a single instance (a node/link/graph), by determining a compact subgraph leading to its prediction. PGM-Explainer <ref type="bibr" target="#b18">(Vu &amp; Thai, 2020)</ref> explored the dependencies of explained features in the form of conditional probability, which is naturally designed for explaining a single instance.</p><p>However, verifying if a target GNN works as expected often requires a considerable amount of explanations for providing a global view of explanations. For this end, PGExplainer <ref type="bibr" target="#b14">(Luo et al., 2020)</ref> learns a multilayer perceptron (MLP) to explain multiple instances collectively. However, PGExplainer heavily relies on node embeddings from the target GNN, which may not be obtained without knowing its internal model structure and parameters. Besides, PG-Explainer can not explain any graph tasks without explicit motifs. Taking MUTAG as an example, PGExplainer assumes that NH 2 and NO 2 are the motifs for the mutagen graphs and filters out the instances without these two motifs. We verified that the assumption might not be reasonable by looking at the dataset statistics (provided in Appendix B). More specifically, PGExplainer fails to explain the instances without explicit motifs under their assumptions. Motivated by this observation, we aim to provide fast and accurate explanations for any GNN-based models without the limitations above.</p><p>In this work, we propose a new methodology, called Gem, to provide interpretable explanations for any GNNs on the graph using causal explanation models. To the best of our knowledge, while the notion of causality has been used for interpretable machine learning on images or texts, this is the first effort from a causal perspective to explain graph neural networks. Specifically, our causal objective is built upon the notion of Granger causality, which comes from the pioneering work of Wiener and Granger <ref type="bibr" target="#b1">(Bressler &amp; Seth, 2011;</ref><ref type="bibr" target="#b4">Granger, 1969;</ref><ref type="bibr" target="#b21">Wiener, 1956)</ref>. Granger causality declares a causal relationship x i → Y between variables x i and Y if we are better able to predict Y using all available information than if the information apart from x i had been used. In the graph domain, if the absence of an edge/node x i decreases the ability to predict Y , then there is a causal relationship between this edge/node and its corresponding prediction. Based on the insights from neuroscience <ref type="bibr" target="#b0">(Biswal et al., 1997)</ref>, we extend the notion of Granger causality to characterize the explanation of an instance by its local subgraphs.</p><p>We note that the concept of Granger causality is probabilistic, and the graph data is inherently interdependent, i.e., edges or nodes are correlated variables. Directly applying Granger causality may lead to incorrectly detected causal relations. In addition, we envision that the resulting explanations should be human-intelligible and valid. For example, in some applications such as chemistry, an explanation for the mutagen graph is a functional group and should be connected. Accordingly, we propose an approximate computation strategy that makes our method viable for graph data with interdependency, under reasonable assumptions on the causal objective.</p><p>In particular, we incorporate various graph rules, such as the connectivity check, to encourage the obtained explanations to be valid and human-intelligible. Then we train causal explanation models that learn to distill compact subgraphs, causing the outputs of the target GNNs. This approach is flexible and general since it has no requirements on the target model to be explained (commonly referred to as "modelagnostic"), or no assumptions on the learning tasks (explicit motifs for identifying a particular class), and can provide local and global views of the explanations. In particular, it does not require retraining or adapting to the original model. In other words, once trained, Gem can be used to explain the target GNN models with little time.</p><p>Highlights of our original contributions are as follows. We propose a new methodology to explain graph neural networks on the graph from the causal perspective; to the best of our knowledge, such an approach has not been used for interpreting GNNs on the graph so far. We introduce causal objectives for better estimates of the causal effect in our methodology and provide an approximate computation strat-egy to deal with graph data with interdependency. Various graph rules are incorporated to ensure that the obtained explanations are valid. We then use a causal objective to train a graph generative model as the explainer, which can automatically explain the target GNNs with little time. We theoretically analyze that several recent methods, including Gem, all fall into the framework of additive feature attribution methods, which essentially solve the same optimization problem with different approximation methods (provided in Appendix A). We empirically demonstrate that Gem is significantly faster and more accurate than alternative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Setup</head><p>A set of graphs can be represented as</p><formula xml:id="formula_0">G = {G i } N i=1 , where |G| = N . Each graph is denoted as G i = (V i , E i )</formula><p>, where E i denotes the edge set and</p><formula xml:id="formula_1">V i = v i 1 , v i 2 , • • • , v i |Vi|</formula><p>is the node set of graph i. In many applications, nodes are associated with d-dimensional node features</p><formula xml:id="formula_2">X i = x i 1 , x i 2 , • • • , x i |Vi| , x i j ∈ R d .</formula><p>Without loss of generality, we consider the problem of explaining a graph neural network-based classification task. This task can be node classification or graph classification. For graph classification, we associate each graph G i with a label, y i , where In the node classification setting, each node v j ∈ V of a graph G is associated with a corresponding node label y j ∈ Y. Examples of this kind include classifying papers in a citation network, or entities in a social network such as Reddit. The dataset D = {(v j , y j )}</p><formula xml:id="formula_3">y i ∈ Y = {c 1 , c 2 , • • • , c l },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|V |</head><p>j=1 is represented by pairs of nodes and node labels. In general, we use I i to represent an instance, which is equivalent to v i for node classification or G i for graph classification.</p><p>GNN family models. Graph neural networks (GNNs) are a family of graph message passing architectures that incorporate graph structure and node features to learn a dense representation of a node or the entire graph. In essence, GNNs follow a neighborhood aggregation strategy, where the node representations are updated via iteratively aggregating the representations from its neighbors in the graph. Graph convolutional networks (GCNs) use mean pooling <ref type="bibr" target="#b9">(Kipf &amp; Welling, 2017)</ref> for aggregation, while GraphSage aggregates the node features via mean/max/LSTM pooling <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>. Taking GCNs as an example, the basic operator for the neighborhood information aggregation is the element-wise mean. After L iterations of aggregation, a node's representation can capture the structural information within its L-hop graph neighborhood.</p><p>Formally, a graph neural network (GNN) can be written as a function</p><formula xml:id="formula_4">f (•) : G → Y or f (•) : V → Y.</formula><p>The former is a graph-level classifier, and the latter is a node-level classifier.</p><p>Typically, a GNN f (•) is trained with an objective function L : y × ỹ → s that computes a scalar loss s ∈ R after comparing the model's predictive output ỹ to a ground-truth output y. The categorical cross-entropy for classification models is commonly used for such objectives.</p><p>Objective. We are given a pre-trained classification model, represented by f (•), and our ultimate goal is to obtain an explanation model, denoted as f (•) exp , that can provide fast and accurate explanations for the pre-trained model, which can also be called a target GNN. Intrinsically, an explanation is a subgraph that is the most relevant for a predictionthe outcome of the target GNN, denotes as ỹ. Consistent with previous studies in the literature <ref type="bibr" target="#b23">(Yuan et al., 2020)</ref>, we focus on explanations on graph structures. In particular, we specifically do not require access to, or knowledge of, the process by which the classification model produces its output, nor do we require the classification model to be differentiable or any specific form. We allow the explainers to retrieve different predictions by performing queries on f (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In essence, the core of the GNNs is a neighborhood-based aggregation process, where a prediction of an instance is fully determined by its computation graph. Let us use</p><formula xml:id="formula_5">G c i = (V c i , A c i , X c i )</formula><p>to represent the computation graph of an instance i, where V c i is the node set, A c i ∈ {0, 1} indicates the adjacency matrix, and X c i is the feature matrix of the computation graph. Typically, a GNN learns a conditional distribution denoted as P (Y |G c i ), where Y is a random variable representing the class labels. For clarity, let us see an example graph, shown in Figure <ref type="figure" target="#fig_1">1</ref>, which will also be used throughout this paper. In this example, a target GNN is trained for node classification, and the node i is the target node to be explained. Oftentimes, the computation graph of node i is a two-hop subgraph, highlighted in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Therefore, the setting we focus on can be reformulated as the following: we are given a GNN-based classification model that processes the computation graph of an instance (a node or a graph), denoted as G c , and generates the corresponding outputs p (Y |G c ) for predicting ỹ. Unlike the node classification task, when the target GNN is trained for graph classification, the computation graph of an instance will be the entire graph. Accordingly, this work seeks to generate an explanation, a subgraph of G c that is most relevant for predicting ỹ, efficiently and automatically. We use G s to denote the generated explanation. Our setting is general and works for any graph learning tasks, including node classification and graph classification. Our ultimate goal is to encourage a compact subgraph of the computation graph to have a large causal influence on the outcome of the target GNN.</p><p>Differences from PGExplainer. PGExplainer is the most closely related work to our study, as both PGExplainer and Gem adopt parameterized networks to provide local and global views for model explanations. However, PGExplainer relies on node embeddings from the target GNN to learn a multilayer perceptron, which may not be obtained without knowing its internal model structure. In contrast, to explain an instance (a node or a graph), Gem simply inputs the original computation graph into the explainer and outputs a compact explanation graph. In other words, Gem does not require any prior knowledge of the internal model structure (the target GNN) and parameters, or any prior knowledge of the motifs associated with the graph learning tasks. Therefore, it exhibits better generalization abilities. In what follows, we will present Gem, our model-agnostic approach for providing interpretable explanations for any GNNs on a variety of graph learning tasks. The design of Gem is based upon principles of causality, in particular Granger causality <ref type="bibr" target="#b4">(Granger, 1969)</ref>.</p><p>Granger causality <ref type="bibr" target="#b4">(Granger, 1969;</ref><ref type="bibr" target="#b5">1980)</ref>. In general, Granger causality describes the relationships between two (or more) variables when one is causing the other. Specifically, if we are better able to predict variable ỹ using all available information U than if the information apart from variable x i had been used, we say that x i Granger-causes ỹ <ref type="bibr" target="#b5">(Granger, 1980)</ref>, denoted by x i → ỹ2 .</p><p>The crux of our approach is to train an explanation model, or an explainer, to explain the target graph neural network. Specifically, Gem is trained with the guidance built on the first principles of Granger causality. Here we extend Granger causality to the case where a compact subgraph G s of the computation graph is considered the main cause of the corresponding prediction ỹ. This is inspired by a longheld belief in neuroscience that the structural connectivity local to a certain area somehow dictates the function of that  piece <ref type="bibr" target="#b0">(Biswal et al., 1997)</ref>. Due to the inherent property of GNNs, the computation graph contains all the relevant information that causes the prediction of the target GNN. Under the assumption that Granger causality was built upon, we can squeeze the cause of the prediction, G s , from the computation graph. Therefore, we can use the given definition to quantify the degree to which part of the computation graph causes the prediction of the target GNN. In principle, the notion of Granger causality would hold if</p><formula xml:id="formula_6">p (ỹ | G c ) = p (ỹ | G s ) holds.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Causal Objective</head><p>Given a pre-trained/target GNN and an instance G c , we use δ G c to denote the model error of the target GNN when considering the computation graph, while δ G c \{e j } represents the model error excluding the information from the edge e j , where e j ∈ G c . With these two definitions and following the notion of Granger causality, we can quantify the causal contribution of an edge e j to the output of the target GNN. More specifically, the causal contribution of the edge e j is defined as the decrease in model error, formulated as Eq. ( <ref type="formula">1</ref>):</p><formula xml:id="formula_7">∆δ, e j = δ G c \{e j } − δ G c</formula><p>(1)</p><p>To calculate δ G c and δ G c \{e j } , we first compute the outputs corresponding to the computation graph G c and the one excluding edge e j , G c \ {e j }, based on the pre-trained GNN. For simplicity, the pre-trained GNN is denoted as f (•). Then, the associated outputs can be formulated as Eq. ( <ref type="formula">2</ref>) and Eq. ( <ref type="formula" target="#formula_8">3</ref>) respectively:</p><formula xml:id="formula_8">ỹG c = f (G c ) (2) ỹG c \{e j } = f (G c \ {e j })<label>(3)</label></formula><p>Then we compare the outputs of the target GNN, e.g., ỹG c and ỹG c \{e j } , with the ground-truth label y, respectively. In particular, we use the loss function of the pre-trained GNN as the metric to measure the model error, denoted as L. The mathematical formulations are shown as Eq. ( <ref type="formula">4</ref>) and Eq. ( <ref type="formula" target="#formula_9">5</ref>):</p><formula xml:id="formula_9">δ G c = L (y, ỹG c ) (4) δ G c \{e j } = L y, ỹG c \{e j }<label>(5)</label></formula><p>Now, the causal contribution of an edge e j can be measured by the loss difference associated with the computation graph and the one deleting edge e j .</p><p>Recall that we are seeking a "guidance" that can be used to train our explainer, encouraging its outcome to be effective explanations. Intrinsically, ∆δ, e j can be viewed as capturing the individual causal effect (ICE) <ref type="bibr" target="#b3">(Goldstein et al., 2015)</ref> of the input G c with values e j on the output ỹ. Therefore, it is straightforward to obtain the most relevant subgraph for predicting ỹ based on ∆δ, e j , which we call the ground-truth distillation process.</p><p>Ideally, given the edges' causal contributions in a computation graph, we can sort the edges accordingly and distill the top-K most relevant edges as a prediction explanation. However, due to the special representations of the graph data, the casual contributions from the edges are not independent, e.g., a 1-hop neighbor of a node can also be a 2-hop neighbor of the same node due to cycles. To this end, we further incorporate various graph rules to encourage the distillation process to be more effective. We believe that data characteristics are the most crucial factor in deciding which graph rules to use. It is necessary to understand the principle of the learning task, and the limitation of a human-intelligible explanation might be to prevent spurious explanations. In the application of bioinformatics, such as the MUTAG dataset, the explanation is a functional group, and therefore, the distilled top-K edges should be connected. Nevertheless, in graph representation-based Digital Pathology, such as the cell-graphs towards cancer subtyping, the explanation often contains subsets of cells and cellular interactions <ref type="bibr" target="#b7">(Jaume et al., 2020)</ref>. In this particular scenario, the connectivity constraint is unnecessary. The detailed distillation process is presented in Appendix C.</p><p>Using the distilled ground truth, denoted as Gs = Ṽ s , Ãs , Xs , we can train supervised learning models to learn to explain any other GNN models based solely on its outputs, and without the need to retrain the model to be explained. The workflow of Gem is illustrated in Figure <ref type="figure" target="#fig_3">2</ref>. Note that generating an explanation based on the explainer is not necessary in situations where ground-truth labels of the instances are readily available. In those cases, pre-calculated ∆δ, e j and our distillation process can directly be used to explain the pre-trained GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Generative Model as an Explainer</head><p>In principle, any graph generative models that can be trained to output graph-structured data can be used as a causal explanation model. Due to their simplicity and expressiveness, we focus on auto-encoder architectures utilizing graph convolutions <ref type="bibr" target="#b8">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Li et al., 2018a;</ref><ref type="bibr">b)</ref>. In particular, we use a model consisting of a graph convolutional network encoder and an inner product decoder <ref type="bibr" target="#b8">(Kipf &amp; Welling, 2016)</ref>. We leave the exploration of other generative models for future work.</p><p>More concretely, in our explainer, we first apply several graph convolutional layers to aggregate neighborhood information and learn node features. Then we use the inner product decoder to reconstruct the adjacency matrix Âc , a compact subgraph that only contains the most relevant portion of a computation graph to its prediction. In particular, each value in Âc denotes the contribution of a particular edge to the prediction of G c . Formally, the reconstruction process can be formulated as:</p><formula xml:id="formula_10">Z = GCNs (A c , X c )<label>(6)</label></formula><p>Âc = σ ZZ T (7) where A c is the adjacency matrix of the computation graph for the target instance, X c denotes the node features, and Z is the learned node features.</p><p>Explanation for node classification. The output of an explanation for a target node is a compact subgraph of the computation graph that is most influential for the prediction label. To answer the question of "How did a GNN predict that a given node has label ỹ ?", the explainer should capture which node to explain. Specifically, we use a node labeling technique to mark nodes' different roles in a computation graph. The generated node labels are then transformed into vectors (e.g., one-hot encodings) and treated as the nodes' feature vector -X c . Note that the labels here represent the structural information of nodes within a computation graph, which are different from the classification/prediction labels. . An illustration of the node labeling for node explanation generation (best view in color). The node i is the target node to be explained and the graph in the left-hand side is i's computation graph. i is labeled as 0, while j and k are labeled as 1 as they are one-hop away from i.</p><p>The intuition underlying this node labeling technique is that nodes with different relative positions to the target node may have different structural importance to its prediction label ỹ. By incorporating the relative role features, Gem would capture which node to explain in a computation graph.</p><p>Specifically, our node labeling method is derived based on two principles: 1) The target node, denoted as i, always has the distinctive label "0." 2) A node relative position within a computation graph can be described by its radius regarding the center/target node, i.e., d(i, j) and d(i, k). For two nodes, if d(i, j) = d(i, k), then they have the same label. To further elucidate the used node labeling method, let us see an example shown in Figure <ref type="figure" target="#fig_15">3</ref>. Node i is the target node, while both j and k are the one-hop neighbors of i.</p><p>With our node labeling mechanism, node i is labeled as 0, while j and k are labeled as 1 as they are one-hop away from i.</p><p>Training the explainer. We envision that the reconstructed matrix Âc is the weighted adjacency matrix that reflects the contributions of the edges to its prediction. Now we can apply the "guidance" distilled based on the notion of Granger causality, described in Sec. 3.1, to supervise the learning process. In particular, we use the root mean square error between the reconstructed weighted matrices and the true causal contributions distilled based on our proposed distillation process in Sec. 3.1.</p><p>One highlight of our explainer is its flexibility to choose the predictive model, which is commonly referred to as "modelagnosticism" <ref type="bibr" target="#b16">(Ribeiro et al., 2016)</ref>. Guided by the first principles of Granger causality, our explainer enables graph generative models to learn to generate compact subgraphs for model explanations. We do not need to retrain or adapt the predictive model to explain its decisions. Once trained, it can be used to construct explanations using the generative mapping for the target GNN with little time.</p><p>Computational complexity analysis. One may concern that it would be time-consuming to run through the training instances for obtaining the training "guidance." We argue that our method amortizes the estimation cost by training a graph generator to generate explanations for any given instances. In particular, Gem adopts a parameterized graph auto-encoder with GCN layers to generate explanations, which, once trained, can be utilized in the inductive setting to explain new instances. Specifically, the model parameter complexity of Gem is independent of the input graph size as it naturally inherits the advantages of GCNs (empirically verified in Appendix B). With the inductive property, the inference time complexity of Gem is O (|E|), where |E| is the number of edges of the instance to be explained. Sec. 4 empirically verified the computation efficiency of Gem. In a nutshell, our solution transforms the task of producing explanations for a given GNN into a supervised learning task, trained based on the first principles of Granger causality. Then we can address the explanation task with existing supervised graph generative models.</p><p>Extensions to other learning tasks on graphs. Beyond node classification, our explainer can also provide explanations for link prediction and graph classification tasks without modifying its optimization algorithm. The key difference is the node labeling technique for marking the nodes' roles in the computation graph. For example, to generate an explanation for the link prediction task, the explainer model should be able to identify which link to explain. An alternative approach is double-radius node labeling, marking the target link (connecting two target nodes) within the computation graph, proposed by Zhang et al. <ref type="bibr" target="#b24">(Zhang &amp; Chen, 2018</ref>). More concretely, node i's position is determined by its radius with respect to the two target nodes (x, y), i.e., (d(i, x), d(i, y)).</p><p>Note that, due to properties of the graph structure invariant, there is no need to mark a particular node/link for graph classification tasks. Instead, we use a graph labeling method, called the Weisfeiler-Lehman (WL) algorithm <ref type="bibr" target="#b20">(Weisfeiler &amp; Lehman, 1968)</ref>, to capture the structural roles of nodes within a computation graph, which has been widely used in graph isomorphism checking. For more details about the WL algorithm, We refer curious readers to <ref type="bibr" target="#b20">(Weisfeiler &amp; Lehman, 1968</ref>). In Sec. 4, we will empirically show that with the "guidance" based on Granger causality, complemented by node features from graph/node labeling, Gem can provide fast and accurate explanations for any graph learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Settings</head><p>Node classification with synthetic datasets. In the node classification setting, we built two synthetic datasets where ground truth explanations are available. In particular, we followed the data processing as reported in GNNExplainer <ref type="bibr" target="#b22">(Ying et al., 2019)</ref>. The first dataset is BA-shapes, where nodes are labeled based on their structural roles within a house-structured motif, including "top-node," "middle-node," "bottom-node," and "none-node" (the ones not belonging to a house). The second dataset is Tree-cycles, in which nodes are labeled to indicate whether they belong to a cycle, including "cycle-node" and "none-node" (more details of the datasets are provided in Appendix B).</p><p>Graph classification with real-world datasets. For graph classification, we use two benchmark datasets from bioinformatics -Mutag <ref type="bibr" target="#b2">(Debnath et al., 1991)</ref> and NCI1 <ref type="bibr" target="#b19">(Wale et al., 2008)</ref>. Mutag contains 4337 molecule graphs, where nodes represent atoms, and edges denote chemical bonds.</p><p>The graph classes, including the non-mutagenic or the mutagenic class, indicate their mutagenic effects on the Gramnegative bacterium Salmonella typhimurium. NCI1 consists of 4110 instances, each of which is a chemical compound screened for activity against non-small cell lung cancer or ovarian cancer cell lines.</p><p>Baselines. We consider the state-of-the-art baselines that belong to the unified framework of additive feature attribution methods (The proof is provided in Appendix A) <ref type="bibr" target="#b13">(Lundberg &amp; Lee, 2017)</ref>: GNNExplainer <ref type="bibr" target="#b22">(Ying et al., 2019)</ref> and PG-Explainer <ref type="bibr" target="#b14">(Luo et al., 2020)</ref> <ref type="foot" target="#foot_1">3</ref> . GNNExplainer explains for a given instance at a time, while PGExplainer explains multiple instances collectively. Unless otherwise stated, all the hyperparameters of the baselines are the same in the source code. We do not include gradient-based method <ref type="bibr" target="#b22">(Ying et al., 2019)</ref>, graph attention method <ref type="bibr" target="#b17">(Veličković et al., 2018)</ref>, and Gradient <ref type="bibr" target="#b15">(Pope et al., 2019)</ref>, since previous explainers <ref type="bibr" target="#b14">(Luo et al., 2020;</ref><ref type="bibr" target="#b22">Ying et al., 2019)</ref> have shown their superiority over these methods.</p><p>Parameter settings of Gem. For all datasets on different tasks, associate explainers share the same structure <ref type="bibr" target="#b8">(Kipf &amp; Welling, 2016)</ref>. Specifically, we first apply an inference model parameterized by a three-layer GCN with output dimensions 32, 32, and 16. Then the generative model is given by an inner product decoder between latent variables. The explainer models are trained with a learning rate of 0.01. We use hyperparameter K to control the size of the explanation subgraph and compare the performance of Gem with GNNExplainer and PGExplainer. The target GNN model accuracy on four datasets, more implementation details, and experimental results are presented in the Appendix B.</p><p>Evaluation metrics. An essential criterion for explanations is that they must be human interpretable, which implies that the generated explanations should be easy to understand. Taking BA-shapes as an example, the node label is determined by its position within a house-structured motif.</p><p>The explanations for this dataset should be able to highlight the house structure. For interpretability, we use the visu- alized explanations of different methods to analyze their performance qualitatively.</p><p>In addition, explanations seek to answer the question: when a GNN makes a prediction, which parts of the input graph are relevant? Ideally, the generated explanation/subgraph should lead to the same prediction outcome by the pretrained GNN (e.g.</p><formula xml:id="formula_11">p (ỹ | G s ) should be close to p (ỹ | G c )).</formula><p>In other words, a better explainer should be able to generate more compact subgraphs yet maintains the prediction accuracy while the associated explanations are fed into the pre-trained GNN. To this end, we generate the explanations for the test set based on Gem, GNNExplainer and PGExplainer, respectively. Then we use the predictions of the pre-trained GNN for the explanations to calculate the explanation accuracy.</p><p>The rationality of K selection is from the size of the groundtruth structure for the synthetic datasets. We do not have the ground-truth motif for the real-world datasets, therefore we select K according to the distribution of the graph size for each dataset, respectively (reported in the Appendix B). Note that a K that is too small may incur meaningless explanations; for example, the aromatic group is an essential component leading to the mutagenic effect. We evaluate the explanation performance under different K settings, starting from K = 15 for Mutag and NCI1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>In what follows, we summarize the results of our experimental results and discuss our key findings. The explanation accuracy for synthetic datasets and real-world datasets in different K settings are reported in Table <ref type="table" target="#tab_7">1 and Table 2</ref>, respectively. As shown in Table <ref type="table" target="#tab_0">1</ref>, Gem consistently offers the best explanation accuracy in overall cases. In particular, Gem achieves 30% improvement when K = 5 on BA-shapes, compared with GNNExplainer. By looking at the explanations for a target node, shown in Figure <ref type="figure" target="#fig_5">4</ref>, Gem can successfully identify the "house" motif that explains the node label ("middle-node" in red), when K = 6, while the GNNExplainer wrongly attributes the prediction to a The "house" in green is the ground-truth motif that determines the node labels.</p><p>The red node is the target node to be explained (better seen in color).</p><p>node (in orange) that is out of the "house" motif. On Treecycles, GNNExplainer failed to generate effective explanations when K &lt; 8, while Gem and PGExplainer achieves favorable accuracy even when K = 6.</p><p>Note that, for the real-world datasets, there are no explicit motifs (no ground truth motifs) for classification. PGExplainer assumes NO 2 or NH 2 as the motifs for the mutagen graphs and trains an MLP for model explanation with the mutagen graphs including at least one of these two motifs.</p><p>For fair comparisons, we report the results of PGExplainer following its setting reported in <ref type="bibr" target="#b14">(Luo et al., 2020)</ref> and compare them with the results of GNNExplainer and Gem when explaining on mutagen graphs, indicated as PGExplainer-0, GNNExplainer-0, and Gem-0 in Table <ref type="table" target="#tab_1">2</ref>. As GNNExplainer and Gem can explain both classes in the dataset, we report the results of explaining the entire test set using GNNExplainer and Gem (the 4-5th rows in Table <ref type="table" target="#tab_9">2 4</ref> ). For NCI1, PGExplainer fails to explain this dataset without the motif assumption, therefore, we report the results of GNNExplainer and Gem. In general, the results reported successfully verify that our proposed Gem can generate explanations that can consistently yield high explanation accuracies over all datasets.</p><p>To further check the interpretability of the generated explanations, we report the explanation results for Mutag in Figure <ref type="figure">5</ref> (K = 15). The first column shows the initial graphs and corresponding probabilities of being classified as "mutagenic" class by the pre-trained GNN, while the other columns report the explanation subgraphs. Associated probabilities belonging to the "mutagenic" class based on the pre-trained GNN are reported below the subgraphs.</p><p>In the first two cases (the first two rows), Gem can identify the essential components -the aromatic group (carbon  Explanation comparisons on Mutag. The explanation reof different methods are highlighted with black edges, where the gray edges are regarded as unimportant components for the prediction and are discarded by the explainers (better seen in color). The probability under each graph/subgraph denotes the likelihood of being classified into the "mutagenic" class, which is obtained by feeding the associated graph/subgraph into the pre-trained GNN.</p><p>ring) and NO 2 -leading to their labels ( "mutagenic"). Nevertheless, GNNExplainer either only recognizes the aromatic group (the first row) or NO 2 group (the second row), which is not sufficient to be classified into "mutagenic" class by the pre-trained GNN. PGExplainer focuses on identifying the pre-defined motifs -NO 2 and NH 2 . In the first two rows, we observe that PGExplainer can successfully recognize the NO 2 motifs. However, when its explanation subgraphs are fed into the pre-trained GNN, the probabilities of being classified into the correct class are quite low. In the third row of Figure <ref type="figure">5</ref>, we report an instance that belongs to the "mutagenic" class without NO 2 or NH 2 motifs. Only Gem can recognize the essential components of classifying into "mutagenic." Note that, a good explainer for a pre-trained GNN should be able to highlight the important components that lead to its predictions for any instances. In the last row of Figure <ref type="figure">5</ref>, we report an instance that belongs to the "non-mutagenic" class. Though there are no explicit motifs for this class, Gem can successfully generate the explanation, which can be recognized as the "non-mutagenic" class by the target GNN, with a probability of 0.82.</p><p>To verify the effectiveness of Gem in a more statistical view, we measure the resulting change in the pre-trained GNNs'  outcome by computing the difference (the initial graph and the explanation subgraph with K = 15 for Mutag) in log odds and investigate the distributions over the entire test set. The result on the Mutag dataset is reported in Figure <ref type="figure" target="#fig_7">6</ref> (The definition of log-odds difference and more results on other datasets are provided in Appendix B). We can observe that Gem performs consistently better than GNNExplainer and PGExplainer. Specifically, the log-odds difference of Gem is more concentrated around 0, which indicates Gem can well capture the most relevant subgraphs towards the predictions by the pre-trained GNNs.</p><p>Computational performance. PGExplainer and Gem can explain unseen instances in the inductive setting. We measure the average inference time for these two methods. As GNNExplainer explains an instance at a time, we measure its average time cost per explanation for comparisons. As reported in Table <ref type="table" target="#tab_3">3</ref>, we can conclude that Gem consistently explain faster than the baselines overall. Further experiments on the efficiency evaluation, such as training time comparisons, are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In </p><formula xml:id="formula_12">(A s ) = ψ 0 + ψ ij × A s ij (1)</formula><p>where A s is the adjacency matrix of the explanation, A s ij ∈ {0, 1} is a binary variable representing the existence of an edge.</p><p>In this section, we analyze that GNNExplainer (?), PG-Explainer (?) and our explainer, Gem, all fall into the framework of additive feature attribution methods. In what follows, we analyze, these methods for explaining GNNs essentially solve the same optimization problem with different approximation methods.</p><p>In general, to identify a compact subgraph that is important for the GNN's prediction, the explanation problem can be formulated as following optimization problem based on mutual information:</p><formula xml:id="formula_13">max G s ⊆G c MI (Y, G s ) = max G s ⊆G c (H(Y ) − H(Y |G = G s )) s.t. |G s | &lt; K (2)</formula><p>where K is a constraint on G s 's size for a compact explanation.</p><p>When treating G s ⊆ G c as a graph random variable and with convexity assumption, Eq. ( <ref type="formula">2</ref>) can be approximated as solving following optimization problem:</p><formula xml:id="formula_14">min G s ⊆G c H(Y |G = E(G s ))<label>(3)</label></formula><p>The source code were enclosed in the supplementary material.</p><p>To make the optimization tractable, GNNExplainer relaxes the integer constraint on the adjacency matrix and searches for an optimal fractional adjacency matrix. In particular, GNNExplainer factorizes the explanation into a multivariate Bernoulli distribution, formulated as P (G s ) = Π (i,j)∈G s A s ij . For explanations on graph structures, an explanation is selected based on the values of entries in the solution A s . GNNExplainer uses mean-field optimization to solve min A s H(Y |G = E(G s )). Therefore, the weights ψ ij of GNNExplainer under the unified framework are chosen based on the solution A s .</p><p>Gem approximates the solution of the optimization problem from a causal perspective and utilize a graph generative model to generate explanations in an inductive setting. Specifically, Gem assumes that the selections of edges from the G c are conditionally independent to each other. With the assumption of conditional independence, we estimate the edge importance based on the notion of Granger causality. These estimations then are used as the ground-truth to train our explanation model. In particular, we use an autoencoder architecture utilizing graph convolutions (???), due to its simplicity and expressiveness. Following the instantiation of graph auto-encode (?), we use the reparameterization trick for training, our objective is approximated as min Z∼N (µ,σ) H(Y |G = G s ). The weights ψ ij of Gem under the unified framework are then chosen based on the output of the graph generative model.</p><p>PGExplainer factorizes the explanation graph as P (G s ) = Π (i,j)∈G s P (A s ij ), where P (A s ij ) is instantiated with the Bernoulli distribution A s ij ∼ Bern(θ ij ). PGExplainer utilizes the reparameterization trick and approximate the objective as min ∼Uniform(0,1) H(Y |G = G s ). To collectively explain multiple instances, PGExplainer adopts a multilayer perception (MLP) to learn to generate explanations from the target GNN, taking the node embeddings generated from the target GNN as the input. The weights ψ ij of PGExplainer under the unified framework are chosen based on the output of the MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Further Implementation Details and More Experimental Results</head><p>Datasets. BA-shapes was created with a base Barabasi-Albert (BA) graph containing 300 nodes and 80 five-node "house"-structured network motifs.  <ref type="table" target="#tab_0">1</ref>. Note that, we report the average number of nodes and the average number of edges over all the graphs for the real-world datasets. Table <ref type="table" target="#tab_1">2</ref> reports the model accuracy on four datasets, which indicates that the models to be explained are performed reasonably well.</p><p>The motif statistics of Mutag are reported in Table <ref type="table" target="#tab_3">3</ref>. We found that there are 32% of non-mutagenic graphs containing at least NO 2 or NH 2 motifs, which indicates that the assumption made by PGExplainer might not be reasonable. Unless otherwise stated, all models, including GNN classification models and our explainer, are implemented using PyTorch 1 and trained with Adam optimizer. All the experiments were performed on a NVIDIA GTX 1080 Ti GPU with an Intel Core i7-8700K processor.</p><p>For GNN classification models, we use the same setting as GNNExplainer. Specifically, for node classification, we apply three layers of GCNs with output dimensions equal to 20 and perform concatenation to the output of three layers, followed by a linear transformation to obtain the node label. For graph classification, we employ three layers of GCNs with dimensions of 20 and perform global max-pooling to obtain the graph representations. Then a linear transformation layer is applied to obtain the graph label. Figure <ref type="figure" target="#fig_9">1 (a</ref>      Log-odds difference. We measure the resulting change in the pre-trained GNNs' outcome by computing the difference  The results on the other datasets are reported in Figure <ref type="figure" target="#fig_15">3</ref>. We can observe that Gem performs consistently better than GNNExplainer and PGExplainer in general. Specifically, the log-odds difference of Gem is more concentrated around 0, especially on the BA-shapes dataset, which indicates Gem has a better computation accuracy on identifying the most relevant subgraphs towards the predictions by the pretrained GNNs.</p><p>Computational performance. For Gem, we measure the time for distilling the "ground-truth" explanation and the time for training the explainer with a varying number of training instances. As for GNNExplaner, we measure the overall time cost of explanations with a varying number of target instances. The training time of PGExplainer and the explanation time of GNNExplainer are measured by reusing the source code released by the authors. Figure <ref type="figure" target="#fig_5">4</ref> report the computational performance for four datasets. We can conclude that Gem amortizes the explanation cost by training a graph generator to generate explanations for any given instances and consistently explain faster than the baseline overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distillation Process</head><p>We are given a pre-trained classification model, denoted as f (•), and the training set that is used to train f (•). Procedure Distillation Process presents the detailed algorithm of generating the ground-truth explanation G s for a training instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Related Work</head><p>Explanations seek the answers to the questions of "what if" and "why," which are arguably and inherently causal. The theory of causal inference is one method by which such questions might be answered (?). Recently, causal interpretability has gained increasing attention in explaining machine learning models (???). There are several viable formalisms of causality, such as Granger causality (?), causal Bayesian networks (?), and structural causal models (?).  learning models on images as a causal learning task, and proposed a causal explanation model that can learn to estimate the feature importance towards its prediction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and l is the number of categories. The dataset D = {(G i , y i )} N i=1 is represented by pairs of graph instances and graph labels. Examples of such task include classifying the drug molecule graphs according to their functionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. An illustration of the computation graph (best viewed in color). Node i is the target node to be explained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of Gem. 1) The distillation process of generating the ground-truth explanations based on the first principles of Granger causality; 2) Training the explainer that can be used to generate explanations for the target GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure3. An illustration of the node labeling for node explanation generation (best view in color). The node i is the target node to be explained and the graph in the left-hand side is i's computation graph. i is labeled as 0, while j and k are labeled as 1 as they are one-hop away from i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. Explanation comparisons on BA-shapes. The "house" in green is the ground-truth motif that determines the node labels. The red node is the target node to be explained (better seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure5. Explanation comparisons on Mutag. The explanation reof different methods are highlighted with black edges, where the gray edges are regarded as unimportant components for the prediction and are discarded by the explainers (better seen in color). The probability under each graph/subgraph denotes the likelihood of being classified into the "mutagenic" class, which is obtained by feeding the associated graph/subgraph into the pre-trained GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Log-odds difference comparisons on Mutag (more dense distribution around 0 is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>) and 1 (b) are the model architectures for node classification and graph classification, receptively.1 https://pytorch.org</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Model architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The distribution of the graph size on real-world datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>∆log-odds = log-odds (p(G c )) − log-odds (p(G s )) (4)where log-odds(p) = log p 1−p , and p(G c ) and p(G s ) are the outputs of the pre-trained GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Prior works of this research line usually are designed to explain the importance of each component of a neural network on its prediction. Chattopadhyay et al. (?) proposed an attribution method based on the first principles of causality, particularly the Structural Causal Model and do(•) calculus. Schwab et al. (?) framed the explanation task for deep</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Log-odds difference comparisons on other datasets (more dense distribution around 0 is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Explanation Accuracy on Synthetic Datasets (%). GNNExplainer 82.4 88.2 91.2 91.2 94.1 14.3 46.8 74.6 91.4 96.1 PGExplainer 71.9 90.7 92.0 93.3 94.1 94.4 80.6 77.0 82.4 89.4</figDesc><table><row><cell>K</cell><cell>5</cell><cell>BA-SHAPES 6 7 8</cell><cell>9</cell><cell>6</cell><cell>TREE-CYCLES 7 8 9 10</cell></row><row><cell>Gem</cell><cell cols="5">93.4 97.1 97.1 97.1 99.3 86.1 87.5 92.5 93.9 95.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Explanation Accuracy on Real-World Datasets (%).</figDesc><table><row><cell>K</cell><cell>MUTAG 15 20 25 30 15 20 25 30 NCI1</cell></row><row><cell cols="2">Gem-0 GNNExplainer-0 60.0 67.6 68.9 75.8 − 64.0 78.1 81.0 85.0 − PGExplainer-0 22.5 38.5 57.6 72.3 − Gem 66.3 78.0 82.1 83.4 56.9 65.3 68.9 72.8 − − − − − − − − − GNNExplainer 67.1 74.9 75.8 80.9 59.3 61.8 69.6 72.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Inference Time per Instance (ms).</figDesc><table><row><cell>DATASETS</cell><cell cols="3">BA-SHAPES TREE-CYCLES MUTAG NCI1</cell></row><row><cell>GNNEXPLAINER PGEXPLAINER GEM</cell><cell>265.2 6.7 0.5</cell><cell>204.5 6.5 0.5</cell><cell>257.6 259.8 5.5 − 0.05 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>-level balanced binary tree and 80 six-node cycle motifs. The statistics of four datasets are presented in Table</figDesc><table><row><cell>arXiv:2104.06643v1 [cs.LG] 14 Apr 2021 055 109 108 107 106 105 104 103 102 101 100 099 098 097 096 095 094 093 092 091 090 089 088 087 086 085 084 083 082 081 080 079 078 077 076 075 074 073 072 071 070 069 068 067 066 065 064 063 062 061 060 059 058 057 056</cell><cell>with a base 8</cell><cell>Tree-cycles was built</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Data Statistics of Four Datasets.</figDesc><table><row><cell cols="4">DATASETS BA-SHAPES TREE-CYCLES MUTAG NCI1</cell></row><row><cell>#GRAPHS #NODES #EDGES #LABELS</cell><cell>1 700 4, 110 4</cell><cell>1 871 1, 950 2</cell><cell>4, 337 4, 110 29 30 30 32 2 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Model Accuracy of Four Datasets (%).</figDesc><table><row><cell cols="4">DATASETS BA-SHAPES TREE-CYCLES MUTAG NCI1</cell></row><row><cell>ACCURACY</cell><cell>94.1</cell><cell>97.1</cell><cell>88.5 78.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>Motif Statistics on Mutag (Number of Instances Containing Associated Motif).</figDesc><table><row><cell>MOTIF</cell><cell>NO2 NH2</cell></row><row><cell cols="2">MUTAGENIC NON-MUTAGENIC 97 251 596 501</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Data Splitting for Four Datasets.Figure1(c) depicts the model architecture of Gem for generating explanations. We use mean square error as the loss for training Gem. In particular, it was optimized using Adam optimizer with a learning rate of 0.01 and 0.001 for explaining graph and node classification model, respectively. We train at batch size 32 for 100 epochs. Table4shows the detailed data splitting for model training, testing, and validation. Note that both classification models and our explanation models use the same data splitting. Figure2depicts the distribution of the graph size on two real-world datasets in terms of the number of edges.</figDesc><table><row><cell cols="4">DATASETS #OF TRAINING #OF TESTING #OF VALIDATION</cell></row><row><cell>BA-SHAPES TREE-CYCLES MUTAG NCI1</cell><cell>300 270 3, 468 3, 031</cell><cell>50 45 434 410</cell><cell>50 45 434 411</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and investigate the distributions over the entire test set. The log-odds difference is formulated as:</figDesc><table><row><cell>110 164 163 162 161 160 159 158 157 156 155 154 153 152 151 150 149 148 147 146 145 144 143 142 141 140 139 138 137 136 135 134 133 132 131 130 129 128 127 126 125 124 123 122 121 120 119 118 117 116 115 114 113 112 111</cell><cell>in log odds</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Schwab et al.'s proposal is built upon the notion of Granger causality.</figDesc><table><row><cell>165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We are aware of the drawbacks of reusing notations. xi and ỹ in this definition represent any random variables for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We use the source code released by the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">The comparisons with PGExplainer on NCI1 were omitted as it fails to explain on NCI1, indicated as "−" in Table2 and Table 3.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While such methods can provide meaningful explanations for deep models on images, they cannot be directly applied to interpret graph neural networks, due to the inherent property of graph representations.</p><p>Algorithm 1 Distillation Process: Distill the top-k most relevant edges for each computation graph Require: Given the computation graph G c = (V c , A c , X c ). Ensure: Calculate the model error of the computation graph δ G c according to Eq. ( <ref type="formula">4</ref>) 1: for edge e j ∈ G c do 2:</p><p>Calculate the causal contribution ∆δ, e j according to Eq. (1) -Eq. ( <ref type="formula">5</ref>). 3: end for 4: Remove edges with the least casual contribution and re-calculate the causal contribution of the generated subgraph. 5: E c sorted ← sort the edges in ascending order based on the causal contributions. 6: Initialize G s ← G c 7: for ∀ edge e j ∈ E c sorted do 8:</p><p>Calculate the model error of the subgraph, denoted as δ G s \{ej } according to Eq. ( <ref type="formula">3</ref>) and Eq. ( <ref type="formula">5</ref>). 9: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous assessment of flow and bold signals in resting-state functional connectivity maps</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Kylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMR in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="165" to="170" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><surname>Wiener-Granger Causality</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Well Established Methodology</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="323" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-Activity Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. Correlation with Molecular Orbital Energies and Hydrophobicity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="65" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating Causal Relations by Econometric Models and Cross-Spectral Methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Testing for Causality: a Personal Viewpoint</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="329" to="352" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feroce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scognamiglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Anniciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00311</idno>
		<title level="m">Goksel, O., and Gabrani, M. Towards Explainable Graph Representations in Digital Pathology</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Bayesian Deep Learning Workshop</title>
				<meeting>NIPS Bayesian Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning Deep Generative Models of Graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Objective De Novo Drug Design with Conditional Graph Generative Model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guardian: Evaluating Trust in Online Social Networks with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Communications</title>
				<meeting>IEEE International Conference on Computer Communications</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameterized Explainer for Graph Neural Network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explainability Methods for Graph Convolutional Neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10772" to="10781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model-Agnostic Interpretability of Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Workshop on Human Interpretability in Machine Learning</title>
				<meeting>ICML Workshop on Human Interpretability in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic Graphical Model Explanations for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><surname>Pgm-Explainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Reduction of a Graph to a Canonical Form and an Algebra Arising during this Reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Theory of Prediction. Modern Mathematics for Engineers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<biblScope unit="page" from="165" to="190" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating Explanations for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Gnnexplainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9244" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Model-Level Explanations of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Xgnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD. ACM</title>
				<meeting>SIGKDD. ACM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting Multicellular Function Through Multi-Layer Tissue Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling Polypharmacy Side Effects with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
