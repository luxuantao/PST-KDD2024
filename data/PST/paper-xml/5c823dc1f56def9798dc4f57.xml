<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-30">January 30, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Rafael</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sabino</forename><surname>Parmezan</surname></persName>
							<email>parmezan@usp.br</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory of Computational Intelligence</orgName>
								<orgName type="institution" key="instit1">Instituto de Ciências Matemáticas e de Computação</orgName>
								<orgName type="institution" key="instit2">Universidade de São Paulo</orgName>
								<address>
									<addrLine>Av. Trabalhador São-carlense, 400</addrLine>
									<postCode>13566-590</postCode>
									<settlement>São Carlos</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vinicius</forename><forename type="middle">M A</forename><surname>Souza</surname></persName>
							<email>vmasouza@icmc.usp.br</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory of Computational Intelligence</orgName>
								<orgName type="institution" key="instit1">Instituto de Ciências Matemáticas e de Computação</orgName>
								<orgName type="institution" key="instit2">Universidade de São Paulo</orgName>
								<address>
									<addrLine>Av. Trabalhador São-carlense, 400</addrLine>
									<postCode>13566-590</postCode>
									<settlement>São Carlos</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
							<email>gbatista@icmc.usp.br</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory of Computational Intelligence</orgName>
								<orgName type="institution" key="instit1">Instituto de Ciências Matemáticas e de Computação</orgName>
								<orgName type="institution" key="instit2">Universidade de São Paulo</orgName>
								<address>
									<addrLine>Av. Trabalhador São-carlense, 400</addrLine>
									<postCode>13566-590</postCode>
									<settlement>São Carlos</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">The terms &quot;prediction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-30">January 30, 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">973C924046EB8FB50956ED3C563471B2</idno>
					<idno type="DOI">10.1016/j.ins.2019.01.076</idno>
					<note type="submission">Received date: 2 June 2018 Revised date: 28 January 2019 Accepted date: 30 January 2019 Preprint submitted to Information Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Univariate analysis</term>
					<term>automatic parameter tuning</term>
					<term>multi-step-ahead prediction</term>
					<term>time series forecasting</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model, Information Sciences (2019),</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A systematic literature review of the last decade • One of the most extensive, impartial and comprehensible evaluations ever done • Recommendation of the most suitable predictors based on a critical analysis • A guideline for models selection, parameters setting and validation of results</p><p>• A new online data repository for time series forecasting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The technological advances in computing area, including databases systems, machine learning, and cloud computing have leveraged off the conversion of data into useful information and knowledge to support decision making. These advances contributed to the development of computer systems capable of storing, analyzing and managing an increasing amount of data.</p><p>Data can be represented in different formats, from the most basic, such as numeric and nominal, to more complex ones, such as audio and video. However, the storage of temporal information, which allows the chronological organization of the collected data, is one of the data representations that has attracted most attention of researchers and driven the creation of large databases <ref type="bibr" target="#b13">[14]</ref>.</p><p>Temporal data mining is a process to extract useful knowledge from time series. Prediction 1 is one of the tasks contemplated by temporal data mining, which is motivated by the challenge of reducing future uncertainty, especially due to the volatility of some phenomena <ref type="bibr" target="#b4">[5]</ref>. Some examples are the prediction of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the closing price of a company's stock each day, prediction of unemployment for a state each quarter, and energy load forecasting. Thus, predictors constitute an essential tool in many areas for decision making as commercial and financial strategies or politics, and their adequate use can cause social and economic impacts.</p><p>The methods for time series prediction rely on the idea that historical data include intrinsic patterns which convey useful information for the future description of the phenomenon investigated. These patterns are usually non-trivial to identify, and their discovery is one of the primary goals of the time series processing: the circumstances the patterns found will repeat and what types of changes they may suffer over time <ref type="bibr" target="#b30">[31]</ref>.</p><p>The design of a model for time series prediction focuses on the application of algorithms. Under certain assumptions about the data, the model captures the variables involved and represents the existing dynamic relations, summarizing them in a robust and potentially flexible mathematical structure. The structure can be used to predict future data, as well as to help to understand the process that originated the data.</p><p>The application of statistical methods based on autoregression and Moving Averages (MA) are considered the state-of-the-art for time series modeling and prediction for over a half-century <ref type="bibr" target="#b15">[16]</ref>. The algorithms that implement these methods assume the data follow a known distribution. Based on that information, function parameters are defined to fit a model to the data. However, the usage of these parametric methods requires sophisticated mathematical concepts as well as vast technical expertise for the establishment of the model's parameters.</p><p>The use of fundamentals of descriptive statistics can guide the parameters definition <ref type="bibr" target="#b4">[5]</ref>. In several cases, autocorrelation-based functions automatize this task. The results can be interpreted via correlograms and by the application of techniques for obtaining input arguments from the minimization of information criteria, which penalizes the model for the number of parameters required for its adjustment.</p><p>More recently, several studies have employed non-parametric modeling based on machine learning algorithms for time series prediction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>. One of the main advantages of this approach is that it does not presuppose the data distribution nature.</p><p>Empirical research has demonstrated that machine learning algorithms for time series prediction provide very competitive results, frequently outperforming statistical models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>. Nevertheless, to the best of our knowledge and according to the systematic review conducted in this work, there are no articles in the literature containing robust empirical studies focused on the comparison between parametric and non-parametric methods. Thorough research regarding the application of these algorithms on benchmark datasets and in the presence of properties that challenge time series modeling, such as trend, non-stationarity and distribution shifts, could consolidate the efficiency and effectiveness of each method.</p><p>The present study fills this gap by providing an objective comparison of the most popular statistical and machine learning models for univariate time series prediction. The primary purpose of this research is to empirically identify the behavior of these predictors concerning datasets with specific characteristics. With this in mind, we provide a comprehensive guide for users to choose the best predictive models for their applications. The three major contributions of this paper can be summarized as follows:</p><p>• Planning and conduction of a systematic review and meta-analysis to position this research in the corresponding state-of-the-art. We have selected 117 publications, between the years 2009 and 2018, whose content helped to answer ten scientific questions;</p><p>• A critical analysis and algorithms recommendation based on the execution of one of the most extensive, impartial and comprehensible experimental evaluations ever done for time series prediction. Using a set of 95 time series from synthetic and real domains, we face eleven predictive algorithms, seven parametric and four non-parametric, employing two multi-step-ahead projection strategies and four performance evaluation measures. The findings of our experimental comparison will facilitate further research on this topic since they provide a better insight into the predictive performance of the methods currently available in the literature. In addition to discussing which algorithms should be used as baseline and topline when investigating the proposition of novel models, we highlight the advantages and drawbacks of each method. We performed this analysis according to characteristics of the data, and we hope that it can guide practitioners in statistics and machine learning;</p><p>• Building an online archive, namely of ICMC-USP Time Series Prediction Repository <ref type="bibr" target="#b31">[32]</ref>, which grants access to all materials produced in this work. Thereby, other researchers can reuse the avail-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>able datasets to replicate our results and compare their methods more rigorously against different predictors.</p><p>We emphasize that the outlined experimental protocol and our discussion on its usage are a guideline for model selection, parameters setting, and employment of statistical and machine learning algorithms for time series prediction.</p><p>The remaining of this paper is structured as follows: Section 2 presents the related work and the statistics derived from a meta-analysis of the literature, which was guided by the results of a systematic review. Section 3 reports the main definitions and notations about time series. Section 4 describes the temporal data prediction approaches with their usual methods and some techniques that help in its parameters estimation. Section 5 specifies the configuration of the experiments, which includes datasets, algorithms and performance measures. Section 6 presents results and discussion, while Section 7 punctuates the limitations, recommendations, and practical implications of the outcomes. Finally, Section 8 reports the conclusions and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Systematic Review and Meta-analysis of the Literature</head><p>For over a half-century, statistical methods based on autoregression and MA have influenced the temporal data processing and analysis fields. Although some studies have stated, between the seventies and eighties, that the parametric models could not be readily adapted to many real applications, they resisted over the years <ref type="bibr" target="#b15">[16]</ref>. The preference for these methods made them reached the condition of state-of-art for time series modeling and prediction.</p><p>In the last two decades, with the rise of the data mining process, there is an increasing interest in the adaptation of machine learning methods, especially those for regression tasks, to support analysis with time dependence. Due to their simplicity and comprehensibility, the non-parametric techniques have established themselves as serious candidates to the classical models, so that scientific competitions have been undertaken to encourage both the improvement of these algorithms as the development of new solutions <ref type="bibr" target="#b0">[1]</ref>.</p><p>The researchers from statistics and machine learning communities have contributed to several aspects of the prediction process, such as the assistance in selecting the most promising model <ref type="bibr" target="#b25">[26]</ref>, the study of the deseasonalization effects on the projection of future values <ref type="bibr" target="#b2">[3]</ref>, and the construction of hybrid models by a combination of statistical and machine learning methods <ref type="bibr" target="#b1">[2]</ref>. In this sense, the quality of parametric and non-parametric models has been explored mainly in annual or biennial competitions aimed at assessing the performance of prediction algorithms on a considerable amount of time series data <ref type="bibr" target="#b2">[3]</ref>.</p><p>To position this study in the corresponding state-of-the-art, we performed a meta-analysis of the literature. We conducted this meta-analysis using the results of a systematic review of papers published in the last ten years. Our supplementary material details the systematic review protocol, which includes ten research questions, a search key, and five search sources as well as the selected publications and a detailed results interpretation. Throughout the remainder of this section, we highlight the most relevant points found and describe the most related papers to the present work.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> displays a distribution bar chart, by publication year, regarding the number of selected papers at the end of the systematic review. In particular, the graph shows that from January 2009 to December 2018, were published annually about twelve articles, with two peaks in 2010 and 2015. The inspection of the 117 papers discovered by the systematic review allowed us to elaborate a broad and in-depth meta-analysis, of which Fig. <ref type="figure" target="#fig_2">2</ref> illustrates some results. Fig. <ref type="figure" target="#fig_2">2</ref>(a) shows the percentage of use of each prediction approach in 68 papers with real applications. Fig. <ref type="figure" target="#fig_2">2</ref>   Table <ref type="table" target="#tab_1">1</ref> compares our study with other 11 experimental papers previously published. This comparison uses the following criteria: the number of parametric and non-parametric predictors compared; the amount of synthetic and real time series selected; the number of measures chosen to evaluate the algorithms' performance; and if any statistical significance test was applied to support the comparison of results. Most of these articles assess predictors on datasets from specific domains. Although <ref type="bibr" target="#b0">[1]</ref> considered a significant number of datasets, the lengths of the series are short with values between 81 and 126 observations. In general, such studies have concluded that no particular traditional model can provide the best predictions. The supplementary material presents a description of these papers. As summarized in the last row of Table <ref type="table" target="#tab_1">1</ref>, our proposal differs from the literature not only by comparing eleven popular predictive algorithms using two multi-step-ahead strategies on 95 time series, but also includes a more rigorous experimental design supported by a new multi-criteria performance measure. Besides discussing which algorithms should be used as baseline and topline when investigating the proposition of novel models, we highlighted the advantages and drawbacks of each method for certain types of datasets. We compare the models' performances with the support of statistical significance tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fundamentals of Time Series</head><p>A time series Z of size m can be formulated as an ordered sequence of observations, i.e., Z = (z 1 , z 2 , . . . , z m ) where z t ∈ is an observation at time t. In this work, we assume the time series are discrete and uniformly sampled over time.</p><p>When the time series values are synthesized by a mathematical function y = f (time), the series is said deterministic. When the time series comprises, in addition to a mathematical time function, a random term , y = f (time, ), the series is stochastic or nondeterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Another relevant feature of a time series is the stationarity. A stationary series develops randomly around a constant average, reflecting some stable equilibrium <ref type="bibr" target="#b30">[31]</ref>. Attention to this property is essential since some methods assume the stationarity condition. A typical transformation takes successive differences of the original time series. The first difference, defined as ∆z t = z tz t-1 is usually enough to make the series stationary.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref>(a) displays a real time series that expounds, in tons, the monthly chocolate production in Australia from January 1958 to December 1990. These measurements, provided by the Australian Bureau of Statistics, as well as all datasets adopted in this paper, are available at the ICMC-USP Time Series Prediction Repository <ref type="bibr" target="#b31">[32]</ref>. Fig. <ref type="figure" target="#fig_3">3</ref>(a) shows that the time series values do not oscillate around a fixed level. Instead, there exists an increasing behavior whose the period of variation remains constant as the level increases. We can avail of temporal data decomposition techniques to explore those properties. The decomposition techniques, beyond allowing the identification of components that act in a series, enable to obtain patterns (via indexes and equations) that may be coupled to a computational model for prediction of future values. The three major components of a time series are: Trend (T ) is a long-term increase or decrease in the data which can assume a great variety of patterns (e.g., linear, exponential, damped, and polynomial) <ref type="bibr" target="#b30">[31]</ref>. Real time series with an increasing trend can be found in phenomena related to the demographic development, gradual change of consumption habits, and demand for technologies in the social sectors. The decreasing trend, in turn, can be found in series concerning the mortality rates, epidemics, and unemployment. We can use regression models and the MA method to obtain this component <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>. We use the trend to estimate the level, i.e., the value or the typical range of values that the variable assumes if there is no increasing or decreasing trend in the long term. Fig. <ref type="figure" target="#fig_3">3</ref>(b) shows the trend, estimated using MA with 12 periods (monthly data), of the chocolate production time series;</p><p>Seasonality (S) is the occurrence of cyclic patterns of variation that repeat, at relatively constant time intervals, along with the trend component. Examples of seasonal patterns are the increase in sales of air conditioners in summer and warm clothing in winter. Average percentage, percentage relation, relation between MA, and relative links are some of the algorithms that allow computing the seasonality from the extraction of seasonal indexes <ref type="bibr" target="#b40">[41]</ref>;</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>Residue (R) is the short-term fluctuations that are neither systematic nor predictable. In the real world, unforeseen events cause such instabilities, such as natural disasters, terrorist attacks, and strikes. In practical terms, the residual component is what remains after the estimation of T and S components, and their removals from the time series <ref type="bibr" target="#b30">[31]</ref>. We can define the residue R of a period t of a series Z, in agreement with the additive and multiplicative models, as R t = Z t -(T t + S t ) and R t = Z t ÷ (T t × S t ), respectively.</p><p>For a better analysis and understanding, we can reformulate the time series Z, according to Eqs. 1 and 2, by an additive or multiplicative decomposition of its components.</p><formula xml:id="formula_1">Z t = T t + S t + R t (1) Z t = T t × S t × R t<label>(2)</label></formula><p>In the additive model (Eq. 1), the interest variable value is the sum of the components values, which contemplate the same unit of the observation Z t . Considering the additive decomposition, Fig. <ref type="figure" target="#fig_3">3 -(c</ref>) and (e) -shows the seasonality provided by the relation between MA, and the residue of the chocolate production time series.</p><p>Differently, in the multiplicative model (Eq. 2), only the trend has the same unit of the investigated variable. The other components exhibit values that can modify the trend, i.e., they assume values larger, smaller or exactly equal to 1. Adopting the multiplicative decomposition, Fig. <ref type="figure" target="#fig_3">3 -(d</ref>) and (f) -outlines the seasonality, obtained via relation between MA, and the residue of the chocolate production time series.</p><p>We note that not every data sequence will have all the three mentioned components, even when the classical decomposition is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Time Series Prediction</head><p>The time series prediction process covers six steps, as illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>.   The first step partitions the time series in two sequences: one before the prediction horizon, which is intended to the model training (building and fit); and another after that period, which is used to test (evaluate) the quality of the fitted model.</p><p>The second step chooses the predictive model structure based on data characteristics and estimates the parameters using some search technique. Usually, the algorithm that implements this technique receives as input the training sequence, which is subdivided into subsequences (samples) for training and validation, and a set of predefined parameters. At each iteration, the algorithm seeks for parameters values that minimize the predictive error of the model.</p><p>The third step builds the model with the previously found parameters values and fits the training sequence data. This model is then extrapolated, in the fourth step, for the periods of the test sequence. Evidently, the prediction error of the model reflects the chosen values for the parameters. Such error can be amplified for long time horizons.</p><p>The fourth step also chooses the strategy to predict the values of a time series several periods ahead (prediction horizon h &gt; 1). The most intuitive strategy is known as multi-step (or recursive), where the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>prediction of h &gt; 1 is conducted h successive times considering a predictive model with h = 1 <ref type="bibr" target="#b2">[3]</ref>. After the model's extrapolation, the predicted value or the respective actual value can be employed to calculate the next prediction. In this paper, when the predicted values are used, we called the strategy of multi-stepahead with approximate iteration. Otherwise, when the actual values are adopted, the strategy is called multi-step-ahead with updated iteration.</p><p>The fifth step compares the predicted values to the test sequence to measure the model's accuracy. The performance analysis is essential given that distinct models may have similar adjustments, but result in significantly different predictive values.</p><p>The sixth step makes predictions for future periods of the time series. This step should monitor the prediction error as soon as the actual values of the series arrive. This monitoring aims to indicate when it is necessary to update the model with new data or readjust its parameters since the distribution of most recent data is distinct from old data.</p><p>The time series prediction methods have evolved over the years passing from simple regression techniques to robust statistical and artificial intelligence algorithms. We can group the methods into two approaches according to the prior knowledge about the data distribution (Fig. <ref type="figure" target="#fig_5">5</ref>): (i) parametric (exponential smoothing or based on autoregression and MA), and (ii) non-parametric (global or local). The following subsections discuss these approaches along with the most renowned algorithms for time series prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parametric Methods</head><p>The statistical methods require a priori knowledge about the data distribution to build predictive models. This assumption makes the model depends on a set of parameters, which must be determined to optimize the prediction results. We can divide the models into two groups according to their mathematical complexity <ref type="bibr" target="#b30">[31]</ref>: (i) exponential smoothing models and (ii) ARIMA models.</p><p>The exponential smoothing models decompose the time series into components whose values are smoothed by weights that exponentially decay over time. In the end, an additive or multiplicative structure recomposes the smoothed components to predict future values <ref type="bibr" target="#b14">[15]</ref>.</p><p>In contrast, ARIMA models involve three statistical procedures <ref type="bibr" target="#b4">[5]</ref>: (i) autoregression, (ii) integration, and (iii) MA. Autoregression expresses the correlation between observations, i.e., how much the current values influences the next ones. The integration procedure indicates the number of differences required to guarantee the stationarity of the series. Lastly, the MA part comprises unknown factors that cannot be explained by the time series past values. The SARIMA generalization also allows to model temporal data with seasonal variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Moving Averages</head><p>The MA model of order r, MA(r), is a simple technique that performs an arithmetic average of the last r values of time series to predict the next value. MA considers a constant number of observations to exploit the autocorrelation structure of the prediction residues at the current time with those occurred in previous periods. Eq. 3 defines the MA model, where r is the number of observations included in the average z t+1 .</p><formula xml:id="formula_2">z t+1 = z t + z t-1 + z t-2 + . . . + z t-r+1 r<label>(3)</label></formula><p>The higher the value of r, the more uniform (smoothed) will be the predicted data behavior. Thus, when the series shows small distortions in their patterns or random fluctuations, it is recommended to employ a substantial r value to avoid the influence of noise in the predictions. Otherwise, if the series is nearly devoid of randomness and presents a significant shift in the curves inflection points, it is indicated to use a smaller r value to allow the model to react quickly to the data changes.</p><p>The drawbacks of the MA model are their low accuracy to deal with trend and seasonality. Since the prediction of the next value always involves the addition of new data and the discard of the previous one. Furthermore, the weights assigned to the r observations are typically all equal. Therefore, there is no emphasis on the most recent observations <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Simple Exponential Smoothing</head><p>The SES method is equivalent to MA with r = m, except by the fact that each series value receives a different weight. The weights increase exponentially over time so that the most recent observations exert</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T more influence on the calculation of future predictions <ref type="bibr" target="#b22">[23]</ref>.</p><p>Eq. 4 expresses the SES model structure <ref type="bibr" target="#b14">[15]</ref>, where L denotes the level at time t, α (0 &lt; α &lt; 1) is the weight (or constant smoothing) assigned to historical values, and z t corresponds to the last observed value.</p><formula xml:id="formula_3">L t = αz t + α(1 -α)z t-1 + . . . + α(1 -α) m-1 z 1<label>(4)</label></formula><p>To avoid an expensive computation that involves all observations for each new estimate L, we can reduce Eq. 4 in function of the current value of the time series and the level computed in the previous time. Eq. 5 formalizes the result of this simplification <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_4">L t = αz t + (1 -α)L t-1<label>(5)</label></formula><p>Usually, at the beginning of the SES prediction process, it is supposed that the first fitted value is equal to the first series value, i.e., L 1 = z 1 . In this case, the adjustment procedure starts from the second observation of the time series. The exponential smoothing of the last observed value (z m+1 = L m ) gives the prediction at time m + 1. We call this strategy one-step-ahead. This method does not support an extension to larger horizons. In this case, the fitted value L m gives the prediction of all future values for multiple horizons.</p><p>The flexibility, mathematical simplicity, and reasonable accuracy explain the popularity conferred to the SES method. For making a new prediction, the algorithm needs the most recent observation, the last predicted value, and the parameter α. Among the drawbacks of the method, stands the difficulty in finding the most appropriate value for the smoothing constant <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Holt's Exponential Smoothing</head><p>The SES model when applied to temporal data that present increasing (or decreasing) linear behavior, provides predictions which underestimate (or overestimate) the actual values. To avoid this systematic error, we can make use of methods as Holt's Exponential Smoothing (HES) <ref type="bibr" target="#b14">[15]</ref>.</p><p>The HES model structure, defined by Eqs. 6 and 7 <ref type="bibr" target="#b21">[22]</ref>, is similar to the SES method. However, besides to use the parameter α to soften the level component, the algorithm uses a second smoothing constant (β) for modeling the time series trend.</p><formula xml:id="formula_5">L t = αz t + (1 -α)(L t-1 + T t-1 )<label>(6)</label></formula><formula xml:id="formula_6">T t = β(L t -L t-1 ) + (1 -β)T t-1<label>(7)</label></formula><formula xml:id="formula_7">z t+h = L t + hT t<label>(8)</label></formula><p>The smoothing constants values α and β lie in the range [0, 1], and Eqs. 6 and 7 estimate the level and trend components, respectively. These equations, as well any exponential smoothing method, modify previous estimates when a new observation is computed. Moreover, in Eq. 8, z t+h indicates the prediction value of z at time t + h, where h represents the prediction horizon.</p><p>To implement the HES algorithm recurrence relation, we need to provide its initial values. A widely accepted rule in the literature is to assume</p><formula xml:id="formula_8">L 1 = z 1 and T 1 = z 2 -z 1 .</formula><p>As the method is based on the self-learning concept, the initial values do not affect the predictions. However, this fact does not apply to the smoothing constants, which are difficult to set and bad choices may degrade the predictive performance of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Holt-Winters' Seasonal Exponential Smoothing</head><p>The HW models structure comprises three equations with distinct smoothing constants, which are related to the time series primary components. Given this design outline, such models are divided into two groups <ref type="bibr" target="#b46">[47]</ref>: (i) Multiplicative and (ii) Additive. The choice of a structure depends on the seasonal pattern of the investigated series.</p><p>We can use the Multiplicative model of HW (MHW) to adjust time series in which the amplitude of the seasonal variation rises with the increase of the series average level. The algorithm employs the following equations <ref type="bibr" target="#b46">[47]</ref>:</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T L t = α z t S t-s + (1 -α)(L t-1 + T t-1 )<label>(9)</label></formula><formula xml:id="formula_10">T t = β(L t -L t-1 ) + (1 -β)T t-1<label>(10)</label></formula><formula xml:id="formula_11">S t = γ z t L t + (1 -γ)S t-s<label>(11)</label></formula><formula xml:id="formula_12">z t+h = (L t + hT t )S t-s+h<label>(12)</label></formula><p>In these equations, α, β and γ are smoothing constants whose values lie in the range [0, 1], s indicates the number of observations that make up a seasonal variation, and z t+h represents the prediction value z for the period t + h.</p><p>Analogously to SES and HES methods, the MHW algorithm receives as input a time series and recursively applies the three described equations. Such application should be started at some time in the past, where the values of L, T and S was previously estimated. A simple way to obtain this approximation is through the initialization of level and trend in the same period s. Thus, the level can be determined from the average of the first station (Eq. 13).</p><formula xml:id="formula_13">L s = 1 s (z 1 + z 2 + . . . + z s )<label>(13)</label></formula><p>The trend can be initialized using two complete stations, as defined by Eq. 14.</p><formula xml:id="formula_14">T s = 1 s z s+1 -z 1 s + z s+2 -z 2 s + . . . + z s+s -z s s<label>(14)</label></formula><p>The initial seasonal indexes can be computed by the ratio between the first observations and the average of the first period, as shown in Eq. 15.</p><formula xml:id="formula_15">S 1 = z 1 L s , S 2 = z 2 L s , . . . , S s = z s L s<label>(15)</label></formula><p>The Additive model of HW (AHW) has greater explanatory power in series where the difference between the highest and the lowest demand value within the stations remains constant over time. The algorithm that implements this model uses the following equations:</p><formula xml:id="formula_16">L t = α(z t -S t-s ) + (1 -α)(L t-1 + T t-1 )<label>(16)</label></formula><formula xml:id="formula_17">T t = β(L t -L t-1 ) + (1 -β)T t-1<label>(17)</label></formula><formula xml:id="formula_18">S t = γ(z t -L t ) + (1 -γ)S t-s<label>(18)</label></formula><formula xml:id="formula_19">z t+h = L t + hT t + S t-s+h<label>(19)</label></formula><p>Eq. 10 of MHW is identical to Eq. 17 of AHW. The difference lies in the use of the other equations, in which the seasonal indexes are summed and subtracted, rather than multiplied and divided as in the multiplicative model.</p><p>The variables L and T are commonly initialized by applying the same equations of MHW. The seasonal indexes are calculated according to Eq. 20.</p><formula xml:id="formula_20">S 1 = z 1 -L s , S 2 = z 2 -L s , . . . , S s = z s -L s<label>(20)</label></formula><p>The correct choice of HW models is associated with the morphology of the seasonal variations in the time series, regardless of the existence of the trend component. In these models, when γ = 0 does not mean that the series is devoid of seasonality, but that seasonal rates have been initialized with values that do not need to be fixed along the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_21">M A N U S C R I P T 4.1.5.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARIMA and SARIMA Models</head><p>The ARIMA models of order (p, d, q), i.e. ARIMA(p, d, q), result from the combination of three procedures <ref type="bibr" target="#b4">[5]</ref>: (i) Autoregression (AR(p)), (ii) integration<ref type="foot" target="#foot_0">2</ref> , and (iii) Moving Averages (MA(q)). The simultaneous use of these three components is not a rule to model time series with absence of seasonal patterns, once they can be executed in a conjugated way, i.e., one complementing the other. By this perspective and as the integration procedure can be performed in a preprocessing step, the ARIMA nomenclature is also used to refer to the following structures: ARIM A(p, 0, 0) = AR(p); ARIM A(0, 0, q) = M A(q); and ARIM A(p, 0, q) = ARM A(p, q).</p><p>The main benefit of ARMA is that, to adjust its structure to complex stationary time series, it often uses a number of terms lower than required by models purely AR or purely MA. When a time series is non-stationary, it can be transformed using a data differentiation procedure which ensures such property. This procedure when added to the ARMA structure results in the ARIMA model with order (p, d, q), ARIMA(p, d, q), defined by Eq. 21.</p><formula xml:id="formula_22">I t = δ + p i=1 φ i I t-i + q i=1 θ i e t-i + e t<label>(21)</label></formula><p>In Eq. 21,</p><formula xml:id="formula_23">I t = ∆ d z t = ∆(∆ d-1 z t )</formula><p>and d indicates the difference operator degree; φ p and θ q are, in this order, the parameters of the procedures: autoregressive, with lag length p, and MA, with lag length q; δ reflects the initial level of the model (performs the same function as the intercept in linear regression) and is calculated according to Eq. 22, where µ represents the stationary process average; and e t is the white noise in a distribution with zero average and constant variance σ 2 e . Besides, for each time instant t, it is assumed that e t is independent of the time series past values (z t-1 , z t-2 , . . ., z t-m+1 ).</p><formula xml:id="formula_24">δ = µ(1 -φ 1 -φ 2 -• • • -φ p )<label>(22)</label></formula><p>The constant δ may be omitted in the ARIMA model when the time series under study is non-stationary by nature and, consequently, it was differentiated for obtaining stationarity (d &gt; 1). If the series is stationary in its original form (d = 0), but not with zero average and unit standard deviation, the constant is required. In addition, when the model is devoid of the autoregressive part (AR(p)), it is assumed that the constant is equal to the time series average (δ = µ) <ref type="bibr" target="#b30">[31]</ref>.</p><p>ARIMA can model homogeneous non-stationary series, i.e., time series with a non-explosive trend, as well as stationary series. This model exploits the autocorrelation between the time series values at successive instants, but when the data are observed in periods of less than one year, the series may also have autocorrelation for a seasonal station s. In this context, the seasonal ARIMA models, also known as SARIMA, have in its structure a non-seasonal part (Eq. 21), with parameters (p, d, q), and a seasonal part (Eq. 23), with parameters (P, D, Q) s .</p><formula xml:id="formula_25">I t = δ + P i=1 Φ is I t-is + Q i=1 Θ is e t-is + e t<label>(23)</label></formula><p>In Eq. 23,</p><formula xml:id="formula_26">I t = ∆ D z t = ∆(∆ D-1 z t )</formula><p>and D indicates the degree of the seasonal difference operator; the constant δ is computed according to Eq. 24 and its use follows the same rules as those imposed on the ARIMA structure, but now considering D; Φ P e Θ Q are, in this order, the parameters of the procedures seasonal autoregressive, with lag length P , and of the seasonal MA, with lag length Q; and e t is the white noise that cannot be explained by the model.</p><formula xml:id="formula_27">δ = µ(1 -Φ 1 -Φ 2 -• • • -Φ p )<label>(24)</label></formula><p>The SARIMA(p, d, q)×(P, D, Q) s is denoted by Eq. 25, where the non-seasonal and seasonal parts are summed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><formula xml:id="formula_28">I t = δ + p i=1 φ i I t-i + q i=1 θ i e t-i + P i=1 Φ is I t-is + Q i=1 Θ is e t-is + e t<label>(25)</label></formula><p>In SARIMA models, δ is calculated by Eq. 26 and it can be omitted when d + D &gt; 1. Otherwise, if d + D ≤ 1, the use of δ is required. When the model is devoid of the autoregressive and seasonal autoregressive parts, we must assume that the constant is equal to the time series average (δ = µ) <ref type="bibr" target="#b30">[31]</ref>.</p><formula xml:id="formula_29">δ = µ(1 -φ 1 -φ 2 -• • • -φ p )(1 -Φ 1 -Φ 2 -• • • -Φ p ) (<label>26</label></formula><formula xml:id="formula_30">)</formula><p>The application of SARIMA is especially appropriate when the data have seasonal variations that are not adequately addressed by the first difference (∆z t = z tz t-1 ). A typical example is a time series that describes monthly data. In this series, a dependency between the observations z t and z t-12 is likely to be found.</p><p>The design of an ARIMA or SARIMA model is guided based on the iterative cycle of Box-Jenkins <ref type="bibr" target="#b4">[5]</ref>. This method enables the identification of the stochastic process of data generation and related parameters. The four steps of the iterative cycle are outlined in Fig. <ref type="figure" target="#fig_6">6</ref> and detailed as follow. Therefore, when the series comprises trend, it is recommended to use an ARIMA structure. On the other hand, if the series presents both components of trend and seasonally, it is suggested to adopt a SARIMA structure; 2. Identification of Model Orders: The values of p, d, and q of ARIMA(p, d, q), or the values of p, d, q, P , D, and Q of SARIMA(p, d, q)×(P, D, Q) s , are set with the aid of correlograms or information criteria. Initially, iterative mechanism counts the number of integrations (d or D), in which the data sequence is differentiated as many times as necessary until its variance is less than the variance computed for its original version (without differentiation). Afterward, the analyst must inspect the functions of sample autocorrelation and sample partial autocorrelation of the differentiated time series in the lags 1, 2, 3, . . . to obtain the value of p and q, and in the lags s, s × 2, s × 3, . . . to obtain the value of P and Q. An alternative way to find these parameters values lies in the application of information criteria. The Akaike Information Criterion (AIC), expressed by Eq. 27 <ref type="bibr" target="#b30">[31]</ref>, penalizes the adjustment quality of models with many parameters.</p><formula xml:id="formula_31">AIC = -2 × LL + (log(n) + 1) × N P (<label>27</label></formula><formula xml:id="formula_32">)</formula><p>where LL is the likelihood function logarithm, n refers to the number of observations of the training series, and N P comprises the number of parameters. The idea is to select the model that achieves the lower AIC. Obviously, a model with more parameters may have a better fit, but not necessarily will be preferred regarding AIC; 3. Estimation of Model Coefficients: The preliminary estimates of φ p and Φ P , of the autoregressive component, and θ q and Θ Q , of the MA component, can be obtained using the autocorrelations of the time series integrated into the model identification step. After the assignment of initial values, the coefficients are estimated by maximizing the likelihood function. As the least squares estimators</p><formula xml:id="formula_33">A C C E P T E D M A N U S C R I P T</formula><p>can approximate the maximum likelihood estimators, the said function is typically maximized by nonlinear least squares using the Levenberg-Marquardt algorithm; 4. Diagnosis of the Fitted Model: The model identified as the most promising is examined to ensure that the data dynamic was satisfactorily represented. In practice, the estimates of errors (residues) are analyzed by residual autocorrelation tests. These tests are intended to verify if the residues present white noise behavior, i.e. if their autocorrelations behave randomly and are not significant.</p><p>In the affirmative case, we can extrapolate the model to future times. In the negative case, it will be necessary to select another model and repeat the identification, estimation, and diagnosis steps.</p><p>The employment of ARIMA models requires expertise both in the application domain and in the computational mathematics. Moreover, the analyst perception and experience are essential for the modeling process becomes more practical and less expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Non-parametric Methods</head><p>Machine learning prediction methods, as opposed to statistical models, describe the data properties without the prior knowledge of their distribution. Because they do not depend explicitly on parameters to model the phenomenon's behavior, these methods are simpler to adjust and show reliable performance even when applied to complex and highly nonlinear series. We can divide the machine learning predictors into two approaches <ref type="bibr" target="#b23">[24]</ref>: (i) global and (ii) local.</p><p>In the global approach, the machine learning methods consider all observations of the training series to build a model. It normally involves the transposition of the data sequence into an attribute-value table which is used as input to machine learning regression algorithms. Fig. <ref type="figure">7</ref> shows how to employ this approach.  In Fig. <ref type="figure">7</ref>, the transposition procedure of the sequence Z with size m = 15 into the attribute-value format is obtained sweeping a sliding window of length l = 3. This window is iteratively shifted on the time series in order to collect all the subsequences formed by l consecutive observations. Each extracted subsequence refers to a pair (X i , y i ) where: X i = (x i1 , x i2 , x il ) and corresponds to the temporal pattern of length l; and y i indicates the subsequent value to X i , observed at the instant l + 1. The set of pairs (X ij , y ij ), where j ∈ [1, ml], constitutes the attribute-value table. The idea behind this conversion is to use observations from the past to predict an observation in the future.</p><formula xml:id="formula_34">h = 4 z z 1 z 2 z 3 z 4 z</formula><p>Assuming a prediction horizon h = 4, the data of the table are partitioned into two sets: (i) training set, used to build the model; and (ii) test set, used for model performance evaluation. The prediction accuracy is estimated by comparing the predicted values ŷk with their actual values y k , where k ∈ [1, h]. Fig. <ref type="figure" target="#fig_7">8</ref> exemplifies an application of the multi-step-ahead projection strategy, with approximate and updated iterations, to calculate the estimates ŷk .</p><p>Despite its simplicity, the global approach is not exempt from limitations. The most obvious of them is the fact that the pairs (X ij , y ij ) are considered independent and identically distributed by traditional machine learning algorithms. This assumption leads to a loss of temporal information, which implies in the performance degradation of the resulting regression model. Among the methods that apply this approach, we can cite those that consider polynomial and rational functions. Moreover, there are also those grounded in ANN <ref type="bibr" target="#b45">[46]</ref> and SVM <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The local approach comprises machine learning algorithms that have been adapted to include temporal information in the prediction process. Such methods partition the time series into subsequences whose the closest or most important values related to the current value are combined to produce the future value. These combinations are undertaken by approximation functions, such as simple local average and weighted. Examples of methods that use the local approach are variations of the k NN algorithm <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In the following subsections, we present three state-of-the-art machine learning algorithms that we consider in our experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Artificial Neural Networks</head><p>ANN are computational models inspired by the information processing performed by the human brain <ref type="bibr" target="#b18">[19]</ref>. The Perceptron, exhibited in Fig. <ref type="figure" target="#fig_8">9</ref>, is the simplest form of an ANN used for, besides other tasks, the classification of linearly separable classes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In Fig. <ref type="figure" target="#fig_8">9</ref>, the single neuron comprises l data inputs x i ∈ X. The i th element of X, eventually provided by adjacent neurons, is associated with a synaptic weight w i . This weight can assume a negative or positive value that reflects the importance of the input. The linear combination of inputs with weights, added a threshold (bias) b ∈ , results in the net value (Eq. 28). This value is sent to an activation function f that sets the output y of the neuron. </p><formula xml:id="formula_35">w i x i + b (<label>28</label></formula><formula xml:id="formula_36">)</formula><p>The bias aims to correct, increase or decrease the net value. This correction contributes to achieving a result of f (net) closest to the expected. In the model presented in <ref type="bibr" target="#b29">[30]</ref>, f corresponds to a staircase function. Regarding y, it can be binary with y ∈ {0, 1} or y ∈ {-1, 1}, as well continuous where y ∈ . Furthermore, we may apply other types of activation function.</p><p>A learning process with a finite number of iterations adjusts the synaptic weights of the Perceptron. The learning is conducted by the error correction rule known as the Perceptron convergence algorithm <ref type="bibr" target="#b26">[27]</ref>. This algorithm searches for a weight vector w, such that the two equalities of the step function are satisfied.</p><p>The proposition of the Backpropagation learning algorithm <ref type="bibr" target="#b38">[39]</ref> allowed ANN with more than two layers. One type of ANN with multiple layers, usually trained by the backpropagation algorithm, is the MLP. Fig. <ref type="figure" target="#fig_9">10</ref> shows the structure of a three-layered MLP. The output of the model outlined in Fig. <ref type="figure" target="#fig_9">10</ref>, but considering a single neuron in the output layer, can be represented as follows:</p><formula xml:id="formula_37">A C C E P T E D M A N U S C R I P T y = f n j=1 w j f l i=1 w ij x i + b 0j + b 0</formula><p>An MLP can have one or more layers of neurons between the input and output layers. These intermediate layers are units that do not interact directly with the environment and work as combiners of characteristics. If there are appropriate connections between the input units and a considerable set of intermediate units, one can always find the representation that will produce the correct mapping between the data input and the results output. Although MLP models are more difficult to interpret, their main advantages are the capacity to deal with large volumes of data and proper generalization.</p><p>The MLP accuracy is associated with three topological aspects: (i) determining the number of hidden layers; (ii) definition of the number of neurons in each layer; (iii) specification of synaptic weights that interconnect the neurons in different layers of the network. As reported in <ref type="bibr" target="#b9">[10]</ref>, to produce any mapping, at most two intermediate layers are required with a sufficient number of units per layer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>. On another hand, with only a single intermediate layer is possible to approximate any continuous function.</p><p>We can categorize the Perceptron and MLP models as feed-forward neural networks because in them the neuron-to-neuron signals flow only in one direction: from input to output. Differently, in Recurrent Neural Networks (RNN), connections between neurons form a cycle, and the signals are able to move in different directions. For example, in the Simple Recurrent Network (SRN) <ref type="bibr" target="#b12">[13]</ref>, the state of the hidden layer at a given time is conditioned on its previous state by a context layer, as illustrated in Fig. <ref type="figure" target="#fig_10">11</ref>. This recursion implies a short-term memory, which allows the network to store complex signals for arbitrarily long time periods. The ability to model temporal dependencies makes RNN architectures especially suitable for tasks as speech classification and prediction, where input and/or output covers data sequences that are dependent. Among the several improved RNN, we can cite Bidirectional RNN (BRNN) <ref type="bibr" target="#b41">[42]</ref>, and Nonlinear Autoregressive RNN with Exogenous Inputs (NARX) <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Layer Hidden Layer</head><note type="other">Context Layer Input Layer 1</note><p>In recent years, the concept of fuzzy logic and wavelets has become very attractive in the field of RNN <ref type="bibr" target="#b27">[28]</ref>. Fuzzy logic enables us to reduce the complexity of the data and to deal with uncertainty <ref type="bibr" target="#b6">[7]</ref>; wavelet transform allows us to analyze non-stationary signals to discover their local details <ref type="bibr" target="#b35">[36]</ref>; RNN has self-learning characteristic that increases the accuracy of the model. Their combination contributes to the development of models with fast learning capability that can describe nonlinear systems characterized by uncertainty.</p><p>While theoretically powerful, the recurrent models aforementioned were widely considered to be hard to train due to the so-called vanishing and exploding gradient problems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. A popular solution to deal with this issue is adopting gated architectures, like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which were specifically designed to allow the network to learn much longer-range dependencies <ref type="bibr" target="#b20">[21]</ref>.</p><p>Although LSTM is responsible for several models considered state-of-the-art in the literature, its performance depends heavily on the amount of data available. Besides, the parameterization of LSTM-based networks is still very complex and expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Support Vector Machines</head><p>SVM constitutes a machine learning technique based on the statistical learning theory <ref type="bibr" target="#b44">[45]</ref> with the ability to solve distinct problems as classification and regression <ref type="bibr" target="#b16">[17]</ref>.</p><p>Although SVM models have a similar structure to ANN, they differ in how the learning is conducted. While ANN work by minimizing the empirical risk, i.e., the error minimization of the induced model on the training data, SVM are grounded on the principle of structural risk minimization, which seeks the</p><formula xml:id="formula_38">A C C E P T E D M A N U S C R I P T</formula><p>lowest training error while minimizing an upper bound on the generalization error of the model (model error when applied to test data) <ref type="bibr" target="#b44">[45]</ref>.</p><p>The generalization concept is best understood in the case of binary classification. Thus, given a set of points belonging two classes, an SVM determines the hyperplane that separates them placing the largest possible number of points of a class on the same side, while the distance from each class to the decision surface is maximized. Fig. <ref type="figure" target="#fig_2">12</ref> shows a set of straight lines that discriminate the data into two classes. Between these straight lines, only one maximizes the separation margin (distance between the hyperplane and the nearest sample of each class). The straight line with maximum margin, called optimal separation hyperplane, is the object to be searched during the model training. The technique indicated in Fig. <ref type="figure" target="#fig_2">12</ref> is restricted to linearly separable problems. However, in situations where the samples are not linearly separable, the solution focuses on mapping the input data to a higherdimensional space (feature space). We can achieve this mapping using a kernel function.</p><p>Linear, polynomial, and Radial Basis Function (RBF) kernels are the most applied in practice. Each one of them covers parameters that need to be set. For example, the SVM model with RBF kernel requires two parameters <ref type="bibr" target="#b16">[17]</ref>: (i) C, which is a regularization term that imposes a weight on the training set errors minimization regarding the model complexity minimization; and (ii) σ, that reflects the Gaussian's width of the kernel function. The number of radial functions and their respective centers is determined by the support vectors found.</p><p>The construction of an SVM implies in solving a quadratic problem with linear constraints which depends on the set of input data, parameters, and of the separation margin. During the training phase, the Lagrange multipliers that characterize the support vectors are obtained. These support vectors define the edges of the optimal separation hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">k-Nearest Neighbors</head><p>Similarity-based methods, like the k NN classifier, are characterized by not constructing a model that explicitly describes the training dataset behavior. The model is built by simply storing the data sample. The generalization on the training set is performed every time we asked the algorithm for a new classification.</p><p>The general idea behind the adaptation of k NN for time series prediction is very intuitive. Given a series Z = (z 1 , z 2 , . . . , z m ) in which z t ∈ , the problem is to predict the value z m+h , where h is the prediction horizon. For simplicity, but without loss of generality, the idea will be discussed in a unitary horizon (h = 1), i.e., considering only the prediction of the next value (z m+1 ).</p><p>The modified method uses the last l observations as query Q, and searches for the k most similar subsequences to Q, using a sliding window of size l. Given S l+1 are provided as input to a prediction function f to approximate the value of z m+1 . Eq. 29 is an example of ensemble function.</p><p>In Eq. 29, the prediction function f has the argument S that denotes the set of k most similar subsequences, and S (j) refers to j th nearest neighbor. This is the simplest way of combining the projections since the predictions average considers that all projected values are equally probable to occur in the future.  25 taken as reference query; the blue dotted lines express the most similar subsequences found using some similarity measure, in this case, the Euclidean distance; the blue circles correspond to the observations used for making the prediction; and the red square reflects the value to be predicted. The prediction made by similarity-based methods considers only the previous l observations. Thus, the temporal dependence is restricted to a limited number of previous observations, since usually a certain value is not influenced by observations that happened a long time ago.</p><note type="other">𝑆 𝑙+1 (𝑗) 𝑧 𝑚+1</note><p>Several surveys were conducted to analyze the performance of k NN with different ensemble functions and distance measures. Moreover, a few papers showed that these methods are useful to predict highly nonlinear and complex time series patterns <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref>. A recent study proposed a novel and promising modification of the k NN algorithm for time series prediction, namely k NN -Time Series Prediction with Invariances (k NN-TSPI) <ref type="bibr" target="#b32">[33]</ref>. This proposal differs from the literature by incorporating techniques for amplitude and offset invariance, complexity invariance, and treatment of trivial matches. According to the authors, these three invariances when combined allow more meaningful matching between the reference queries and temporal data subsequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Techniques for Parameter Estimation</head><p>One of the main difficulties faced by researchers in the time series prediction is the search for the best parameter setting to fit a model according to a dataset.</p><p>In theoretical terms, the establishment of all parameters of a model could need the full exploitation of the state space. As this procedure is impractical for most real-world datasets, search algorithms are used to find a suboptimal solution with satisfactory performance and acceptable computational cost. Next, we present the main parameter estimation methods for time series prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Holdout Validation</head><p>The holdout validation technique performs the search for parameters values that minimize the predictive model error over a different number of intervals in the training data. Algorithm 1 describes the logic of this sampling technique to find adequate values for the parameters l and k of the k NN-TSPI method.</p><p>Algorithm 1 iteratively evaluates a set of previously defined parameters. At each iteration and according to the current parameters combination, a new model is built and adjusted on the mh observations of the subsequence S, i.e., z 1 , . . . , z m-h (S ∈ Z). Afterward, the model is extrapolated to a horizon prediction h whose length is equivalent to the validation subsequence (s n-h +1 , . . . , s n ). At the end of this search, the most promising parameters (P) are those which minimize the error between the predicted and validation subsequences. Many measures can measure this error, but the most common is the Mean Square Error (MSE).</p><p>There are different ways to choose the number of observations covered by h . We have adopted h = (max p + h) ÷ 2, where h indicates the number of values to be projected in the test step by the prediction method using the best set of parameters found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Cross-validation</head><p>Cross-validation is a sampling technique broadly used for evaluating models in machine learning. For time series prediction, the technique searches for the most adequate parameter values for global approach  In Algorithm 2, the idea behind the search is similar to that observed in Algorithm 1, except by the content of the 4 th and 7 th lines. In the 4 th line occurs the transposition of the training subsequence S for the attribute-value format using a sliding window of length l, such as illustrated by the training set in Fig. <ref type="figure">7</ref>. In the 7 th line, the attribute-value table T is randomly divided into k samples (kF olds) mutually exclusive, all of approximately the same size. The k th sample is used as a validation set and the k -1 remaining samples are the training set. For each combination of k -1 samples, a model is constructed and adjusted according to the current parameters combination. The prediction error is computed on the validation set k by a loss function as MSE. Evidently, when we use the MSE, the parameterization error is given by the average of the MSE of the k generated models and is considered an estimative of the true error expected on independent data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Box-Jenkins Method</head><p>We can determine the parameters of an ARIMA model via a search mechanism guided by an information criterion that penalizes the models' adjustment with many parameters. This method is called Box-Jenkins. Algorithm 3 exemplifies the use of this method to identify all SARIMA parameters of order (p, d, q)×(P, D, Q) s .</p><p>In Algorithm 3, p, d, q, P , D, and Q are relevant time delays of the time series within a search space pre-determined by the user. The value of max p corresponds, in number of observations, a seasonal period in the series. In situations where this information is not clearly visible, we can apply the technique of scatter plot to obtain max p. Given that the constant δ represents the initial level of the model, a condition for inclusion or omission of δ was inserted respecting the differentiation rules for SARIMA in the 10 th line. In the 16 th line, the most promising parameters are chosen so to minimize the AIC (Eq. 27).</p><p>The values of d and D can be established in a preprocessing step integrating the time series until its variance becomes smaller than its original version (undifferentiated). The prior identification of these values is important to reduce the processing time of Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical Evaluation</head><p>The protocol of our performance evaluation was organized in three steps, as outlined in Fig. <ref type="figure" target="#fig_18">14</ref>.</p><p>Step 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction of Values</head><p>Step 1</p><p>Temporal Datasets  In Step 1, we have selected 95 datasets, which 40 of them are synthetic and 55 are from real domains. These data pose some problems that one usually encounters in a typical one or multi-step-ahead prediction tasks such as the growing trend, non-stationarity, outliers and multiple overlying seasonalities. The datasets and their full descriptions are available online at the ICMC-USP Time Series Prediction Repository <ref type="bibr" target="#b31">[32]</ref>. Such archive is an original contribution of this article and most of the datasets maintained by it are often reported in the literature, as demonstrated by our systematic review. However, before we created the repository, these datasets were scattered among multiple portfolios (online archives, web</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICMC-USP Time Series Prediction Repository</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>pages, supplementary materials, and books). Our idea was to centralize this data in a single repository to facilitate its recovery and usage. In what follows, we present a summary of their properties.</p><p>We designed the 40 synthetic datasets to analyze and understand the performance of algorithms over peculiar characteristics of data. We grouped these data into three categories according to its originating process: (i) deterministic; (ii) stochastic; and (iii) chaotic.</p><p>Table <ref type="table" target="#tab_5">2</ref>(a) shows the main characteristics of each synthetic dataset such as size (m), the maximum number of observations in a seasonal variation (max p), and the prediction horizon (h). In our experimental evaluation, we set the value h as 5% of the series size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>on the MSE of the predictor, normalized by the prediction error of a trivial (or naïve) model. The naïve model assumes that the best value for time t + 1 is the value obtained at time t.</p><formula xml:id="formula_39">T U = h t=1 (z t -ẑt ) 2 h t=1 (z t -z t-1 ) 2 (31) P OCID = h t=1 D t h × 100 (32)</formula><p>The values obtained by Eq. 31 can be interpreted in the following way: if TU &gt; 1, the algorithm's performance is lower than the naïve model; if TU = 1, the algorithm's performance is the same as the naïve model; if TU &lt; 1, the algorithm's performance is higher than the naïve model; and if TU ≤ 0.55, the algorithm of interest is trusted to carry out future predictions.</p><p>Another performance index considered was the POCID, which is formalized by the Eq. 32. In this equation, the term D t stores the value 1 if ( ẑtẑt-1 )(z tz t-1 ) &gt; 0, and 0 otherwise. The idea of this index is to estimate the accuracy of direction's changes of the projected data, i.e., if the future value will increase or decrease when compared to current value. We should use POCID in a complementary way to the analysis of the prediction errors. It is not advisable to make a decision based solely on POCID values.</p><p>Choosing the best model can be a difficult task when different performance measures are available. A method that comprises distinct relevant variables is a key element to workaround such problem. In this direction, we can use the Multi-Criteria Performance Measure (MCPM) developed in <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this work, we employ the MCPM to combine the MSE, TU, and POCID indexes. Differently from the error measures, which yield values that must be minimized, POCID must be maximized. Therefore, we adopt the POCID complement, also called Error Rate (ER), which is determined by ER = 100 -P OCID.  From the values of the adopted evaluation measures, it was possible to compare the investigated algorithms objectively. Friedman's non-parametric statistical test for paired data and multiple comparisons, with a significance level of 5% (p-value &lt; 0.05), followed by Nemenyi posthoc test<ref type="foot" target="#foot_1">3</ref> , was employed to compare the results.</p><p>The experimental protocol execution contemplated the use of the following programming languages: MATLAB and GNU Octave, as well as their packages of functions for time series prediction; R with Forecast package; and Java with Weka library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>We present and discuss the empirical results arranged into three large comparative studies: (i) predictive models applied to synthetic datasets -including an isolated assessment of deterministic, stochastic, and chaotic data; (ii) predictive models applied to real datasets; and (iii) predictive models applied to both</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>synthetic and real datasets. Note that the sequence of our discussion accompanies the complexity increment to model the data and the difficulty of the prediction task.</p><p>We used Critical Difference (CD) diagrams to show the statistical validation results. In these diagrams, the scale indicates the rank position of each predictor according to their average performance <ref type="bibr" target="#b11">[12]</ref>. Algorithms connected by a thick line have not presented Statistically Significant Differences (SSD) in quality. In the supplementary material or at the ICMC-USP Time Series Prediction Repository <ref type="bibr" target="#b31">[32]</ref>, we can see detailed results of MSE, TU and POCID, as well the values found in the step of parameters estimation.</p><p>In a complementary way and due to the particularities of TU and POCID indexes, they had their values summarized in full stacked area charts and bar charts with Standard Deviations (SD), respectively. We also employed a MCPM to provide an overview of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Synthetic Data</head><p>This collection of experiments covers 40 synthetic time series, of which 17 are deterministic, 15 stochastic, and 8 chaotic. In the next subsections we detail our analysis according to these categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Deterministic Time Series</head><p>The computational tests conducted using deterministic data involved 374 configurations (11 predictors × 2 projection strategies × 17 datasets). Fig. <ref type="figure" target="#fig_21">16</ref> presents the CD diagrams concerning the MSE, TU, and POCID indexes. According to the ranking shown in Fig. <ref type="figure" target="#fig_21">16</ref>, k NN-TSPI with approximate iteration was in third place considering the MSE (Fig. <ref type="figure" target="#fig_21">16(a)</ref>) and TU (Fig. <ref type="figure" target="#fig_21">16(b</ref>)) measures, losing without SSD for SARIMA and SVM. For the POCID index (Fig. <ref type="figure" target="#fig_21">16(c</ref>)), the SARIMA, k NN-TSPI and SVM methods, achieved the best results but without SSD among them.</p><p>For updated iteration, MLP presented the best MSE (Fig. <ref type="figure" target="#fig_21">16(d)</ref>) and TU (Fig. <ref type="figure" target="#fig_21">16</ref>(e)) averages. However, in terms of POCID (Fig. <ref type="figure" target="#fig_21">16(f</ref>)), such configuration was in the second position without SSD compared to SVM.</p><p>In Fig. <ref type="figure" target="#fig_22">17</ref>, we show for each method, how many datasets presented TU values in one of their four possible ranges. We can note that when using approximate iteration (Fig. <ref type="figure" target="#fig_22">17(a)</ref>), SARIMA was reliable in 14 of 17 datasets (TU ≤ 0.55). k NN-TSPI reached the second best performance, so that for 14 datasets (12 + 2) its use was preferable to the trivial model (TU &lt; 1). As expected, SES failed to beat the naïve model for no one dataset. This fact reinforces that it does not support the prediction via approximate iteration.</p><p>As for updated iteration (Fig. <ref type="figure" target="#fig_22">17(b</ref>)), MLP achieved reliable results for 17 datasets (TU ≤ 0.55). From the same point of view, Holt-Winters models (AHW and MHW) presented the second best performance among all algorithms. In other words, their use was preferable (TU &lt; 1) to the trivial model in 17 datasets (16 + 1), of which 16 provided reliable modelings (TU ≤ 0.55).</p><p>Analyzing both projection strategies in Fig. <ref type="figure" target="#fig_22">17</ref>, we can see that, in general, the k NN-TSPI performance was more stable than the other methods to predict deterministic time series.</p><p>In Fig. <ref type="figure" target="#fig_24">18</ref>, we display the average results of POCID and their respective SD for each algorithm and projection strategy. SARIMA with approximate iteration achieved an average hit rate of 98.29% (SD = 3.83%) on the projection horizons trends. In contrast, k NN-TSPI with approximate iteration reached an average hit rate of 92.99% (SD = 18.44%). Considering the projection strategy with updated iteration, the highest hit rates were obtained by MLP -99.39% (SD = 1.24%) -and SVM -99.36% (SD = 2.42%).   Fig. <ref type="figure" target="#fig_25">19</ref> shows the CD diagrams with respect to the MCPM total area values. SARIMA provided the best results when configured with approximate iteration (Fig. <ref type="figure" target="#fig_25">19(a)</ref>). On the other hand, employing updated iteration (Fig. <ref type="figure" target="#fig_25">19(b)</ref>), MLP recorded the smallest prediction errors. Nevertheless, SVM was the algorithm that obtained, for both projection strategies, the most stable performance in terms of prediction error and hit rate on the projection horizons trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Stochastic Time Series</head><p>The experiments performed from stochastic data encompassed 330 configurations (11 predictors × 2 projection strategies × 15 datasets). The CD diagrams portrayed in Fig 20 summarizes these computational tests according to MSE, TU, and POCID. ence margin. On average, LSTM with approximate iteration presented the best POCID rates (Fig. <ref type="figure" target="#fig_26">20(c</ref>)), followed by SVM.</p><p>Considering the updated iteration, SVM achieved the third best result of MSE (Fig. <ref type="figure" target="#fig_26">20(d)</ref>) and TU (Fig. <ref type="figure" target="#fig_26">20(e)</ref>). The first two ranking positions were occupied by SARIMA and ARIMA, respectively. As for the POCID values (Fig. <ref type="figure" target="#fig_26">20(f</ref>)), k NN-TSPI outperformed LSTM and SARIMA without SSD.</p><p>In Fig. <ref type="figure" target="#fig_27">21</ref>, the ranges of values derived from TU coefficient evidence that SARIMA, with approximate and updated iterations, culminated in the most promising method to predict time series with stochastic behavior.</p><p>TU &gt; 1.00 TU = 1.00 0.55 &lt; TU &lt; 1.00 TU ≤ 0.55 In Fig. <ref type="figure" target="#fig_2">22</ref>, we can note that when employing approximate iteration, the highest hit rates were reached by MHW -51.71% (SD = 5.39%) -, AHW -50.73% = 3.51%) and HES -50.65% k NN-TSPI exhibited the eighth best result, an hit rate of 47.83% (SD = 8.46%). In contrast, the updated iteration, it showed the highest average hit rate on the prediction horizons trends -49.20% (SD = 6.47%). The poorest POCID values were obtained by SES -27.94% (SD = 3.52%) -, HES -31.47% (SD = 2.41%) -, and ARIMA -37.02% (SD = 14.09%). Fig. <ref type="figure" target="#fig_3">23</ref> covers the CD diagrams designed from the MCPM total area values. According to the multicriteria analysis, SARIMA and k NN-TSPI were the best algorithms regardless of the employed projection strategy. Nevertheless, they did not present SSD regarding the ARIMA, MLP, LSTM, and SVM methods. Although k NN-TSPI has been more accurate, SARIMA provided the lowest error rates. This fact is due to SARIMA's own structure, which often includes an MA procedure that covers estimates of the innovation factor (white noise) that cannot be explained by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">Chaotic Time Series</head><p>The computational tests conducted using chaotic data involved 176 configurations (11 predictors × 2 projection strategies × 8 datasets). As depicted in the CD diagrams of Fig. <ref type="figure" target="#fig_29">24</ref>, the algorithms with approximate iteration MA, SVM, SES and LSTM showed, in increasing order and without SSD, the best results of MSE (Fig. <ref type="figure" target="#fig_29">24(a)</ref>) and TU (Fig. <ref type="figure" target="#fig_29">24(b)</ref>). The models with approximate iteration SVM, LSTM, and MLP assumed respectively the first, second, and third positions in the POCID average ranking (Fig. <ref type="figure" target="#fig_29">24(c)</ref>).</p><p>Concerning the updated iteration, MLP and SVM achieved the best results in the three performance measures (Fig. <ref type="figure" target="#fig_29">24 -(d)</ref>, (e), and (f)). On the other hand, MA exhibited the highest prediction errors and the lowest hit rates on the projection horizons trends.  In Fig. <ref type="figure" target="#fig_30">25</ref>, the statistics derived from TU coefficient show that, using the approximate iteration (Fig. <ref type="figure" target="#fig_30">25(a)</ref>), the predictive models were convenient to predict, approximately, two of eight datasets (TU &lt; 1). In this scenario, HES, AHW and MLP obtained, for all datasets, a performance lower than the trivial model (TU &gt; 1). Such results reinforce the difficulty in predicting chaotic time series, especially when the employed projection strategy favors the error propagation along the prediction horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head><p>TU &gt; 1.00 TU = 1.00 0.55 &lt; TU &lt; 1.00 TU ≤ 0.55 Examining the updated iteration in Fig. <ref type="figure" target="#fig_30">25</ref>(b), we can see that, in general, MLP, SVM and k NN-TSPI were reliable for modeling seven of eight datasets (TU ≤ 0.55). We expected such results since machine learning models are non-parametric. Although this premise is also valid for LSTM, we tend to have overfitting with such a network since it has many parameters and our datasets are not so large.</p><p>In Fig. <ref type="figure" target="#fig_6">26</ref>, we expose the POCID averages and their respective SD. Analyzing the projection strategy with approximate iteration, SVM presented the best POCID values -72.90% (SD = 14.47%) -, while SES -0.98% (SD = 1.62%) -and MA -33.76% (SD = 20.85%) -showed the poorest hit rates on the projection horizons trends. As for the projection strategy with updated iteration, the best POCID values were achieved by MLP -97.41% (SD = 3.30%) -, followed by SVM -95.93% (SD = 9.93%). The MA model maintained the poorest overall performance -67.78% (SD = 31.16%). Fig. <ref type="figure" target="#fig_2">27</ref> illustrates the CD diagrams with respect to the MCPM total area values. Inspecting both projection strategies, we can note that SVM and k NN-TSPI were more stable than the other methods for predicting chaotic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4.">Overall Comparison</head><p>After analyzing and discussing the results according to the different characteristics of the data, we are now in a position to interpreting the outcomes considering all the synthetic data. The experiments performed using all synthetic time series encompassed 880 configurations (11 predictors × 2 projection strategies × 40 datasets). The CD diagrams of Fig. <ref type="figure" target="#fig_7">28</ref>  In agreement with these diagrams, SARIMA with approximate iteration showed the best values of MSE (Fig. <ref type="figure" target="#fig_7">28(a)</ref>) and TU (Fig. <ref type="figure" target="#fig_7">28(b)</ref>). Differently, when we examined the hit rates on the projection horizons trends (Fig. <ref type="figure" target="#fig_7">28(c</ref>)), such configuration occupied the second position without SSD in comparison with SVM. Considering the updated iteration, SVM, MLP and k NN-TSPI assumed, in this order and by a small difference margin, the first, second and third positions in the rankings derived from MSE (Fig. <ref type="figure" target="#fig_7">28(d)</ref>), TU (Fig. <ref type="figure" target="#fig_7">28(e</ref>)), and POCID (Fig. <ref type="figure" target="#fig_7">28(f)</ref>).</p><p>Looking at both projection strategies in Fig. <ref type="figure" target="#fig_7">28</ref>, k NN-TSPI exhibited the third best performance for all evaluation measures, except when treated of the POCID index for the projection strategy with updated iteration. Besides, SVM was very competitive regarding the results obtained from SARIMA.  Analyzing the approximate iteration (Fig. <ref type="figure" target="#fig_33">29(a)</ref>), SARIMA was adequate in 27 (25 + 2) datasets (TU &lt; 1), which 25 were very well modeled (TU ≤ 0.55). In contrast, HES failed to outperform the naïve method for no one of the 40 datasets. Considering the updated iteration (Fig. <ref type="figure" target="#fig_33">29(b</ref>)), the use of MLP, SVM and k NN-TSPI was appropriate for 39 datasets (TU &lt; 1), which 26 resulted in reliable predictive models (TU ≤ 0.55).</p><p>Fig. <ref type="figure" target="#fig_3">30</ref> demonstrates that the machine learning methods MLP, SVM, and k NN-TSPI were able to achieve approximately 73.55% of precision (SD = 26.19%) on the projection horizons trends, regardless of the projection strategy adopted. Fig. <ref type="figure" target="#fig_3">31</ref> contemplates the CD diagrams concerning the MCPM values considering all synthetic datasets. We can see that k NN-TSPI with approximate iteration achieved the best multi-criteria results (Fig. <ref type="figure" target="#fig_3">31(a)</ref>), surpassing by a minimum difference SVM, SARIMA, MLP, and LSTM. On the other hand, SVM with updated iteration showed the smallest prediction errors and the highest hit rates on the projection horizons trends (Fig. <ref type="figure" target="#fig_3">31(b</ref>)), being very competitive with MLP and k NN-TSPI. In relation to stability, the machine learning algorithms outperformed the state-of-the-art statistical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Real Data</head><p>The computational tests conducted using real data totaled 1,210 configurations (11 predictors × 2 projection strategies × 55 datasets). Fig. <ref type="figure" target="#fig_3">32</ref>  In Fig. <ref type="figure" target="#fig_3">32</ref>, we can note that SARIMA achieved the best results for both projection strategies given the three evaluated measures. In general, models based on simple exponential smoothing obtained the worst results, being that the MA method maintained the poorest overall performance. We did not verify SSD between the machine learning algorithms concerning the ARIMA and SARIMA models.</p><p>The four ranges of TU values indicated in Fig. <ref type="figure" target="#fig_3">33</ref> show that, using approximate iteration (Fig. <ref type="figure" target="#fig_3">33(a)</ref>), SARIMA was adequate to predict 30 (20 + 10) of 55 datasets (TU &lt; 1). The SVM algorithm was suitable for modeling 27 <ref type="bibr">(11 + 16)</ref> time series, while the ARIMA and k NN-TSPI methods were preferable to the trivial model for 22 (12 + 10) of 55 datasets.</p><p>For the updated iteration (Fig. <ref type="figure" target="#fig_3">33(b</ref>)), SARIMA was convenient (TU &lt; 1) to predict 43 (22 + 21) datasets. Over this total, 22 provided a solid modeling which is reliable to make future projections (TU ≤ 0.55). SVM and k NN-TSPI were favorable (TU &lt; 1) to predict the values of 42 (18 + 24) and 38 <ref type="bibr">(16 + 22)</ref> datasets, respectively.</p><p>In Fig. <ref type="figure" target="#fig_4">34</ref> we display the POCID results for real time series. We can see that when using approximate iteration, k NN-TSPI obtained an average hit rate on the prediction horizons trends equivalent to 61.86% (SD = 18.83%), competing with SARIMA -61.64% (SD = 26.29%) -and MLP -58.07% (SD = 18.04%).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Overall Comparison</head><p>Our last comparison comprises all the 95 datasets, i.e., involves synthetic and real datasets with high variability in their characteristics. We evaluated a total of 2,090 configurations (11 predictors × 2 projection strategies × 95 datasets). The CD diagrams of Fig. <ref type="figure" target="#fig_6">36</ref> summarizes such results according to the MSE, TU, and POCID measures. In this figure, it is possible to notice how competitive the machine learning methods were in relation to the ARIMA and SARIMA state-of-the-art models. In Fig. <ref type="figure" target="#fig_3">37</ref>, the statistics derived from TU values show that, applying the approximate iteration (Fig. <ref type="figure" target="#fig_3">37(a)</ref>), SARIMA was preferable to the naïve model in 57 (45 + 12) of 95 datasets (TU &lt; 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The performance of SVM and k NN-TSPI were better than the trivial model (TU &lt; 1) in 48 <ref type="bibr">(25 + 23)</ref> and 46 <ref type="bibr">(26 + 20)</ref> datasets, respectively. For the updated iteration (Fig. <ref type="figure" target="#fig_3">37(b)</ref>), SVM and k NN-TSPI were the algorithms that demonstrated the smallest projection errors for approximately 80 datasets (TU &lt; 1).</p><p>TU &gt; 1.00 TU = 1.00 0.55 &lt; TU &lt; 1.00 TU ≤ 0.55 Fig. <ref type="figure" target="#fig_8">39</ref> illustrates the CD diagrams with respect to the MCPM total area values for synthetic and real data. The outcomes of this multi-criteria analysis indicate that SARIMA, MLP, SVM, and k NN-TSPI are the most promising methods for time series modeling and prediction. Such fact is in line with our initial hypothesis, i.e., that machine learning algorithms offer results similar to or better than those reached by state-of-the-art statistical methods with fewer parameters and without the requirement of a priori knowledge of data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations, Recommendations, and Practical Implications of the Outcomes</head><p>We limited our study to predictors that deal with univariate data. Such methods receive as input unidimensional time series without considering possible explanatory variables. In future works, we want to explore the multivariate scenario which still constitutes an important gap in the literature.</p><p>Table <ref type="table" target="#tab_10">4</ref> summarizes the main characteristics of the statistical and machine learning models discussed and evaluated in this paper. The table exhibits the following information: the algorithm's name, the prediction approach, the number of parameters (#P) required by each predictor, the sampling method for the parameter estimation, if the algorithm supports the multi-step-ahead projection strategy (h &gt; 1), and if the method is prepared to deal with non-stationarity, trend (T ), and seasonality (S ). Note that the computational complexity is empirically related to the number of parameters of a model. Thus, SARIMA is the most expensive and complex among the evaluated methods.  interesting tool to guide practitioners and researchers to choose the most promising method according to the data characteristics and the type of multi-step-ahead projection strategy desired. The multi-step-ahead projection with approximate iteration faces some difficulties, such as reduced performance and increase uncertainty given the error accumulation. These problems become more visible as the prediction horizon grows and the predicted values are not replaced by the actual values observed. However, in most of the real applications, it is the only strategy feasible to predict large horizons.</p><p>In agreement with the aforementioned projection strategy, SARIMA is the candidate to the most promising method to model and predict deterministic time series (Fig. <ref type="figure" target="#fig_38">40</ref>(a.1)), followed by k NN-TSPI, SVM, and MLP. These four algorithms also obtained the highest POCID values. Machine learning models have an advantage over more complex methods because the use of default parameters can generate suitable predictive models. This would not be possible for methods like SARIMA and ARIMA.</p><p>Since k NN-TSPI and SARIMA presented very close results, they are candidates to the most appropriate model for stochastic time series (Fig. <ref type="figure" target="#fig_39">40(a.</ref>2)). Coincidentally, AHW and MHW achieved the highest POCID values. Also, MHW did not present SSD compared to k NN-TSPI, SARIMA, SVM, LSTM, MLP, MA, and ARIMA. It is important to highlight that ARIMA and SARIMA usually outperform exponential smoothing algorithms when the time series are relatively long and well-behaved. If the data present many irregularities, the results of ARIMA and SARIMA will be lower than those obtained by exponential smoothing models.</p><p>SVM was the more stable method for chaotic series (Fig. <ref type="figure" target="#fig_38">40</ref>(a.3)). Note that SVM did not show SSD when compared with k NN-TSPI, reinforcing the competitiveness of similarity-based algorithms. This fact becomes even more evident in the evaluation with 40 synthetic datasets (Fig. <ref type="figure" target="#fig_38">40</ref>(a.4)): k NN-TSPI exhibited the best multi-criteria performance, remaining without SSD concerning SVM, SARIMA, MLP, LSTM, and MA. If on the one hand, LSTM performs well when there are lots of training data compared to the number of weights to be learned, on the other hand, we do not recommend its use in scenarios with little data because of the high risk of overfitting. SARIMA and SVM, followed by k NN-TSPI, LSTM and MA, demonstrated the best results on real datasets, as we can see in Fig. <ref type="figure" target="#fig_38">40</ref>(a.5). k NN-TSPI is the best candidate to model and predict real time series, given it obtained POCID hit rates greater than SARIMA and SVM. with trend and seasonality, show reasonable results concerning the projections horizons trends. In respect to topline models, we emphasize the performance of SARIMA, SVM, and k NN-TSPI.</p><p>We are certain that the study of machine learning methods is right now at the same maturity stage as temporal data modeling researchers using statistical models. From the practical point of view, the results put forward in this work will serve as a reference for the advance of time series prediction field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>The comprehensive review performed in this work allowed us to identify an important gap in the literature: the lack of an objective comparison between parametric and non-parametric models for time series prediction. In this sense, to present a relevant contribution to the areas of statistics and machine learning, this paper investigated several methods from both fields for univariate time series prediction.</p><p>Our study focused on the presentation of the most popular predictive algorithms and their confrontation considering 95 datasets. To the best of our knowledge, we not only conducted one of the most extensive experimental evaluations ever done for time series prediction, but we also carry out a broad analysis of 2,090 results.</p><p>We detailed discuss all procedures related to predictive model induction and evaluation, from its conception to its extrapolation. The empirical assessment carry out sought to meet two goals: (i) to make feasible the consolidation of the efficiency and effectiveness of each method analyzed; and (ii) to characterize the different algorithms for model building, describing the advantages and disadvantages of the use of each one.</p><p>Besides recommendations of predictors to be used as baseline and topline, we also highlighted the benefits of our experimental protocol and how it can be used as a reference for models selection, parameters setting, and the employment of statistical and machine learning methods for time series prediction.</p><p>The datasets considered in this work, as well as the estimated parameters and the results achieved, are available at the ICMC-USP Time Series Prediction Repository <ref type="bibr" target="#b31">[32]</ref>. We developed this online archive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>to provide open access to all the materials employed and produced in this study. Such repository is an initiative that aims to encourage the reproduction of our results and which will support more rigorous evaluations of new prediction algorithms.</p><p>The outcomes from the overall comparison that covered 95 datasets (40 synthetic time series and 55 real time series) indicate that SARIMA, SVM and k NN-TSPI are the most promising methods for temporal data modeling and prediction. Such fact is in line with our initial hypothesis, i.e., that machine learning algorithms offer results similar to or better than those reached by state-of-the-art statistical methods. The similarity-based algorithm with invariances, proposed by us in <ref type="bibr" target="#b32">[33]</ref>, still contemplates the advantages of being easy to explain, adjust, and embed into any device.</p><p>As future works, we intend to explore the properties inherent to similarity-based time series prediction, such as invariances to distortions in temporal data, distance measures, complexity estimates applied to complexity-invariant distances, and prediction functions. An empirical analysis of these particularities will allow a better understanding of the predictive performance of similarity-based methods and the conclusions drawn may guide future research with the k NN-TSPI algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of papers published by year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) portrays the frequency in A C C E P T E D M A N U S C R I P Twhich the most popular methods appeared in the 117 publications. Fig.2(c) graphically summarizes the algorithms most used as baselines in 29 empirical studies involving both statistical and machine learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Summary of the meta-analysis results. The acronyms are: Artificial Neural Networks (ANN), ARIMA models -Autoregressive Integrated Moving Average (ARIMA) or Seasonal ARIMA (SARIMA) -, Support Vector Machines (SVM), k -Nearest Neighbors (k NN), Fuzzy Logic (FL), Deep Learning (DL), Bayesian Neural Networks (BNN), Simple Exponential Smoothing (SES), Wavelet Transform (WT), Holt-Winters (HW) models, and Gaussian Process (GP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Components extracted from the monthly chocolate production time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time series prediction process</figDesc><graphic coords="8,165.39,504.20,103.07,54.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Hierarchy of approaches for time series prediction</figDesc><graphic coords="8,346.46,387.10,191.45,171.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Activity flow diagram for building an ARIMA or SARIMA model</figDesc><graphic coords="13,161.35,267.76,248.22,116.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Representation of the multi-step-ahead prediction strategy for the global approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Structure of Perceptron</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Structure of MLP with a single hidden layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Structure of SRN. Recurrent link between hidden and context layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 Figure 12 :</head><label>112</label><figDesc>Figure 12: Optimal separation hyperplane and its supporting hyperplanes. The ordered axes x1 and x2 represent the dimensions of the samples in a 2D space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>l as the k most similar subsequences of Q, the algorithm uses the next observations of each subsequence S (j) l+1 with 1 ≤ j ≤ k to predict z m+1 . Thereby, the values of S (j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13</head><label>13</label><figDesc>displays an application of the described method with l = 25 and k = 3. The dotted line in gray represents the observations that belong to the time series; the green line indicates the subsequence of length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 : 3 f</head><label>133</label><figDesc>Figure 13: An example that illustrates the k NN algorithm for time series prediction with l = 25 and k = 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>5 forC 8 if error &lt; min error then 9 min error = error; 10 l best = l; 11 C 12 σ 17 P</head><label>58910111217</label><figDesc>← 0 : 0.25 : 1 do 6 for σ ← 0.005 : 0.05 : 0.25 do 7 error = cross validation(T, kF olds, C, σ, model = "SVM"); best = C; best = σ; ← {l best , C best , σ best }; 18 return P; 19 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Algorithm 3 : 2 best aic = ∞; 3 n 13 f 15 f 27 P</head><label>323131527</label><figDesc>Box-Jenkins Method // S represents a training subsequence /* max ord1 e max ord2 are three-position vectors whose values indicate the maximum lag lengths of the non-seasonal and seasonal parts, respectively */ /* max p is an upper bound for the number of observations constituting a seasonal station in the historical series */ /* P comprises the parameters list which resulted in the least prediction error */ Input: S, max ord1, max ord2, max p Output: P 1 begin = length(S); 4 for p ← 0 : max ord1[1] do 5 for d ← 0 : max ord1[2] do 6 for q ← 0 : max ord1[3] do 7 for P ← 0 : max ord2[1] do 8 for D ← 0 : max ord2[2] do 9 for Q ← 0 : max ord2[3] do 10 if d + D ≤ 1 then 11 f it = sarima(S, [p, d, q], [P, D, Q], max p, δ = TRUE); 12 else it = sarima(S, [p, d, q], [P, D, Q], max p, δ = FALSE); 14 end it aic = -2 * f it.loglik + (log(n) + 1) * length(f it.coef );16 if f it aic &lt; best aic then 17 best aic ← f it aic; 18 best f it ← f it; 19 best model ← [p, d, q, P, D, Q, max p]; ← {best aic, best f it, best model}; 28 return P; 29 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Experimental setup</figDesc><graphic coords="20,208.94,541.77,210.01,103.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig. 15 illustrates how to calculate the MCPM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Multi-Criteria Performance MeasureA radar chart consisting of three axes, each one representing an individual performance measure, is illustrated in Fig.15. The final value of MCPM is achieved by the sum of the total area of each triangle. Lower values of MCPM correspond to a better predictive performance for an algorithm, as portrayed in the right side of Fig.15.From the values of the adopted evaluation measures, it was possible to compare the investigated algorithms objectively. Friedman's non-parametric statistical test for paired data and multiple comparisons, with a significance level of 5% (p-value &lt; 0.05), followed by Nemenyi posthoc test 3 , was employed to compare the results.The experimental protocol execution contemplated the use of the following programming languages: MATLAB and GNU Octave, as well as their packages of functions for time series prediction; R with Forecast package; and Java with Weka library.</figDesc><graphic coords="23,340.54,377.41,69.94,66.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: CD diagrams for the MSE, TU, and POCID values coming from the predictors on deterministic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Performance of the prediction methods for the four ranges of TU values in deterministic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Averages and SD of POCID obtained by the prediction methods on deterministic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: MCPM over deterministic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: CD diagrams for the MSE, TU, and POCID values coming from the predictors on stochastic time series In Fig 20, k NN-TSPI with approximate iteration occupied the third position in the rankings of MSE (Fig. 20(a)) and TU (Fig. 20(b)). Such configuration has lost to SARIMA and SVM by a small differ-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Performance of the prediction methods for the four ranges of TU values in stochastic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 22 :Figure 23 :</head><label>2223</label><figDesc>Figure 22: Averages and SD of POCID obtained by the prediction methods on stochastic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: CD diagrams for the MSE, TU, and POCID values coming from the predictors on chaotic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Performance of the prediction methods for the four ranges of TU values in chaotic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 26 :Figure 27 :</head><label>2627</label><figDesc>Figure 26: Averages and SD of POCID obtained by the prediction methods on chaotic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>Fig. 29 highlights this result in terms of TU values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Performance of the prediction methods for the four ranges of TU values in synthetic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>10 Figure 30 :Figure 31 :</head><label>103031</label><figDesc>Figure 30: Averages and SD of POCID obtained by the prediction methods on synthetic time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 33 :Figure 34 :Figure 35 :</head><label>333435</label><figDesc>Figure 33: Performance of the prediction methods for the four ranges of TU values in real time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Fig. 35</head><label>35</label><figDesc>Fig.35portrays the CD diagrams designed from the MCPM total area values. SARIMA and SVM culminated in the best overall results. k NN-TSPI, in turn, also provided good results for both projection strategies and without SSD in comparison to SARIMA, MLP, and SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 37 :Figure 38 :Figure 39 :</head><label>373839</label><figDesc>Figure 37: Performance of the prediction methods for four ranges of TU values in synthetic and real time series From the graphic representation of the averages and SD of the POCID values in Fig 38, it is possible to verify that the hit rates over the projections horizons trends are slightly distributed among the SARIMA, MLP, SVM, and k NN-TSPI models. Each one of these algorithms, regardless of the projection strategy employed, recorded a POCID average of approximately 64.89% (SD = 23.33%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Fig. 40</head><label>40</label><figDesc>summarizes the key results of this work. This figure shows, for each time series group, the rank position of the predictors given the multi-criteria performance measure. Such illustration can be an A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 40 :</head><label>40</label><figDesc>Figure 40: Overview of our findings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 41 :</head><label>41</label><figDesc>Figure 41: Predictions obtained for the out of sample period</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Some properties of empirical studies reported in related work</figDesc><table><row><cell>Paper</cell><cell></cell><cell>#Predictor(s)</cell><cell>#Synthetic</cell><cell>#Real</cell><cell>#Performance</cell><cell>Statistical</cell></row><row><cell></cell><cell cols="2">Parametric Non-parametric</cell><cell>Dataset(s)</cell><cell>Dataset(s)</cell><cell>Measure(s)</cell><cell>Test(s)</cell></row><row><cell>[33]</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>55</cell><cell>3</cell></row><row><cell>[50]</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>9</cell><cell>3</cell></row><row><cell>[37]</cell><cell>1</cell><cell>7</cell><cell>5</cell><cell>35</cell><cell>1</cell></row><row><cell>[25]</cell><cell>1</cell><cell>2</cell><cell>-</cell><cell>6</cell><cell>1</cell></row><row><cell>[9]</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>7</cell><cell>2</cell></row><row><cell>[1]</cell><cell>-</cell><cell>8</cell><cell>-</cell><cell>1,045</cell><cell>2</cell></row><row><cell>[6]</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>[44]</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>3</cell></row><row><cell>[8]</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell>10</cell><cell>1</cell></row><row><cell>[49]</cell><cell>3</cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>4</cell></row><row><cell>[18]</cell><cell>6</cell><cell>3</cell><cell>-</cell><cell>10</cell><cell>2</cell></row><row><cell>This paper</cell><cell>7</cell><cell>4</cell><cell>40</cell><cell>55</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 2 details this sampling technique to search values for the parameters l, C, and σ of the SVM regression algorithm.</figDesc><table><row><cell cols="2">Algorithm 1: Holdout Validation</cell></row><row><cell cols="2">/* S represents a training subsequence of length n extracted from a time series Z of size m</cell><cell>*/</cell></row><row><cell cols="2">/* max p is an upper bound for number of observations constituting a seasonal station in the historical series</cell><cell>*/</cell></row><row><cell cols="2">/* h indicates the amount of values to be predicted by the best model identified /* P comprises the parameters list which resulted in the least prediction error Input: S, max p, h Output: P 1 begin</cell><cell>*/ */</cell></row><row><cell>2 3 4 5 6</cell><cell>min error = ∞; h = (max p + h) ÷ 2; for l ← 3 : 2 : max p do for k ← 1 : 2 : 9 do error = knn tspi(S, l, k, h );</cell></row><row><cell>7</cell><cell>if error &lt; min error then</cell></row><row><cell>8</cell><cell>min error = error;</cell></row><row><cell>9</cell><cell>l best = l;</cell></row><row><cell>10</cell><cell>k best = k;</cell></row><row><cell>11</cell><cell>end</cell></row><row><cell>12</cell><cell>end</cell></row><row><cell>13 14 15</cell><cell>end P ← {l best , k best }; return P;</cell></row><row><cell>16 end</cell><cell></cell></row><row><cell cols="2">methods. Algorithm 2: Cross-validation</cell></row><row><cell cols="2">// S represents a training subsequence</cell></row><row><cell cols="2">/* max p specifies an upper bound for the number of observations constituting a seasonal station in the historical series</cell><cell>*/</cell></row><row><cell cols="2">/* kFolds is the number of partitions on which the training data sample will be split /* P comprises the parameters list which resulted in the least prediction error Input: S, max p, h Output: P 1 begin</cell><cell>*/ */</cell></row><row><cell>2</cell><cell>min error = ∞;</cell></row></table><note><p>3 for l ← 3 : 2 : max p do 4 T ← generate data table(S, l);</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Summary of characteristics and settings of the benchmark datasets</figDesc><table><row><cell></cell><cell cols="3">(a) Synthetic data</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Real data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ID</cell><cell>Dataset</cell><cell>m</cell><cell>max p</cell><cell>h (m × 5%)</cell><cell>ID</cell><cell>Dataset</cell><cell>Acquisition</cell><cell>Start</cell><cell>Finish</cell><cell>m</cell><cell>max p</cell><cell>h</cell></row><row><cell></cell><cell>Fourier A:</cell><cell>790</cell><cell></cell><cell></cell><cell>01.A</cell><cell>Fortaleza</cell><cell>Annual</cell><cell>1849</cell><cell>1997</cell><cell>149</cell><cell>6</cell><cell>7</cell></row><row><cell>01.D 02.D 03.D</cell><cell>• Constant Level • Increasing Trend • Decreasing Trend Fourier B:</cell><cell>790</cell><cell>25 25 25</cell><cell>40 40 40</cell><cell>02.A 03.D 04.D 05.D</cell><cell>Manchas Atmosfera: • Temperatura • Umidade Relativa do Ar Banespa</cell><cell>Annual Daily Daily</cell><cell>1749 Jan-01-1997 Jan-03-1995</cell><cell>1942 Dec-31-1997 Dec-27-2000</cell><cell>176 365 1,499</cell><cell>11 7 7 7</cell><cell>12 31 31 88</cell></row><row><cell>04.D 05.D 06.D</cell><cell>• Constant Level • Increasing Trend • Decreasing Trend</cell><cell></cell><cell>38 38 38</cell><cell>40 40 40</cell><cell>06.D 07.D 08.D 09.D</cell><cell>CEMIG IBV Patient Demand Petrobras</cell><cell>Daily Daily Daily Daily</cell><cell cols="2">Jan-03-1995 Jan-03-1995 Jan-01-2007 Mar-31-2009 Dec-27-2000 Dec-27-2000 Jan-03-1995 Dec-27-2000</cell><cell>1,499 1,499 821 1,499</cell><cell>7 7 7 7</cell><cell>88 88 90 88</cell></row><row><cell></cell><cell>Fourier C:</cell><cell>790</cell><cell></cell><cell></cell><cell></cell><cell>Poluição:</cell><cell>Daily</cell><cell>Jan-01-1997</cell><cell>Dec-31-1997</cell><cell>365</cell><cell></cell><cell></cell></row><row><cell cols="2">07.D 08.D 09.D 10.D 11.D 12.D 13.D 14.D 15.D 16.D Multiplicative Seasonality • Constant Level • Increasing Trend • Decreasing Trend Fourier D: • Constant Level • Increasing Trend • Decreasing Trend Seasonal Dependence: • Constant Level • Increasing Trend • Decreasing Trend</cell><cell>790 2,200 590</cell><cell>34 34 34 38 38 38 25 25 25 14</cell><cell>40 40 40 40 40 40 110 110 110 30</cell><cell>10.D 11.D 12.D 13.D 14.D 15.D 16.D 17.D 18.D 19.D 20.D 21.D 22.D 23.M</cell><cell>• PM10 • SO2 • CO • O3 • NO2 Star Stock Market: • Amsterdam • Frankfurt • London • Hong Kong • Japan • Singapore • New York</cell><cell>Daily Daily</cell><cell>1922 Jan-06-1986</cell><cell>1924 Dec-31-1997</cell><cell>600 3,128</cell><cell>7 7 7 7 7 7 7 7 7 7 7 7 7</cell><cell>31 31 31 31 31 25 92 92 92 92 92 92 92</cell></row><row><cell cols="2">17.D High Frequency</cell><cell>550</cell><cell>63</cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CCA:</cell><cell>1,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>18.S 19.S 20.S 21.S 22.S 23.S</cell><cell>• Constant Level • Seasonal Patterns • Increasing Trend • Decreasing Trend • Upward Shift • Downward Shift</cell><cell></cell><cell>12 30 12 12 12 12</cell><cell>50 50 50 50 50 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CCB:</cell><cell>1,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>24.S 25.S 26.S 27.S 28.S 29.S</cell><cell>• Constant Level • Double Seasonality • Increasing Trend • Decreasing Trend • Upward Shift • Downward Shift</cell><cell></cell><cell>30 30 30 30 30 30</cell><cell>50 50 50 50 50 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SDN:</cell><cell>2,200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30.S 31.S 32.S</cell><cell>• Constant Level • Increasing Trend • Decreasing Trend</cell><cell></cell><cell>25 25 25</cell><cell>110 110 110</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>33.C</cell><cell>Logistic Map</cell><cell>550</cell><cell>4</cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>34.C</cell><cell>Hénon Map</cell><cell>3,000</cell><cell>3</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35.C</cell><cell>Mackey-Glass System</cell><cell>3,000</cell><cell>7</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>36.C</cell><cell>Lorenz System</cell><cell>3,000</cell><cell>25</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>37.C</cell><cell>Rössler System</cell><cell>3,000</cell><cell>14</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Chaotic Signals:</cell><cell>550</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>38.C 39.C</cell><cell>• A • B</cell><cell></cell><cell>22 7</cell><cell>28 28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40.C</cell><cell>ECGSYN</cell><cell>3,000</cell><cell>60</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>summarizes such results. CD diagrams for the MSE, TU, and POCID values coming from the predictors on synthetic time series</figDesc><table><row><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell></row><row><cell cols="2">SARIMA SVM k NN-TSPI MLP LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HES ARIMA MHW AHW SES MA</cell><cell cols="2">SARIMA SVM k NN-TSPI LSTM MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HES ARIMA MHW AHW SES MA</cell><cell cols="2">SVM SARIMA MLP k NN-TSPI LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SES MA ARIMA HES MHW AHW</cell></row><row><cell cols="9">(a) Approximate iteration -MSE</cell><cell cols="9">(b) Approximate iteration -TU</cell><cell cols="9">(c) Approximate iteration -POCID</cell></row><row><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell></row><row><cell cols="2">SVM MLP k NN-TSPI HES SARIMA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MA SES ARIMA LSTM MHW AHW</cell><cell cols="2">SVM MLP k NN-TSPI HES AHW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MA SES ARIMA SARIMA MHW LSTM</cell><cell cols="2">SVM MLP k NN-TSPI AHW LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SES MA ARIMA SARIMA HES MHW</cell></row><row><cell cols="9">(d) Updated iteration -MSE</cell><cell cols="9">(e) Updated iteration -TU</cell><cell cols="9">(f) Updated iteration -POCID</cell></row><row><cell cols="3">Figure 28:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>presents the CD diagrams regarding the MSE, TU, and POCID measures obtained from the aforementioned experiments.</figDesc><table><row><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell></row><row><cell cols="2">SARIMA SVM ARIMA k NN-TSPI LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HES AHW MHW MLP SES MA</cell><cell cols="2">SARIMA SVM ARIMA LSTM k NN-TSPI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HES AHW MHW MLP SES MA</cell><cell cols="2">SARIMA k NN-TSPI MLP SVM MHW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SES ARIMA MA HES AHW LSTM</cell></row><row><cell cols="9">(a) Approximate iteration -MSE</cell><cell cols="9">(b) Approximate iteration -TU</cell><cell cols="9">(c) Approximate iteration -POCID</cell></row><row><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11</cell></row><row><cell cols="2">SARIMA SVM ARIMA k NN-TSPI MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MA HES MHW AHW LSTM SES</cell><cell cols="2">SARIMA SVM ARIMA k NN-TSPI MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MA HES MHW AHW LSTM SES</cell><cell cols="2">SARIMA k NN-TSPI SVM MLP LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SES MA HES MHW ARIMA AHW</cell></row><row><cell cols="9">(d) Updated iteration -MSE</cell><cell cols="9">(e) Updated iteration -TU</cell><cell cols="9">(f) Updated iteration -POCID</cell></row><row><cell cols="27">Figure 32: CD diagrams for the MSE, TU, and POCID values coming from the predictors on real time series</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Prediction algorithms and their properties</figDesc><table><row><cell>Algorithm</cell><cell>Prediction</cell><cell>#P</cell><cell>Technique for</cell><cell>h &gt; 1 Non-stationarity</cell><cell>T</cell><cell>S</cell></row><row><cell></cell><cell>Approach</cell><cell></cell><cell>Parameter Estimation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MA</cell><cell>Parametric</cell><cell>1</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SES</cell><cell>Parametric</cell><cell>1</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HES</cell><cell>Parametric</cell><cell>2</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AHW</cell><cell>Parametric</cell><cell>4</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MHW</cell><cell>Parametric</cell><cell>4</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ARIMA</cell><cell>Parametric</cell><cell>4</cell><cell>Box-Jenkins Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SARIMA</cell><cell>Parametric</cell><cell>7</cell><cell>Box-Jenkins Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell>Non-parametric</cell><cell>5</cell><cell>Cross-validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM</cell><cell>Non-parametric</cell><cell>6</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM</cell><cell>Non-parametric</cell><cell>3</cell><cell>Cross-validation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>k NN-TSPI</cell><cell>Non-parametric</cell><cell>2</cell><cell>Holdout Validation</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>An operation which consists of taking successive differences from the original series Z. The first difference is denoted by ∆zt = ztzt-1; the second difference is defined as ∆ 2 zt = ∆(∆zt) = ∆(ztzt-1); finally, the d th difference equals ∆ d zt = ∆(∆ d-1 zt).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>All statistical tests were performed using KEEL Software Tool for Windows, http://www.keel.es.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by the São Paulo Research Foundation [grant numbers 2013/10978-8, 2016/04986-6, and 2018/05859-3]; and the Brazilian National Council for Scientific and Technological Development [grant number 306631/2016-4].</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Step 2, we estimate the parameters according to the particularities of each method. In this context, the non-parametric algorithms SVM and MLP, which are applied according to the global approach, had their parameters determined through cross-validation in ten partitions and the minimization of the MSE (Algorithm 2). On the other hand, the parameters of the parametric models ARIMA and SARIMA were defined using the Box-Jenkins method and the minimization of the AIC (Eq. 27) and maximum likelihood Table <ref type="table">3</ref> shows the eleven predictors evaluated, as well as a description of their parameters and the range of values considered in the estimation step.  We previously evaluated, in addition to the single-layer MLP, two other candidate algorithms: SRN and RNN with two layers. RNN obtained a lower predictive performance than the SRN and MLP models. In contrast, SRN and MLP achieved comparable results, since we did not find statistically significant differences between their performances. As MLP is a more straightforward approach and has faster execution time, we prefer to keep only the results of this neural network in this paper.</p><p>After estimating the best parameters, predictive models were constructed and adjusted to the training data. Each model was posteriorly extrapolated to predict h periods ahead in agreement with the two projection strategies presented in Section 4 and designated as: (i) multi-step-ahead with approximate iteration; and (ii) multi-step-ahead with updated iteration.</p><p>In Step 3, the projected data were compared with the test data using MSE measure, Theil's U (TU) coefficient, and hit rate Prediction Of Change In Direction (POCID).</p><p>The MSE measure, denoted by Eq. 30, works with the difference (error) between the actual value observed (z t ) and the predicted value ( ẑt ).</p><p>In Eq. 30, the quadratic sum of the prediction error is divided by the number of observations. Thus, an efficient predictor will have an MSE value close to zero. The TU coefficient, expressed by Eq. 31, is based</p><p>SVM, SARIMA and k NN-TSPI exposed, in this order, the best multi-criteria results when executed over the 95 datasets (Fig. <ref type="figure">40</ref>(a.6)). It is relevant to observe how the error eventually propagated along the projection horizon influences the performance of MLP and ARIMA.</p><p>In applications where the data acquisition period is not extreme, we can use the multi-step-ahead projection with updated iteration. These applications acquire the observations at a reasonable deadline which allows to feed (or update) the model continuously over time. For this projection strategy, MLP is a proper candidate to model and predict deterministic time series (Fig. <ref type="figure">40(b.1</ref>)). Beyond to present the smallest prediction errors, it was able to hit the extrapolated horizons trend accurately. Also note that MLP did not show SSD regarding two machine learning algorithms (SVM and k NN-TSPI) and three exponential smoothing methods (AHW, HES and MHW). In this scenario, despite the competitiveness of the exponential smoothing models, the machine learning algorithms culminated in the best POCID rates.</p><p>SARIMA and k NN-TSPI seem to be the most appropriate models for dealing with stochastic time series (Fig. <ref type="figure">40(b.</ref>2)). k NN-TSPI still stands out for having achieved the highest POCID values. It is important to emphasize that SARIMA and k NN-TSPI did not present SSD concerning three parametric (ARIMA, MA and MHW) and three non-parametric (SVM, LSTM and MLP) methods.</p><p>We can consider MLP, SVM and k NN-TSPI the most suitable algorithms to model and predict chaotic time series (Fig. <ref type="figure">40(b.</ref>3)). MLP obtained the smallest errors and the best POCID rates, followed by SVM and k NN-TSPI. We observe a similar behavior on the 40 synthetic datasets (Fig. <ref type="figure">40(b.4</ref>)), where SVM, MLP and k NN-TSPI were the most appropriate methods both concerning the low prediction error and the high hit rates on the projection horizons trends.</p><p>SARIMA, SVM, k NN-TSPI, and ARIMA are the most promising algorithms for real time series (Fig. <ref type="figure">40(b.5</ref>)). ARIMA's performance was very close to k NN-TSPI. Also, SARIMA and k NN-TSPI showed the highest POCID values.</p><p>For all the 95 datasets (Fig. <ref type="figure">40</ref>(b.6)), SVM surpassed k NN-TSPI and SARIMA by a small margin. These three methods did not exhibit large performance differences regarding MLP. Besides, they achieved similar POCID results.</p><p>Considering both projection strategies, SVM was the algorithm that obtained, regarding prediction error and POCID rates, the most stable performance for deterministic time series (Fig. <ref type="figure">40 -(</ref>a.1) and (b.1)). In contrast, SARIMA and k NN-TSPI were the best models for stochastic data (Fig. <ref type="figure">40 -(a.</ref>2) and (b.2)). Although k NN-TSPI was more accurate concerning the projection horizons trends, SARIMA provided the lowest error rates. Regarding the chaotic time series (Fig. <ref type="figure">40 -(a.</ref>3) and (b.3)), SVM and k NN-TSPI were more stable than the other predictors.</p><p>The learning algorithms k NN-TSPI and SVM outperformed the state-of-the-art statistical methods (SARIMA and ARIMA) when examining all the 40 synthetic datasets (Fig. <ref type="figure">40 -(a.4</ref>) and (b.4)).</p><p>Given the 55 real datasets (Fig. <ref type="figure">40</ref> -(a.5) and (b.5)), SARIMA and SVM showed very similar performances. k NN-TSPI also provided good results for both projection strategies and without SSD in comparison to SARIMA and SVM.</p><p>On the overall comparison involving 95 datasets (Fig. <ref type="figure">40</ref> -(a.6) and (b.6)), the multi-criteria analysis indicates that SARIMA, SVM and k NN-TSPI are the most promising methods for temporal data modeling and prediction.</p><p>To compare the prediction quality of SARIMA, MLP, SVM, and k NN-TSPI, we show eight examples in Fig. <ref type="figure">41</ref>. We chose these time series to illustrate some cases where the modeling is challenging.</p><p>In Fig. <ref type="figure">41</ref>, we can see the impact of error propagation in the multi-step-ahead projection with approximate iteration when compared to the updated iteration. Among the four algorithms, k NN-TSPI stands out for its robustness and stability concerning the projection horizons trends. An interesting case where the algorithm performed very well is the example illustrated in Fig. <ref type="figure">41(d.1</ref>).</p><p>Although SARIMA and SVM have achieved about the same prediction accuracy as the similarity-based method, the algorithm with invariances is simpler to understand, encode, and adjust. While SARIMA has seven parameters and SVM has three, k NN-TSPI have only two. Most importantly, the two input arguments of k NN-TSPI are intuitive and can be easily estimated based on the data seasonality. The first parameter (l) is the query length in number of observations, and the second (k) is the number of similar subsequences required to make a prediction.</p><p>The use of baseline and topline methods is an essential guideline for conducting empirical assessments. In this direction, we suggest MA and HW as baseline models. MA is quite simple and generally performs better than the naïve (one-step-ahead) technique. HW methods, besides being indicated for time series</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Supplementary material associated with this article can be found, in the online version, at https: //goo.gl/ouK3sj. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical comparison of machine learning models for time series forecasting</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Gayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>El-Shishiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Reviews</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="594" to="621" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Forecast combinations of computational intelligence and linear models for the NN5 time series forecasting competition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Andrawis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>El-Shishiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="672" to="688" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben Taieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7067" to="7083" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: Forecasting and control</title>
		<meeting><address><addrLine>New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Wiley Series in Probability and Statistics. 5 edn.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Application of machine learning techniques for supply chain demand forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laframboise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vahidov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1140" to="1154" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid fuzzy time series model based on granular computing for stock price forecasting</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="227" to="241" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forecasting tourism demand to Catalonia: Neural networks vs. time series models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Claveria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Torra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Modelling</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="220" to="228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for time lag selection to forecast seasonal time series using neural networks and support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method with neural networks for the classification of fruits and vegetables</title>
		<author>
			<persName><forename type="first">J</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="7207" to="7220" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A review on time series data mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="181" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exponential smoothing: The state of the art</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">25 years of time series forecasting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G D</forename><surname>Gooijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="473" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Support vector machines for classification and regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., Faculty of Engineering</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Southampton, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Southampton</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Forecasting accuracy evaluation of tourist arrivals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Antonakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Filis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Tourism Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="112" to="127" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural networks and learning machines</title>
		<meeting><address><addrLine>Upper Saddle River, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Uncertainty, Fuzziness and Knowledge-based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forecasting seasonals and trends by exponentially weighted moving averages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A state space framework for automatic forecasting using exponential smoothing methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="454" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterization and prediction of runoff dynamics: A nonlinear dynamical view</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Water Resources</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparison of various forecasting methods for autocorrelated time series</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kandananond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering Business Management</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning for time series forecasting and forecast combination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="2006" to="2016" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An introduction to computing with neural nets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wavelet fuzzy neural networks for identification and predictive control of dynamic systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3046" to="3058" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exponentially weighted moving average control schemes: properties and enhancements</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Saccucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The bulletin of mathematical biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to time series analysis and forecasting</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kulahci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Statistics</title>
		<meeting><address><addrLine>New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>2 edn.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R S</forename><surname>Parmezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<ptr target="https://goo.gl/uzxGZJ" />
		<title level="m">ICMC-USP time series prediction repository</title>
		<meeting><address><addrLine>São Carlos, Brasil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A study of the use of complexity measures in the similarity search process adopted by kNN algorithm for time series prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R S</forename><surname>Parmezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
		<meeting><address><addrLine>Miami, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metalearning for choosing feature selection algorithms in data mining: Proposal of a new framework</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R S</forename><surname>Parmezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data driven modelling based on recurrent interval-valued metacognitive scaffolding fuzzy neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic forecasting of hourly electricity price by generalization of ELM for usage in improved wavelet neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niknam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Khooban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A time-dependent enhanced support vector machine for time series regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ristanoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="946" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="386" to="408" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation, in: Parallel distributed processing: explorations in the microstructure of cognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Time series prediction using support vector machines: A survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Sapankevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Schaum&apos;s outline of probability and statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Computational capabilities of recurrent NARX neural networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comparison of univariate time series methods for forecasting intraday arrivals at a call center</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Science and Statistics</title>
		<imprint>
			<date type="published" when="1999">2 edn., 1999</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Back propagation neural network with adaptive differential evolution algorithm for time series forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="863" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Forecasting sales by exponentially weighted moving averages</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Winters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="324" to="342" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nearest neighbor model for multiple-time-step prediction of short-term traffic condition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Transportation Engineering</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A comparative analysis of univariate time series methods for estimating and forecasting daily spam in United States</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-second Americas Conference on Information Systems</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Applications and comparisons of four time series models in epidemiological surveillance data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">88075</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
