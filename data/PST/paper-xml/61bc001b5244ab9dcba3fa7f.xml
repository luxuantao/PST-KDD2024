<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ditch the Gold Standard: Re-evaluating Conversational Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-21">21 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huihan</forename><surname>Li</surname></persName>
							<email>huihanl@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
							<email>tianyug@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manan</forename><surname>Goenka</surname></persName>
							<email>mgoenka@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ditch the Gold Standard: Re-evaluating Conversational Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-21">21 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.08812v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of humanmachine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and goldhistory evaluation in terms of model ranking. We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational question answering aims to build machines to answer questions in conversations and has the promise to revolutionize the way humans interact with machines for information seeking. With recent development of large-scale datasets <ref type="bibr" target="#b4">(Choi et al., 2018;</ref><ref type="bibr" target="#b29">Saeidi et al., 2018;</ref><ref type="bibr" target="#b28">Reddy et al., 2019;</ref><ref type="bibr" target="#b2">Campos et al., 2020)</ref>, rapid progress has been made in better modeling of conversational QA systems.</p><p>Current conversational QA datasets are collected by crowdsourcing human-human conversations, where the questioner asks questions about a specific topic, and the answerer provides answers based on an evidence passage and the conversational history. When evaluating conversational QA systems, a set of held-out conversations are used for asking models questions in turn. Since the evaluation builds on pre-collected conversations, the gold history of the conversation is always provided, regardless of models' actual predictions (Figure <ref type="figure" target="#fig_0">1(b)</ref>). Although current systems achieve near-human F1 scores on this static evaluation, it is questionable whether this can faithfully reflect models' true performance in real-world applications. To what extent do humanmachine conversations deviate from human-human conversations? What will happen if models have no access to ground-truth answers in a conversation?</p><p>To answer these questions and better understand the performance of conversational QA systems, we carry out the first large-scale human evaluation with four state-of-the-art models on the QuAC dataset <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> by having human evaluators converse with the models and judge the correctness of their answers. We collected 1,446 human-machine conversations in total, with 15,059 question-answer pairs. Through careful analysis, we notice a significant distribution shift from human-human conversations and identify a clear inconsistency of model performance between current evaluation protocol and human judgements.</p><p>This finding motivates us to improve automatic evaluation such that it is better aligned with human evaluation. <ref type="bibr" target="#b19">Mandya et al. (2020)</ref>; <ref type="bibr" target="#b34">Siblini et al. (2021)</ref> identify a similar issue in gold-history evaluation and propose to use models' own predictions for automatic evaluation. However, predictedhistory evaluation poses another challenge: since all the questions have been collected beforehand, using predicted history will invalidate some of the questions because of changes in the conversational history (see Figure <ref type="figure" target="#fig_0">1</ref>(c) for an example).</p><p>Following this intuition, we propose a question Topic: Spandau Ballet (English pop band)</p><p>What was the band's first success album at the international level?</p><p>They achieved platinum status.</p><p>1985.</p><p>What year did this happen?</p><p>What was the band's first success album at the international level?</p><p>They achieved platinum status.</p><p>"Only When You Leave".</p><p>What songs were in it?</p><p>Gold answer: "Parade" from 1984.</p><p>What was the band's first success album at the international level?</p><p>They achieved platinum status.</p><p>???</p><p>(a) Human evaluation (b) Automatic evaluation w/ What songs were in it?</p><p>(c) Automatic evaluation w/ predicted history gold history rewriting mechanism, which automatically detects and rewrites invalid questions with predicted history (Figure <ref type="figure" target="#fig_3">4</ref>). We use a coreference resolution model <ref type="bibr" target="#b15">(Lee et al., 2018)</ref> to detect inconsistency of conference in question text conditioned on predicted history and gold history, and then rewrite those questions by substituting with correct mentions, so that the questions are resolvable in the predicted context. Compared to predicted-history evaluation, we find that incorporating this rewriting mechanism aligns better with human evaluation.</p><p>Finally, we also investigate the impact of different modeling strategies based on human evaluation. We find that both accurately detecting unanswerable questions and explicitly modeling question dependencies in conversations are crucial for model performance. Equipped with all the insights, we discuss directions for conversational QA modeling. We release our human evaluation dataset and hope that our findings can shed light on future development of better conversational QA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>2.1 Evaluation of conversational QA Evaluation of conversational QA in real-world consists of three components: an evidence passage P , a (human) questioner H that has no access to P , 2 and a model M that has access to P . The questioner asks questions about P and the model answers them based on P and the conversational history thus far (see an example in Figure <ref type="figure" target="#fig_0">1(a)</ref>). Formally, for the i-th turn, the human asks a ques-2 Existing conversational QA datasets make different assumptions: For example, QuAC <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> assumes no access but CoQA assumes the questioner to have access. tion based on the previous conversation,</p><formula xml:id="formula_0">Q i ∼ H(Q 1 , A 1 , ..., Q i−1 , A i−1 ),<label>(1)</label></formula><p>and then the model answers it based on both the history and the passage,</p><formula xml:id="formula_1">A i ∼ M(P, Q 1 , A 1 , ..., Q i−1 , A i−1 , Q i ),<label>(2)</label></formula><p>where Q i and A i represent the question and the answer at the i-th turn. If the question is unanswerable from P , we simply denote A i as CANNOT ANSWER. The model M is then evaluated by the correctness of answers.</p><p>Evaluating conversational QA systems requires human in the loop and is hence expensive. Instead, current benchmarks use automatic evaluation with gold history (Auto-Gold) and collect a set of humanhuman conversations for automatic evaluation. For each passage, one annotator asks questions without seeing the passage, while the other annotator provides the answers. Denote the collected questions and answers as Q * i and A * i . In gold-history evaluation, the model is inquired with pre-collected questions Q * i and the gold answers as history:</p><formula xml:id="formula_2">A i ∼ M(P, Q * 1 , A * 1 , ..., Q * i−1 , A * i−1 , Q * i ),<label>(3)</label></formula><p>and we evaluate the model by comparing A i to A * i (measured by word-level F1). This process does not require human effort but cannot truly reflect the distribution of human-machine conversations, because unlike human questioners who may ask different questions based on different model predictions, this static process ignores model predictions and always asks the pre-collected question.</p><p>In this work, we choose the QuAC dataset <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> as our primary evaluation because it is closer to real-world information-seeking conversations, where the questioner cannot see the evidence passage during the dataset collection. It prevents the questioner asking questions that simply overlaps with the passage and encourages unanswerable questions. QuAC also adopts extractive question answering that restricts the answer as a span of text, which is generally considered easier to evaluate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models</head><p>For human evaluation and analysis, we choose the following four conversational QA models with different model architectures and training strategies:</p><p>BERT. It is a simple BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> baseline which concatenates the previous two turns of question-answer pairs, the question, and the passage as the input and predicts the answer span. <ref type="foot" target="#foot_0">3</ref>This model is the same as the "BERT + PHQA" baseline in <ref type="bibr" target="#b25">Qu et al. (2019a)</ref>.</p><p>GraphFlow. <ref type="bibr" target="#b24">Chen et al. (2020)</ref> propose a recurrent graph neural network on top of BERT embeddings to model the dependencies between the question, the history and the passage.</p><p>HAM. <ref type="bibr" target="#b26">Qu et al. (2019b)</ref> propose a history attention mechanism (HAM) to softly select the most relevant previous turns.</p><p>ExCorD. <ref type="bibr" target="#b14">Kim et al. (2021)</ref> train a question rewriting model on CANARD <ref type="bibr" target="#b7">(Elgohary et al., 2019)</ref> to generate context-independent questions, and then use both the original and the generated questions to train the QA model. This model achieves the current state-of-the-art on QuAC (67.7% F1).</p><p>For all the models except BERT, we use the original implementations for a direct comparison. We report their performance on both standard benchmark and our evaluation in Table <ref type="table" target="#tab_1">2</ref>.</p><p>3 Human Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conversation collection</head><p>In this section, we carry out a large-scale human evaluation with the four models discussed above. We collect human-machine conversations using 100 passages from the QuAC development set on Amazon Mechanical Turk. <ref type="foot" target="#foot_1">4</ref> We also design a set of qualification questions to make sure that the annotators fully understand our annotation guideline. For each model and each passage, we collect three conversations from three different annotators.</p><p>We collect each conversation in two steps:</p><p>(1) The annotator has no access to the passage and asks questions. The model extracts the answer span from the passage or returns CANNOT ANSWER in a human-machine conversation interface. <ref type="foot" target="#foot_2">5</ref> We provide the title, the section title, the background of the passage, and the first question from QuAC as a prompt to annotators. Annotators are required to ask at least 8 and at most 12 questions. We encourage context-dependent questions, but also allow open questions like "What else is interesting?" if asking a follow-up question is difficult. (2) After the conversation ends, the annotator is shown the passage and asked to check whether the model predictions are correct or not.</p><p>We noticed that the annotators are biased when evaluating the correctness of answers. For questions to which the model answered CANNOT ANSWER, annotators tend to mark the answer as incorrect without checking if the question is answerable. Additionally, for answers with the correct types (e.g. a date as an answer to "When was it?"), annotators tend to mark it as correct without verifying it from the passage. Therefore, we asked another group of annotators to verify question answerability and answer correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer validation</head><p>For each collected conversation, we ask two additional annotators to validate the annotations. First, each annotator reads the passage before seeing the conversation. Then, the annotator sees the question (and question only) and selects whether the question is (a) ungrammatical, (b) unanswerable, or (c) answerable. If the annotator chooses "answerable", the interface then reveals the answer and asks about its correctness. If the answer is "incorrect", the annotator selects the correct answer span from the passage. We discard all questions that both annotators find "ungrammatical" and the correctness is taken as the majority of the 3 annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation statistics</head><p>In total, we collected 1,446 human-machine conversations and 15,059 question-answer pairs. We release this collection as an important source that Deciding the correctness of answers is challenging even for humans in some cases, especially when questions are short and ambiguous. We measure annotators' agreement and calculate the Fleiss' Kappa <ref type="bibr" target="#b8">(Fleiss, 1971)</ref> on the agreement between annotators in the validation phase. We achieve κ = 0.598 (moderate agreement) of overall annotation agreement. Focusing on answerability annotation, we have κ = 0.679 (substantial agreement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Disagreements between Human and Gold-history Evaluation</head><p>We now compare the results from our human evaluation and gold-history (automatic) evaluation. Note that the two sets of numbers are not directly comparable: (1) the human evaluation reports accuracy, while the automatic evaluation reports F1 scores;</p><p>(2) the absolute numbers of human evaluation are much higher than those of automatic evaluations. For example, for the BERT model, the human evaluation accuracy is 82.6% while the automatic evaluation F1 is only 63.2%. The reason is that, in automatic evaluations, the gold answers cannot capture all possible correct answers to open-ended questions or questions with multiple answers; however, the human annotators can evaluate the correctness of answers easily. Nevertheless, we can compare relative rankings between different models.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows different trends between human evaluation and gold-history evaluation (Auto-Gold). Current standard evaluation cannot reflect model performance in human-machine conversations: (1) Human evaluation and Auto-Gold rank BERT and GraphFlow differently; especially, GraphFlow performs much better in automatic evaluation, but worse in human evaluation. (2) The gap between HAM and ExCorD is significant (F1 of 65.4% vs 67.7%) in the automatic evaluation but the two models perform similarly in human evaluation (accuracy of 87.8% vs 87.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Strategies for Automatic Evaluation</head><p>The inconsistency between human evaluation and gold-history evaluation suggests that we need better ways to evaluate and develop our conversational QA models. When being deployed in realistic scenarios, the models would never have access to the ground truth (gold answers) in previous turns and are only exposed to the conversational history and the passage. Intuitively, we can simply replace gold answers by the predicted answers of models and we name this as predicted-history evaluation (Auto-Pred). Formally, the model makes predictions based on the questions and its own answers:</p><formula xml:id="formula_3">A i ∼ M(P, Q * 1 , A 1 , ..., Q * i−1 , A i−1 , Q * i ). (4)</formula><p>This evaluation has been suggested by several recent works <ref type="bibr" target="#b19">(Mandya et al., 2020;</ref><ref type="bibr" target="#b34">Siblini et al., 2021)</ref>, which reported a significant performance drop using predicted history. We observe the same performance degradation, shown in </p><formula xml:id="formula_4">(Q * 1 , A 1 , ..., Q * i−1 , A i−1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predicted history invalidates questions</head><p>We examined 100 QuAC conversations with the best-performing model (ExCorD) and identified three categories of invalid questions caused by predicted history. We find that 23% of the questions become invalid after using the predicted history.</p><p>We summarize the types of invalid questions as follows (see detailed examples in Figure <ref type="figure">3</ref>):</p><p>• Unresolved coreference (44.0%). The question becomes invalid for containing either a pronoun or a definite noun phrase that refers to an entity unresolvable without the gold history. • Incoherence (39.1%). The question is incoherent with the conversation flow (e.g., mentioning an entity non-existent in predicted history).</p><p>While humans may still answer the question using the passage, this leads to an unnatural conversation and a train-test discrepancy for models. • Correct answer changed (16.9%). The answer to this question with the predicted history changes from when it is based on the gold history.</p><p>We further analyze the reasons for the biggest "unresolved coreference" category and find that the model either gives an incorrect answer to the previous question ("incorrect prediction", 39.8%), or the model predicts a different (yet correct) answer to What was the band's first success album at the international level?</p><p>Became the band's last American hit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What songs were in it</head><p>Coreference resolution Gold answer: "Parade" from 1984.</p><p>They achieved platinum status .</p><p>Coreference results using predicted and gold history do not match.</p><p>What songs were in "Parade"</p><p>Rewritten by gold history coreference results.</p><p>First single "Only When You Leave" .</p><p>Gold answer: "Only When You Leave" .</p><p>How did it do on the charts?  an open question ("open question", 37.0%), or the model returns CANNOT ANSWER incorrectly ("no prediction", 9.5%), or the gold answer is longer than prediction and the next question depends on the extra part ("extra gold information", 13.6%). Invalid questions result in compounding errors, which may further affect how the model interprets the following questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation with question substitution</head><p>Among all the invalid question categories, "unresolved coreference" questions are the most critical ones. They lead to incorrect interpretations of questions and hence wrong answers. We propose to improve our evaluation by first detecting these questions using a state-of-the-art coreference resolution system <ref type="bibr" target="#b15">(Lee et al., 2018)</ref> <ref type="foot" target="#foot_3">6</ref> , and then substituting them with either rewriting the questions inplace and replacing the questions with their contextindependent counterparts. Detecting invalid questions. We make the assumption that if the coreference model resolves mentions in Q * i differently between using gold history</p><formula xml:id="formula_5">(Q * 1 , A * 1 , ..., A * i−1 , Q * i ) and predicted history (Q * 1 , A 1 , ..., A i−1 , Q * i ), then Q * i is identified as hav- ing an unresolved coreference issue.</formula><p>The inputs to the coreference model for Q * i are the following: assuming e * j and e j have a shared mention in Q * i . We determine whether e * j = e j by checking if F1(s * j , s j ) &gt; 0, where s * j and s j are the first mention of e * j and e j respectively, and F1 is the wordlevel F1 score, i.e., e * j = e j as long as their first mentions have word overlap. The reason we take the F1 instead of exact match to check whether the entities are the same is stated in Appendix A.</p><formula xml:id="formula_6">S * i = [BG; Q * i−k ; A * i−k ; Q * i−k+1 ; A * i−k+1 ; ...; Q * i ] S i = [BG; Q * i−k ; A i−k ; Q * i−k+1 ; A i−k+1 ; ...; Q * i ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question rewriting through entity substitution.</head><p>Our first strategy is to substitute the entity names in Q * i with entities in E * , if Q * i is invalid. The rewritten question, instead of the original one, will be used in the conversation history and fed into the model. We denote this evaluation method as rewritten-question evaluation (Auto-Rewrite), and Figure <ref type="figure" target="#fig_3">4</ref> illustrates a concrete example.</p><p>To analyze how well Auto-Rewrite does in detecting and rewriting questions, we manually check 100 conversations of ExCorD from the QuAC development set. We find that Auto-Rewrite can detect invalid questions with a precision of 72% and a recall of 72% (more detailed analysis in Appendix B). An example of correctly detected and rewritten question is presented in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>Question replacement using CANARD. Another strategy is to replace the invalid questions with context-independent questions. The CANARD 7 We are only interested in the entities mentioned in the current question Q * i and we filter out named entities (e.g., the National Football League) because they can be understood without coreference resolution.</p><p>dataset <ref type="bibr" target="#b7">(Elgohary et al., 2019)</ref> provides such a resource, which contains human-rewritten contextindependent version of QuAC's questions. Recent works <ref type="bibr" target="#b1">(Anantha et al., 2021;</ref><ref type="bibr" target="#b7">Elgohary et al., 2019)</ref> have proposed training sequence-to-sequence models on such dataset to rewrite questions; however, since the performance of the question-rewriting models is upper bounded by the human-rewritten version, we simply use CANARD for question replacement. We denote this strategy as replacedquestion evaluation (Auto-Replace). Because collecting context-independent questions is expensive, Auto-Replace is limited to evaluating models on QuAC; it is also possible to be extended to other datasets by training a question rewriting model, as demonstrated in existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Automatic vs Human Evaluation</head><p>In this section, we compare human evaluation with all the automatic evaluations we have introduced: gold-history (Auto-Gold), predicted-history (Auto-Pred), and our proposed Auto-Rewrite and Auto-Replace evaluations. We first explain the metrics we use in the comparison ( §6.1) and then discuss the findings ( §6.2 and §6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Agreement metrics</head><p>Model performance and rankings. We first consider using model performance reported by different evaluation methods. Considering numbers of automatic and human evaluations are not directly comparable, we also calculate models' rankings and compare whether the rankings are consistent between automatic and human evaluations. Model performance is reported in Table <ref type="table" target="#tab_1">2</ref>. In human evaluation, GraphFlow &lt; BERT &lt; HAM ≈ ExCorD; in Auto-Gold, BERT &lt; GraphFlow &lt; HAM &lt; Ex-CorD; in other automatic evaluations, GraphFlow &lt; BERT &lt; HAM &lt; ExCorD.  Statistics of unanswerable questions. Percentage of unanswerable questions is an important aspect in conversations. Automatic evaluations using static datasets have a fixed number of unanswerable questions, while in human evaluation, the percentage of unanswerable questions asked by human annotators varies with different models. The statistics of unanswerable questions is shown in Table <ref type="table">3</ref>.</p><p>Pairwise agreement. For a more fine-grained evaluation, we perform a passage-level comparison for every pair of models. More specifically, for every single passage we use one automatic metric to decide whether model A outperforms model B (or vice versa) and examine the percentage of passages that the automatic metric agrees with human evaluation. For example, if the pairwise agreement of BERT/ExCorD between human evaluation and Auto-Gold is 52%, it means that Auto-Gold and human evaluation agree on 52% passages in terms of which model is better. Higher agreement means the automatic evaluation is closer to human evaluation. Figure <ref type="figure" target="#fig_4">5</ref> shows the results of pairwise agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Automatic evaluations have a significant distribution shift from human evaluation</head><p>We found that automatic evaluations have a significant distribution shift from human evaluation. We draw this conclusion from the following points.</p><p>• Human evaluation shows a much higher model performance than all automatic evaluations, as shown in Table <ref type="table" target="#tab_1">2</ref>. Two reasons may cause this large discrepancy: (a) Many conversational QA questions have multiple possible answers, and it is hard for the static dataset in automatic evaluations to capture all the answers. It is not an issue in human evaluation because all answers are judged by human evaluators. (b) There are more unanswerable questions and open questions in human evaluation (reason discussed in the next paragraph), which are easier-for example, models are almost always correct when answering questions like "What else is interesting?".</p><p>• Human evaluation has a much higher unanswerable question rate, as shown in Table <ref type="table">3</ref>. The reason is that in human-human data collection, the answers are usually correct and the questioners can ask followup questions upon the highquality conversation; in human-machine interactions, since the models can make mistakes, the conversation flow is less fluent and it is harder to have followup questions. Thus, questioners chatting with models tend to ask more open or unanswerable questions.</p><p>• All automatic evaluation methods have a pairwise agreement lower than 70% with human evaluation, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. This suggests that all automatic evaluations cannot faithfully reflect the model performance of human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Auto-Rewrite is closer to human evaluation</head><p>First, we can clearly see that among all automatic evaluations, Auto-Gold deviates the most from the human evaluation. From Table <ref type="table" target="#tab_1">2</ref>, only Auto-Gold shows different rankings from human evaluation, while Auto-Pred, Auto-Rewrite, and Auto-Replace show consistent rankings to human judgments.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, we see that Auto-Gold has the lowest agreement with human evaluation; among others, Auto-Rewrite better agrees with human evaluation for most model pairs. Surprisingly, Auto-Rewrite is even better than Auto-Replace-which uses human-written context independent questions-in most cases. After checking the Auto-Replace conversations, we found that human-written context independent questions are usually much longer than QuAC questions and introduce extra information into the context, which leads to out-of-domain challenges for conversational QA models (example in Appendix C). It shows that our rewriting strategy can better reflect real-world performance of conversational QA systems. However, Auto-Rewrite is not perfect-we see that when comparing G/E or G/H, Auto-Pred is better than Auto-Rewrite; in all model pairs, the agreement between human evaluation and Auto-Rewrite is still lower than 70%. This calls for further effort in designing better automatic evaluation in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Towards Better Conversational QA</head><p>With insights drawn from human evaluation and comparison with automatic evaluations, we discuss the impact of different modeling strategies, as well as future directions towards building better conversational question answering systems.</p><p>Modeling question dependencies on conversational context. When we focus on answerable questions (Table <ref type="table" target="#tab_1">2</ref>), we notice that GraphFlow, HAM and ExCorD perform much better than BERT. We compare the modeling differences of the four systems in Figure <ref type="figure">6</ref>, and identify that all the three better systems explicitly model the question dependencies on the conversation history and the passage: both GraphFlow and HAM highlight repeated mentions in questions and conversation history by special embeddings (turn marker and PosHAE) and use attention mechanism to select the most relevant part from the context; ExCorD adopts a question rewriting module that generates context-independent questions given the history and passage. All those designs help models better understand the question in a conversational context. Figure <ref type="figure" target="#fig_5">7</ref> gives an example where GraphFlow, HAM and ExCorD resolved the question from long conversation history while BERT failed.</p><p>Unanswerable question detection. Table <ref type="table" target="#tab_4">4</ref> demonstrates models' performance in detecting unanswerable questions. We notice that Graph-Flow predicts much fewer unanswerable questions than the other three models, and has a high precision and a low recall in unanswerable detection. This is because GraphFlow uses a separate network for predicting unanswerable questions, which is harder to calibrate, while the other models jointly predict unanswerable questions and answer spans. This behavior has two effects: (a) GraphFlow's overall performance is dragged down by its poor unanswerable detection result (Table <ref type="table" target="#tab_1">2</ref>). (b) In human evaluation, annotators ask fewer unanswerable questions with GraphFlow (Table <ref type="table">3</ref>)-when the model outputs more, regardless of correctness, the human questioner has a higher chance to ask passage-related followup questions. Both suggest that how well the model detects unanswerable questions significantly affects its performance and the flow in human-machine conversations.</p><p>Optimizing towards the new testing protocols. Most existing works on conversational QA modeling focus on optimizing towards Auto-Gold evaluation. Since Auto-Gold has a large gap from the real-world evaluation, more efforts are needed in optimizing towards the human evaluation, or Auto-Rewrite, which better reflects human evaluation. One potential direction is to improve models' robustness given noisy conversation history, which simulates the inaccurate history in real world that consists of models' own predictions. In fact, prior works <ref type="bibr" target="#b19">(Mandya et al., 2020;</ref><ref type="bibr" target="#b34">Siblini et al., 2021)</ref> that used predicted history in training showed that it benefits the models in predicted-history evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Conversational question answering. In recent years, several conversational question answering datasets have emerged, such as QuAC (Choi CANNOT ANSWER G/H/E: Gold, The Portland Zoo, Osceola, Fox...  <ref type="bibr" target="#b1">Anantha et al., 2021;</ref><ref type="bibr" target="#b24">Qu et al., 2020)</ref> Different from single-turn QA datasets <ref type="bibr" target="#b27">(Rajpurkar et al., 2016)</ref>, conversational QA requires the model to understand the question in the context of conversational history. There have been many methods proposed to improve conversational QA performance <ref type="bibr" target="#b22">(Ohsugi et al., 2019;</ref><ref type="bibr" target="#b24">Chen et al., 2020;</ref><ref type="bibr" target="#b26">Qu et al., 2019b;</ref><ref type="bibr" target="#b14">Kim et al., 2021)</ref> and significant improvements have been made on conversational QA benchmarks. Besides text-based conversational QA tasks, there also exist conversational QA benchmarks that require external knowledge or other modalities <ref type="bibr" target="#b29">(Saeidi et al., 2018;</ref><ref type="bibr" target="#b30">Saha et al., 2018;</ref><ref type="bibr" target="#b11">Guo et al., 2018;</ref><ref type="bibr" target="#b5">Das et al., 2017)</ref>.</p><p>Only recently has it been noticed that the current method of evaluating conversational QA models is flawed. <ref type="bibr" target="#b19">Mandya et al. (2020)</ref>; <ref type="bibr" target="#b34">Siblini et al. (2021)</ref> point out that using gold answers in history is not consistent with real-world scenarios and propose to use predicted history for evaluation. Different from prior works, in this paper, we conduct a large scale human evaluation to provide evidence for why goldhistory evaluation is sub-optimal. In addition, we point out that even predicted-history evaluation has issues with invalid questions, for which we propose rewriting questions to further mitigate the gap.</p><p>Automatic evaluation of dialogue systems. Automatically evaluating dialogue systems is difficult due to the nature of conversations. In recent years, the NLP community has cautiously re-evaluated and identified flaws in many popular automated evaluation strategies of dialogue systems <ref type="bibr" target="#b17">(Liu et al., 2016;</ref><ref type="bibr" target="#b31">Sai et al., 2019)</ref>, and have proposed new evaluation protocols to align more with human evaluation in a real-world setting: Huang et al. ( <ref type="formula">2020</ref>  <ref type="formula">2020</ref>) train models to predict the relatedness score between references and model outputs, which are shown to be better than BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref> or ROGUE <ref type="bibr" target="#b16">(Lin, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we carry out the first large-scale human evaluation on conversational QA systems. We show that current standard automatic evaluation with gold history cannot reflect models' performance in human evaluation, and that humanmachine conversations have a large distribution shift from static conversational QA datasets of human-human conversations. To tackle these problems, we propose to use predicted history with rewriting invalid questions for evaluation, which reduces the gap between automatic evaluations and real-world human evaluation. Based on the insights from the human evaluation results, we also nalyze current conversational QA systems and identify promising directions for future development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Invalid Question Detection</head><p>In question rewriting, we use F1 instead of exact match to check whether two entites are the same. The reason is that sometimes the prediction may mention the same entity as the gold answer does, but with different names. Figure <ref type="figure">8</ref> gives an example. Thus to avoid the false positive of detecting invalid questions, we take the F1 metric. B Quality of Rewriting Questions Detection. After manually checking 100 conversations of ExCorD from the QuAC development set, we find that Auto-Rewrite can detect invalid questions with a precision of 72% and a recall of 72%.</p><p>We notice that the coreference model sometimes detects the pronoun of the main character in the passage as insolvable, although it almost shows up in every question. This issue causes the low precision but is not a serious problem in our case -whether rewriting the pronoun of the main character does not affect models' prediction much, because the model always sees the passage and knows who the main character is.</p><p>Rewriting. Among all correctly detected invalid questions, we further check the quality of rewriting, and in 68% of the times Auto-Rewrite gives a correct context-independent questions. The most common error is being ungrammatical. For example, using the gold history of "... Dee Dee claimed that Spector once pulled a gun on him", the original question "Did they arrest him for doing this?" was rewritten to "Did they arrest Phillip Harvey Spector for doing pulled?" While this creates a distribution shift on question formats, it is still better than putting an invalid question in the flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Issue with Context Independent Questions</head><p>Figure <ref type="figure">9</ref> shows an example where extra information in context-independent questions confuses the model and leads to incorrect prediction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of human and automatic evaluations with gold or predicted history. The model answers the first question incorrectly. (a) A human questioner asks the next question based on current predictions. (b) Automatic evaluation with gold history poses pre-collected questions with gold answers as conversational history, regardless of model predictions. (c) Using predicted history in automatic evaluation makes the next question invalid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model performance of human evaluation (accuracy, left) and Auto-Gold (F1, right). Scales for accuracy and F1 are different. Human evaluation and Auto-Gold rank BERT and GraphFlow differently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: An example of question rewriting. We rewrite the second question with referent in the gold history, because predicted and gold history have different coreference results. We do not rewrite the third question as coreference results are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Pairwise agreement of different model pairs comparing automatic evaluations to human evaluation. B: BERT; G: GraphFlow; H: HAM; E: ExCorD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example of BERT failing to resolve the festival in Q *4 , while all other models with explicit dependency modelings succeeded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>); Ye et al. (2021) evaluate the coherence of the dialogue systems; Gupta et al. (2019) explore to use multiple references for evaluation; Mehri and Eskenazi (2020) propose an unsupervised and reference-free evaluation; Lowe et al. (2017); Tao et al. (2018); Ghazarian et al. (2019); Shimanaka et al. (2019); Sai et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Q * 1 :Figure 8 :</head><label>18</label><figDesc>Figure 8: An example that the prediction may mention the same entity as the gold answer does with slightly different names.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 9: The context-independent question Q P by Auto-Replace contains extra information that confuses the model. The rewritten question Q W did not change the original question and led to a correct answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of conversations (# C) and questions (# Q) collected in human evaluation, using 100 passages from the QuAC development set. We also add QuAC development set for reference. GF: GraphFlow. complements existing conversational QA datasets. Numbers of conversations and question-answer pairs collected for each model are shown in Table1. The data distribution of this collection is very different from the original QuAC dataset (human-human</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Human Evaluation</cell><cell>QuAC</cell></row><row><cell></cell><cell>BERT</cell><cell>GF</cell><cell cols="2">HAM ExCorD</cell><cell></cell></row><row><cell># C</cell><cell>357</cell><cell>359</cell><cell>373</cell><cell>357</cell><cell>1,000</cell></row><row><cell cols="4"># Q 3,755 3,666 3,959</cell><cell>3,679</cell><cell>7,354</cell></row></table><note>conversations): we see more open questions and unanswerable questions, due to less fluent conversation flow caused by model mistakes, and that models cannot provide feedback to questioner about whether an answer is worth following up like human answerers do (more analysis in §6.2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Steve Di Giorgio returned to the band... A 1 : ... bassist Greg Christian had left Testament again...</figDesc><table><row><cell>Unresolved coreference (44.0%)</cell></row><row><cell>Q  *  1 : What was Frenzal Rhomb's first song? A  *  1 : Punch in the Face.</cell></row><row><cell>A 1 : CANNOT ANSWER.</cell></row><row><cell>Q  *  2 : How did it fare?</cell></row><row><cell>Incoherence (39.1%)</cell></row><row><cell>Q  *  1 : Did Billy Graham succeed in becoming a chaplain? A  *  1 : He contracted mumps shortly after...</cell></row><row><cell>A 1 : After a period of recuperation in Florida, he ...</cell></row><row><cell>Q  *  2 : Did he retire after his mumps diagnosis?</cell></row><row><cell>Correct answer changed (16.9%)</cell></row><row><cell>Q  *  1 : Are there any other interesting aspects? A  *  1 : ... Q  *  2 : What happened following this change in crew?</cell></row><row><cell>Figure 3: Examples of invalid questions with predicted</cell></row><row><cell>history. Some are shortened for better demonstration.</cell></row><row><cell>Q  *  i , A  *  i : questions and gold answers from the collected</cell></row><row><cell>dataset, A i : model predictions.</cell></row><row><cell>may become unnatural or invalid when the history</cell></row><row><cell>is changed to</cell></row></table><note>. However, another issue naturally arises with predicted history: Q * i s were written by the dataset annotators based on (Q *1 , A * 1 , ..., Q * i−1 , A * i−1 ), which</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performance in automatic and human evaluations. We report overall performance on all questions and also performance on answerable questions only.where BG is the background, S * i and S i denote the inputs for gold and predicted history. After the coreference model returns entity cluster information given S * i and S i , we extract a list of entities E</figDesc><table><row><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Answerable questions</cell><cell></cell></row><row><cell></cell><cell cols="8">BERT GraphFlow HAM ExCorD BERT GraphFlow HAM ExCorD</cell></row><row><cell>Auto-Gold (F1)</cell><cell>63.2</cell><cell>64.9</cell><cell>65.4</cell><cell>67.7</cell><cell>61.8</cell><cell>66.6</cell><cell>64.5</cell><cell>66.4</cell></row><row><cell>Auto-Pred (F1)</cell><cell>54.6</cell><cell>49.6</cell><cell>57.2</cell><cell>61.2</cell><cell>52.7</cell><cell>54.5</cell><cell>54.6</cell><cell>59.2</cell></row><row><cell>Auto-Rewrite (F1)</cell><cell>54.5</cell><cell>48.2</cell><cell>57.3</cell><cell>61.9</cell><cell>51.2</cell><cell>51.9</cell><cell>55.1</cell><cell>59.7</cell></row><row><cell>Auto-Replace (F1)</cell><cell>54.2</cell><cell>47.8</cell><cell>57.1</cell><cell>61.7</cell><cell>50.7</cell><cell>51.7</cell><cell>54.8</cell><cell>59.7</cell></row><row><cell cols="2">Human (Accuracy) 82.6</cell><cell>81.0</cell><cell>87.8</cell><cell>87.9</cell><cell>75.9</cell><cell>83.2</cell><cell>84.8</cell><cell>85.3</cell></row></table><note>* = {e * 1 , ..., e * |E * | } and E = {e 1 , ..., e |E| }. 7 We say Q * i is valid only if E * = E, that is, |E * | = |E| and e * j = e j , ∀e j ∈ E,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The percentage of models' predicted unanswerable questions, and the precision and recall for detecting unanswerable questions in different evaluations. B: BERT; G: GraphFlow; H: HAM; E: ExCorD.</figDesc><table><row><cell></cell><cell cols="4">Predicted unanswerable Q.</cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell></cell><cell cols="2">Recall</cell></row><row><cell></cell><cell>B</cell><cell>G</cell><cell>H</cell><cell>E</cell><cell>B</cell><cell>G</cell><cell>H</cell><cell>E</cell><cell>B</cell><cell>G</cell><cell>H</cell><cell>E</cell></row><row><cell>Auto-Gold</cell><cell cols="3">27.1 21.5 27.1</cell><cell>28.3</cell><cell cols="8">56.8 62.3 57.1 57.9 68.1 59.3 68.4 72.5</cell></row><row><cell>Auto-Pred</cell><cell cols="3">27.8 13.8 28.6</cell><cell>28.9</cell><cell cols="8">50.0 53.9 52.3 53.3 61.4 33.0 66.1 68.2</cell></row><row><cell cols="4">Auto-Rewrite 27.3 13.1 25.1</cell><cell>26.0</cell><cell cols="8">48.6 55.0 52.4 53.9 65.7 35.7 65.1 69.4</cell></row><row><cell cols="4">Auto-Replace 27.5 12.9 25.2</cell><cell>25.7</cell><cell cols="8">48.6 54.2 52.1 53.8 66.1 34.7 64.9 68.4</cell></row><row><cell>Human</cell><cell cols="3">42.3 14.7 37.2</cell><cell>36.0</cell><cell cols="8">75.0 93.0 86.8 87.4 95.2 72.5 93.7 93.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">We use bert-base-uncased as the encoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">We restrict the annotators from English-speaking countries, and those who have finished at least 1,000 HITS with an acceptance rate of &gt;95%. The compensation rate for Amazon Mechanical Turk workers is calculated using $15/h.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">We used ParlAI<ref type="bibr" target="#b21">(Miller et al., 2017)</ref> to build the interface.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">We use the coreference model from AllenNLP<ref type="bibr" target="#b9">(Gardner et al., 2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Alexander Wettig and other members of the Princeton NLP group, and the anonymous reviewers for their valuable feedback. This research is supported by a Graduate Fellowship at Princeton University and the James Mi *91 Research Innovation Fund for Data Science.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Topiocqa: Open-domain conversational question answering with topic switching</title>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Vaibhav Adlakha</surname></persName>
		</author>
		<author>
			<persName><surname>Dhuliawala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00768</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kaheer Suleman, Harm de Vries, and Siva Reddy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open-domain question answering goes conversational via question rewriting</title>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Chappidi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.44</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="520" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DoQA -accessing domain-specific FAQs via conversational QA</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Ander Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.652</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">Jan Deriu. 2020</date>
			<biblScope unit="page" from="7302" to="7314" />
		</imprint>
	</monogr>
	<note>Mark Cieliebak, and Eneko Agirre</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">QuAC: Question answering in context</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1241</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can you unpack that? learning to rewrite questions-in-context</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1605</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5918" to="5924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><surname>Joseph L Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2501</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
				<meeting>Workshop for NLP Open Source Software (NLP-OSS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Better automatic evaluation of open-domain dialogue systems with contextualized embeddings</title>
		<author>
			<persName><forename type="first">Johnny Tian-Zheng</forename><surname>Sarik Ghazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10635</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dialog-to-action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2942" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating evaluation of open-domain dialogue systems with human generated multiple references</title>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Bigham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5944</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
				<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GRADE: Automatic graphenhanced coherence metric for evaluating opendomain dialogue systems</title>
		<author>
			<persName><forename type="first">Lishan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.742</idno>
	</analytic>
	<monogr>
		<title level="m">emnlp</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9230" to="9240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learn to resolve conversational dependency: A consistency training framework for conversational question answering</title>
		<author>
			<persName><forename type="first">Gangwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6130" to="6141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards an automatic Turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Angelard-Gontier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1116" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Do not let the history haunt you: Mitigating compounding errors in conversational question answering</title>
		<author>
			<persName><forename type="first">Angrosh</forename><surname>Mandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frans</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><surname>Coenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">USR: An unsupervised and reference free evaluation metric for dialog generation</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.64</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="681" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ParlAI: A dialog research software platform</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-2014</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple but effective method to incorporate multi-turn context with BERT for conversational machine comprehension</title>
		<author>
			<persName><forename type="first">Yasuhito</forename><surname>Ohsugi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP for Conversational AI</title>
				<meeting>the First Workshop on NLP for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Open-Retrieval Conversational Question Answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401110</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="539" to="548" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bert with history answer embedding for conversational question answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331341</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1133" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentive history selection for conversational question answering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="1391" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CoQA: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00266</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics (TACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="249" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interpretation of natural language rules in conversational machine reading</title>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2087" to="2097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10314</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Re-evaluating adem: A deeper look at scoring dialogue responses</title>
		<author>
			<persName><forename type="first">Mithun</forename><surname>Ananya B Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Das Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukundhan</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6220" to="6227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Kumar Mohankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00347</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics (TACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="810" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Shimanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12679</idno>
		<title level="m">Machine translation evaluation with bert regressor</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards a more robust evaluation for conversational question answering</title>
		<author>
			<persName><forename type="first">Wissam</forename><surname>Siblini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Sayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Kessaci</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.130</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1028" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems</title>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards quantifiable dialogue coherence evaluation</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liucun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lishan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.211</idno>
	</analytic>
	<monogr>
		<title level="m">acl</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2718" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
