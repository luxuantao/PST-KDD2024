<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeText: A Deep Text Ranking Framework with BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-06">6 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
							<email>wguo@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaowei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
							<email>sidwang@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiji</forename><surname>Gao</surname></persName>
							<email>hgao@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ananth</forename><surname>Sankar</surname></persName>
							<email>ansankar@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zimeng</forename><surname>Yang</surname></persName>
							<email>zyang@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
							<email>qguo@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
							<email>lizhang@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
							<email>blong@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
							<email>bchen@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
							<email>dagarwal@linkedin.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeText: A Deep Text Ranking Framework with BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-06">6 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2008.02460v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ranking</term>
					<term>Deep Language Models</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ranking is the most important component in a search system. Most search systems deal with large amounts of natural language data, hence an effective ranking system requires a deep understanding of text semantics. Recently, deep learning based natural language processing (deep NLP) models have generated promising results on ranking systems. BERT is one of the most successful models that learn contextual embedding, which has been applied to capture complex query-document relations for search ranking. However, this is generally done by exhaustively interacting each query word with each document word, which is inefficient for online serving in search product systems. In this paper, we investigate how to build an efficient BERT-based ranking model for industry use cases. The solution is further extended to a general ranking framework, DeText, that is open sourced and can be applied to various ranking productions. Offline and online experiments of DeText on three real-world search systems present significant improvement over state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Search systems provide relevant documents to users who are looking for specific information through queries. A user receives a list of ranked documents ordered by search relevance, where ranking plays a crucial role to model such relevance that directly affects consequential user interactions and experience. Most search systems deal with a large amount of natural language data from queries, profiles, and documents. An effective search system requires a deep understanding of the context and semantics behind natural language data to power ranking relevance.</p><p>Traditional ranking approaches largely rely on word/phrase exact matching features, which has a limited ability to capture contextual and deep semantic information. In the recent decade, deep learning based natural language processing technologies present an unprecedented opportunity to understand the deep semantics of natural language data through embedding representation <ref type="bibr" target="#b12">[13]</ref>. Moreover, to enhance contextual modeling, contextual embedding such as BERT <ref type="bibr" target="#b7">[8]</ref> has been proposed and extensively evaluated on various NLP tasks with significant improvements over existing techniques.</p><p>However, promoting the power of BERT in ranking is a nontrivial task. The current effective approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> integrate BERT as an embedding generation component in the ranking model, with the input a concatenated string of query and document texts. BERT is then fine tuned with ranking loss. The inherent transformer layer <ref type="bibr" target="#b26">[27]</ref> in BERT allows direct context sharing between query words and document words, exploiting the power of contextual modeling in BERT to the greatest extent, as the query word embeddings can incorporate many matching signals in documents. This approach, in the category of interaction based models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, comes with a significant challenge in online serving: a) the heavy BERT computation on the fly is not affordable in a real world search system; and b) the interaction based structure, as applied to concatenated query and document, precludes any embedding pre-computing that can reduce computation.</p><p>To enable an efficient BERT-based ranking model for industry use cases, we propose to use representation based structure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. Instead of applying BERT to a concatenated string of query and document texts, it generates query and document embeddings independently. It then computes the matching signals based on the query and document embeddings. This approach makes it feasible for pre-computing document embedding; thus, the online system only needs to do BERT real-time computation for queries. By independently computing query and document embeddings, however, we may lose the enhancement on the direct context sharing between queries and documents at word-level <ref type="bibr" target="#b21">[22]</ref>. This trade-off makes it a challenge to develop a BERT-based ranking model that is both effective and efficient.</p><p>In this work, we investigated the BERT-based ranking model solution with representation-based structure, and conducted comprehensive offline and online experiments on real-world search products. Furthermore, we extended the model solution into a general ranking framework, DeText (Deep Text Ranking Framework), that is able to support several state-of-the-art deep NLP components in addition to BERT. The framework comes with great flexibility to adapt to various industry use cases. For example, BERT can be applied for ranking components that have rich natural language paraphrasing; CNN can be applied when ease of deployment is a top concern for a specific system.</p><p>Beyond the ranking framework, we also summarized experience on developing an effective and efficient ranking solution with deep NLP technology, and how to balance effectiveness and efficiency for industry usage in general. We shared practical lessons of improving relevance performance while maintaining a low latency, as well as general guidance in deploying deep ranking models into search production.</p><p>The contribution of this paper is summarized below:</p><p>? We developed a representation based ranking solution powered by BERT and successfully launched it to LinkedIn's commercial search engines.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first introduce how Deep NLP models extract text embeddings, discuss their application in ranking, and then introduce the status of ranking model productionization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep NLP based Ranking Models</head><p>There are two categories of deep NLP based ranking models: representation based and interaction based models. Representation based models learn independent embeddings for the query and the document. DSSM <ref type="bibr" target="#b12">[13]</ref> averages the word embeddings as the query/document embeddings. Following this work, CLSM/LSTM-RNN <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> encodes word order information using CNN <ref type="bibr" target="#b15">[16]</ref>/LSTM <ref type="bibr" target="#b11">[12]</ref>, respectively. All these three works assume that there is only one field on the document side, and the document score is the cosine similarity score of the query/document embedding. NRM-F <ref type="bibr" target="#b29">[30]</ref> adds more fields in the document side and achieves better performance. One major weakness of representation based networks is the failure to capture local lexical matching, since the text embedding, e.g., a 128 dimensional vector, cannot summarize all the information in the original text.</p><p>To overcome the issue, interaction based models compare each part of the query with each part of the document. In DRMM <ref type="bibr" target="#b10">[11]</ref>, a cosine similarity is computed for each word embedding in the query and each word embedding in the document. The final document score is computed based on the pairwise word similarity score histogram. K-NRM <ref type="bibr" target="#b27">[28]</ref> and Conv-KNRM <ref type="bibr" target="#b6">[7]</ref> extended DRMM by kernel pooling and pairwise ngram similarity, respectively. Recently, BERT <ref type="bibr" target="#b7">[8]</ref> has shown superior performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> in ranking. It is considered an interaction based model, since the query string and document string are concatenated as one sentence, where transformer layer <ref type="bibr" target="#b26">[27]</ref> compares every word pair in that sentence.</p><p>In experiments of previous works, interaction based methods usually produce better relevance results than representation based methods, at the cost of longer computation time introduced by the pairwise word comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Productionizing Deep Neural Ranker</head><p>Commercial search engines have a strict requirement on the serving latency. Despite better relevance performance, the interaction based ranking approaches are not scalable due to the heavy interaction computation. Therefore, to our best knowledge, the representation based approaches are generally used for production.</p><p>With representation based approaches, existing work uses embedding pre-computing, either for documents <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> or for member profiles (personalization) <ref type="bibr" target="#b9">[10]</ref>. It requires a huge amount of hard disk space to store the embedding, as well as a sophisticated system design to refresh the embeddings when there are any document/profile changes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEARCH SYSTEMS AT LINKEDIN</head><p>There are many search ranking systems at LinkedIn. Figure <ref type="figure" target="#fig_1">1</ref> shows three examples: people search that retrieves member profile documents; job search that ranks job post documents; and help center search that returns FAQ documents. The number of unique documents in each search system is listed in Table <ref type="table" target="#tab_3">1</ref>. In general, the common part of these ranking systems is to discover the relevant documents, based on many hand-crafted features. Similar to other vertical searches such as Yelp or IMDB, the documents at LinkedIn are semi-structured with multiple fields. For example, member profiles contain headline, job title, company, etc. In general, the retrieval and ranking process needs to be finished around one or several hundred milliseconds. The data from these three search verticals are different in nature. The queries and documents in help center search are the most similar to natural language, i.e., the text data is more likely to be a normal sentence with proper syntax, and majority queries are paraphrases of the problem that users want to address in help center search. People search is on the other end of the spectrum: the queries and documents are mostly entities without grammar; exact keywords matching such as company names is important. Job search data lies in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DETEXT FRAMEWORK FOR BERT-BASED RANKING MODEL</head><p>In this section, we propose a BERT-based ranking framework using representation-based structure. The framework can be extended to support other neural network components, such as CNN and LSTM, for deep natural language processing. Specifically, we refer to the BERT-based ranking model as DeText-BERT, and directly illustrate the model using the open sourced DeText framework as shown in Figure <ref type="figure">2</ref>.</p><p>We design the DeText framework to be (1) general and flexible enough to cover most use cases of ranking modules in search systems; (2) able to reach a good balance between efficiency and effectiveness for practical use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>As illustrated in Figure <ref type="figure">2</ref>, given multiple source (queries, user profiles) / target (documents) texts and traditional features, the DeText framework contains 6 components: Token Embedding Layer, Text Embedding Layer, Interaction Layer, Traditional Feature Processing, Multilayer-Perceptron Layer, and Learning-to-rank Layer. Specifically, DeText-BERT model uses BERT as the text embedding layer. In the rest of this section, we will illustrate the details of each component.</p><p>Input Text Data. The input text data is generalized as source and target texts. The source texts could be queries or user profiles.</p><p>The target text could be documents. Both source and target could have multiple fields, which is different from most previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>, where only two fields (query and document) are available. There are several advantages of using multiple fields: 1). enable personalization with text fields from user profiles, and 2). achieve better and more robust results.</p><p>Token Embedding Layer. The sequence of text tokens is transformed into an embedding matrix E. For text with m tokens, the matrix has a size of d ?m, where d is the number of token embedding dimensions. Depending on the text encoding methods, different token granularities are used: in CNN/LSTM, the tokens are words; in BERT, the tokens are subwords <ref type="bibr" target="#b23">[24]</ref>.</p><p>Text Embedding Layer. Under the representation based model structure, embedding is extracted independently for each text field. The embedding can be generated through various neural network components for deep natural language processing, such as BERT, CNN, LSTM, etc. The outcome of this layer is a d-dimensional embedding vector. More details are discussed in Section 4.3 and 5.2.</p><p>Interaction Layer. The interaction between source and target only happens after the text embedding is generated, which is the key difference of representation based methods from interaction based methods. Table <ref type="table" target="#tab_4">2</ref> summarizes the different interaction methods, where u s /u t is the source/target field embedding, respectively. Note that for every source and target pair, cosine similarity generates one feature, while the Hadamard product/concatenation generates many features (a vector). Traditional Feature Processing. The existing hand-crafted features, such as personalization features, social networks features, user behavior features, are usually informative for ranking. To integrate them with deep NLP features, we use standard normalization and elementwise rescaling <ref type="bibr" target="#b0">[1]</ref> to better process the features:</p><formula xml:id="formula_0">x (1) i = x i -? ? x (2) i = wx (1) i + b</formula><p>where mean ? and standard deviation ? are pre-computed from training data, and w and b are learned in the DeText-BERT model. MLP Layer. Deep features, as the output of the interaction layer, are concatenated with the traditional features as the final features, followed by a Multilayer-Perceptron (MLP) <ref type="bibr" target="#b18">[19]</ref> layer to compute the final document score. The hidden layer in MLP is able to extract the non-linear correlations of deep features and traditional features.</p><p>LTR Layer. The last layer is the learning-to-rank layer that takes multiple target scores as input. DeText provides the flexibility of pointwise, pairwise or listwise LTR <ref type="bibr" target="#b2">[3]</ref>, as well as Lambda rank <ref type="bibr" target="#b3">[4]</ref>. Binary classification loss (pointwise learning-to-rank) can be used for systems where click probability is important to model, while pairwise/listwise LTR can be used for systems where only relative position matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Flexibility of DeText</head><p>The DeText framework enables model flexibility to adapt to demands of different productions, in terms of input data layer (multiple source/target fields), text embedding layer (CNN vs BERT), interaction layer (cosine/hadamard/concat), LTR (pointwise/pairwise/listwise), etc.</p><p>By enhancing the model flexibility, we can optimize the model effectiveness while maintaining efficiency. Firstly, representation based methods are used to bound the time complexity. Secondly, the flexibility of input data/interaction layer, together with traditional feature handling, enable us to experiment and develop scalable neural network models with strong relevance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DeText-BERT for Ranking</head><p>To use BERT in ranking model, we follow the approach of finetuning on pretrained BERT model <ref type="bibr" target="#b7">[8]</ref>: The BERT model is firstly pretrained on unsupervised data, and then fine-tuned in ranking framework with supervised clickthrough data. To extract the text embedding, we use the embedding of a special token "[CLS]". The source/target embedding will later go through the interaction layer to generate deep features.</p><p>Previous work <ref type="bibr" target="#b21">[22]</ref> shows that directly training a representation based BERT ranking model does not yield good results. This is because the BERT fine-tuning requires a small learning rate (around 1e-5). Therefore, two optimizers are used in DeText, each with a different learning rate responsible for a different part of the model. For example, in our experiments (Section 6.1.4), we set 1e-5 for BERT components, and 1e-3 for other components. Using this dual learning rates strategy, a successful representation based ranking model with BERT can be trained.</p><p>In order to reduce the online serving latency and capture domainspecific semantics, we also pretrained a compact BERT model on LinkedIn's in-domain data, named as LiBERT. More detailed can be found in Section 6.1.5.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Support CNN for Ranking in DeText</head><p>DeText framework can support CNN for deep natural language processing in the text embedding layer. It is worth noting that we use word tokens instead of triletters as in prior work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, since the latter lifts the computation (by an order of the character length of words). We follow the previous work <ref type="bibr" target="#b13">[14]</ref> to generate the text embedding from word embedding matrix E. Specifically, it uses a one-dimensional CNN along the sentence length dimension. After max-pooling, the resulting text embedding vector has f elements, where f is the number of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ONLINE DEPLOYMENT STRATEGY</head><p>The major challenge of deploying deep neural models comes from serving latency. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, two different deployment strategies, document pre-computing and two pass ranking, are designed for BERT based models and CNN based models, respectively. They are discussed in detail in the following subsections. Note that the online deployment strategies only affect ranking components; the document retrieval components stay the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DeText-BERT with Document Embedding</head><p>Pre-computing</p><p>The multiple transformer layers in BERT is computationally timeconsuming. Since DeText uses the representation based method, we are able to adopt the document embedding pre-computing approach for search ranking, as shown in the boxed section (doc pre-computing) in Figure <ref type="figure" target="#fig_3">3</ref>. For offline, document embeddings are pre-computed with BERT and saved in an embedding store, which is a fast key-value store where key is the document id, and value is the pre-computed document embedding vectors. The store is refreshed on a regular basis (e.g., daily). For online, after document candidates are retrieved from search index, the corresponding document ids are used to fetch the document embeddings from pre-computed embedding store. The benefit of this approach is to have the heavy BERT online computation only happen on the queries. It can significantly save online computation time, since the document texts are much larger than the query texts. In the setting of 10 documents for one query, the online latency can be reduced from hundreds of milliseconds to tens of milliseconds with this deployment strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DeText-CNN with Real-time Inference</head><p>DeText-CNN can be adopted with a different online integration strategy: real-time inference for both sources and targets. Compared to document embedding pre-computing, real-time inference can simplify online system design without the need of pre-computing or refreshing document embeddings. Hence the real-time inference could be a lightweight solution readily applied for many search engines. In this paper, we show that real-time inference can be achieved by (1) choosing a compact DeText structure without hurting relevance performance too much, and (2) two pass ranking that reduces 99 percentile (P99) latency. We find that a compact CNN structure with small dimensions can perform well in our experiments (Table <ref type="table" target="#tab_10">7</ref>). This is mainly because traditional handcrafted features from the production systems already contain valuable information for ranking, so that the CNN model can be focusing on the signals that are missing in the traditional features.</p><p>Even with a simple network, the CNN computation time grows linearly with the number of retrieved documents. Therefore, we use a two pass ranking schema to bound the latency (Figure <ref type="figure" target="#fig_3">3</ref>, two pass ranking box). The first ranker is a MLP <ref type="bibr" target="#b18">[19]</ref> with one hidden layer without the deep features, which is fast. After ranking, only the top ranked hundreds of documents are sent to the DeText-CNN model. <ref type="foot" target="#foot_0">2</ref> This two pass ranking framework has several benefits: 1). easy to implement and deploy; 2). the MLP ranker can filter out a large amount of irrelevant documents, which provides a relatively small candidate set with high recall for CNN ranker; and 3). the latency is bounded since CNN is applied to a small set of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we discuss the offline and online experiments of DeText on search ranking tasks in English traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setting</head><p>6.1.1 Datasets. The models are tested on three document ranking datasets, i.e., people search, job search, and help center search. The training data is collected from clickthrough data, which are sampled from 2 month traffic: 5 million queries for people search, 1.5 million queries for job search, 340 thousand queries for help center. Both development and test set have 50 thousand queries for each vertical from the later month. One query usually has 10 or more documents. Multiple document fields are used as the target fields of DeText: 1). In people search, the documents are the member profiles; three profile fields are used: headline, current position, past position. 2). In job search, the job post title, company name are used. 3). In help center search, document title and example question (illustrates the typical question for the document) are used.</p><p>6.1.2 Metrics. For both offline/online metrics, only relative metric improvement over baseline models instead of absolute values are presented, due to the company confidential policy. The online metrics are defined in Table <ref type="table" target="#tab_6">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Baseline Models.</head><p>The production models are trained with XGBoost <ref type="bibr" target="#b4">[5]</ref>. The hyper-parameters (pairwise vs listwise, number of trees, etc) are optimized by both manual tuning and auto hyper-parameter tuning, which are proven effective in LinkedIn's commercial search engines.</p><p>For each vertical search, there are existing hand-crafted traditional features. These features contain valuable information for the specific search engine verified by both offline and online experiments. The features can be categorized into three classes: 1). Text matching features. It includes not only exact matching features such as cosine similarity and jaccard similarity, but also semantic matching features, i.e., named entity ids that are obtained by applying in-house entity linking tools <ref type="bibr" target="#b24">[25]</ref>; 2). Personalization features. For example, in people search, the social network distance between the searcher and the retrieved profiles; in job search, the searcher's title overlapping with the job post title; and 3). Document popularity features. For example, in people search, static rank of a member profile; in job search/help center, the clickthrough rate of a job post/FAQ document, respectively.</p><p>6.1.4 DeText Models. Two models are evaluated in this section: DeText-LiBERT (BERT model pretrained on LinkedIn data) and Detext-CNN. The default setting of DeText training is introduced below, unless specified otherwise: 1). Token embedding layer: For DeText-CNN models, we always pretrain word embedding on the LinkedIn textual training data with Glove <ref type="bibr" target="#b20">[21]</ref>, which leads to comparable or better results than no word pretraining or existing word embedding trained on out-domain data. For DeText-LiBERT models, the word embeddings are from a BERT model pretrained on LinkedIn data. 2). Text embedding layer: For DeText-CNN, the CNN filter window size is fixed as 3 for all text fields (we do not observe significant gain from using multiple window sizes), and the number of filters is fixed as 64. For DeText-LiBERT, the model structure is described in Section 6.1.5. 3). Interaction layer: The best combination of "cosine similarity and hadamard" is used for each dataset. 4). Feature processing layer: Both normalization and element-wise re-scaling are performed on the traditional features. 5). MLP layer: one hidden layer of size 200. 6). Learning-to-rank layer: we stick to listwise, since we find listwise ranking performs better (people search and job search) or comparable (help center) to pairwise ranking.</p><p>Regarding training, both DeText-CNN and DeText-LiBERT models are trained for 2 epochs. Adam optimizer <ref type="bibr" target="#b14">[15]</ref> is used with learning rate 1e-3; for the BERT component, the learning rate is 1e-5. Each minibatch contains 256 queries with associated documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">BERT Pretraining on LinkedIn Data.</head><p>The LinkedIn text data has many domain-specific terms, such as "Pinterest", "LinkedIn", resulting in a very different vocabulary from Wikipedia, as used by Google BERT (e.g., BERT BASE ) pretraining. Thus, we pretrained a LiBERT on domain specific data, and then fine tuned the parameters during DeText-LiBERT training.</p><p>In order to reduce model serving latency, we use a smaller architecture compared to Google's BERT base model <ref type="bibr" target="#b7">[8]</ref>: (6 layers, 512 hidden, 8 heads). The resulting model has 34 million parameters, 1/3 of Google's BERT BASE model. We also use less data (around 1/5) than BERT BASE model, 685 million words vs 3.3 billion words. The statistics of LinkedIn data is listed in Table <ref type="table" target="#tab_7">4</ref>. Although LiB-ERT pretraining is conducted in an unsupervised learning manner, we collected pretraining data from the time period prior to the three verticals' training data collection period, to ensure there is no potential data leaking bias.  Overall: Table <ref type="table" target="#tab_8">5</ref> summarizes the offline NDCG percentage lift in the three search datasets. To understand the impact of deep NLP on text data, we included one baseline model DeText-MLP (DeText with only MLP and LTR layers on traditional features). Since DeText-MLP does not use any text embedding, it has comparable results as XGBoost, which is also observed in previous works <ref type="bibr" target="#b16">[17]</ref>. For DeText-CNN, it consistently outperforms the strong production baseline model by a large margin. DeText-LiBERT is able to further improve the NDCG scores. The performance of DeText-CNN/DeText-LiBERT shows that deep learning models are able to capture a lot of semantic textual matching, hence a necessary complement to the existing hand-crafted features. Meanwhile, it is worth noting that in Table <ref type="table" target="#tab_8">5</ref> deep learning models achieve the largest improvement on help center, followed by job search and people search. This is mainly caused by the genre of the data, as discussed in Section 3: 1). In the help center, there are many paraphrases of the same scenarios, for example, query "how to hide my profile updates" to FAQ document "Sharing profile changes with your network". 2). In people search, exact matching is much more important as compared to the other two searches, for example, if the query contains the company word "twitter", generally we should not return a member profile who works at "facebook", even though the word embedding of the two companies could be similar. 3). Job search has less paraphrasing than help center, but more search exploration compared to people search. LiBERT v BERT BASE : The impact of pretrained BERT on LinkedIn data is evaluated and shown in Table <ref type="table" target="#tab_9">6</ref>. In people search and job search, DeText-LiBERT significantly outperforms google's BERT BASE , i.e., DeText-BERT BASE , which should be attributed to     <ref type="table" target="#tab_10">7</ref>. We observed with a large number of filters, the gain on people and job search is relatively small (less than +0.4%). This is probably because the powerful hand-crafted features on people/job search are already integrated in the DeText model. Based on the results, we decided to adopt the CNN model with 64 filters in production to reduce the online serving latency.</p><p>Text Embedding Interaction: Table <ref type="table" target="#tab_11">8</ref> shows the impact of different text embedding interaction methods. We used cosine similarity as a baseline, and gradually added features computed by other interaction methods. The experiments show that using both cosine similarity and hadamard product features can produce the best or 2nd best results. Traditional Features: We evaluated the importance of processing traditional features, as shown in table <ref type="table" target="#tab_12">9</ref>. The first row, where no traditional features are used, proves that the traditional features are crucial in people/job search to capture social networks and personalization signals. In addition, both feature element-wise rescaling and normalization techniques are helpful; applying them together yields the best results. Multiple Fields: Table <ref type="table" target="#tab_13">10</ref> shows the impact of multiple document fields (using all the fields described in Section 6.1.1). To provide a dedicated comparison, we excluded the traditional features in this experiment. The results demonstrate that using multiple document fields can significantly improve the relevance performance. This is a practical solution for many real-world applications, since the documents in vertical search engines could be semi-structured with many text fields containing additional valuable information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Online</head><p>Experiments. We performed online experiments in the production environment with model candidates showing promising offline performance. The experiments are conducted with each model under at least 20% traffic for more than two weeks, and the best models are later fully ramped to production. All reported metrics are statistically significant (p-value &lt; 0.05) over the production baseline XGBoost.</p><p>For LiBERT models, document embeddings are refreshed daily. However, in job search, there are many new job postings on an hourly basis, which requires the embedding precomputing in a more frequent manner such as near-line update. Due to the computational resources and product priority, we leave the online experiment of DeText-LiBERT on job search to future work.</p><p>Table <ref type="table" target="#tab_14">11</ref> summarizes the experiments of DeText-CNN/DeText-LiBERT on three search engines. From CTR@5 on people and job search, we observed a similar trend in online/offline metrics: the improvement on job search is larger than on people search. Furthermore, DeText-LiBERT is consistently better than DeText-CNN in people search and help center, indicating the importance of contextual embedding on capturing deep semantics between queries and documents in search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Latency Performance.</head><p>To better understand the latency performance, the offline P99 latency on people search is provided in Table <ref type="table" target="#tab_15">12</ref>. Similar patterns on job search and help center are observed, and they are not presented due to limited space. For each worker, there are thousands of documents to score. The CNN model  in the two pass ranking will score hundreds of documents. All numbers are computed by a Intel(R) Xeon(R) 8-core CPU E5-2620 v4 @ 2.10GHz machine and 64-GB memory.</p><p>We also compared with another variant, all-decoding, that is to score all the retrieved documents on the fly. By comparing the first two settings in Table <ref type="table" target="#tab_15">12</ref>, it proves two pass ranking is effective at reducing the P99 latency. Meanwhile, the online A/B test does not show significant relevance difference between all-decoding and two pass ranking strategies.</p><p>With the document precomputing strategy, we are able to fully ramp the DeText-LiBERT models to production within latency requirements. In addition, we are interested in the LiBERT performance w.r.t. BERT BASE . Our experiments suggest that DeText-LiBERT is faster than DeText-BERT BASE , due to the smaller model structure of the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Extension of DeText to Other Tasks</head><p>In this section, we show the great potential of applying DeText to applications beyond search ranking. We conducted extra experiments on two additional ranking tasks: job recommendation and query auto completion from job search.</p><p>For job recommendation, we model the job application probability. The input is a tuple of user id, job post id, and whether the user applied for the job or not. The source fields are from user profiles, including headline, job title, company, and skill. The target fields are from job posts, including job title, job company, job skill, and job country. We used logistic regression as a baseline that is close to production setting, and evaluated with AUC <ref type="bibr" target="#b8">[9]</ref> metrics. For fair comparison, point-wise ranking (binary classification) is used with no hidden layer of MLP in DeText. Traditional features are kept the same as in the baseline model.</p><p>For query auto completion, the source fields are from member profiles, including headline, job title, and company; the target is the completed query. The baseline model is XGBoost with traditional hand-crafted features. We used the same set of traditional features in DeText with listwise LTR, and evaluated with MRR@10 [2], which is the reciprocal of the rank position of the correct answer.</p><p>Table <ref type="table" target="#tab_16">13</ref> shows the offline results. DeText outperforms the baseline models in both tasks by a large margin, indicating that DeText is flexible enough to be applied in other ranking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LESSONS LEARNED</head><p>We have conducted various experiments on several ranking tasks, where multiple practical methods are used regarding offline relevance, online deployment, latency optimization, etc. In this section, we summarize the interesting findings and practical solutions into lessons, which could be helpful for both academic and industry practitioners who apply deep NLP models for ranking tasks.</p><p>Deep NLP model performance w.r.t. language genre. Deep NLP models, especially BERT, are strong at handling paraphrasing. Help center is a good fit, since the queries are close to natural language with rich variation. For people search where queries are mostly named entities, the improvement of both CNN and BERT is smaller. Job search lies in between.</p><p>Pretraining BERT on in-domain data makes a big relevance difference. The common practice of using BERT is to pretrain on general domain such as Wikipedia, and then fine-tune it for a specific task. Our experiments suggest that for vertical search systems, it is better to pretrain BERT on in-domain data. Table <ref type="table" target="#tab_9">6</ref> shows that, with only 1/3 of the parameters of BERT BASE , LiBERT significantly outperforms BERT BASE on people search and job search, while reaching a similar performance on help center.</p><p>Handling traditional features. Production models are strong and robust with many hand-crafted traditional features. We observed that 1). after carefully handling these features (Table <ref type="table" target="#tab_12">9</ref>), deep ranking models can achieve better performance than the production models. 2). When combining the traditional features with the BERT model, different learning rates should be used.</p><p>Latency reduction solutions. Latency is one of the biggest challenges to productionize deep learning models, especially the search ranking tasks that involve many documents for one search. In this paper, we present several effective solutions:</p><p>? For heavy models such as BERT, document pre-computing can save a large amount of computation. Note that the prerequisite is representation based structure. ? With two pass ranking, we can deploy a compact CNN based ranking model for real time inference in production for both queries and documents.</p><p>? Pretraining a BERT model on in-domain data can maintain the same level of relevance performance, while significantly reducing computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 www.github.com/linkedin/detext (a) People Search (b) Job Search (c) Help Center</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The first two figures show the search result of "cloud computing" in people search/job search, respectively. The last figure shows an example of query "ask for recommendation" in help center search.</figDesc><graphic url="image-7.png" coords="2,468.41,77.71,86.38,163.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Document embedding pre-computing and twopass ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1  </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Summary of three vertical searches.</figDesc><table><row><cell></cell><cell cols="3">People Job Help Center</cell></row><row><cell>No. of Unique Docs</cell><cell>600M</cell><cell>20M</cell><cell>2,700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Interaction features.</figDesc><table><row><cell>Cosine similarity</cell><cell>u ? s ut ?us ???ut ?</cell><cell>one feature per source/target pair</cell></row><row><cell>Hadamard product</cell><cell>u q ? u d</cell><cell>d features per source/target pair</cell></row><row><cell>Concatenation</cell><cell>u q ? u d</cell><cell>d features per text field</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The DeText framework. In this figure, there are two source fields, and two target fields.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Traditional feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>processing</cell></row><row><cell></cell><cell></cell><cell>CNN/LSTM</cell><cell>CNN/LSTM</cell><cell>CNN/LSTM</cell><cell>CNN/LSTM</cell></row><row><cell></cell><cell></cell><cell>/BERT</cell><cell>/BERT</cell><cell>/BERT</cell><cell>/BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>d ? m 2</cell><cell>d ? n 1</cell><cell>d ? n 2</cell></row><row><cell></cell><cell>Token</cell><cell></cell><cell></cell></row><row><cell></cell><cell>embedding</cell><cell></cell><cell></cell></row><row><cell></cell><cell>layer</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>source_field 2</cell><cell>target_field 1</cell><cell>target_field 2</cell></row><row><cell>Index Query</cell><cell>Doc IDs MLP Model Figure 2: Retrieval Documents (1st pass)</cell><cell>Ranking (doc pre-computing) DeText Model Doc Embeddings Offline Precomputed Embedding Store (Regularly refreshed) DeText Model Top k (2nd pass) Documents</cell><cell>Ranked Documents Ranked Documents</cell></row><row><cell></cell><cell></cell><cell>Ranking (two pass ranking)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Online metrics definitions. Proportion of searches that received a click at top 5 items Session Success Rate Proportion of search sessions that received a click. A new session is created if the user has no activity in help center search for 30 minutes.</figDesc><table><row><cell>Metric</cell><cell>Definition</cell></row><row><cell>CTR@5</cell><cell></cell></row><row><cell>Job Apply</cell><cell>Job search metric. Number of job applications from search.</cell></row><row><cell>Happy Path Rate</cell><cell>Help center search metric. Proportion of users who searched</cell></row><row><cell></cell><cell>and clicked a document without using help center search</cell></row><row><cell></cell><cell>again in that day, nor creating a ticket.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>LinkedIn data for BERT pretraining.</figDesc><table><row><cell>Data Source</cell><cell>Description</cell><cell># Words</cell></row><row><cell>Search queries</cell><cell>Query reformulation pairs</cell><cell>204M</cell></row><row><cell cols="2">Member profiles Member headlines and summaries</cell><cell>98M</cell></row><row><cell></cell><cell cols="2">Member position titles and descriptions 105M</cell></row><row><cell>Job posts</cell><cell>Job titles and descriptions</cell><cell>217M</cell></row><row><cell>Help center</cell><cell>Queries and doc titles</cell><cell>61M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Offline NDCG@10 score percentage lift in three searches over the production baseline XGBoost. ? and ? denote statistically significant improvements at p &lt; 0.05 using a two-tailed t-test over XGBoost and DeText-CNN, respectively.</figDesc><table><row><cell>Models</cell><cell cols="3">People Search Job Search Help Center</cell></row><row><cell>DeText-MLP</cell><cell>-0.07%</cell><cell>+0.05%</cell><cell>+0.15%</cell></row><row><cell>DeText-CNN</cell><cell>+3.02%  ?</cell><cell>+4.65%  ?</cell><cell>+11.56%  ?</cell></row><row><cell>DeText-LiBERT</cell><cell>+3.38%  ? ?</cell><cell>+6.14%  ? ?</cell><cell>+13.94%  ? ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>General BERT vs in-domain BERT on NDCG@10.</figDesc><table><row><cell>Model</cell><cell cols="3">People Search Job Search Help Center</cell></row><row><cell>DeText-CNN</cell><cell>+3.02%</cell><cell>+4.65%</cell><cell>+11.56%</cell></row><row><cell>DeText-BERT BASE</cell><cell>+3.08%</cell><cell>+3.60%</cell><cell>+13.80%</cell></row><row><cell>DeText-LiBERT</cell><cell>+3.38%</cell><cell>+6.14%</cell><cell>+13.94%</cell></row><row><cell cols="3">6.2 Search Ranking Experiments</cell><cell></cell></row></table><note><p>6.2.1 Offline Experiments. All the relative percentage lift is calculated w.r.t the production baseline model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Number of CNN filters in DeText-CNN on NDCG@10.</figDesc><table><row><cell cols="4">#Filters People Search Job Search Help Center</cell></row><row><cell>64</cell><cell>+3.02%</cell><cell>+4.65%</cell><cell>+11.56%</cell></row><row><cell>128</cell><cell>+3.07%</cell><cell>+4.81%</cell><cell>+11.94%</cell></row><row><cell>256</cell><cell>+3.10%</cell><cell>+4.82%</cell><cell>+12.37%</cell></row><row><cell>512</cell><cell>+3.16%</cell><cell>+4.92%</cell><cell>+12.74%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Text embedding interaction in DeText-CNN on NDCG@10.</figDesc><table><row><cell>Interaction</cell><cell>People</cell><cell>Job</cell><cell>Help Center</cell></row><row><cell>cosine</cell><cell cols="2">+2.67% +4.25%</cell><cell>+11.02%</cell></row><row><cell>cosine, hadamard</cell><cell cols="2">+3.02% +4.65%</cell><cell>+11.56%</cell></row><row><cell>cosine, concat</cell><cell cols="2">+2.39% +4.62%</cell><cell>+10.09%</cell></row><row><cell cols="3">cosine, hadamard, concat +2.84% +4.73%</cell><cell>+11.49%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Traditional features for DeText-CNN models on NDCG@10. The first row does not use any traditional features.</figDesc><table><row><cell cols="4">Trad-ftr Rescale Norm People</cell><cell>Job</cell><cell>Help Center</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">-4.52% -9.98%</cell><cell>+11.07%</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">+2.31% +3.17%</cell><cell>+11.13%</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">+2.47% +3.44%</cell><cell>+11.55%</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">+2.71% +4.49%</cell><cell>+11.24%</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">+3.02% +4.65%</cell><cell>+11.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>The impact of using multiple fields. In the single target field setting, the most important field is used: headline for people search and job post title for job search. In this experiment, all the traditional features are excluded. Note that DeText-CNN with a single target field is a special version of CLSM model<ref type="bibr" target="#b25">[26]</ref> that operates on words. To better understand the trade-off on DeText-CNN models w.r.t. efficiency and effectiveness, we experimented with different numbers of CNN filters, shown in Table</figDesc><table><row><cell>Model</cell><cell>#fields</cell><cell>People</cell><cell>Job</cell></row><row><cell>DeText-CNN (CLSM on words)</cell><cell>single</cell><cell cols="2">-5.20% -12.83%</cell></row><row><cell>DeText-LiBERT</cell><cell>single</cell><cell cols="2">-3.14% -10.30%</cell></row><row><cell>DeText-CNN</cell><cell cols="2">multiple -4.52%</cell><cell>-9.98%</cell></row><row><cell>DeText-LiBERT</cell><cell cols="2">multiple -2.51%</cell><cell>-7.20%</cell></row><row><cell cols="4">the pretraining on in-domain data. In the help center where vo-</cell></row><row><cell cols="4">cabulary and language are closer to Wikipedia, LiBERT can still</cell></row><row><cell cols="4">achieve comparable results. It is worth noting LiBERT has only 1/3</cell></row><row><cell>of the parameters of BERT BASE .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Limit of CNN:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Online experiments of DeText-CNN and DeText-LiBERT.</figDesc><table><row><cell>Search</cell><cell>Model</cell><cell>Metrics</cell><cell>Percentage Lift</cell></row><row><cell cols="2">People search DeText-CNN</cell><cell>CTR@5</cell><cell>+1.13%</cell></row><row><cell></cell><cell></cell><cell cols="2">Session Success Rate neutral</cell></row><row><cell></cell><cell cols="2">DeText-LiBERT CTR@5</cell><cell>+1.56%</cell></row><row><cell></cell><cell></cell><cell cols="2">Session Success Rate +0.23%</cell></row><row><cell>Job search</cell><cell>DeText-CNN</cell><cell>CTR@5</cell><cell>+3.16%</cell></row><row><cell></cell><cell></cell><cell>Job Apply</cell><cell>+0.73%</cell></row><row><cell>Help center</cell><cell>DeText-CNN</cell><cell>Happy Path Rate</cell><cell>+15.0%</cell></row><row><cell></cell><cell></cell><cell cols="2">Session Success Rate +6.1%</cell></row><row><cell></cell><cell cols="2">DeText-LiBERT Happy Path Rate</cell><cell>+26.1%</cell></row><row><cell></cell><cell></cell><cell cols="2">Session Success Rate +11.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>The latency at 99 percentile on people search.</figDesc><table><row><cell>Model</cell><cell cols="2">Deployment Strategy Time</cell></row><row><cell>DeText-CNN People</cell><cell>all-decoding</cell><cell>+55ms</cell></row><row><cell>DeText-CNN People</cell><cell>two pass ranking</cell><cell>+21ms</cell></row><row><cell>DeText-LiBERT people</cell><cell>doc pre-computing</cell><cell>+43ms</cell></row><row><cell cols="2">DeText-BERT BASE people doc pre-computing</cell><cell>+71ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Offline experiments of DeText-CNN on job recommendation and query auto completion datasets. Both improvements are statistically significant at p &lt; 0.05.</figDesc><table><row><cell>Tasks</cell><cell>Metrics</cell><cell>Percentage Lift</cell></row><row><cell>Job Recommendation</cell><cell>AUC</cell><cell>+3.01%</cell></row><row><cell cols="3">Query Auto Completion MRR@10 +4.72%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>this paper, we propose the DeText (deep text) ranking framework with BERT/CNN based ranking model for practical usage in industry. To accommodate the requirements of different ranking productions, DeText allows flexible configuration, such as input data, text embedding extraction, traditional feature handling, etc. These choices enable us to experiment and develop scalable neural network models with strong relevance performance. Our offline experiments show that DeText-LiBERT/DeText-CNN consistently outperforms the strong production baselines. The resulting models are deployed into three vertical searches in LinkedIn's commercial search engines.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that the top hundreds of documents are in one worker, while the online ranking is distributed to many workers. Each worker is responsible for retrieving and ranking the documents on its own shard.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature normalization and likelihoodbased similarity measures for image retrieval</title>
		<author>
			<persName><forename type="first">Selim</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Context-sensitive query auto-completion</title>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Yossef</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Kraus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>In WWW</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From ranknet to lambdarank to lambdamart: An overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeper Text Understanding for IR with Contextual Neural Language Modeling</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time personalization using embeddings for search ranking at airbnb</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J??rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining Decision Trees and Neural Networks for Learning-to-Rank in Personal Search</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multilayer perceptron, fuzzy sets, and classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushmita</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<title level="m">Understanding the Behaviors of BERT in Ranking</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Krishnaram Kenthapadi, and Sahin Cem Geyik. 2018. Towards Deep and Representation Learning for Talent Search at LinkedIn</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gungor</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cagri</forename><surname>Ozcaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianren</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Context-aware map from entities to canonical forms</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Merhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Jiang</surname></persName>
		</author>
		<idno>App. 15/189</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">974</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ranking relevance in yahoo search</title>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Dawei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mianwei</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chikashi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Nobata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural ranking models with multiple document fields</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Nick Craswell, and Saurabh Tiwary</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
