<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-22">22 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWKV Foundation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Barcelona</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Charm Therapeutics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara 7 Zendesk</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Arcadinho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>9 Peking University 10 Storyteller.io 11 Crisis24</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Chung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Grella</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kiran</forename><surname>Kranthi</surname></persName>
						</author>
						<author>
							<persName><surname>Gv</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">New York University 13 National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuzheng</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haowen</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Przemys?aw</forename><surname>Kazienko</surname></persName>
							<affiliation key="aff8">
								<orgName type="laboratory">Lab 18</orgName>
								<orgName type="institution">Wroclaw University of Science and Technology</orgName>
								<address>
									<addrLine>15 Databaker Technology Co. Ltd 16 Purdue University 17 Criteo AI</addrLine>
									<settlement>Epita</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Koco</surname></persName>
							<affiliation key="aff8">
								<orgName type="laboratory">Lab 18</orgName>
								<orgName type="institution">Wroclaw University of Science and Technology</orgName>
								<address>
									<addrLine>15 Databaker Technology Co. Ltd 16 Purdue University 17 Criteo AI</addrLine>
									<settlement>Epita</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bart?omiej</forename><surname>Koptyra</surname></persName>
							<affiliation key="aff8">
								<orgName type="laboratory">Lab 18</orgName>
								<orgName type="institution">Wroclaw University of Science and Technology</orgName>
								<address>
									<addrLine>15 Databaker Technology Co. Ltd 16 Purdue University 17 Criteo AI</addrLine>
									<settlement>Epita</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hayden</forename><surname>Lau</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Sri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ipsit</forename><surname>Mantri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ferdinand</forename><surname>Mom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Atsushi</forename><surname>Saito</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="institution">Nextremer Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
							<affiliation key="aff10">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johan</forename><forename type="middle">S</forename><surname>Wind</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">University of Oslo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stanis?aw</forename><surname>Wo?niak</surname></persName>
							<affiliation key="aff8">
								<orgName type="laboratory">Lab 18</orgName>
								<orgName type="institution">Wroclaw University of Science and Technology</orgName>
								<address>
									<addrLine>15 Databaker Technology Co. Ltd 16 Purdue University 17 Criteo AI</addrLine>
									<settlement>Epita</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruichong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>9 Peking University 10 Storyteller.io 11 Crisis24</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">EleutherAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qihang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff12">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff13">
								<orgName type="institution">Kuaishou Technology Co</orgName>
								<address>
									<country>Ltd</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff14">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui-Jie</forename><surname>Zhu</surname></persName>
							<affiliation key="aff15">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
							<affiliation key="aff16">
								<orgName type="institution">University of Electronic Science and Technology</orgName>
								<address>
									<country>China 27 RuoxinTech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tamas</forename><surname>Sar- Los</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">2022</forename><surname>R?</surname></persName>
							<affiliation key="aff10">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Ef</surname></persName>
						</author>
						<author>
							<persName><forename type="first">?aglar</forename><surname>G?l?ehre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
						</author>
						<title level="a" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-22">22 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.13048v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning techniques have made significant strides in artificial intelligence, playing a pivotal role in various scientific and industrial applications. These applications often involve complex sequential data processing tasks that include natural language understanding, conversational AI, time-series analysis, and even indirect modalities that can be reframed as sequences, such as images and graphs <ref type="bibr" target="#b9">(Brown et al., 2020;</ref><ref type="bibr" target="#b17">Ismail Fawaz et al., 2019;</ref><ref type="bibr" target="#b63">Wu et al., 2020;</ref><ref type="bibr" target="#b0">Albalak et al., 2022)</ref>. Predominant among these techniques are RNNs, convolutional neural networks (CNNs), and the Transformer models <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>.</p><p>Each of these has distinct drawbacks that restrict their efficiency in certain scenarios. RNNs suffer from the vanishing gradient problem, making them difficult to train for long sequences. Additionally, they cannot be parallelized in the time dimension during training, which restricts their scalability <ref type="bibr" target="#b13">(Hochreiter, 1998;</ref><ref type="bibr" target="#b29">Le and Zuidema, 2016)</ref>. CNNs, on the other hand, are only adept at capturing local patterns, which limits their capacity to deal with long-range dependencies, crucial to many sequence processing tasks <ref type="bibr" target="#b2">(Bai et al., 2018)</ref>.</p><p>Transformer models emerged as a powerful alternative due to their ability to handle both local and long-range dependencies and their capability for parallelized training <ref type="bibr" target="#b55">(Tay et al., 2022)</ref>. Recent models such as GPT-3 <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>, <ref type="bibr">ChatGPT (OpenAI, 2022;</ref><ref type="bibr" target="#b27">Koco? et al., 2023)</ref>, <ref type="bibr">GPT-4 (Ope-nAI, 2023)</ref>, LLaMA <ref type="bibr" target="#b57">(Touvron et al., 2023)</ref>, and Chinchilla <ref type="bibr" target="#b15">(Hoffmann et al., 2022)</ref> exemplify the capability of this architecture, pushing the frontiers of what's possible in NLP. Despite these significant advancements, the self-attention mechanism inherent to Transformers poses unique challenges,  <ref type="bibr" target="#b26">(Kitaev et al., 2020)</ref>, Linear Transformer <ref type="bibr" target="#b25">(Katharopoulos et al., 2020)</ref>, Performer <ref type="bibr">(Choromanski et al., 2020)</ref>, AFT <ref type="bibr" target="#b69">(Zhai et al., 2021)</ref>, MEGA <ref type="bibr" target="#b33">(Ma et al., 2023)</ref>. Here T denotes the sequence length, d the feature dimension, and c is MEGA's chunk size of quadratic attention.</p><p>primarily due to its quadratic complexity. This complexity renders the architecture computationally expensive and memory-intensive for tasks involving long input sequences or in resource-constrained situations. These limitations have spurred a wealth of research aiming to improve the scaling properties of Transformers, often at the expense of some of the properties that make it so effective <ref type="bibr" target="#b62">(Wang et al., 2020;</ref><ref type="bibr" target="#b66">Zaheer et al., 2020;</ref><ref type="bibr">Dao et al., 2022a)</ref>.</p><p>To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, a novel architecture that effectively combines the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV is carefully designed to alleviate the memory bottleneck and quadratic scaling associated with Transformers <ref type="bibr" target="#b25">(Katharopoulos et al., 2020)</ref> with a more efficient linear scaling, while still preserving the rich, expressive properties that make the Transformer a dominant architecture in the field.</p><p>One of the defining characteristics of RWKV is its ability to offer parallelized training and robust scalability, similar to Transformers. Moreover, we have reformulated the attention mechanism in RWKV to introduce a variant of linear attention, eschewing the traditional dot-product token interaction in favor of more effective channeldirected attention. This approach contrasts significantly with the traditional Transformer architecture, where specific token interactions predominantly drive attention. The implementation of linear attention in RWKV is carried out without approximation, which offers a considerable improvement in efficiency and enhances the scalability, see Table <ref type="table" target="#tab_0">1</ref>.</p><p>The overarching motivation behind developing RWKV is to bridge the gap between computational efficiency and expressive capacity in neural net-work architectures. It offers a promising and viable solution for handling tasks involving large-scale models with billions of parameters, exhibiting competitive performance at a fraction of the computational cost. Our experimental results suggest that RWKV could be a valuable tool for addressing the ongoing challenges in scaling and deploying AI models across various domains, particularly those involving sequential data processing. Thus, RWKV paves the way for the next generation of more sustainable and computationally efficient AI models for sequence processing tasks.</p><p>Our contributions in this paper are as follows:</p><p>? We introduce the RWKV network architecture, which combines the advantages of RNNs and Transformers while mitigating their known limitations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, a number of techniques have been proposed to address the limitations of transformers.</p><p>Optimizing Attention Mechanism Many transformer variants ("x-formers") have been introduced to reduce the complexity of transformers <ref type="bibr" target="#b55">(Tay et al., 2022)</ref>, including sparse attention <ref type="bibr" target="#b4">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b26">Kitaev et al., 2020;</ref><ref type="bibr">Guo et al., 2022)</ref>, approximating the full attention matrix <ref type="bibr" target="#b62">(Wang et al., 2020;</ref><ref type="bibr" target="#b32">Ma et al., 2021;</ref><ref type="bibr">Choromanski et al., 2020)</ref>, combining chunked attention with gating <ref type="bibr" target="#b33">(Ma et al., 2023)</ref> and other efficient methods <ref type="bibr" target="#b25">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b18">Jaegle et al., 2021)</ref>. Some recent works like FlashAttention <ref type="bibr">(Dao et al., 2022a)</ref> and others <ref type="bibr" target="#b49">(Rabe and Staats, 2022;</ref><ref type="bibr" target="#b19">Jang et al., 2019)</ref> share similarities with RWKV's chunked computation scheme. Despite being memory-efficient, their time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV achieves better space and time complexity during inference by formulating a linear attention as an RNN.</p><p>Attention Free Models Another line of research replaces the attention mechanism with other modules to scale to long sequences. MLP-Mixer and others <ref type="bibr" target="#b56">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b31">Liu et al., 2021)</ref> proposed the replacement of attention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer (AFT) <ref type="bibr" target="#b69">(Zhai et al., 2021)</ref> replaces dot-product self-attention with a computationally efficient alternative which can be seen as a multi-head attention where each feature dimension corresponds to a head. Inspired by AFT, RWKV takes a similar approach but modifies the interaction weights for simplicity such that it can be transformed into an RNN. In parallel, RNNstyle <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Chung et al., 2014)</ref> recursive components have also been modified to increase context length, such as the Recurrent Memory Transformer <ref type="bibr" target="#b11">(Bulatov et al., 2022</ref><ref type="bibr" target="#b10">(Bulatov et al., , 2023) )</ref> and Linear Recurrent Units <ref type="bibr" target="#b41">(Orvieto et al., 2023)</ref>. State space models (SSM) like S4 <ref type="bibr">(Gu et al., 2022)</ref> and its variants <ref type="bibr">(Dao et al., 2022b;</ref><ref type="bibr" target="#b46">Poli et al., 2023)</ref> are also proposed.</p><p>Notably, Quasi-Recurrent neural network (QRNN) <ref type="bibr" target="#b8">(Bradbury et al., 2017</ref>) uses both convolutional layers and recurrent pooling functions across timesteps and channels. While QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as an attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN, RWKV includes a parametrized channel-mixing module (see the green blocks in Fig. <ref type="figure" target="#fig_0">1c</ref>) that is parallelizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Here we briefly review the fundamentals of RNNs and Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Neural Networks (RNNs)</head><p>Popular RNN architectures such as LSTM <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref> and <ref type="bibr">GRU (Chung et al., 2014)</ref> are characterized by the following formulation (shown for LSTM, others can be reasoned similarly):</p><formula xml:id="formula_0">f t = ? g (W f x t + U f h t-1 + b f ), (1) i t = ? g (W i x t + U i h t-1 + b i ), (2) o t = ? g (W o x t + U o h t-1 + b o ), (3) ct = ? c (W c x t + U c h t-1 + b c ), (4) c t = f t c t-1 + i t ct , (5) h t = o t ? h (c t ).<label>(6)</label></formula><p>The data flow of RNNs is shown in Fig. <ref type="figure" target="#fig_0">1a</ref>. Although RNNs can be factored into two linear blocks (W and U ) and an RNN-specific block (1)-( <ref type="formula" target="#formula_0">6</ref>), as noted by <ref type="bibr" target="#b8">Bradbury et al. (2017)</ref>, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformers and AFT</head><p>Introduced by <ref type="bibr" target="#b58">Vaswani et al. (2017)</ref>, Transformers are a class of neural networks that have become the dominant architecture for several NLP tasks. Instead of operating on sequences step-by-step like RNNs, Transformers rely on attention mechanisms to capture relationships between all input and all output tokens:</p><formula xml:id="formula_1">Attn(Q, K, V ) = softmax(QK )V,<label>(7)</label></formula><p>where the multi-headness and scaling factor 1</p><formula xml:id="formula_2">? d k</formula><p>is omitted for convenience. The core QK multiplication is an ensemble of pairwise attention scores between each token in a sequence, which can be decomposed as vector operations:</p><formula xml:id="formula_3">Attn(Q, K, V ) t = T i=1 e q t k i v i T i=1 e q t k i .<label>(8)</label></formula><p>In AFT <ref type="bibr" target="#b69">(Zhai et al., 2021)</ref>, this is alternately formulated as</p><formula xml:id="formula_4">Attn + (W, K, V ) t = t i=1 e w t,i +k i v i t i=1 e w t,i +k i , (<label>9</label></formula><formula xml:id="formula_5">)</formula><p>where {w t,i } ? R T ?T is the learned pair-wise position biases, and each w t,i is a scalar. Inspired by AFT, we let each w t,i in RWKV be a channel-wise time decay vector multiplied by the relative position, traced backwards from current time as it decays:</p><formula xml:id="formula_6">w t,i = -(t -i)w,<label>(10)</label></formula><p>where w ? (R ?0 ) d , with d the number of channels. We require w to be non-negative to ensure that e w t,i ? 1 and the per-channel weights decay backwards in time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Receptance Weighted Key Value (RWKV) Model</head><p>The RWKV architecture derives its name from the four primary model elements used in the timemixing and channel-mixing blocks:</p><p>? R: Receptance vector acting as the acceptance of past information. ? W : Weight is the positional weight decay vector. A trainable model parameter. ? K: Key is a vector analogous to K in traditional attention. ? V : Value is a vector analogous to V in traditional attention.</p><p>Interactions between the main elements for every timestep are multiplicative, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> 4.1 High-Level Summary</p><p>The RWKV architecture is comprised of a series of stacked residual blocks, each formed by a timemixing and a channel-mixing sub-blocks with recurrent structures.</p><p>The recurrence is formulated both as a linear interpolation between the current input and the input at the previous time step (a technique we refer to as time-shift mixing or token shift, indicated by the diagonal lines in Fig. <ref type="figure" target="#fig_2">3</ref>), which can be adjusted independently for every linear projection of the input embedding (e.g., R, K, V in time-mixing, and R, K in channel-mixing), and as the time-dependent update of the W KV which is formalized in equation 14. The W KV computation is similar to AFT <ref type="bibr" target="#b69">(Zhai et al., 2021)</ref>, but W is now a channel-wise vector multiplied by relative position rather than a pairwise matrix in AFT. We also introduce a vector U for separately attending to the current token in order to compensate for potential degeneration of W (see Appendix G for more details).   The time-mixing block is given by:</p><formula xml:id="formula_7">r t = W r ? (? r x t + (1 -? r )x t-1 ), (11) k t = W k ? (? k x t + (1 -? k )x t-1 ), (12) v t = W v ? (? v x t + (1 -? v )x t-1 ), (<label>13</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">wkv t = t-1 i=1 e -(t-1-i)w+k i v i + e u+kt v t t-1 i=1 e -(t-1-i)w+k i + e u+kt ,<label>(14)</label></formula><formula xml:id="formula_10">o t = W o ? (?(r t ) wkv t ),<label>(15)</label></formula><p>where the W KV computation, wkv t , plays the role of Attn(Q, K, V ) in Transformers without incurring a quadratic cost as interactions are between scalars. Intuitively, as time t increases, the vector o t is dependent on a long history, represented by the summation of an increasing number of terms. For the target position t, RWKV performs a weighted summation in the positional interval of [1, t], and then multiplies with the receptance ?(r). Therefore, interactions are multiplicative inside a given timestep and summed over different timesteps.</p><p>Further, the channel-mixing block is given by:</p><formula xml:id="formula_11">r t = W r ? (? r x t + (1 -? r )x t-1 ),<label>(16)</label></formula><formula xml:id="formula_12">k t = W k ? (? k x t + (1 -? k )x t-1 ),<label>(17)</label></formula><formula xml:id="formula_13">o t = ?(r t ) (W v ? max(k t , 0) 2 ),<label>(18)</label></formula><p>where we adopt squared ReLU activation <ref type="bibr" target="#b53">(So et al., 2021)</ref>. Note that in both time-mixing and channelmixing, by taking the sigmoid of the receptance, we're intuitively using it as a "forget gate" to eliminate unnecessary historical information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformer-like Parallelization</head><p>RWKV can be efficiently parallelized in what we call a time-parallel mode, reminiscent of Transformers. The time complexity of processing a batch of sequences in a single layer is O(BT d 2 ), which mainly consists of matrix multiplications W , ? {r, k, v, o} (assuming B sequences, T maximum tokens and d channels). Meanwhile, updating attention scores wkv t requires a serial scan (see Appendix B for more detail) and has complexity O(BT d).</p><p>The matrix multiplications can be parallelized akin to W , ? {Q, K, V, O} in typical Transformers. The element-wise W KV computation is time-dependent, but can be readily parallelized along the other two dimensions <ref type="bibr" target="#b30">(Lei et al., 2018)</ref> </p><formula xml:id="formula_14">3 .</formula><p>Additionally, token shift is implemented as a simple offset in the temporal dimension at each block using PyTorch <ref type="bibr" target="#b45">(Paszke et al., 2019</ref>) library as nn.ZeroPad2d((0,0,1,-1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RNN-like Sequential Decoding</head><p>It is common in recurrent networks to use output at state t as input at state t + 1. This is especially evident in the autoregressive decoding inference of a language model, requiring each token to be computed before fed into the next step, making it possible for RWKV to take advantage of its RNNlike structure, referred to as time-sequential mode.</p><p>In such circumstances, RWKV can be conveniently formulated recursively for decoding during inference, as shown in Appendix B, which leverages the advantage that each output token is dependent only on the latest state, which is of constant size, irrespective of the sequence length.</p><p>It then behaves as an RNN decoder, yielding constant speed and memory footprint with respect to the sequence length, enabling the processing of longer sequences more efficiently. In contrast, selfattention typically requires a KV cache growing linearly with respect to the sequence length, resulting in degraded efficiency and increasing memory footprint and time as the sequence grows longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Software Implementation</head><p>RWKV is originally implemented using the Pytorch Deep Learning Library <ref type="bibr" target="#b45">(Paszke et al., 2019)</ref> and a custom CUDA kernel for the W KV computation explained in 4.7. Although RWKV is a general recurrent network, its current implementation focuses in the task of language modeling (RWKV-LM). The model architecture is comprised of an embedding layer, for which we follow the setup described in Section 4.7 and several identical residual blocks applied sequentially as seen in Fig. <ref type="figure" target="#fig_1">2</ref> and 3 following the principles outlined in Section 4.6. After the last block, a simple output projection head composed by a LayerNorm <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> and a linear projection is used to obtain the logits to be used in the next-token prediction task and calculate the cross entropy loss during training. Both the embeddings generated after the last residual block and the logits could also be used later for downstream NLP tasks. Training is performed in time-parallel mode (Section 4.2) while autoregressive inference and a potential chat interface 4 leverage the time-sequential mode (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Gradient Stability and Layer Stacking</head><p>The RWKV architecture has been designed as a fusion of both Transformers and RNNs, offering the advantage of stable gradients and deeper architectures of Transformers compared to traditional RNNs while being efficient in inference.</p><p>Previous work has sought to tackle the problem of gradient stability in RNNs with a variety of techniques including using non-saturated activation functions <ref type="bibr">(Chandar et al., 2019)</ref>, gating mechanism <ref type="bibr">(Gu et al., 2019)</ref>, gradient clipping <ref type="bibr" target="#b44">(Pascanu et al., 2012)</ref>, and adding constraints <ref type="bibr" target="#b23">(Kanai et al., 2017;</ref><ref type="bibr" target="#b37">Miller and Hardt, 2018)</ref>. While these techniques have seen little success, RWKV avoids the problem inherently by utilizing softmax in conjunction with RNN-style updates.</p><p>The RWKV model features a single-step process for updating attention-like scores, which includes a time-dependent softmax operation that helps numerical stability and guards against vanishing gradients (for rigorous proof, see Appendix F). Intuitively, this operation ensures the gradient is propagated along the most relevant path. Layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> is another key aspect of the architecture which enhances the training dynamics of deep neural networks by stabilizing gradients, addressing both vanishing and exploding gradient issues.</p><p>These design elements not only contribute to the RWKV architecture's stability and learning capabilities but enable the stacking of multiple layers in a manner that surpasses the capabilities of any existing RNN. In doing so, the model is able to capture more complex patterns across various levels of abstraction (see also Appendix G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Harnessing Temporal Structure for Sequential Data Processing</head><p>RWKV captures and propagates sequential information through the combination of three mechanisms: recurrence, time decay and token shift.</p><p>The recurrence in the time-mixing block of RWKV is the basis for the model's capacity to capture intricate relationships between sequence elements and to propagate locality information through time.</p><p>The time decay mechanism (e -w and e u in equation 14), maintains sensitivity to the positional relationship between sequence elements. By gradually diminishing the influence of past information over time, the model preserves a sense of temporal locality and progression, which is essential for sequential processing. This treatment of positional information in sequential data exhibits similarities to the Attention with Linear Biases (ALiBi) model <ref type="bibr" target="#b47">(Press et al., 2022)</ref>, where the linear biases facilitate input length extrapolation. In this context, the RWKV architecture can be perceived as a trainable version of ALiBi, seamlessly incorporating positional information without the necessity for explicit encoding. It can also be seen as an extension of the gated convolution introduced in <ref type="bibr" target="#b69">Zhai et al. (2021)</ref> to the full sequence length until a given step.</p><p>The token shift or time-shift mixing, or (diagonal arrows in Figure <ref type="figure" target="#fig_2">3</ref>), also contributes to the model's adaptation to sequential data. By linearly interpolating between the current input and the previous time step input, the model naturally aggregates and gates information in the input channels. The overall structure of time-shift mixing bears resemblance to the causal convolution with no dilations in WaveNet <ref type="bibr">(van den Oord et al., 2016)</ref>, which is a classical architecture used for forecasting time series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Additional Optimizations</head><p>Custom Kernels To address inefficiencies in the W KV computation due to the sequential nature of the task when using standard deep learning frameworks, we implement a custom CUDA kernel so as to launch a single compute kernel in training accelerators. All other parts of the model are matrix multiplications and point-wise operations that can already be efficiently parallelized.</p><p>FFN with R gate Prior research <ref type="bibr" target="#b56">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b31">Liu et al., 2021;</ref><ref type="bibr" target="#b65">Yu et al., 2022)</ref> suggests that self-attention may not be as essential in Transformer-based vision tasks as previously thought. Although it provided us with some insights, replacing self-attention entirely in natural language tasks could be too drastic. In our study, we partially dismantle the attention mechanism by replacing the fixed QKV formula with KV and introducing a new time-decaying factor W . This approach enables us to incorporate token and channelmixing components akin to MLP-mixer <ref type="bibr" target="#b56">(Tolstikhin et al., 2021)</ref> and a gating unit R similar to gMLP <ref type="bibr" target="#b31">(Liu et al., 2021)</ref>, which enhance the performance of our RWKV model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small Init Embedding</head><p>During the initial stage of training a transformer model <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>, we observe that the embedding matrix undergoes slow changes, which pose a challenge for the model to deviate from its initial noisy embedding state. To mitigate this issue, we propose an approach that involves initializing the embedding matrix with small values and subsequently applying an additional LayerNorm operation. By implementing this technique, we accelerate and stabilize the training process, enabling the training of deep architectures with post-LN components. The effectiveness of this approach is demonstrated in Figure <ref type="figure" target="#fig_7">8</ref>, where it is shown to facilitate improved convergence by allowing the model to quickly transition away from the initially small embedding. This is achieved through small changes following a single step, which in turn lead to substantial alterations in directions and subsequently significant changes after the LayerNorm operation.</p><p>Custom Initialization Building on principles from previous works <ref type="bibr" target="#b38">(He et al., 2016;</ref><ref type="bibr" target="#b22">Jumper et al., 2021)</ref>, we initialize parameters to values as similar as possible to an identity mapping while breaking symmetry so there is a clean information path. Most weights are initialized to zero. No biases are used for linear layers. Specific formulas are given in Appendix D. We find the choice of initialization to be significant in convergence speed and quality (see Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluations</head><p>In this section, we focus on evaluating to answer the following questions:</p><p>? RQ1:</p><p>Is RWKV competitive against quadratic transformer architectures with equal number of parameters and training tokens? ? RQ2: When increasing the number of parameters, does RWKV remain competitive against quadratic transformer architectures? ? RQ3: Does increasing parameters of RWKV yield better language modeling loss, when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process?</p><p>Addressing RQ1 and RQ2, from Fig. <ref type="figure">4</ref>, we can see that RWKV is very competitive on six benchmarks (Winogrande, PIQA, ARC-C, ARC-E, LAMBADA, and SciQ) against major open source quadratic complexity transformer models: Pythia <ref type="bibr" target="#b5">(Biderman et al., 2023)</ref>, OPT <ref type="bibr" target="#b71">(Zhang et al., 2022)</ref> and <ref type="bibr">BLOOM (Scao et al., 2022)</ref>. RWKV even outperforms Pythia and GPT-Neo <ref type="bibr" target="#b7">(Black et al., 2022)</ref> in four tasks: PIQA, OBQA, ARC-E, and COPA (See details in Appendix H). For RQ3, Fig. <ref type="figure">5</ref> shows that increasing context length leads to lower test loss on the Pile, an indication that RWKV can make effective use of long contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference Experiments</head><p>We benchmark inference requirements according to size and family. Specifically, we evaluate text generation speed and memory requirements on a typical compute platforms including CPU (x86) and GPU (NVIDIA A100 80GB). For all our experiments we use float32 precision. We include all model parameters in parameter count, including both embedding and non-embedding layers. Performance under different quantization setups is left to further work. See Appendix I for more results. Additionally, we carried out comparative studies on RWKV-4 and ChatGPT / GPT-4, see Appendix J. They revealed that RWKV-4 is very sensitive to prompt engineering. When the prompts were adjusted from the ones used for GPT to more suitable for RWKV, the F1-measure performance increased even from 44.2% to 74.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>There are several promising directions for future work on the RWKV architecture:</p><p>? Increasing model expressivity with enhanced time-decay formulations and exploring initial model states while maintaining efficiency. ? Further improving RWKV computational efficiency by applying parallel scan in the wkv t step to reduce the computational cost to O(B log(T )d). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies which allow it to capture locality and long-range dependencies, while addressing limitations of current architectures by: (1) replacing the quadratic QK attention by a scalar formulation with linear cost, (2) reformulating recurrence and sequential inductive biases to unlock efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.</p><p>We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.</p><p>RWKV opens a new door to scalable and efficient architectures to model complex relation-ships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations</head><p>While our proposed RWKV model has demonstrated promising results regarding training and memory efficiency during inference, some limitations should be acknowledged and addressed in future work. First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model's performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model's recurrent architecture inherently limits its ability to "look back" at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information, it is mechanistically limited compared to full selfattention.</p><p>Another limitation of this work is the increased importance of prompt engineering in comparison to standard Transformer models. The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model's continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Time-Mixing Block as an RNN Cell</head><p>As stated in 4.3, the RWKV time-mixing block can be formulated as an RNN, as the W KV computation can be written in such a recursive form:</p><formula xml:id="formula_15">a 0 , b 0 = 0,<label>(19)</label></formula><formula xml:id="formula_16">wkv t = a t-1 + e u+kt v t b t-1 + e u+kt ,<label>(20)</label></formula><formula xml:id="formula_17">a t = e -w a t-1 + e kt v t ,<label>(21)</label></formula><formula xml:id="formula_18">b t = e -w b t-1 + e kt . (<label>22</label></formula><formula xml:id="formula_19">)</formula><p>The dataflow of the RNN-like time-mixing is shown in Fig. <ref type="figure" target="#fig_5">7</ref>, where the hidden states h is the numerator-denominator tuple (a, b). To avoid overflow in calculating e kt , a numerical trick is used in the official implementation. Note that</p><formula xml:id="formula_20">a 1 = e -w a 0 + e k 0 v 0 = e k 0 v 0 , (23) b 1 = e -w b 0 + e k 0 = e k 0 ,<label>(24)</label></formula><p>and we set a 1 = v 0 , b 1 = 1, p 0 = k 0 , where p t-1 stores the shared exponents of a t and b t . Now the above recursion can be converted into a numerical safe version, for each time step t &gt; 1:</p><formula xml:id="formula_21">q := max(p t-1 , u + k t ),<label>(25)</label></formula><formula xml:id="formula_22">a * t = e p t-1 -q a t-1 + e u+kt-q v t , (<label>26</label></formula><formula xml:id="formula_23">) b * t = e p t-1 -q b t-1 + e u+kt-q , (<label>27</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">wkv t = a * t b * t . (<label>28</label></formula><formula xml:id="formula_26">)</formula><p>The update to a t , b t and their shared exponent are also carried out in similar fashion:</p><formula xml:id="formula_27">q := max(p t-1 -w, k t ),<label>(29)</label></formula><formula xml:id="formula_28">a t = e p t-1 -w-q a t-1 + e kt-q v t ,<label>(30)</label></formula><p>b t = e p t-1 -w-q b t-1 + e kt-q , (31) p t = q.</p><p>(32)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Parameter and FLOP Count for the RWKV Models</head><p>The following section provides an overview of the different RWKV model architectures along with their respective parameter and FLOP counts in Table 2. The number of parameters for each model is computed using the formula: #parameters = 2V D + 13D 2 L + D(11L + 4) where V = 50277 is the vocabulary size, D represents the Model Dimension and L corresponds to the number of layers.</p><p>FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D 2 L), which is the twice (add and multiply) the number of parameters in linear layers. The backwards pass FLOPs can be approximated as twice that of the forward pass. So the total is 6(V D + 13D 2 L) per token for training (3x fw FLOPs). It is noteworthy that FLOPs are independent of the context length, unlike regular transformers. The FLOP approximations in this paper are in line with the methodology used by <ref type="bibr" target="#b24">Kaplan et al. (2020)</ref>.</p><p>Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant. Another approximation is based on the number of non-embedding parameters multiplied by 2. This gives 2(V D + 13D 2 L + D(11L + 4)) resulting in 1.6% more FLOPs for 14B model and 8% more FLOPs for 169M model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Parameter initializations</head><p>We describe the specific parameter initializations below and motivate the design choices. Parameters belonging to residual blocks are often adjusted by layer depth and total number of layers. Let # denote the vocabulary size, s denote the embedding dimension, d denote the hidden size (we use d = 4s), L the number of layers, l the layer index (from 0 to L-1), we use the following initializations:</p><p>? Embeddings are initialized to U(?1e-4) as explained in 4.7 ? For the channel-mixing blocks (11), ? k i and ? r i are initialized to</p><formula xml:id="formula_29">( i s ) 1-l L</formula><p>? For the time-mixing blocks ( <ref type="formula" target="#formula_11">16</ref>), initializations are</p><formula xml:id="formula_30">? k i = ( i s ) 1-l L , ? v i = ( i s ) 1-l L + 0.3l L-1 and ? r i = 0.5( i s ) 1-l L</formula><p>? w i (14), also known as "time decay", is initialized to -5+8?( i d-1 ) 0.7+ 1.3l L-1 . Intuitively, it is the discount factor applied to previous tokens over time.</p><p>? u i (14), also known as "bonus", is set to 0.5(((i + 1) mod 3) -1) + log 0.3. It is the special weighting applied to the current token in equation 14. The alternating zigzag pattern initially creates subtle variations in the tensor elements, which are intended to help the model treat different dimensions of the embedding distinctively. ? W o (15) (time-mixing) and W v (channelmixing) are initialized to N (0, d s = 2) ? All W r , W k , W v weights are initialized to 0 so the model can start learning from the beginning without noisy signals.</p><p>? All LayerNorm weights start from 1 and biases from 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Small Init Embedding</head><p>This section presents experimental validation of small initialization embedding. The experimental setup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution with a mean of 0.0 and a standard deviation of 0.02, which is a commonly used initialization method in models like BERT and GPT. On the other hand, in the small initialization of the embedding (small init emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4, which is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4 is used. However, this difference is negligible and does not affect our conclusions. The experiments were conducted with a batch size of 400. As depicted in the figure <ref type="figure" target="#fig_7">8</ref>, the loss curve for the small init emb exhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal distribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Gradient Stability in RWKV</head><p>In this section, we present a mathematical description of the gradient stability property in RWKV, focusing specifically on the time-mixing block. By gradient stability we mean that if the inputs x t are bounded and the model parameters are fixed, then the gradients with respect to W k and W v are uniformly bounded for all T (thus not exploding). Consequently, we can control the amount each x t contributes to the gradient at T in a naturally decaying fashion by the weight decay mechanism w (thus not vanishing unless desired). First, we make the simplification that there are no token shifts, this will not affect the final conclusion. In this scenario, wkv T can be written as</p><formula xml:id="formula_31">wkv T = T t=1 K e t v t T t=1 K e t = E(v t ) = S(v t ) S(1) ,<label>(33)</label></formula><p>where</p><formula xml:id="formula_32">v t = W v x t , ?(v t ) i ?(W v ) i,j = (x t ) j , K e t = e W k xt+w T,t , ?(K e t ) i ?(W k ) i,j = (x t ) j (K e t ) i ,</formula><p>and S(?) and E(?) are shorthand for denoting sums and averages over weights K e t . The loss function at position T can be written as</p><formula xml:id="formula_33">L T = l(f (wkv T ), y T ).<label>(34)</label></formula><p>Because wkv T relates to (W k ) i,j and (W v ) i,j only through the i-th channel (wkv T ) i , we have</p><formula xml:id="formula_34">?L T ?(W v ) i,j = ?L T ?(wkv T ) i ?(wkv T ) i ?(W v ) i,j .<label>(35)</label></formula><p>The first part of above equation contains trivial operations like output layers, and other layers of time-mixing, which can be proven inductively. The second part of above equation can be bounded as</p><formula xml:id="formula_35">?(wkv T ) i ?(W v ) i,j = ?E i [(v t ) i ] ?(W v ) i,j = |E i [(x t ) j ]| ? max t |(x t ) j |,<label>(36)</label></formula><p>which is irrelevant to T . Similarly,</p><formula xml:id="formula_36">?(wkv T ) i ?(W k ) i,j = ? S i [(v t ) i ] S i (1) /?(W k ) i,j = S i [(x t ) j (v t ) i ] S i (1) - S i [(x t ) j ]S i [(v t ) i ] S i (1) 2 = E i [(x t ) j (v t ) i ] -E i [(x t ) j ]E i [(v t ) i ] = cov i ((x t ) j , (v t ) i )<label>(37)</label></formula><p>can also be bounded. Note that wkv's softmax operation contains at least two non-zero terms (u and w), so the above "covariance" will not degenerate into 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Model Behavior Visualization</head><p>In Figure <ref type="figure" target="#fig_8">9</ref>, we present visualizations of some behavior of the RWKV model. The top plot illustrates the time decays (e -w ) in each layer of the RWKV-169M model, sorted along the channel axis. Notably, several decays in the last layers are very close or equal to one, implying that certain information is preserved and propagated throughout the model's temporal context. Meanwhile, many decays in the initial layer are close to zero, which corresponds to local operations in wkv (14), likely to be associated with tasks such as text parsing or lexical analysis. (Note that the local operations in wkv is due to the extra parameter u, when e -w is degenerated into 0.) These patterns of time decays are partly learned, but also come from parameter initialization as it speeds up training.</p><p>The bottom plot shows the information retrieval and propagation path in the RWKV-430M model. The experiment follows the causal trace method introduced by <ref type="bibr" target="#b35">Meng et al. (2022)</ref>, where we 1. Run the model once, and record all states and activation of each layer during the computation; 2. Corrupt the input embeddings of the subject using noise ("The Eiffel Tower" in this example);</p><p>3. Restore the states and activation of a certain layer at a certain token during the computation, and record the log-probability of the model outputting the correct answer ("Paris").</p><p>Unlike transformers, RWKV relies on recursive propagation of information in the time dimension. In this case, the fact that "the Eiffel Tower is located in Paris" is retrieved in layer 4. It is then passed down to the subsequent layers. In layer 20, mostly, the information is propagated through time until reaching where it is needed. Finally, it is passed down to the last layer for outputting the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Evaluation Details</head><p>The results for following tasks are in Table <ref type="table" target="#tab_6">3</ref> and<ref type="table" target="#tab_7">4</ref>. Tasks:</p><p>? LAMBADA <ref type="bibr" target="#b43">(Paperno et al., 2016)</ref>. A benchmark dataset that evaluates the model's contextual reasoning and language comprehension abilities by presenting context-target pairs, where the objective is to predict the most probable target token. ? PIQA <ref type="bibr" target="#b6">(Bisk et al., 2020)</ref>. A benchmark for the task of physical common sense reasoning, which consists of a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution). ? HellaSwag <ref type="bibr" target="#b67">(Zellers et al., 2019)</ref> A novel benchmark for commonsense Natural Language Inference (NLI) which is build by adversarial filtering against transformer models. ? Winogrande <ref type="bibr" target="#b68">(Zellers et al., 2020)</ref> A dataset designed to evaluate the acquisition of common sense reasoning by neural language models, aiming to determine whether we are accurately assessing the true capabilities of machine common sense. ? StoryCloze <ref type="bibr" target="#b38">(Mostafazadeh et al., 2016)</ref> A benchmark to present a novel approach to assess comprehension of narratives, narrative generation, and script acquisition, focusing on commonsense reasoning. ? ARC Challenge <ref type="bibr" target="#b36">(Clark et al., 2018)</ref> A dataset designed for multiple-choice question answering, encompassing science exam questions ranging from third grade to ninth grade. ? ARC Easy An easy subset of ARC.</p><p>? HeadQA <ref type="bibr" target="#b59">(Vilares and G?mez-Rodr?guez, 2019)</ref> A benchmark consisting of graduatelevel questions encompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology. ? OpenBookQA <ref type="bibr" target="#b36">(Mihaylov et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Inference results</head><p>Figures 10 and 11 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements for LLM inference in float32 precision. We benchmark the following model families and sizes:</p><p>? RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b      <ref type="bibr">et al., 2019)</ref>, WNLI <ref type="bibr" target="#b61">(Wang et al., 2018</ref><ref type="bibr">), GoEmotions (Demszky et al., 2020)</ref>, and PolEmo2 <ref type="bibr" target="#b28">(Koco? et al., 2019)</ref> benchmarks. SOTA is provided as a supplementary reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Importance of prompt construction and comparison to GPT models</head><p>Inspired by article <ref type="bibr" target="#b27">(Koco? et al., 2023)</ref>, we compared the zero-shot performance of the RWKV-4-Raven-14B with ChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual entailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited in readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive proper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse than ChatGPT and GPT-4 in specific task performance. We suspect that this disparity is likely caused by the choice of prompts used to generate the answers. Given that prompts are in natural language and do not consider that RWKV is an RNN, so it can not look back inside an instruction. When the instruction style was adapted to respect that RNNs is not capable for retrospective processing, quality on some datasets increased significantly (ex. for RTE <ref type="bibr" target="#b60">(Wang et al., 2019)</ref> F1 Macro increased from 44.2% to 74.8%). We hypothesize that RWKV models are more sensitive to the position of the components in the context, as RNN-based architectures cannot look back and readjust the weight of previous information. For better performance, desired information should be after the question. Example of ChatGPT prompt to RTE: Having premise &lt;here is a premise&gt; judge if the following hypothesis &lt;here is a hypothesis&gt; are logically connected with the premise? Answer "entailment" if yes, or "not_entailment" if no. RWKV prompt taking into account the characteristics of the RNN: Can you tell me if the hypothesis is entailment or is not entailment to the premise? Table <ref type="table">7</ref>: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion <ref type="bibr" target="#b64">(Wulczyn et al., 2017)</ref>, Sarcasm <ref type="bibr" target="#b52">(Siddiqui, 2019)</ref>, Unhealthy <ref type="bibr" target="#b48">(Price et al., 2020)</ref>, <ref type="bibr">MathQA (Cobbe et al., 2021)</ref>, and TweetSent <ref type="bibr" target="#b3">(Barbieri et al., 2020)</ref> benchmarks. SOTA is provided as a supplementary reference.</p><p>premise: &lt;here is a premise&gt; hypothesis: &lt;here is a hypothesis&gt; While separating the instruction from the input is relatively easy to do, other aspects of prompt engineering are harder to quantify. Testing the approach of stating the input after the question on multiple other tasks, shown in tab. 7, suggests that better prompts might reduce the disparity between models. Raven achieves comparable result to Chat-GPT on unhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While this approach in prompting looks necessary, it alone is not enough to replace the capability of having free access to the whole context. Therefore, prompt engineering seems to be of significantly more importance to the RNN models compared to standard transformers. It is entirely possible that good prompts to RNN models do not mean additional restrictions, but should simply be constructed using completely different guidelines. While authors of a forementioned paper <ref type="bibr" target="#b27">(Koco? et al., 2023)</ref> perform chain-of-thought to improve results on MathQA dataset, trying the same approach on the Raven model achieved a very low accuracy of 48.11%. Despite that, using a very similar prompt without the chain-of-thought outperformed ChatGPT on this dataset. It would not be surprising if, thanks to the hidden state, RNN models do not need additional steps during inference, and adding those only makes the model forget the information it analyzed and deemed important. This further emphasizes the importance of the order of information the model receives.</p><p>The template used to prompt the Raven model in MathQA with chain-of-thought prompting: Write the reasoning and highlight the answer to the question at the end in format: 'Answer: '. The question is: &lt;here is question&gt;</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Computation structure of the RWKV in comparison toQRNN and RNN (Vanilla, LSTM, GRU, etc)   architectures. Color codes: orange indicates time-mixing, convolutions or matrix multiplications, and the continuous block indicates that these computations can proceed simultaneously; blue signifies parameterless functions that operate concurrently along the channel or feature dimension (element-wise). Green indicates channel-mixing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RWKV block elements (left) and RWKV residual block with a final head for language modeling (right) architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: RWKV architecture for language modelling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Cumulative time during text generation for different LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Zero-Shot Performance: The horizontal axis is a number of parameters and the vertical axis is accuracy.</figDesc><graphic url="image-4.png" coords="8,89.45,207.78,136.06,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: RWKV time-mixing block formulated as an RNN cell. Color codes: yellow (?) denotes the token shift, red (1) denotes the denominator, blue (2) denotes the numerator, pink (3) denotes the fraction computations in 14. h denotes the numerator-denominator tuple (a, b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: of small initialization embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Model behavior visualizations of the RWKV model.</figDesc><graphic url="image-7.png" coords="16,333.74,252.23,158.18,72.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not accounted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Text generation inference time for LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity comparison with different Transformers: Reformer</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>RWKV model architectures and associated FLOP counts</figDesc><table><row><cell cols="3">Name Layers Model Dimension</cell><cell>Parameters</cell><cell>FLOPs per token</cell></row><row><cell>169 M</cell><cell>12</cell><cell>768</cell><cell>1.693 ? 10 8</cell><cell>2.613 ? 10 8</cell></row><row><cell>430 M</cell><cell>24</cell><cell>1024</cell><cell>4.304 ? 10 8</cell><cell>7.573 ? 10 8</cell></row><row><cell>1.5 B</cell><cell>24</cell><cell>2048</cell><cell>1.515 ? 10 9</cell><cell>2.823 ? 10 9</cell></row><row><cell>3 B</cell><cell>32</cell><cell>2560</cell><cell>2.985 ? 10 9</cell><cell>5.710 ? 10 9</cell></row><row><cell>7 B</cell><cell>32</cell><cell>4096</cell><cell>7.393 ? 10 9</cell><cell>1.437 ? 10 10</cell></row><row><cell>14 B</cell><cell>40</cell><cell>5120</cell><cell>1.415 ? 10 10</cell><cell>2.778 ? 10 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Zero-Shot Performance of the model on Common Sense Reasoning Tasks.</figDesc><table><row><cell>? Bloom (Scao et al., 2022): 560m, 1b, 3b</cell></row><row><cell>? OPT (Zhang et al., 2022): 125m, 350m, 1.3b,</cell></row><row><cell>2.7b, 6.7b, 13b</cell></row><row><cell>? GPT-Neo (Black et al., 2022): 125m, 1.3b,</cell></row><row><cell>2.7b</cell></row><row><cell>? Pythia (Biderman et al., 2023): 160m, 410m,</cell></row><row><cell>1.4b, 2.8b, 6.7b, 12b</cell></row><row><cell>Missing models in are due to Out Of Memory</cell></row><row><cell>(OOM) errors. A comparison at 512 tokens is</cell></row><row><cell>shown in Figure 11 as some large transformer mod-</cell></row><row><cell>els produced an OOM when inferencing longer se-</cell></row></table><note><p>* Interpolation of Pythia and GPT-Neo models</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Zero-Shot Performance of various models on different tasks.</figDesc><table><row><cell>Method</cell><cell>L</cell><cell>d</cell><cell>T</cell><cell cols="3">Train bpc Test bpc Time Complexity Space Complexity</cell></row><row><cell>Transformer</cell><cell cols="4">12 512 1024 0.977</cell><cell>1.137</cell><cell>O(T 2 d)</cell><cell>O(T 2 + T d)</cell></row><row><cell>Transformer</cell><cell cols="4">24 256 1024 1.039</cell><cell>1.130</cell><cell>O(T 2 d)</cell><cell>O(T 2 + T d)</cell></row><row><cell>Reformer</cell><cell cols="4">12 512 1024 1.040</cell><cell>1.195</cell><cell>O(T log T d)</cell><cell>O(T log T + T d)</cell></row><row><cell>Synthesizer</cell><cell cols="4">12 512 1024 0.994</cell><cell>1.298</cell><cell>O(T 2 d)</cell><cell>O(T 2 + T d)</cell></row><row><cell cols="5">Linear Transformer 12 512 1024 0.981</cell><cell>1.207</cell><cell>O(T d 2 )</cell><cell>O(T d + d 2 )</cell></row><row><cell>Performer</cell><cell cols="4">12 512 1024 1.002</cell><cell>1.199</cell><cell>O(T d 2 log d)</cell><cell>O(T d log d + d 2 log d)</cell></row><row><cell>AFT-simple</cell><cell cols="4">12 512 1024 0.854</cell><cell>1.180</cell><cell>O(T d)</cell><cell>O(T d)</cell></row><row><cell>RWKV-RNN</cell><cell>6</cell><cell cols="3">512 1024 0.720</cell><cell>-</cell><cell>O(Td)</cell><cell>O(d)</cell></row></table><note><p>* Interpolation of Pythia and GPT-Neo models</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Enwik8 results, measured in bits per character (bpc): the lower the better. Baseline comparisons are made with Reformer<ref type="bibr" target="#b26">(Kitaev et al., 2020)</ref>, Synthesizer<ref type="bibr" target="#b54">(Tay et al., 2020)</ref> (the best performing dense version), Linear Transformer<ref type="bibr" target="#b25">(Katharopoulos et al., 2020</ref>), Performer (Choromanski et al., 2020). L, d, and T denote the number of blocks (network depth), dimension of features, and sequence length, respectively. Both Linear Transformer and Performer are implemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.</figDesc><table><row><cell>quences. For GPU experiments, we use an NVIDIA</cell></row><row><cell>A100 with 80GB of VRAM. For CPU experiments,</cell></row><row><cell>we use an AMD EPYC processor with 30 CPU</cell></row><row><cell>cores and 200 GiB RAM.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://huggingface.co/RWKV</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>If the sequence is very long, more sophisticated methods such as<ref type="bibr" target="#b34">Martin and Cundy (2017)</ref> that parallelize over sequence length could be used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/BlinkDL/ChatRWKV</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/BlinkDL/ChatRWKV</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We acknowledge <rs type="person">EleutherAI and StabilityAI</rs> for compute access and technical support in development of RWKV. We also acknowledge the members of the RWKV Discord server for their help and work on further extending the applicability of RWKV to different domains. Finally, we thank <rs type="person">Stella Biderman</rs> for feedback on the paper.</p></div>
			</div>
			<div type="funding">
<div><p>ficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR).</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Author Contributions</head><p>Bo Peng Original RWKV idea, original code, performance optimizations, original experiments, and trained RWKV models from 0.1B to 14B.</p><p>Eric Alcaide Manuscript (initial draft sections 1, 2; sections 4, 7 and 8; revision and proofreading; final version ). Figures <ref type="bibr">(2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">7)</ref>. Experiments section 6. Appendices D, I. Contributions to Appendix K.</p><p>Quentin Anthony Led writing the paper. Manuscript (initial draft sections 1, 2, 3; revision and proofreading; final version).</p><p>Zhenyuan Zhang Manuscript (revision and proofreading) Figure <ref type="figure">3</ref>. Experiments Appendix G. Contributions to Appendices B and K.</p><p>Kranthi Kiran GV Manuscript (sections 2 and 5; contributions to section 3; revision and proofreading). Tables <ref type="table">3</ref> and<ref type="table">4</ref>. Appendix C.</p><p>Xiangru Tang Manuscript (sections 2 and 3; contributions to abstract; revision and proofreading). Contributions to Appendix K.</p><p>Matteo Grella Manuscript (sections 4.5, 4.6, 8; contributions to sections 1, 7 and 9; proofreading and revision). Contributions to Appendix B.</p><p>Ferdinand Mom Manuscript (contributions to section <ref type="bibr">1, 2, 4.3, 4.6; proofreading and revision)</ref>. Contributions to Appendix B.</p><p>Atsushi Saito Manuscript (sections 3 and 5; contributions to section 2). Figures <ref type="figure">1a ,</ref><ref type="figure">1b,</ref><ref type="figure">1c</ref>. Contributions to Appendix H Krishna Sri Ipsit Mantri Figure <ref type="figure">4</ref> Rui-Jie Zhu Tables <ref type="table">1</ref> and<ref type="table">5</ref>. Experiments for table <ref type="table">5</ref>.</p><p>Peng Zhou Contributions to Table <ref type="table">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qihang Zhao Manuscript (proofreading and revision). Contributions to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Cases</head><p>In this part, we present a few instances of outputs produced by RWKV model using a Chat interface 56 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWKV-4-Raven-14B</head><p>Tell me about ravens.</p><p>Ravens are large, black birds with a distinctive white head and neck. They are found in most parts of the world, including North America, Europe, Asia, and Australia. Ravens are known for their intelligence and problem-solving abilities. They are also considered to be symbols of death and transformation in many cultures. Ravens are often associated with the afterlife or death because they have been known to eat carrion or even other birds. In some cultures, ravens are also believed to be messengers of the gods or guardians of treasure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWKV-4-Raven-14B</head><p>Explain the concept of machine learning to a 10-year-old.</p><p>Machine learning is like a super-smart computer that can learn on its own. It can recognize patterns and make decisions based on what it has learned. Just like how you might remember something from the past, the computer can remember what it's learned and use it to make predictions or decisions in the future. It's like having a smart friend who always knows what to do! RWKV-4-Raven-14B Write a short poem describing nature's beauty in springtime.</p><p>Nature's beauty in springtime is rare, A symphony of colors, a gentle surprise. The flowers bloom, the birds sing and sing, As the sun shines down, warming our skin.</p><p>The trees sway in the gentle breeze, As we take in their beauty with ease. The sky is blue, the clouds are white, A sight to behold, it's truly divine.</p><p>Below is the numerical stable formula for updating the state variable in RWKV:</p><p>Please write out the corresponding pytorch code to do so, where both a t and b t are torch.Tensor([n])</p><p>Here is the PyTorch code to update the state variable in RWKV using the given formula:</p><p>This code defines a function update_state that takes in the state variables a and b, the control variables u and v, and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp function to compute the exponential of the control variables and the log of the normalization factor. Finally, it uses these values to update the state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWKV-4-Raven-14B</head><p>Explain the following metaphor: Life is like cats.</p><p>The metaphor "Life is like cats" suggests that life can be unpredictable and full of unexpected twists and turns, just like a cat's behavior. It implies that we cannot fully control or predict the outcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies that life is complex and multifaceted, with many different variables and factors at play. Just as a cat can have multiple personalities or moods, life can have many twists and turns that are difficult to predict or understand. Overall, the metaphor suggests that life is complex and unpredictable, much like a cat's behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWKV-4-Raven-14B</head><p>Test the convergence of ? i=0 2 -i .</p><p>The given series is a geometric series with the first term a 0 = 2 and common ratio r = 1 2 . We can find the sum of this series using the formula for a geometric series:</p><p>So, the sum of the given series is 2 .</p><p>RWKV-4-Raven-14B</p><p>I would like to cook some Chinese food at home. Do you have any suggestions on the meal I could choose?</p><p>Yes, of course! If you are not very experienced in cooking Chinese food, you could start with making noodles or rice, then make a dish using one of those. I also think making stir-fry with some meat is an excellent idea.</p><p>I want to make something more special. What about red braised pork belly?</p><p>Yes, red braised pork belly is also very popular and tasty.</p><p>Tell me the steps to cook.</p><p>You will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine. Put all of these in a pot, cook on medium heat and simmer until the meat is tender. After that, remove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices into it, then boil until it's almost done.</p><p>What do I need to be careful with the most during the cooking process?</p><p>To cook pork belly properly, it is very important to monitor the heat of the stove and be patient with the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew. You can add a few ingredients like soy sauce or black beans to give the pork more flavor.</p><p>Tell me more about the proper heat and timing.</p><p>If the temperature of the water is too high, it can damage the collagen and connective tissues, resulting in tough pork belly. It is essential to control the temperature carefully, especially when braising the pork. Additionally, keep the water simmering but not boiling to maintain a low simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat. Lastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most appropriate for slow cooking pork belly.</p><p>I see. Thank you for your advice.</p><p>You're welcome! It was a pleasure chatting with you. Let me know if you need any further assistance in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWKV-4-Raven-14B</head><p>If all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your answer in detail.</p><p>Yes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are sweet.</p><p>If birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.</p><p>No, penguins cannot fly. Penguins are birds, but they are not capable of flight like other birds.</p><p>They have wings and can glide for short distances, but they cannot take off or fly long distances like other birds.</p><p>The day before two days after the day before tomorrow is Saturday. What day is it today?</p><p>Please explain your answer in detail.</p><p>Today is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.</p><p>If five cats can catch five mice in five minutes, how long will it take one cat to catch one mouse? Please explain your answer in detail.</p><p>If five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one minute. To find out how long it will take one cat to catch one mouse, we can use the concept of proportionality.</p><p>Let's assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught by a cat in one minute is equal to the number of cats multiplied by the number of mice caught by each cat: x = 5 * 1 = 5 So, it will take 5 minutes for one cat to catch one mouse.</p><p>RWKV-4-Raven-14B</p><p>Summarize the following paragraph.</p><p>&lt;The Abstract of this paper goes here&gt;</p><p>The paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. The model is able to scale to tens of billions of parameters and exhibits linear computational complexity during training and inference, making it a promising alternative to Transformers for sequence processing tasks.</p><p>Please further summarize it in one sentence.</p><p>The researchers have developed a new architecture that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs, which allows for linear scaling in memory and computational requirements.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FETA: A benchmark for few-sample task transfer in open-domain dialogue</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Jandaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Pryor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Yoffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10936" to="10953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Shaojie Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TweetEval: Unified benchmark and comparative evaluation for tweet classification</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.148</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1644" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01373</idno>
		<title level="m">Pythia: A suite for analyzing large language models across training and scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo</idno>
		<ptr target="https://doi.org/10.5281/zenodo" />
		<imprint>
			<date type="published" when="2021">2022. 2021</date>
			<biblScope unit="page">5297715</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scaling transformer to 1m tokens and beyond with rmt</title>
		<author>
			<persName><forename type="first">Aydar</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><forename type="middle">S</forename><surname>Burtsev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent memory transformer</title>
		<author>
			<persName><forename type="first">Aydar</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11079" to="11091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Training compute-optimal large language models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mnnfast: A fast and scalable system architecture for memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Hanhwi</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonsung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Eon</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="250" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<idno type="DOI">10.18653/v1/W17-4413</idno>
		<editor>Matt Gardner Johannes Welbl Nelson F. Liu</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03819-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Preventing gradient explosions in gated recurrent units</title>
		<author>
			<persName><forename type="first">Sekitoshi</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotetsu</forename><surname>Iwamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno>ArXiv, abs/2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koco?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Cichecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliwier</forename><surname>Kaszyca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Kochanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominika</forename><surname>Szyd?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julita</forename><surname>Bielaniewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Gruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkadiusz</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Kanclerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Koco?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart?omiej</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiktoria</forename><surname>Mieleszczenko-Kowszewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mi?kowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Oleksy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Piasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Radli?ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Wojtasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanis?aw</forename><surname>Wo?niak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemys?aw</forename><surname>Kazienko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Chatgpt: Jack of all trades, master of none</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-level sentiment analysis of polemo 2.0: Extended corpus of multi-domain consumer reviews</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koco?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mi?kowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Za?ko-Zieli?ska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="980" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive lstms</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1477</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4470" to="4481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Pay attention to mlps</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2441" to="2453" />
			<date type="published" when="2021">2021</date>
			<pubPlace>Luna</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mega: Moving average equipped gated attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Parallelizing linear recurrent neural nets over sequence length</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
		<idno>ArXiv, abs/1709.04057</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Locating and editing factual associations in GPT</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<title level="m">Stable recurrent models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Introducing chatgpt</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Resurrecting recurrent neural networks for long sequences</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.06349</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germ?n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Pytorch: An imperative style, high-performance deep learning library</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10866</idno>
		<title level="m">Hyena hierarchy: Towards larger convolutional language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Six attributes of unhealthy conversations</title>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Gifford-Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jory</forename><surname>Flemming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saul</forename><surname>Musker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maayan</forename><surname>Roichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.alw-1.15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Online Abuse and Harms</title>
		<meeting>the Fourth Workshop on Online Abuse and Harms</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="114" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Selfattention does not need o(n 2 ) memory</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Staats</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagn?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gall?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">SARCASMANIA: Sarcasm Exposed</title>
		<author>
			<persName><forename type="first">Ramsha</forename><surname>Siddiqui</surname></persName>
		</author>
		<ptr target="http://www.kaggle.com/rmsharks4/sarcasmania-dataset" />
		<imprint>
			<date type="published" when="2019-02">2019. February-2023</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Primer: Searching for efficient transformers for language modeling</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Manke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/2109.08668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>CoRR, abs/2105.01601</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models. A?ron van den Oord</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno>ArXiv, abs/1609.03499</idno>
		<editor>Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu</editor>
		<imprint>
			<date type="published" when="2016">2023. 2016</date>
		</imprint>
	</monogr>
	<note>Wavenet: A generative model for raw audio</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Head-qa: A healthcare dataset for complex reasoning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>G?mez-Rodr?guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ex machina: Personal attacks seen at scale</title>
		<author>
			<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052591</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-04-03">2017. 2017. April 3-7, 2017</date>
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>An attention free transformer</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Alon Albalak Manuscript</title>
		<imprint/>
	</monogr>
	<note>abstract and sections 1, 9; proofreading and revision</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Contributions Appendix J. Ruichong Zhang Manuscript (proofreading and revision); Contributions to Figure 5 and Appendix K</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuscript</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stanis?aw Wo?niak Appendix J. Bart?omiej Koptyra Contributions to Appendix J. RWKV-4</title>
		<imprint/>
	</monogr>
	<note>Contributions to Appendix J. Przemys?aw Kazienko Manuscript (section 6; proofreading and revision)</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gpt-Neox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="20" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
