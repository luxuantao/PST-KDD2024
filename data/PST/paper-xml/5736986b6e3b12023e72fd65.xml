<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
							<email>kandasamy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
							<email>akshaykr@cs.cmu.edu</email>
							<affiliation key="aff1">
								<address>
									<settlement>Microsoft Research</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
							<email>bapoczos@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
							<email>robins@hsph.harvard.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49BCD2526D5BD64A2E951AA1C60C535E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are derived from the von Mises expansion and are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entropies, divergences, and mutual informations are classical information-theoretic quantities that play fundamental roles in statistics, machine learning, and across the mathematical sciences. In addition to their use as analytical tools, they arise in a variety of applications including hypothesis testing, parameter estimation, feature selection, and optimal experimental design. In many of these applications, it is important to estimate these functionals from data so that they can be used in downstream algorithmic or scientific tasks. In this paper, we develop a recipe for estimating statistical functionals of one or more nonparametric distributions based on the notion of influence functions.</p><p>Entropy estimators are used in applications ranging from independent components analysis <ref type="bibr" target="#b14">[15]</ref>, intrinsic dimension estimation <ref type="bibr" target="#b3">[4]</ref> and several signal processing applications <ref type="bibr" target="#b8">[9]</ref>. Divergence estimators are useful in statistical tasks such as two-sample testing. Recently they have also gained popularity as they are used to measure (dis)-similarity between objects that are modeled as distributions, in what is known as the "machine learning on distributions" framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. Mutual information estimators have been used in in learning tree-structured Markov random fields <ref type="bibr" target="#b18">[19]</ref>, feature selection <ref type="bibr" target="#b24">[25]</ref>, clustering <ref type="bibr" target="#b17">[18]</ref> and neuron classification <ref type="bibr" target="#b30">[31]</ref>. In the parametric setting, conditional divergence and conditional mutual information estimators are used for conditional two sample testing or as building blocks for structure learning in graphical models. Nonparametric estimators for these quantities could potentially allow us to generalise several of these algorithms to the nonparametric domain. Our approach gives sample-efficient estimators for all these quantities (and many others), which often outperfom the existing estimators both theoretically and empirically.</p><p>Our approach to estimating these functionals is based on post-hoc correction of a preliminary estimator using the Von Mises Expansion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. This idea has been used before in the semiparametric statistics literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. However, most studies are restricted to functionals of one distribution and have focused on a "data-split" approach which splits the samples for density estimation and functional estimation. While the data-split (DS) estimator is known to achieve the parametric con-vergence rate for sufficiently smooth densities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>, in practical settings, as we show in our simulations, splitting the data results in poor empirical performance.</p><p>In this paper we introduce the method of influence function based nonparametric estimators to the machine learning community and expand on this technique in several novel and important ways. The main contributions of this paper are:</p><p>1. We propose a "leave-one-out" (LOO) technique to estimate functionals of a single distribution.</p><p>We prove that it has the same convergence rates as the DS estimator. However, the LOO estimator has better empirical performance in our simulations since it makes efficient use of the data.</p><p>2. We extend both DS and LOO methods to functionals of multiple distributions and analyse their convergence. Under sufficient smoothness both estimators achieve the parametric rate and the DS estimator has a limiting normal distribution.</p><p>3. We prove a lower bound for estimating functionals of multiple distributions. We use this to establish minimax optimality of the DS and LOO estimators under sufficient smoothness.</p><p>4. We use the approach to construct and implement estimators for various entropy, divergence, mutual information quantities and their conditional versions. A subset of these functionals are listed in Table <ref type="table">1</ref> in the Appendix. Our software is publicly available at github.com/kirthevasank/if-estimators.</p><p>5. We compare our estimators against several other approaches in simulation. Despite the generality of our approach, our estimators are competitive with and in many cases superior to existing specialised approaches for specific functionals. We also demonstrate how our estimators can be used in machine learning applications via an image clustering task.</p><p>Our focus on information theoretic quantities is due to their relevance in machine learning applications, rather than a limitation of our approach. Indeed our techniques apply to any smooth functional.</p><p>History: We provide a brief history of the post-hoc correction technique and influence functions. We defer a detailed discussion of other approaches to estimating functionals to Section 5. To our knowledge, the first paper using a post-hoc correction estimator was that of Bickel and Ritov <ref type="bibr" target="#b1">[2]</ref>. The line of work following this paper analysed integral functionals of a single one dimensional density of the form ν(p) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. A recent paper by Krishnamurthy et al. <ref type="bibr" target="#b11">[12]</ref> also extends this line to functionals of multiple densities, but only considers polynomial functionals of the form p α q β for densities p and q. All approaches above of use data splitting. Our work contributes to this line of research in two ways: we extend the technique to a more general class of functionals and study the empirically superior LOO estimator.</p><p>A fundamental quantity in the design of our estimators is the influence function, which appears both in robust and semiparametric statistics. Indeed, our work is inspired by that of Robins et al. <ref type="bibr" target="#b29">[30]</ref> and Emery et al. <ref type="bibr" target="#b5">[6]</ref> who propose a (data-split) influence-function based estimator for functionals of a single distribution. Their analysis for nonparametric problems rely on ideas from semiparametric statistics: they define influence functions for parametric models and then analyse estimators by looking at all parametric submodels through the true parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let X be a compact metric space equipped with a measure µ, e.g. the Lebesgue measure. Let F and G be measures over X that are absolutely continuous w.r.t µ. Let f, g ∈ L 2 (X ) be the Radon-Nikodym derivatives with respect to µ. We focus on estimating functionals of the form:</p><formula xml:id="formula_0">T (F ) = T (f ) = φ ν(f )dµ or T (F, G) = T (f, g) = φ ν(f, g)dµ ,<label>(1)</label></formula><p>where φ, ν are real valued Lipschitz functions that twice differentiable. Our framework permits more general functionals (e.g. functionals based on the conditional densities), but we will focus on this form for ease of exposition. To facilitate presentation of the main definitions, it is easiest to work with functionals of one distribution T (F ). Define M to be the set of all measures that are absolutely continuous w.r.t µ, whose Radon-Nikodym derivatives belong to L 2 (X ).</p><p>Central to our development is the Von Mises expansion (VME), which is the distributional analog of the Taylor expansion. For this we introduce the Gâteaux derivative which imposes a notion of differentiability in topological spaces. We then introduce the influence function.</p><p>Definition 1. Let P, H ∈ M and U : M → R be any functional. The map U : M → R where U (H; P ) = ∂U (P +tH) ∂t | t=0 is called the Gâteaux derivative at P if the derivative exists and is linear and continuous in H. U is Gâteaux differentiable at P if the Gâteaux derivative exists at P . Definition 2. Let U be Gâteaux differentiable at P . A function ψ(•; P ) : X → R which satisfies U (Q -P ; P ) = ψ(x; P )dQ(x), is the influence function of U w.r.t the distribution P .</p><p>By the Riesz representation theorem, the influence function exists uniquely since the domain of U is a bijection of L 2 (X ) and consequently a Hilbert space. The classical work of Fernholz <ref type="bibr" target="#b6">[7]</ref> defines the influence function in terms of the Gâteaux derivative by,</p><formula xml:id="formula_1">ψ(x; P ) = U (δ x -P ; P ) = ∂U ((1 -t)P + tδ x ) ∂t t=0 ,<label>(2)</label></formula><p>where δ x is the dirac delta function at x. While our functionals are defined only on non-atomic distributions, we can still use (2) to compute the influence function. The function computed this way can be shown to satisfy Definition 2.</p><p>Based on the above, the first order VME is,</p><formula xml:id="formula_2">U (Q) = U (P ) + U (Q -P ; P ) + R 2 (P, Q) = U (P ) + ψ(x; P )dQ(x) + R 2 (P, Q),<label>(3)</label></formula><p>where R 2 is the second order remainder. Gâteaux differentiability alone will not be sufficient for our purposes. In what follows, we will assign Q → F and P → F , where F , F are the true and estimated distributions. We would like to bound the remainder in terms of a distance between F and F . For functionals T of the form (1), we restrict the domain to be only measures with continuous densities, Then, we can control R 2 using the L 2 metric of the densities. This essentially means that our functionals satisfy a stronger form of differentiability called Fréchet differentiability <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> in the L 2 metric. Consequently, we can write all derivatives in terms of the densities, and the VME reduces to a functional Taylor expansion on the densities (Lemmas 9, 10 in Appendix A):</p><formula xml:id="formula_3">T (q) = T (p) + φ ν(p) (q -p)ν (p) + R 2 (p, q) = T (p) + ψ(x; p)q(x)dµ(x) + O( p -q 2 2 ).<label>(4)</label></formula><p>This expansion will be the basis for our estimators.</p><p>These ideas generalise to functionals of multiple distributions and to settings where the functional involves quantities other than the density. A functional T (P, Q) of two distributions has two Gâteaux derivatives, T i (•; P, Q) for i = 1, 2 formed by perturbing the ith argument with the other fixed. The influence functions ψ 1 , ψ 2 satisfy, ∀P 1 , P 2 ∈ M,</p><formula xml:id="formula_4">T 1 (Q 1 -P 1 ; P 1 , P 2 ) = ∂T (P 1 + t(Q 1 -P 1 ), P 2 ) ∂t t=0 = ψ 1 (u; P 1 , P 2 )dQ 1 (u),<label>(5)</label></formula><formula xml:id="formula_5">T 2 (Q 2 -P 2 ; P 1 , P 2 ) = ∂T (P 1 , P 2 + t(Q 2 -P 2 )) ∂t t=0 = ψ 2 (u; P 1 , P 2 )dQ 2 (u).</formula><p>The VME can be written as,</p><formula xml:id="formula_6">T (q 1 , q 2 ) = T (p 1 , p 2 ) + ψ 1 (x; p 1 , p 2 )q 1 (x)dx + ψ 2 (x; p 1 , p 2 )q 2 (x)dx + O( p 1 -q 1 2 2 ) + O( p 2 -q 2 2 2 ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Estimating Functionals</head><p>First consider estimating a functional of a single distribution, T (f ) = φ( ν(f )dµ) from samples X n 1 ∼ f . We wish to find an estimator T with low expected mean squared error</p><formula xml:id="formula_7">(MSE) E[( T -T ) 2 ].</formula><p>Using the VME (4), Emery et al. <ref type="bibr" target="#b5">[6]</ref> and Robins et al. <ref type="bibr" target="#b29">[30]</ref> suggest a natural estimator. If we use half of the data X n/2 1</p><p>to construct an estimate f (1) of the density f , then by (4):</p><formula xml:id="formula_8">T (f ) -T ( f (1) ) = ψ(x; f (1) )f (x)dµ + O( f -f (1) 2 2 ).</formula><p>As the influence function does not depend on (the unknown) F , the first term on the right hand side is simply an expectation of ψ(X; f (1) ) w.r.t F . We can use the second half of the data X n n/2+1 to estimate this expectation with its sample mean. This leads to the following preliminary estimator:</p><formula xml:id="formula_9">T (1) DS = T ( f (1) ) + 1 n/2 n i=n/2+1 ψ(X i ; f (1) ).<label>(7)</label></formula><p>We can similarly construct an estimator T</p><p>(2)</p><p>DS by using X n n/2+1 for density estimation and X n/2 1 for averaging. Our final estimator is obtained via</p><formula xml:id="formula_10">T DS = ( T<label>(1)</label></formula><formula xml:id="formula_11">DS + T (2) DS )/2.</formula><p>In what follows, we shall refer to this estimator as the Data-Split (DS) estimator. The DS estimator for functionals of one distribution has appeared before in the statistics literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The rate of convergence of this estimator is determined by the O( f -f (1) 2</p><p>2 ) error in the VME and the n -1 rate for estimating an expectation. Lower bounds from several literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref> confirm minimax optimality of the DS estimator when f is sufficiently smooth. The data splitting trick is common approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> as the analysis is straightforward. While in theory DS estimators enjoy good rates of convergence, data splitting is unsatisfying from a practical standpoint since using only half the data each for estimation and averaging invariably decreases the accuracy.</p><p>To make more effective use of the sample, we propose a Leave-One-Out (LOO) version of the above estimator,</p><formula xml:id="formula_12">T LOO = 1 n n i=1 T ( f-i ) + ψ(X i ; f-i ) .<label>(8)</label></formula><p>where f-i is a density estimate using all the samples X n 1 except for X i . We prove that the LOO Estimator achieves the same rate of convergence as the DS estimator but empirically performs much better. Our analysis is specialised to the case where f-i is a kernel density estimate (Section 4).</p><p>We can extend this method to estimate functionals of two distributions. Say we have n i.i.d samples X n 1 from f and m samples Y m 1 from g. Akin to the one distribution case, we propose the following DS and LOO versions.</p><formula xml:id="formula_13">T (1) DS = T ( f (1) , ĝ(1) ) + 1 n/2 n i=n/2+1 ψ f (X i ; f (1) , ĝ(1) ) + 1 m/2 m j=m/2+1 ψ g (Y j ; f (1) , ĝ(1) ). (9) T LOO = 1 max(n, m) max(n,m) i=1 T ( f-i , ĝ-i ) + ψ f (X i ; f-i , ĝ-i ) + ψ g (Y i ; f-i , ĝ-i ) .<label>(10)</label></formula><p>Here, ĝ(1) , ĝ-i are defined similar to f (1) , f-i . For the DS estimator, we swap the samples to compute T</p><p>DS and average. For the LOO estimator, if n &gt; m we cycle through the points Y m 1 until we have summed over all X n 1 or vice versa. T LOO is asymmetric when n = m. A seemingly natural alternative would be to sum over all nm pairings of X i 's and Y j 's. However, this is computationally more expensive. Moreover, a straightforward modification of our proof in Appendix D.2 shows that both approaches converge at the same rate if n and m are of the same order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples:</head><p>We demonstrate the generality of our framework by presenting estimators for several entropies, divergences mutual informations and their conditional versions in Table <ref type="table">1</ref> (Appendix H). For many functionals in the table, these are the first computationally efficient estimators proposed. We hope this table will serve as a good reference for practitioners. For several functionals (e.g. conditional and unconditional Rényi-α divergence, conditional Tsallis-α mutual information) the estimators are not listed only because the expressions are too long to fit into the table. Our software implements a total of 17 functionals which include all the estimators in the table. In Appendix F we illustrate how to apply our framework to derive an estimator for any functional via an example.</p><p>As will be discussed in Section 5, when compared to other alternatives, our technique has several favourable properties: the computational complexity of our method is O(n 2 ) when compared to O(n 3 ) of other methods; for several functionals we do not require numeric integration; unlike most other methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>, we do not require any tuning of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>Some smoothness assumptions on the densities are warranted to make estimation tractable. We use the Hölder class, which is now standard in nonparametrics literature. Moreover, define the Bounded Hölder Class Σ(s, L, B , B) to be {f ∈ Σ(s, L) : B &lt; f &lt; B}. Note that large s implies higher smoothness. Given n samples X n 1 from a d-dimensional density f , the kernel density estimator (KDE) with bandwidth h is f</p><formula xml:id="formula_15">(t) = 1/(nh d ) n i=1 K t-Xi h . Here K : R d → R is a smoothing kernel [35]. When f ∈ Σ(s, L), by selecting h ∈ Θ(n -1 2s+d ) the KDE achieves the minimax rate of O P (n -2s</formula><p>2s+d ) in mean squared error. Further, if f is in the bounded Hölder class Σ(s, L, B , B) one can truncate the KDE from below at B and from above at B and achieve the same convergence rate <ref type="bibr" target="#b2">[3]</ref>. In our analysis, the density estimators f (1) , f-i , ĝ(1) , ĝ-i are formed by either a KDE or a truncated KDE, and we will make use of these results.</p><p>We will also need the following regularity condition on the influence function. This is satisfied for smooth functionals including those in Table <ref type="table">1</ref>. We demonstrate this in our example in Appendix F. Assumption 4. For a functional T (f ) of one distribution, the influence function ψ satisfies,</p><formula xml:id="formula_16">E (ψ(X; f ) -ψ(X; f )) 2 ∈ O( f -f 2 ) as f -f 2 → 0.</formula><p>For a functional T (f, g) of two distributions, the influence functions ψ f , ψ g satisfy,</p><formula xml:id="formula_17">E f (ψ f (X; f , g ) -ψ f (X; f, g)) 2 ∈ O( f -f 2 + g -g 2 ) as f -f 2 , g -g 2 → 0. E g (ψ g (Y ; f , g ) -ψ g (Y ; f, g)) 2 ∈ O( f -f 2 + g -g 2 ) as f -f 2 , g -g 2 → 0.</formula><p>Under the above assumptions, Emery et al. <ref type="bibr" target="#b5">[6]</ref>, Robins et al. <ref type="bibr" target="#b29">[30]</ref> show that the DS estimator on a single distribution achieves MSE E[( T DS -T (f )) 2 ] ∈ O(n -4s 2s+d +n -1 ) and further is asymptotically normal when s &gt; d/2. Their analysis in the semiparametric setting contains the nonparametric setting as a special case. In Appendix B we review these results with a simpler self contained analysis that directly uses the VME and has more interpretable assumptions. An attractive property of our proof is that it is agnostic to the density estimator used provided it achieves the correct rates.</p><p>For the LOO estimator (Equation ( <ref type="formula" target="#formula_12">8</ref>)), we establish the following result. The key technical challenge in analysing the LOO estimator (when compared to the DS estimator) is in bounding the variance as there are several correlated terms in the summation. The bounded difference inequality is a popular trick used in such settings, but this requires a supremum on the influence functions which leads to significantly worse rates. Instead we use the Efron-Stein inequality which provides an integrated version of bounded differences that can recover the correct rate when coupled with Assumption 4. Our proof is contingent on the use of the KDE as the density estimator. While our empirical studies indicate that T LOO 's limiting distribution is normal (Fig <ref type="figure" target="#fig_5">2(c</ref>)), the proof seems challenging due to the correlation between terms in the summation. We conjecture that T LOO is indeed asymptotically normal but for now leave it to future work.</p><p>We reiterate that while the convergence rates are the same for both DS and LOO estimators, the data splitting degrades empirical performance of T DS as we show in our simulations. Now we turn our attention to functionals of two distributions. When analysing asymptotics we will assume that as n, m → ∞, n/(n + m) → ζ ∈ (0, 1). Denote N = n + m. For the DS estimator <ref type="bibr" target="#b8">(9)</ref> we generalise our analysis for one distribution to establish the theorem below.</p><p>Theorem 6 (Convergence/Asymptotic Normality of DS Estimator for T (f, g)). Let f, g ∈ Σ(s, L, B, B ) and ψ f , ψ g satisfy Assumption 4. Then, E[(</p><formula xml:id="formula_18">T DS -T (f, g)) 2 ] is O(n -4s 2s+d + m -4s 2s+d )</formula><p>when s &lt; d/2 and O(n -1 + m -1 ) when s ≥ d/2. Further, when s &gt; d/2 and when ψ f , ψ g = 0,</p><formula xml:id="formula_19">T DS is asymptotically normal, √ N ( T DS -T (f, g)) D -→ N 0, 1 ζ V f [ψ f (X; f, g)] + 1 1 -ζ V g [ψ g (Y ; f, g)] .<label>(11)</label></formula><p>The convergence rate is analogous to the one distribution case with the estimator achieving the parametric rate under similar smoothness conditions. The asymptotic normality result allows us to construct asymptotic confidence intervals for the functional. Even though the asymptotic variance of the influence function is not known, by Slutzky's theorem any consistent estimate of the variance gives a valid asymptotic confidence interval. In fact, we can use an influence function based estimator for the asymptotic variance, since it is also a differentiable functional of the densities. We demonstrate this in our example in Appendix F.</p><p>The condition ψ f , ψ g = 0 is somewhat technical. When both ψ f and ψ g are zero, the first order terms vanishes and the estimator converges very fast (at rate 1/n 2 ). However, the asymptotic behavior of the estimator is unclear. While this degeneracy occurs only on a meagre set, it does arise for important choices, such as the null hypothesis f = g in two-sample testing problems.</p><p>Finally, for the LOO estimator <ref type="bibr" target="#b9">(10)</ref> on two distributions we have the following result. Convergence is analogous to the one distribution setting and the parametric rate is achieved when s &gt; d/2.</p><p>Theorem 7 (Convergence of LOO Estimator for T (f, g)). Let f, g ∈ Σ(s, L, B, B ) and ψ f , ψ g satisfy Assumption 4. Then, E[(</p><formula xml:id="formula_20">T LOO -T (f, g)) 2 ] is O(n -4s 2s+d + m -4s 2s+d ) when s &lt; d/2 and O(n -1 + m -1 ) when s ≥ d/2.</formula><p>For many functionals, a Hölderian assumption (Σ(s, L)) alone is sufficient to guarantee the rates in Theorems 5,6 and 7. However, for some functionals (such as the α-divergences) we require f , ĝ, f, g to be bounded above and below. Existing results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> demonstrate that estimating such quantities is difficult without this assumption. Now we turn our attention to the question of statistical difficulty. Via lower bounds given by Birgé and Massart <ref type="bibr" target="#b2">[3]</ref> and Laurent <ref type="bibr" target="#b13">[14]</ref> we know that the DS and LOO estimators are minimax optimal when s &gt; d/2 for functionals of one distribution. In the following theorem, we present a lower bound for estimating functionals of two distributions.</p><p>Theorem 8 (Lower Bound for T (f, g)). Let f, g ∈ Σ(s, L) and T be any estimator for T (f, g). Define τ = min{8s/(4s + d), 1}. Then there exists a strictly positive constant c such that,</p><formula xml:id="formula_21">lim inf n→∞ inf T sup f,g∈Σ(s,L) E ( T -T (f, g)) 2 ≥ c n -τ + m -τ .</formula><p>Our proof, given in Appendix E, is based on LeCam's method <ref type="bibr" target="#b34">[35]</ref> and generalises the analysis of Birgé and Massart <ref type="bibr" target="#b2">[3]</ref> for functionals of one distribution. This establishes minimax optimality of the DS/LOO estimators for functionals of two distributions when s ≥ d/2. However, when s &lt; d/2 there is a gap between our upper and lower bounds. It is natural to ask if it is possible to improve on our rates in this regime. A series of work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> shows that, for integral functionals of one distribution, one can achieve the n -1 rate when s &gt; d/4 by estimating the second order term in the functional Taylor expansion. This second order correction was also done for polynomial functionals of two distributions with similar statistical gains <ref type="bibr" target="#b11">[12]</ref>. While we believe this is possible here, these estimators are conceptually complicated and computationally expensive -requiring O(n 3 + m 3 ) running time compared to the O(n 2 + m 2 ) running time for our estimator. The first order estimator has a favorable balance between statistical and computational efficiency. Further, not much is known about the limiting distribution of second order estimators.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with Other Approaches</head><p>Estimation of statistical functionals under nonparametric assumptions has received considerable attention over the last few decades. A large body of work has focused on estimating the Shannon entropy-Beirlant et al. <ref type="bibr" target="#b0">[1]</ref> gives a nice review of results and techniques. More recent work in the single-distribution setting includes estimation of Rényi and Tsallis entropies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. There are also several papers extending some of these techniques to divergence estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Many of the existing methods can be categorised as plug-in methods: they are based on estimating the densities either via a KDE or using k-Nearest Neighbors (k-NN) and evaluating the functional on these estimates. Plug-in methods are conceptually simple but unfortunately suffer several drawbacks. First, they typically have worse convergence rate than our approach, achieving the parametric rate only when s ≥ d as opposed to s ≥ d/2 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>. Secondly, using either the KDE or k-NN, obtaining the best rates for plug-in methods requires undersmoothing the density estimate and we are not aware for principled approaches for selecting this smoothing parameter. In contrast, the bandwidth used in our estimators is the optimal bandwidth for density estimation so we can select it using a number of approaches, e.g. cross validation. This is convenient from a practitioners perspective as the bandwidth can be selected automatically, a convenience that other estimators do not enjoy. Secondly, plugin methods based on the KDE always require computationally burdensome numeric integration. In our approach, numeric integration can be avoided for many functionals of interest (See Table <ref type="table">1</ref>).</p><p>Another line of work focuses more specifically on estimating f -Divergences. Nguyen et al. <ref type="bibr" target="#b21">[22]</ref> estimate f -divergences by solving a convex program and analyse the method when the likelihood ratio of the densities belongs to an RKHS. Comparing the theoretical results is not straightforward as it is not clear how to port the RKHS assumption to our setting. Further, the size of the convex program increases with the sample size which is problematic for large samples. Moon and Hero <ref type="bibr" target="#b20">[21]</ref> use a weighted ensemble estimator for f -divergences. They establish asymptotic normality and the parametric convergence rate only when s ≥ d, which is a stronger smoothness assumption than is required by our technique. Both these works only consider f -divergences, whereas our method has wider applicability and includes f -divergences as a special case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We compare the estimators derived using our methods on a series of synthetic examples. We compare against the methods in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b32">33]</ref>. Software for the estimators was obtained either  directly from the papers or from Szabó <ref type="bibr" target="#b33">[34]</ref>. For the DS/LOO estimators, we estimate the density via a KDE with the smoothing kernels constructed using Legendre polynomials <ref type="bibr" target="#b34">[35]</ref>. In both cases and for the plug in estimator we choose the bandwidth by performing 5-fold cross validation. The integration for the plug in estimator is approximated numerically.</p><formula xml:id="formula_22">(a) -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Quantiles of N (0, 1) Quantiles of n -1/2 ( TDS -T )/σ (b) -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Quantiles of N (0, 1) Quantiles of n -1/2 ( TLOO -T )/σ (c)</formula><p>We test the estimators on a series of synthetic datasets in 1 -4 dimension. The specifics of the densities used in the examples and methods compared to are given in Appendix G. The results are shown in Figures <ref type="figure" target="#fig_3">1</ref> and<ref type="figure" target="#fig_5">2</ref>. We make the following observations. In most cases the LOO estimator performs best. The DS estimator approaches the LOO estimator when there are many samples but is generally inferior to the LOO estimator with few samples. This, as we have explained before is because data splitting does not make efficient use of the data. The k-NN estimator for divergences <ref type="bibr" target="#b27">[28]</ref> requires choosing a k. For this estimator, we used the default setting for k given in the software. As performance is sensitive to the choice of k, it performs well in some cases but poorly in other cases. We reiterate that the hyper-parameter of our estimator (bandwidth of the kernel) can be selected automatically using cross validation.</p><p>Next, we test the DS and LOO estimators for asymptotic normality on a 4-dimensional Hellinger divergence estimation problem. We use 4000 samples for estimation. We repeat this experiment 200 times and compare the empiriical asymptotic distribution (i.e. the √ 4000( T -T (f, g))/ S values where S is the estimated asymptotic variance) to a N (0, 1) distribution on a QQ plot. The results in Figure <ref type="figure" target="#fig_5">2</ref> suggest that both estimators are asymptotically normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image clustering:</head><p>We demonstrate the use of our nonparametric divergence estimators in an image clustering task on the ETH-80 datset <ref type="bibr" target="#b15">[16]</ref>. Using our Hellinger divergence estimator we achieved an accuracy of 92.47% whereas a naive spectral clustering approach achieved only 70.18%. When we used a k-NN estimator for the Hellinger divergence <ref type="bibr" target="#b27">[28]</ref> we achieved 90.04% which attests to the superiority of our method. Since this is not the main focus of this work we defer this to Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We generalise existing results in Von Mises estimation by proposing an empirically superior LOO technique for estimating functionals and extending the framework to functionals of two distributions. We also prove a lower bound for the latter setting. We demonstrate the practical utility of our technique via comparisons against other alternatives and an image clustering application. An open problem arising out of our work is to derive the limiting distribution of the LOO estimator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 ..</head><label>3</label><figDesc>Let X ⊂ R d be a compact space. For any r = (r 1 , . . . , r d ), r i ∈ N, define |r| = i r i and D r = The Hölder class Σ(s, L) is the set of functions on L 2 (X ) satisfying,|D r f (x) -D r f (y)| ≤ L x -y s-r ,for all r s.t. |r| ≤ s and for all x, y ∈ X .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 5 (</head><label>5</label><figDesc>Convergence of LOO Estimator for T (f )). Let f ∈ Σ(s, L, B, B ) and ψ satisfy Assumption 4. Then, E[( T LOO -T (f )) 2 ] is O(n -4s 2s+d ) when s &lt; d/2 and O(n -1 ) when s ≥ d/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of DS/LOO estimators against alternatives on different functionals. The y-axis is the error | T -T (f, g)| and the x-axis is the number of samples. All curves were produced by averaging over 50 experiments. Discretisation in hyperparameter selection may explain some of the unsmooth curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fig (a): Comparison of the LOO vs DS estimator on estimating the Conditional Tsallis divergence in 4 dimensions. Note that the plug-in estimator is intractable due to numerical integration. There are no other known estimators for the conditional tsallis divergence. Figs (b), (c): QQ plots obtained using 4000 samples for Hellinger divergence estimation in 4 dimensions using the DS and LOO estimators respectively.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by NSF Big Data grant IIS-1247658 and DOE grant DESC0011114.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonparametric entropy estimation: An overview</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Beirlant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Dudewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">László</forename><surname>Györfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">C</forename><surname>Van Der Meulen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical and Statistical Sciences</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating integrated squared density derivatives: sharp best order of convergence estimates</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya'acov</forename><surname>Ritov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhyā: The Indian Journal of Statistics</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimation of integral functionals of a density</title>
		<author>
			<persName><forename type="first">Lucien</forename><surname>Birgé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. of Stat</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On local intrinsic dimension estimation and its applications</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviv</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Divisive Information Theoretic Feature Clustering Algorithm for Text Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramanyam</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Emery</surname></persName>
		</author>
		<author>
			<persName><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><surname>Voiculescu</surname></persName>
		</author>
		<title level="m">Lectures on Prob. Theory and Stat</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Von Mises calculus for statistical functionals</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Fernholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture notes in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new class of random vector entropy estimators and its applications</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Nawaz Goria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><forename type="middle">N</forename><surname>Leonenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">V</forename><surname>Mergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename><surname>Luigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Novi</forename><surname>Inverardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonparametric Statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Applications of entropic spanning graphs</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><surname>Gorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimation of entropy-type integral functionals</title>
		<author>
			<persName><forename type="first">David</forename><surname>Källberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Seleznjev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating nonquadratic functionals of a density using haar wavelets</title>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Kerkyacharian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Stat</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonparametric Estimation of Rényi Divergence and Friends</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Estimating L 2 2 Divergence</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of integral functionals of a density</title>
		<author>
			<persName><forename type="first">Béatrice</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. of Stat</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ICA using spacings estimates of entropy</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing Appearance and Contour Based Methods for Object Categorization</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical inference for the epsilon-entropy and the quadratic Rényi entropy</title>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Leonenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Seleznjev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time adaptive information-theoretic optimization of neurophysiology experiments</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lewi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Butera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exponential concentration for mutual information estimation with application to forests</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new class of Entropy Estimators for Multi-dimensional Densities</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multivariate f-divergence Estimation With Confidence</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the Entropy Estimators</title>
		<author>
			<persName><forename type="first">Alizadeh</forename><surname>Havva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">Alizadeh</forename><surname>Noughabi</surname></persName>
		</author>
		<author>
			<persName><surname>Noughabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Computation and Simulation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</title>
		<author>
			<persName><forename type="first">Dávid</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">Hanchuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fulmi</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IEEE PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">KL divergence estimation of continuous distributions</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pérez-Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISIT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the estimation of alpha-divergences</title>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions</title>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Entropy and Kullback-Leibler Divergence Estimation based on Szegos Theorem</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ramırez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Vıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Santamarıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Crespo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>EUSIPCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quadratic semiparametric Von Mises Calculus</title>
		<author>
			<persName><forename type="first">James</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aad</forename><forename type="middle">W</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metrika</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An Information Theoretic Approach to the Functional Classification of Neurons</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Schneidman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exponential Concentration of a Density Functional Estimator</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast Multidimensional Entropy Estimation by k-d Partitioning</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Information Theoretical Estimators Toolbox</title>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Szabó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Nonparametric Estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Aad</forename><forename type="middle">W</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Vaart</surname></persName>
		</author>
		<title level="m">Asymptotic Statistics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Divergence estimation for multidimensional densities via k-nearest-neighbor distances</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
