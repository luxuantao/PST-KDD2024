<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enabling FPGAs in Hyperscale Data Centers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jagath</forename><surname>Weerasinghe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich</orgName>
								<address>
									<addrLine>Säumerstrasse 4</addrLine>
									<postCode>8803</postCode>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francois</forename><surname>Abel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich</orgName>
								<address>
									<addrLine>Säumerstrasse 4</addrLine>
									<postCode>8803</postCode>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Hagleitner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Zurich</orgName>
								<address>
									<addrLine>Säumerstrasse 4</addrLine>
									<postCode>8803</postCode>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><surname>Herkersdorf</surname></persName>
							<email>herkersdorf@tum.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Integrated Systems</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enabling FPGAs in Hyperscale Data Centers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F53CD158DA0E0FD1FFB7DACBDD7CBC8</idno>
					<idno type="DOI">10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.199</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>FPGA</term>
					<term>hyperscale data centers</term>
					<term>cloud computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>FPGAs (Field Programmable Gate Arrays) are making their way into data centers (DCs) and are used to offload and accelerate specific services, but they are not yet available to cloud users. This puts the cloud deployment of compute-intensive workloads at a disadvantage compared with on-site infrastructure installations, where the performance and energy efficiency of FPGAs are increasingly being exploited for application-specific accelerators and heterogeneous computing.</p><p>The cloud is housed in DCs, and DCs are based on ever shrinking servers. Today, we observe the emergence of hyperscale data centers, which are based on densely packaged servers. The shrinking form factor brings the potential to deploy FPGAs on a large scale in such DCs. These FPGAs must be deployed as independent DC resources, and they must be accessible to the cloud users. Therefore, we propose to change the traditional paradigm of the CPU-FPGA interface by decoupling the FPGA from the CPU and connecting the FPGA as a standalone resource to the DC network.</p><p>This allows cloud vendors to offer an FPGA to users in a similar way as a standard server. As existing infrastructure-asa-service (IaaS) mechanisms are not suitable, we propose a new OpenStack (open source cloud computing software) service to integrate FPGAs in the cloud. This proposal is complemented by a framework that enables cloud users to combine multiple FPGAs into a programmable fabric. The proposed architecture and framework address the scalability problem that makes it difficult to provision large numbers of FPGAs. Together, they offer a novel solution for processing large and heterogeneous data sets in the cloud.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The use of FPGAs in application-specific processing and heterogeneous computing domains has been popular for 20 years or so. In contrast, in cloud DCs FPGAs have only just started to be used for offloading and accelerating specific application workloads <ref type="bibr" target="#b0">[1]</ref>. In both cases, FPGAs are usually treated as co-processor workers under the control of a CPU master and are attached over a high-speed point-to-point interconnect such as the PCIe or the HyperTransport buses <ref type="bibr" target="#b1">[2]</ref>. To provide such an FPGA as an independent resource that can be consumed by users in the cloud, it must be abstracted from its attached CPU, which is not straight forward to achieve in such a master-worker programming model <ref type="bibr" target="#b2">[3]</ref>. As a result, FPGAs have not made significant inroads into the cloud yet.</p><p>To accelerate and enable the large-scale deployment of FPGAs in future DCs, we advocate for a change of paradigm in the CPU-FPGA and FPGA-FPGA interfaces. We propose an architecture that sets the FPGA free from the CPU and its PCIe-bus by connecting the FPGA directly to the DC network as a standalone resource. Cloud vendors can then provision these FPGA resources in a similar manner to CPU, memory and storage resources.</p><p>Enabling FPGAs on a large scale opens new opportunities for both cloud customers and cloud vendors. From the customers' perspective, FPGAs can be rented, used and released, similar to cloud infrastructure resources such as virtual machines (VMs) and storage. For example, IaaS users can rent FPGAs for education (e.g., university classes), research (e.g., building HPC systems) and testing (e.g., evaluation prior to deployment in real environments) purposes. From the Platform as a Service (PaaS) vendors' perspective, FPGAs can be used to offer acceleration as a service to the application developers on cloud platforms. For example, PaaS vendors can provide FPGA-accelerated application interfaces to PaaS users. A Software as a Service (SaaS) vendor can use FPGAs to provide acceleration as a service and also to improve user experience. Acceleration of Bing web search service is such an example <ref type="bibr" target="#b0">[1]</ref>. From the FPGA vendors' perspective, the cloud expands the FPGA user base and also opens new paths for them to market their own products. For example, new products can be placed on the cloud so that users can try them out before actually purchasing them. In summary, the deployment of FPGAs in DCs will benefit both the users and the various cloud service providers and operators.</p><p>Meanwhile, the servers, which make up a cloud DC, are continuously shrinking in terms of the form factor. This leads to the emergence of a new class of hyperscale data centers (HSDC) based on small and dense server packaging. This miniaturization of the DC servers is a game-changing requirement that will transform the traditional way of instantiating and operating an FPGA in a DC infrastructure. First, miniaturized servers aim to leverage the advanced semiconductor manufacturing processes for the gates by integrating a complete server system on chip (SoC) for increased density and power efficiency. As a result, legacy memory controllers and high-speed I/Os are embedded on chip, thus eliminating the need for an external PCIe bus to support these I/Os. Second, the shrinking of the DC form factor unit will enable the deployment of a large number of network-attached FPGAs, exceeding by far the scaling capacity of traditional PCIe bus attachments. Once such network-attached FPGAs become available on a large scale in DCs, vendors will be able to rent them out on the cloud. However, as existing serverprovisioning mechanisms are not suitable for this purpose, we propose a new resource-provisioning service in OpenStack for integrating such standalone FPGAs in the cloud. Also, as users will be able to request multiple FPGAs from the cloud, we want to provide them with the possibility to implement a programmable interconnection network of FPGAs in a costeffective, scalable and flexible manner on the cloud. Therefore, we also propose a framework to interconnect multiple FPGAs in a user defined topology, and for the cloud vendor to deploy such a topology in its infrastructure. We expect this software-defined approach of FPGA networking to offer new technical perspectives and solutions for processing large and heterogeneous data sets in the cloud.</p><p>The remainder of this paper is organized as follows: Section II introduces the concept of a hyperscale data center based on the DOME mircoserver <ref type="bibr" target="#b3">[4]</ref>, and Section III analyzes prior art. The rationale behind the network-attached FPGA proposal is elaborated in Section IV. Section V presents the networkattached FPGA architecture, the cloud integration service for FPGAs and the framework for setting up a fabric of FPGAs on the cloud. Initial scaling perspectives and sizing are discussed in Section VI, and we conclude in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HYPERSCALE DATA CENTERS</head><p>The scaling of modern DCs has been fuelled by the continuous shrinking of the server node infrastructure. After the tower-, rack-and blade-server form factors, a new class of hyperscale servers (HSS) is emerging. In an HSS, the form factor is almost exclusively optimized for the performanceper-cost metric. This is achieved by increasing the density of CPUs per real estate and by sharing the cost of resources such as networking, storage, power supply, management and cooling.</p><p>At the time of writing, there are several HSSs on the market <ref type="bibr" target="#b4">[5]</ref> [6] <ref type="bibr" target="#b6">[7]</ref> and at the research stage <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b7">[8]</ref>. Among these, the HSS of DOME <ref type="bibr" target="#b3">[4]</ref> has the objective of building the world's highest density and most energy efficient rack unit. In this paper, we refer to that specific type of HSS for supporting our discussion on hyperscale data centers (HSDC). Figure <ref type="figure" target="#fig_1">1-(b)</ref> shows the packaging concept of the HSS rack chassis (19" by 2U 1 ) proposed in <ref type="bibr" target="#b3">[4]</ref>. In essence, this HSS is disaggregated <ref type="bibr" target="#b8">[9]</ref> into multiple node boards (Figure <ref type="figure" target="#fig_1">1-(a)</ref>), each the size of a double-height dual in-line memory module (DIMM -133mm x 55mm), which are densely plugged into a carrier base board. Each node board is a standalone resource, and the DC network is used to interconnect all the resources. A node board with a CPU and its DRAM is called a compute module. Similarly, a node board with solid-state disks (SSD) is called a storage module, and a node board with an Ethernet switch is referred to as a networking module. The use of a homogeneous form factor is a significant contributor to the overall cost minimization of an HSS. The carrier base board is a passive board that provides system management, 10 GbE networking between the node boards and multiple 40 GbE uplinks. Figure <ref type="figure" target="#fig_1">1-(d)</ref> shows an example of a network interconnect based on a fat-tree topology (a.k.a folded Clos topology) <ref type="bibr" target="#b9">[10]</ref> that interconnects four racks of 16 chassis, each with 128 node boards and two networking modules. This ultra-dense packaging is combined with an innovative cooling system <ref type="bibr" target="#b10">[11]</ref> that enables the integration of as many as 128 compute modules into a 2U chassis (Figure <ref type="figure" target="#fig_1">1-(b</ref>)). For a 19" rack with 16 such chassis, this amounts to 2K compute modules and ∼100 TB of DRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRIOR ART</head><p>There are a few previous attempts to enable FPGAs in the cloud. Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Byma et al. <ref type="bibr" target="#b11">[12]</ref> proposed frameworks to integrate virtualized FPGAs into the cloud using the OpenStack infrastructure manager. In both cases, FPGAs are virtualized by partitioning them to multiple slots, where each slot or virtual FPGA is a partially reconfigurable region in a physical FPGA which is attached over a PCIe-bus.</p><p>In <ref type="bibr" target="#b2">[3]</ref>, a virtual FPGA model is present in each virtual machine (VM) and acts as the communication channel between the applications running in the VM and the virtual FPGA. The commands and data communicated by the virtual FPGA model are transferred to the virtual FPGA by the hypervisor. There are a few drawbacks in this framework particularly from the perspective of a cloud-based framework. First, users can not deploy their own designs in the FPGAs, instead a limited set of applications offered by the cloud vendor has to be used. Second, if a user needs to deploy an application using several virtual FPGAs connected together, the data have to be copied back and forth through the VM and hypervisor stack to feed the next FPGA. Third, VM migration disrupts the use of the virtual FPGA because the physical FPGA is tightly coupled to the hypervisor.</p><p>In contrast to <ref type="bibr" target="#b2">[3]</ref>, the framework proposed by <ref type="bibr" target="#b11">[12]</ref> allows users to deploy their own application in the FPGAs and allows those applications to be accessed over the Ethernet network. In addition to that, <ref type="bibr" target="#b11">[12]</ref> has shown that the OpenStack Glance image service can be used for managing FPGA bit streams, which is an important factor when integrating FPGAs into OpenStack. However, from the perspective of a cloud deployment, also this framework has a few drawbacks. First, as network, a plain Ethernet connection is offered to the virtual FPGAs which limits the flexibility of the applications that can run on FPGAs. Second, even though multiple virtual FPGAs are enabled in a single physical FPGA, an isolation method, such as VLAN or overlay virtual network (OVN) for multitenant deployments is not supported, which is indispensable in deploying applications in the cloud.</p><p>Catapult <ref type="bibr" target="#b0">[1]</ref> is a highly customized, application-specific FPGA-based reconfigurable fabric designed to accelerate pageranking algorithms in the Bing web search engine. It is a good example to show the potential of FPGAs on a large scale in cloud DCs. Authors claim that compared with a softwareonly approach, Catapult can achieve 95% improvement in ranking throughput for a fixed latency. In Catapult, similarly to the above-mentioned systems, FPGAs are PCIe-attached. But for scaling, these PCIe-attached FPGAs are connected by a dedicated serial network, which breaks the homogeneity of the DC network and increases the complexity of the system. This complexity overhead can be traded off for the significant boost in the overall performance. However, in the case of general-purpose cloud DCs, maintaining such a customized infrastructure is not acceptable because the FPGAs are used in diverse kinds of applications at different scales similarly to other DC resources such as servers and storage.</p><p>All these systems deploy FPGAs tightly coupled to a sever over the PCIe bus. In <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>, FPGAs are accessed through the PCIe bus, whereas <ref type="bibr" target="#b11">[12]</ref> uses a plain Ethernet connection. In <ref type="bibr" target="#b0">[1]</ref>, FPGAs are chained in a dedicated network for scaling. In contrast to those systems, the focus of our proposal is to consider the FPGA as a DC network-connected standalone resource with compute, memory and networking capabilities. In the context of an HSDC, the FPGA resource is therefore considered as a hyperscale server-class computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FPGA ATTACHMENT OPTIONS</head><p>The miniaturization of the DC servers is a game-changing requirement that will transform the traditional way of instantiating and operating an FPGA in a DC infrastructure. We consider three approaches for setting up a large number of FPGAs into an HSDC. One option is to incorporate the FPGA onto the same board as the CPU when a tight or coherent memory coupling between the two devices is desired (Figure <ref type="figure" target="#fig_2">2-(a)</ref>). We do not expect such a close coupling to be generalized outside the scope of very specific applications, such as web searching or textanalytics processing <ref type="bibr" target="#b12">[13]</ref>. First, it breaks the homogeneity of the compute module in an environment where server homogeneity is sought to reduce the management overhead and provide flexibility across compatible hardware platforms. Second, in large DCs, failed resources can be kept in place for months and years without being repaired or replaced, in what is often referred to as a fail-in-place strategy. Therefore, an FPGA will become unusable and its resources wasted if its host CPU fails. Third, the footprint of the FPGA takes a significant real estate away from the compute module -the layout of a large FPGA on a printed circuit board is somehow equivalent to the footprint of a DDR3 memory channel, i.e. 8-16GB-, which may require the size of the module to be increased (e.g., by doubling the height of the standard node board from 2U to 4U). Finally, the power consumption and power dissipation of such a duo may exceed the design capacity of a standard node board.</p><p>The second and by far the most popular option in use today is to implement the FPGA on a daughter-card and communicate with the CPU over a high-speed point-to-point interconnect such as the PCIe-bus (Figure <ref type="figure" target="#fig_2">2-(b)</ref>). This path provides a better balance of power and physical space and is already put to use by FPGAs <ref type="bibr" target="#b0">[1]</ref> as well as graphics processing units (GPU) in current DCs. However, this type of interface comes with the following two drawbacks when used in a DC. First, the use of the FPGA(s) is tightly bonded to the workload of the CPU, and the fewer the PCIe-buses per CPU, the higher is the chance of under-provisioning the FPGA(s), and vice-versa. Catapult <ref type="bibr" target="#b0">[1]</ref> uses one PCIe-attached FPGA per CPU and solves this inelastic issue by deploying a secondary inter-FPGA network at the price of additional cost, increased cabling and management complexity. Second, server applications are often migrated within DCs. The PCIeattached FPGA(s) affected must then be detached from the bus before being migrated to a destination where an identical number and type of FPGAs must exist, thus hindering the entire migration process. Finally, despite the wide use of this attachment model in high-performance computing, we do not believe that is a way forward for the deployment of FPGAs in the cloud because it confines this type of programmable technology to the role of coarse accelerator in the service of a traditional CPU-centric platform.</p><p>The third and preferred method for deploying FPGAs in an HSDC is to set the FPGA free from the traditional CPU-FPGA attachment by hooking up the FPGA directly to the HSDC network (Figure <ref type="figure" target="#fig_2">2-(c)</ref>). The main implication of this scheme is that the FPGA must be turned into a standalone appliance capable of communicating with a host CPU over the network of the DC. From a practical point of view, and with respect to the HSDC concept of section II, this is an FPGA module equipped with an FPGA, some optional local memory and a network controller interface (NIC). Joining a NIC to an FPGA enables that FPGA to communicate with other DC resources, such as servers, disks, I/O and other FPGA modules. Multiple such FPGA modules can then be deployed in the HSDC independently of the number of CPUs, thus overcoming the limitations of the two previous options.</p><p>The networking layer of such an FPGA module can be implemented with either a discrete or an integrated NIC. A discrete NIC (e.g., dual 10 GbE NIC) is a sizable applicationspecific integrated circuit (ASIC) typically featuring 500+ pins, 400+ mm 2 of packaging, and 5 to 15 W of power consumption. The footprint and power consumption of such an ASIC do not favour a shared-board implementation with the FPGA (see above discussion on sharing board space between an FPGA and a CPU). Inserting a discrete component also adds a point of failure in the system. Integrating the NIC into the reconfigurable fabric of the FPGA alleviates these issues and is becoming practical with the latest FPGA devices which can implement a 10 Gb/s network protocol stack in less than 5-10% of their total resources <ref type="bibr" target="#b13">[14]</ref>. Normally, the Ethernet media access controller (MAC) of such a protocol stack connects to an external physical layer device (PHY) whose task is to perform encoding/decoding and serialization/deserialization functions, as well as a transceiver, such as the enhanced small form-factor pluggable transceiver (SFP+) whose task is to physically move the data bits over the media according to a specific physical layer standard. However, in the case of a HSDC chassis, the need for an external PHY and an external transceiver can be skipped by selecting the appropriate FPGA device from a family. First, because of the dense packing of an HSDC, the modules plugged on the same chassis base board are all located within short distance and do not require an external transceiver to communicate with each other. Second, all mid-and high-end networking-oriented FPGAs offer integrated high-speed transceivers that already support most of the popular PHYs. These integrated transceivers operate at line rates up to 32 Gb/s, and they commonly support the 10GBASE-KR (10 Gb/s) and 40GBASE-KR4 (40 Gb/s) Ethernet standards, which we seek for interconnecting our modules over a distance of up to 1 meter of copper printed circuit board and two connectors. This removal of an external PHY and transceiver is a key contributor in the overall power, latency, cost and area savings. In summary, we advocate a direct attachment of the FPGA to the DC network by means of an integrated NIC, and refer to such a standalone FPGA as a network-attached FPGA. This paves the way towards using FPGAs in resource-centric DCs <ref type="bibr" target="#b8">[9]</ref>. The combination of such network-attached FPGAs with emerging software-defined networking (SDN) technologies brings new technical perspectives and market value propositions, such as building large and programmable fabrics of FPGAs on the cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SYSTEM ARCHITECTURE</head><p>In this section, we propose and describe A) the architecture of such a network-attached FPGA, B) the way it is integrated into a cloud environment, and C) how it can be deployed and used on a large scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network-attached FPGA Architecture</head><p>The high-level architecture of the proposed networkattached FPGA concept is shown in Figure <ref type="figure" target="#fig_4">3</ref>. It contains an FPGA and an optional off-chip memory. The FPGA is split into three main parts: i) a user logic part used for implementing customized applications, ii) a network service layer (NSL), In the context of an HSDC, the FPGA concept of Figure <ref type="figure" target="#fig_4">3</ref> is matched to the double-height DIMM form factor defined in section II and is therefore referred to as an FPGA module. The architecture of such a network-attached FPGA module is now explained in detail with reference to Figure <ref type="figure">4</ref>.</p><p>1) User Logic (vFPGA): Multiple user applications can be hosted on a single physical FPGA (pFPGA), somehow similar to multiple VMs running on the same hypervisor. Each user gets a partition of the entire user logic and uses it to implement its applications. This partitioning is achieved by a feature called partial reconfiguration, a technology used to dynamically reconfigure a region of the FPGA while other regions are running untouched<ref type="foot" target="#foot_0">2</ref> . We refer to such a partition of user logic as a virtual FPGA (vFPGA), and it is depicted in Figure <ref type="figure">4</ref> as vFPGA1 and vFPGA2<ref type="foot" target="#foot_1">3</ref> . For the sake of simplicity, in this discussion we assume there is only one vFPGA in the user logic. A vFPGA is assigned an ID (vFPGAID), an IP address, a MAC address and a tenant ID. The vFPGA connects to the DC network through the NSL, and can therefore communicate with other vFPGAs. A vFPGA can also have off-chip local memory assigned to it.</p><p>2) Network Service Layer (NSL): The NSL is a HW implementation of the physical, data link, network and transport layers (L1-L4) used in a typical protocol layered architecture. These layers are mapped into the following three components: i) an Ethernet media access controller (MAC) , ii) a network and transport stack, and iii) an application interface.</p><p>a) Ethernet MAC: The Ethernet MAC implements the data link layer of the Ethernet standard. The MAC performs functions such as frame delineation, cyclic redundancy check (CRC), virtual LAN extraction and collection of statistics.</p><p>b) Network and Transport Stack: The network and transport stack provides a HW implementation of L3-L4 protocols. Applications running on a cloud HW infrastructure are inherently diverse. These applications impose different communication requirements on the infrastructure. For example, one system may require a reliable, stream-based connection such as TCP for inter-application communication, whereas another system may need an unreliable, message-oriented communication, such as UDP. For the applications where latency is critical, RoCE might be preferred. Having this network and transport stack implemented in HW within the FPGA provides low-latency and enables to instantiate these protocols on demand. Again, we leverage partial reconfiguration feature of the FPGA to achieve such a flexibility.</p><p>Usually, a network protocol stack contains a control plane and a data plane. The control plane learns how to forward packets, whereas the data plane performs the actual packet forwarding based on the rules learnt by the control plane. Usually, these two planes sit close to each other in a network stack, with the control plane distributed over the network. With the emergence of SDN, we observe that these planes are getting separated from each other. In the FPGA, it is important to use as few resources as possible for the NSL in order to leave more space for the user logic. To minimize the complexity of the stack, inspired by the SDN concepts, we decouple the control plane from the HW implementation of the data plane and place it in software.</p><p>The vFPGAs must be securely isolated in multi-tenant environments. For this isolation, it is important to use widely used techniques such as VLANs or OVNs in order to coexist with other infrastructure resources. Therefore, we implement a tunnel endpoint (TEP) of an OVN in the network and transport stack. The TEP implemented in FPGA hardware also provides an acceleration, as software-based TEPs degrade both the network and CPU performance significantly <ref type="bibr" target="#b14">[15]</ref>.</p><p>The forwarding data base (FDB) sitting in the network and transport stack contains the information on established connections belonging to connection-oriented protocols and the information on allowed packets from connection-less protocols. This information includes mac addresses, IP addresses, OVN IDs and port numbers belonging to source and destination vFPGAs. The control plane running in a centralized network management software feeds this information to the FDB through the ML.</p><p>c) Application Interface: The application interface comprises FIFO-based connection interfaces resembling socket buffers in a TCP or UDP connection. The vFPGA reads from and writes to these FIFOs to communicate with other vFPGAs. One or more FIFO interfaces can be assigned to a single vFPGA.</p><p>3) Management Layer (ML): The management layer contains a memory manager and a management stack. The memory manager enables access to memory assigned to vFPGAs and the management stack enables the vFPGAs to be remotely managed by a centralized management software.</p><p>a) Memory Manager: The memory manager contains a memory controller and a virtualization layer. The memory controller provides the interface for accessing memory from the vFPGAs. The virtualization layer allows the physical memory to be partitioned and shared between different vFPGAs in the same device. This layer is configured through the management stack according to the vFPGA memory requirements. It uses the vFPGAID to calculate the offset when accessing the physical memory that belongs to a particular vFPGA.</p><p>b) Management Stack: The management stack runs a set of agents to enable the centralized resource-management software to manage the FPGA remotely. The agents include functions such as device registration, network and memory configuration, FPGA reconfiguration, and a service to make the FPGA nodes discoverable. The management stack may run on an embedded operating system in a soft core processor or preferably in a hard core processor, like the processing system in a Xilinx FPGA Zynq device. The network connection of the embedded OS is then shared with the HW network stack of the NSL to reduce the number of physical network connections to the FPGA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cloud integration</head><p>Cloud integration is the process of making the abovementioned vFPGAs available in the cloud so that users can rent them. In this section, we present a framework for integrating FPGAs in the cloud that consists of a new accelerator service for OpenStack, a way to integrate FPGAs into OpenStack, a way to provision FPGAs on the cloud, and a way for the user to rent an FPGA on the cloud.</p><p>1) Accelerator Service for OpenStack: We propose a new service for OpenStack to enable network-attached FPGAs. In previous research, FPGAs <ref type="bibr" target="#b11">[12]</ref> [3] and GPUs <ref type="bibr" target="#b15">[16]</ref> have been integrated into the cloud by using the Nova compute service in OpenStack. In those cases, heterogeneous devices are PCIeattached and are usually requested as an option with virtual machines or as a single appliance, which requires a few simple operations to make the device ready for use.</p><p>In our deployment, in contrast, standalone FPGAs are requested independent of a host because we want to consider them as a new class of compute resource. Therefore, similar to Nova, Cinder and Neutron in OpenStack, which translate high-level service API calls into device-specific commands for compute, storage and network resources, we propose the accelerator service shown in Figure <ref type="figure">5</ref>, to integrate and provision FPGAs in the cloud. In the figure, the parts in red show the new extensions we propose for OpenStack. To setup network connections with the standalone FPGAs we need to carry out management tasks. For that, we use an SDN stack connected to the Neutron network service, and we call it the network manager. Here we explain the high-level functionality of the accelerator-service and the network-manager components.</p><p>Accelerator Service: The accelerator service comprises an API front end, a scheduler, a queue, a data base of FPGA resources (DB), and a worker. The API front end receives the accelerator service calls from the users through the OpenStack dashboard or through a command line interface, and dispatches them to the relevant components in the accelerator service. The DB contains the information on pFPGA resources. The scheduler matches the user-requested vFPGA to the user logic of a pFPGA by searching the information in the DB, and forwards the result to the worker. The worker executes four main tasks: i) registration of FPGA nodes in the DB; ii) retrieving vFPGA bit streams from the Swift object store; iii) forwarding service calls to FPGA plug-ins, and iv) forwarding network management tasks to the network manager through the Neutron service. The queue is just there to pass service calls between the API front end, the scheduler and the worker. The FPGA plug-in translates the generic service calls received from the worker into device-specific commands and forwards them to the relevant FPGA devices. We foresee the need for one specific plug-in per FPGA vendor to be hooked to the worker. Other heterogeneous devices like GPUs and DSPs will be hooked to the worker in a similar manner.</p><p>Network Manager: The network manager is connected to the OpenStack Neutron service through a plug-in. The network manager has an API front end, a set of applications, a network topology discovery service, a virtualization layer, and an SDN controller. The API front end receives network service calls from the accelerator-worker through the Neutron and exposes applications running in the network manager. These applications include connection management, security and service level agreements (shown in red in the network manager in Figure <ref type="figure">5</ref>). The virtualization layer provides a simplified view of the overall DC network, including FPGA devices, to the above applications. The SDN controller configures both the FPGAs and network switches according to the commands received by the applications through the virtualization layer.</p><p>2) Integrating FPGAs into OpenStack: In this sub section, the process of integrating FPGAs into OpenStack is outlined. The IaaS vendor executes this process as explained below.</p><p>When the IaaS vendor powers up an FPGA module, the ML of the FPGA starts up with a pre-configured IP address. This IP address is called the management IP. The accelerator service and the network manager use this management IP to communicate with the ML for executing management tasks. Second, the network-attached FPGA module is registered in Fig. <ref type="figure">5</ref>. OpenStack Architecture with Network-attached FPGAs the accelerator-DB in the OpenStack accelerator service. This is achieved by triggering the registration process after entering the management IP into the accelerator service. Then the accelerator service acquires the FPGA module information automatically from the ML over the network and stores them in the FPGA resource pool in the accelerator-DB. Third, a few special files 4 needed for vFPGA bitstream generation are uploaded to the OpenStack Swift object store.</p><p>3) Provisioning an FPGA on the Cloud: From the IaaS vendors' perspective, let's now look at the process of provisioning a single vFPGA. When a request for renting a vFPGA arrives, the accelerator-scheduler searches the FPGA pool to find a user logic resource that matches the vFPGA request. Once matched, the tenant ID and an IP address are configured for the vFPGA in the associated pFPGA. After that, the vFPGA is offered to the user with a few special files which are used to generate a bitstream for user application.</p><p>4) Renting an FPGA on the Cloud: From the user's perspective, the process of renting a single vFPGA on the cloud and configuring a bitstream to it is as follows. First, the user specifies the resources that it wants to rent by using a GUI provided by the IaaS vendor. This includes FPGA-internal resources, such as logic cells, DSP slices and Block RAM as well as module resources, such as DC network bandwidth and memory capacity. The IaaS vendor uses this specification to provision a vFPGA as explained above.</p><p>Upon success, a reference to the provisioned vFPGA is returned to the user with a vFPGAID, an IP address and the files needed to compile a design for that vFPGA. Second, the user compiles it's design to a bitstream and uploads it to the OpenStack Swift object store through the Glance image service. Finally, the user associates the uploaded bitstream 4 Users need these files to generate a bitstream for the vFPGA Fig. <ref type="figure">6</ref>. Two Examples of FPGA Fabrics with the returned vFPGAID and requests the accelerator service to boot that vFPGA. At the successful conclusion of the renting process, the vFPGA and its associated memory are accessible over the DC network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fabric of FPGAs on the Cloud</head><p>Motivated by the success of large-scale SW-based distributed applications such as those based on MapReduce and deep learning <ref type="bibr" target="#b16">[17]</ref>, we want to give the users a possibility to distribute their applications on a large number of FPGAs. This sub section describes a framework for interconnecting such a large number of FPGAs in the cloud that offers the potential for FPGAs to be used in large-scale distributed applications.</p><p>We refer to a multiple number of FPGAs connected in a particular topology as an FPGA fabric. When the interconnects of this fabric are reconfigurable, we refer to it as a programmable fabric. Users can define their programmable fabrics of vFPGAs on the cloud and rent them using the proposed framework.  These two fabrics are used to build two different types of applications. As an example, a fabric of FPGAs arranged in a pipeline, shown in Figure <ref type="figure">6-(a)</ref>, is used in Catapult <ref type="bibr" target="#b0">[1]</ref> for accelerating page-ranking algorithms, which we discussed in prior art. Figure <ref type="figure">6-(b</ref>)) shows a high-level view of a fabric that can be used for mapping and reducing type of operations.</p><p>1) Renting an FPGA Fabric on the Cloud: The renting and provisioning steps of such a fabric in the cloud are as follows. First, user decides on the required number of vFPGAs and customizes them as mentioned above in the case of a single vFPGA. Second, the user defines its fabric topology by connecting those customized vFPGAs on a GUI or with a script. We call this fabric a vFPGA Fabric (vFF). In vFF, the number of network connections between two vFPGAs can be selected. If a network connection is required between a vFPGA and the SW applications that uses the vFF (explained in the next sub section), it is also configured in this step. Third, the user rents the defined vFF from the IaaS vendor. At this step, the user-defined fabric description is passed to the OpenStack accelerator service. Then, similar to a single vFPGA explained earlier, the accelerator service matches the vFF to the hardware infrastructure as shown in Figure <ref type="figure" target="#fig_6">7</ref>. In addition to the steps followed when matching a single vFPGA, the scheduler considers the proximity of vFPGAs and optimal resource utilization when matching a vFF to the hardware infrastructure. After that, the accelerator service requests the network manager to configure the NSL of assoicated pFPGAs and intermediate network switches to form the fabric in HW infrastructure. Fourth, the user associates a bitstream with each vFPGA of the vFF and requests to boot the fabric. Finally, on successful provisioning, an ID representing the fabric (vFFID) is returned to the users that is used in the programming phase to access the vFF.</p><p>2) Using an FPGA Fabric from SW Applications: The way a vFF is used from a SW application is explained here. We consider the pipeline-based vFF shown in Figure <ref type="figure">6-(</ref>  example and show how it can be used from a SW application. We assume this fabric runs an application based on data-flow computing. The text-analytics acceleration engine explained in <ref type="bibr" target="#b12">[13]</ref> is an example of such an application. Also, we assume that the L4 protocol used is a connection-oriented protocol such as TCP.</p><p>To make the applications agnostic to the network protocols and to facilitate the programming, we propose a library and an API to use both the vFPGAs and vFFs. The vFFID returned at the end of the fabric-deployment phase is used to access the vFF from the SW applications by means of the vFFID. Below are the steps for accessing a vFF. First, the vFF is initialized from the SW application. This initiates a connection for sending data to the vFF as shown by (1)a-conn in Figure <ref type="figure" target="#fig_8">8</ref> . The immediate SDN-enabled switch triggers this connection-request packet and forwards it to the network manager. On behalf of the first vFPGA in the pipeline, the network manager establishes the connection and updates the relevant FDB entries in the associated pFPGA. For receiving data, the library starts a local listener and tells the network manager to initiate a connection on behalf of the last vFPGA in the pipeline ((1)-b-conn). Then, the SW application can start sending data and receiving the result by calling send() and receive() on the vFFH, respectively. If configured by the user at the vFF definition stage, connections are created for sending back intermediate results from the vFPGA to the SW application. When close() is called on the vFFH, the connections established are closed detaching the fabric from the SW application. The connections for accessing memory associated with each vFPGA are also established in a similar manner through the network manager in the fabric initialization phase. The SW applications can write to and read from the memory using the vFPGAID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. OUTLOOK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Resource Estimation</head><p>This section preestimates the FPGA resource utilization for the NSL and the ML. According to the commercial implemen-tations, an UDP and a TCP engine consume approximately 5K <ref type="bibr" target="#b17">[18]</ref> and 10K <ref type="bibr" target="#b13">[14]</ref> lookup tables (LUTs), respectively. A memory controller requires around 4K LUTs <ref type="bibr" target="#b18">[19]</ref>. Assuming that the management stack in the ML is implemented in a hard core processor, we estimate the total resource utilization for both the NSL and the ML to be approximately 30K LUTs, which accounts for 15 to 20% of the total LUT resources available in a modern FPGA device such as the Xilinx 7 series. This resource utilization is comparable to previous attempts of enabling FPGAs in the cloud based on PCIe <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b11">[12]</ref>, which use 20-25% of FPGA resources for communication and management tasks. However, as we already discussed, the network-attached FPGA we propose alleviate most of the limitations posed by the PCIe-bus enabling large scale deployments in the DCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scaling Perspectives</head><p>As explained earlier, the network-attached FPGA module enables large-scale deployment of FPGAs in DCs similarly to compute modules. Table <ref type="table">I</ref> shows the single precision floatingpoint compute performance of a full rack of resources an HSDC built using FreeScale T4240-based <ref type="bibr" target="#b19">[20]</ref> compute modules <ref type="bibr" target="#b10">[11]</ref> and Xilinx Zynq 7100 SoC-based <ref type="bibr" target="#b20">[21]</ref> FPGA modules. Such a full rack of FPGAs can achieve close to 1000 TFLOPS and provides the user with the impressive number of 71 million configurable logic blocks (CLBs). Here, we assume that only 32U of rack space is used, out of 42U, for deploying above resources. FPGAs must be deployed in data centers, and they must be made available to the cloud users. As FPGAs can be reconfigured for specific workloads, users can leverage them to improve the performance and energy efficiency of their processing in the cloud. The provisioning of FPGAs as standalone resources with direct connections to the data center network is the key enabler for a large-scale deployment of FPGAs in the cloud. This is a profound change of paradigm in the CPU-FPGA interaction. The standalone network attachment promotes the FPGA to the rank of a peer processor in the data center. Data centers must take this paradigm shift into account if they want to host FPGAs and other similar heterogeneous computing resources on a large scale in the future. Some emerging hyperscale data centers are already embracing this trend by moving away from the traditional server-centric architecture towards a more resource-centric one. These hyperscale data centers are paving the way for large scale deployment of standalone network-attached FPGAs.</p><p>Once these FPGAs are plentiful in hyperscale data centers, the accelerator provisioning service of the OpenStack software can offer them to cloud users. As a result, a user can lease large numbers of FPGAs on the cloud and can request them to be interconnected into a preferred topology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 1U = one rack unit = 1.75 inches (44.45 mm)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Hyperscale Data Center: (a) Compute module with 48 GB DRAM (on the back). (b) 2U Rack chassis with 128 compute modules. (c) Four racks with 8K compute modules. (d) An example DC network interconnect.</figDesc><graphic coords="3,65.96,73.01,323.90,269.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Options for Attaching an FPGA to a CPU</figDesc><graphic coords="3,323.49,390.50,220.27,67.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Finally, the integrated version of the NIC provides the agility to implement a specific protocol stack on demand, such as Virtual Extensible LAN (VxLAN), Internet Protocol version 4 (IPV4), version 6 (IPv6), Transmission Control Protocol (TCP), User Datagram Protocol (UDP) or Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) Alternatively, it can also adapt to emerging new protocols, such as Generic Network Virtualization Encapsulation (Geneve) and Network Virtualization Overlays (NVO3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. High-level Architecture of the Network-attached FPGA Module</figDesc><graphic coords="5,103.84,73.00,147.08,108.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 6 shows such two fabrics in which vFPGAs with different sizes are shown in different patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. FPGA Fabric Deployment; SW: Network Switch</figDesc><graphic coords="8,79.32,71.00,195.87,189.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>a) as an</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. FPGA Fabric Programming Model</figDesc><graphic coords="8,335.19,73.00,196.10,162.13" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This partial reconfiguration feature is not further discussed as it exceeds the scope of this paper</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Note that in the figure, vFPGA1 and vFPGA2 are not proportional in size to the NSL and the ML</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reconfigurable fabric for accelerating large-scale datacenter services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 41st Annual International Symposium on Computer Architecuture, ser. ISCA &apos;14</title>
		<meeting>eeding of the 41st Annual International Symposium on Computer Architecuture, ser. ISCA &apos;14<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Programming models for reconfigurable application accelerators</title>
		<author>
			<persName><forename type="first">W</forename><surname>Muiris</surname></persName>
		</author>
		<idno>PMEA 2009</idno>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Programming Models for Emerging Architectures</title>
		<imprint>
			<date type="published" when="2009">sept 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enabling FPGAs in the cloud</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Conference on Computing Frontiers, ser. CF &apos;14</title>
		<meeting>the 11th ACM Conference on Computing Frontiers, ser. CF &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The DOME embedded 64 bit microserver demonstrator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on IC Design Technology (ICICDT)</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="203" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">HP Moonshot: An accelerator for hyperscale workloads</title>
		<author>
			<persName><surname>Hewlett-Packard</surname></persName>
		</author>
		<ptr target="www.hp.com" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Seamicro SM15000 fabric compute systems</title>
		<author>
			<persName><surname>Seamicro</surname></persName>
		</author>
		<ptr target="http://www.seamicro.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dell poweredge C5220 microserver</title>
		<author>
			<persName><surname>Dell</surname></persName>
		</author>
		<ptr target="http://www.dell.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The machine: A new kind of computer</title>
		<author>
			<persName><forename type="first">Hewlett</forename><surname>Packard</surname></persName>
		</author>
		<ptr target="http://www.hpl.hp.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Network support for resource disaggregation in nextgeneration datacenters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks, ser. HotNets-XII</title>
		<meeting>the Twelfth ACM Workshop on Hot Topics in Networks, ser. HotNets-XII<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fat-trees: Universal networks for hardware-efficient supercomputing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=4492.4495" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="892" to="901" />
			<date type="published" when="1985-10">Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual function heat-spreading and performance of the IBM/ASTRON DOME 64-bit μserver demonstrator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luijten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IC Design Technology (ICICDT), 2014 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FPGAs in the cloud: Booting virtualized hardware accelerators with openstack</title>
		<author>
			<persName><forename type="first">S</forename><surname>Byma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 22Nd International Symposium on Field-Programmable Custom Computing Machines, ser. FCCM &apos;14</title>
		<meeting>the 2014 IEEE 22Nd International Symposium on Field-Programmable Custom Computing Machines, ser. FCCM &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Giving text analytics a boost</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">TCP/UDP/IP Network Protocol Accelerator</title>
		<author>
			<persName><surname>Mle</surname></persName>
		</author>
		<ptr target="http://www.missinglinkelectronics.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the cost of tunnel endpoint processing in overlay virtual networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utility and Cloud Computing (UCC), 2014 IEEE/ACM 7th International Conference on</title>
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
			<biblScope unit="page" from="756" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Heterogeneous cloud computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Crago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Cluster Computing (CLUSTER)</title>
		<imprint>
			<date type="published" when="2011-09">Sept 2011</date>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">UDPIP -Hardware UDP/IP Stack Core</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="http://www.xilinx.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FPGA implementation of a configurable cache/scratchpad memory with virtualized user-level RDMA capability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kalokerinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAMOS &apos;09. International Symposium on</title>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">T4240 product brief</title>
		<author>
			<persName><surname>Freescale</surname></persName>
		</author>
		<ptr target="www.freescale.com" />
		<imprint>
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">DSP solution</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="http://www.xilinx.com/products/technology/dsp.html" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
