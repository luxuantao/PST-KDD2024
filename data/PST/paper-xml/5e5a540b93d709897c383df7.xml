<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thangarajah</forename><surname>Akilan</surname></persName>
							<idno type="ORCID">0000-0002-2972-3291</idno>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Qingming</surname></persName>
						</author>
						<author>
							<persName><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Akilan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><surname>Huo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<postCode>N9B 3P4</postCode>
									<settlement>Windsor</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Toronto Micro Electronics Inc</orgName>
								<address>
									<postCode>L5T 2H7</postCode>
									<settlement>Mississauga</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Lakehead University</orgName>
								<address>
									<addrLine>Thunder Bay</addrLine>
									<postCode>P7B 5E1</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">52A741FC1EF87F74892519BBBC6C2E59</idno>
					<idno type="DOI">10.1109/TITS.2019.2900426</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>foreground-background segmentation</term>
					<term>intelligent systems</term>
					<term>LSTM</term>
					<term>spatiotemporal cues</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The video-based separation of foreground (FG) and background (BG) has been widely studied due to its vital role in many applications, including intelligent transportation and video surveillance. Most of the existing algorithms are based on traditional computer vision techniques that perform pixellevel processing assuming that FG and BG possess distinct visual characteristics. Recently, state-of-the-art solutions exploit deep learning models targeted originally for image classification. Major drawbacks of such a strategy are the lacking delineation of FG regions due to missing temporal information as they segment the FG based on a single frame object detection strategy. To grapple with this issue, we excogitate a 3D convolutional neural network (3D CNN) with long short-term memory (LSTM) pipelines that harness seminal ideas, viz., fully convolutional networking, 3D transpose convolution, and residual feature flows. Thence, an FG-BG segmenter is implemented in an encoderdecoder fashion and trained on representative FG-BG segments. The model devises a strategy called double encoding and slow decoding, which fuses the learned spatio-temporal cues with appropriate feature maps both in the down-sampling and upsampling paths for achieving well generalized FG object representation. Finally, from the Sigmoid confidence map generated by the 3D CNN-LSTM model, the FG is identified automatically by using Nobuyuki Otsu's method and an empirical global threshold. The analysis of experimental results via standard quantitative metrics on 16 benchmark datasets including both indoor and outdoor scenes validates that the proposed 3D CNN-LSTM achieves competitive performance in terms of figure of merit evaluated against prior and state-of-the-art methods. Besides, a failure analysis is conducted on 20 video sequences from the DAVIS 2016 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>detection, the FG-BG segmentation using visual cues has been an integral subsystem. Hence, the video-based intelligent systems have become ubiquitous due to a myriad of easily accessible low-priced camera modules. Such applications face a crucial challenge of processing massive volume of data from multiple feeds at the same time. It is also required for them to tackle with varying environmental factors, like illumination changes, dynamic backgrounds, and so forth <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. These demands perplex the real-time operation of the systems. In the analysis of traffic flow or human activity, the performance of an ITS substantially depends on the robustness of FG-BG segmentation.</p><p>Besides being a core unit of video analytic intelligent framework, the FG-BG segmentation is also an inherent part of various machine-/computer-vision problems, for instance, attention-aware video analysis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, video saliency-based object segmentation and retrieval <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, image quality assessment <ref type="bibr" target="#b6">[7]</ref>, visual tracking <ref type="bibr" target="#b7">[8]</ref>, and human-robot or machine interaction <ref type="bibr" target="#b8">[9]</ref>. The primary objective of FG-BG segmentation is to place a tight mask, where the appearance of an object, a vehicle or human is monitored. Such mask is very informative than using bounding box as it allows a close localization of the FG objects. An example of FG detection and BG suppression is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where the FG-BG separation is employed to identify the traffic flow on a busy highway. It can be achieved by employing several algorithms categorized into five groups: i) Sample-based <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b14">[15]</ref>, ii). Probabilistic-based <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b20">[21]</ref>, iii). Subspace-based <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, iv). Codebook-based <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, and v). Neural network (NN)-based <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b32">[33]</ref>.</p><p>The sample-based algorithms create a generalized BG model based on the evidence collected in local-level, global-level, or a hybrid-level of the two from the past set of N frames, i.e., for each pixel/super-pixel location or region there are N samples stored. If there are k number of samples in the BG that have a distance smaller than a set threshold τ to the incoming pixel/super-pixel or the region in the current frame, then it is classified as BG, otherwise FG. The probabilistic models work on the principle of stochastic process, like Gaussian Mixture Models (GMM) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref> and Conditional Random Field (CRF)-based algorithms <ref type="bibr" target="#b34">[35]</ref>. The subspace-based approaches perform a transformation of data to a subspace, such as Eigenspace or Principal Component Analysis (PCA)-based subspace. Then, they form a BG model using the subspace and estimate the FG. The Codebook generates a dictionary that consists of color, intensity, temporal features, or similar representations. Same properties of a new pixel are compared with the dictionary values to determine its status. The NN-based models formulate the FG-BG segmentation as a structured input-output matching problem. Such models have gained their reputation after series of breakthrough performances in the ImageNet-Large-Scale Visual Recognition Challenge (ILSVRC) since the year of 2012. The NN-based techniques have been exploited for visual semantics/labeling <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, medical image partitioning <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and recently for video FG-BG segmentation <ref type="bibr" target="#b0">[1]</ref> as well. The main challenges in CNN-based FG detection and BG suppression is that dealing with time-dependent motion and the dithering effect at bordering pixels of the FG objects. We address these issues, by excogitating a 3D EnDec CNN that utilizes a strategy called double encoding with micro-autoencoders and slow decoding using residual connections like in ResNet <ref type="bibr" target="#b39">[40]</ref> for lost feature recovery and 3D Conv and LSTM units to handle local to global long-short term spatio-temporal motion of the FG objects. To facilitate the training process, we take advantage of intra-domain transfer learning.</p><p>In summary, this paper focuses on improving a Vanilla image-to-image Conv-LSTM model for enhanced FG object localization. To this end, the key contributions of this paper are as follows:</p><p>i. It introduces a novel technique named double-encoding using autoencoder-like micro modules and slow-decoding using feature passing residual connections. Here, an input feature at a stage during down-sampling process is encoded twice before it reaches completely to the next level of dimension reduced feature map. While, the up-sampling process decodes the feature maps with two sets of residual feature flows from down-sampling stages for every new spatial dimension of the feature space. ii. The time-dependent video cues are handled by 3D convolutions to capture the short temporal motions while the long-short term temporal motions are captured by LSTM modules in the down-sampling and up-sampling stages, respectively. iii. It provides empirical manifest to show the effectiveness of the proposed model compared to a Vanilla Conv-LSTM network. iv. It carries out testing in an exhaustive manner on various video datasets from the benchmark database called change detection 2014 (CDnet) <ref type="bibr" target="#b19">[20]</ref> and failure analysis on DAVIS-2016 dynamic camera video sequences <ref type="bibr" target="#b40">[41]</ref>. The rest of this paper is organized as follows: Section II reviews related literature. Section III elaborates the architectural information. Section IV describes the experimental set-up, analyses the performance, and highlights some key characteristics of the compared existing methods. Finally, Section VI concludes the paper with future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REVIEW: CNN FOR SEGMENTATION</head><p>Deep CNNs have shown state-of-the-art performance in object segmentation/detection/localization over traditional methods, like GMM <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, Graph-cut, Nonparametric models <ref type="bibr" target="#b14">[15]</ref>, Visual background extractor (ViBe) <ref type="bibr" target="#b10">[11]</ref>, and Pixel-Based Adaptive Segmenter (PBAS) <ref type="bibr" target="#b41">[42]</ref>. Here, the FCN <ref type="bibr" target="#b35">[36]</ref> is a pioneer of CNN architecture that reinterprets the standard visual classification network as layers of full 2D convolutional computations without flattened fully connected layers. This model introduces feature-level augmentations through skip connections that combine deep, coarse, semantic detail and shallow, fine, appearance cues from chosen mid-layers. In contrast, our model performs 3D convolutions with LSTM modules and does the coarse-level feature fusion in a structured manner, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The introduction of micro autoencoder blocks are based on the philosophy of increasing the network depth instead of widening for a better feature generalization. Hence, the residual feature flows negate the vanishing gradient of deep networks by carrying important information from earlier to later layers. Although such shortcuts seem, like an addition to the conventional CNN connections, it alleviates training and reduces the number of parameters <ref type="bibr" target="#b39">[40]</ref>.</p><p>An illustration for the ResNet connection is depicted in Fig. <ref type="figure" target="#fig_2">3 (a)</ref>, where X is an input feature, H (X) is a desired transformation, and F(X) is a residual mapping. In <ref type="bibr" target="#b39">[40]</ref>, the feature fusion operation H (X) = F(X) + X is performed by a shortcut connection and element-wise addition. Contrastingly, our model stacks the features depth-wise as</p><formula xml:id="formula_0">H (X) = F(X)</formula><p>X, like in Fig. <ref type="figure" target="#fig_2">3 (b)</ref>, where denotes coarse-level feature concatenation. This favors to have less number of filters in conv layers resulting less computation.</p><p>Ronneberger et al. <ref type="bibr" target="#b38">[39]</ref> restructures the FCN as EnDec CNN, referred U-net for biomedical cell segmentation. In that, the activation maps after each convolution (conv) in the encoding stage are concatenated with the spatially matching activation maps in the decoding stage. It allows the network to exploit the original contextual information to supplement the features after upsampling in the higher layers. In other words, it is a remedy for the lost spatial resolution due to pooling operations or consecutive convolutional kernel striding. The proposed 3D CNN-LSTM model abstracted by Fig. <ref type="figure" target="#fig_1">2</ref> has the following variations from the U-net:</p><p>i. The max-pooling operations achieve invariant features but has a toll on object localization accuracy <ref type="bibr" target="#b42">[43]</ref>. To circumvent this, we perform subsampling process through 3D strided conv (kernel size of 3 and stride of 2). ii. Our model entirely uses 3D conv layers instead of 2D. iii. Our model uses 3D convolutions in the encoding stage, so it employs 3D transpose conv (3D convT) in contrast to the 2D standard upsampling operations. iv. Our model requires a 5D input data (for timedomain: [b, n, H, W, D]), while the U-net takes 4D data ([b, H, W, D]) without consideration of temporal information. The b, n, H, W, D stand for the input's batch size, look back time steps, height, width, and number of channels. The LSTM is an advanced version of Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b43">[44]</ref>. The Constant Error Carousel (CEC) cells in LSTM use an identity activation function and have self-routed connections to themselves with a constant weight of 1.0. So, the errors backpropagate through the LSTMs cannot explode or vanish <ref type="bibr" target="#b44">[45]</ref>. It is considered to be biologically plausible structure, to a certain extent and has been proved to solve previously unlearnable DL tasks involving temporal data. There are many variations of LSTMs, such as decoupled extended Kalman filter LSTM-RNN <ref type="bibr" target="#b45">[46]</ref>, bi-directional <ref type="bibr" target="#b46">[47]</ref>,</p><p>and Connectionist temporal classification (CTC) <ref type="bibr" target="#b47">[48]</ref>. The LSTM unit is applicable to several real-world tasks, like handwriting recognition <ref type="bibr" target="#b48">[49]</ref>, speech/language identification <ref type="bibr" target="#b46">[47]</ref>, robot control/localization <ref type="bibr" target="#b49">[50]</ref>, and driver distraction detection <ref type="bibr" target="#b50">[51]</ref>. Thus, our model also harnesses the LSTM to capture long-short term temporal connections of FG and BG in the consecutive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL: THE 3D CNN-LSTM FOREGROUND SEGMENTER</head><p>We take advantage of a bottom-up implementation strategy. As we want to handle the sequence learning using LSTM modules, we start from a scratch model as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. It is similar to the U-net <ref type="bibr" target="#b38">[39]</ref>, but instead of standard Conv layers, it employs Conv-LSTM2D layers. Then, we improve the scratch to the proposed 3D CNN-LSTM model. We test both the models on selected datasets as a sanity check and Proof of concept (PoC), then based on the empirical prove we finalize the model and carry out extensive experiments on sixteen benchmark CD-net video sequences and limitation analysis on twenty dynamic camera videos from the DAVIS 2016 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Start From Scratch</head><p>Figure <ref type="figure" target="#fig_3">4</ref> overviews the Vanilla Conv-LSTM2D network that is the stepping stone of our proposed model. The size of the kernel k, stride rate s, and dimension of the output are denoted in the following order and enclosing braces (k, s)[b, n, H, W, D] on each layer. Where, b, n, H, W , and D represents the batch size, number of samples taken by the Conv-LSTM modules to capture the temporal information, height and width of the frame, and number of output feature maps, respectively. The network has 24 layers with 298,529 trainable parameters that integrate three major components: encoder, decoder, and classifier. It maintains a constant number of filters ( <ref type="formula">16</ref>) at each layer (except the penultimate layer, that produces 20 feature maps) and the kernel size, k = 3.</p><p>Hence, the spatial dimension of feature maps is linearly reduced by half through striding the kernel at a rate of 2 in the encoding phase. Thus, the last layer of the encoder generates feature maps that have spatial dimension of 15 × 20 as the network's input layer accepts frames with spatial dimension of 240 × 360. To achieve precisely decoded feature maps there are four mini-decoder blocks sequentially networked. Where, each block subsumes a 3D transpose conv, a Conv-LSTM2D, a concatenation, and again a Conv-LSTM2D layer. Hence, the final layer of the decoder produces feature maps with same spatial dimension as the network's input. Every stage in the decoder receives residual cues from the encoding stage via shortcuts as in the U-net. The final classifier module consists of a Batch Normalization (BN) and 3D conv layers with Sigmoi d function as classifier. Thus, the output of the Vanilla model is the FG-BG probability map of the current frame estimated based on the observed n frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed 3D CNN-LSTM Model</head><p>The proposed model overcomes the issue of lacking FG delineation by capturing short and long-short spatiotemporal cues in local and global levels through 3D convolution and LSTM modules. The proposed model is an improved version of the Vanilla network in Fig. <ref type="figure" target="#fig_3">4</ref>. Figure <ref type="figure">5</ref> elucidates the 3D CNN-LSTM model with type of operation carried out in each layer, the associated activation function, and the output dimension. It is optimized for number of parameters and performance. It gets rid of continuous heavy computing ConvLSTM2D layers in the Vanilla model and replaces them with 3D Conv layers. It has three micro-autoencoder blocks in the down sampling path while up-sampling process is slowed down by frequent feature concatenation blocks. End of down-and up-sampling stages a Conv-LSTM module is used to create long-short-term temporal features. Thus, the network becomes deeper with 42 layers and robust than the Vanilla model. Hence, as in the Vanilla model the FG-BG segmentation is handled by the Sigmoid classifier, which generates higher scores for all possible FG objects and lower scores (close to 0) for BG pixels.</p><p>It is noticed that although the 3D CNN-LSTM model is deeper than the Vanilla model, it consumes only 221,367 trainable parameters. It is understandable, since a ConvLSTM2D layer has more nodes than a Conv3D layer. For instance, a ConvLSTM2D with a kernel size of 3 and 16 output feature maps for a four-frame look back input sequence with a dimension of 240 × 320 × 3 subsumes 11,008 trainable parameters while a Conv3D has only 1,312 trainable parameters for the same setting as shown in Table <ref type="table">I</ref>. So by replacing a ConvLSTM2D with a Conv3D, we can achieve ≈ 84% reduction in the trainable parameters. That in return allows us to design a deeper architecture. Thus, our 3D CNN-LSTM neural net becomes 1.75 times deeper and ≈ 25% lesser parameters than the Vanilla model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Conv-LSTM</head><p>The Conv3D is pertinent to spatiotemporal representation learning <ref type="bibr" target="#b51">[52]</ref>. It performs convolutional operations spatiotemporally unlike 2D Conv layer that does only spatially. Thus, a Conv3D extracts short-term or local temporal features resulting in an output volume. Later, these short-term temporal features are fed into Conv-LSTM units to retain longterm or global temporal connectivity of FG cues between </p><formula xml:id="formula_1">C(m, n) = K -1 k=0 K -1 l=0 ω(k, l) * x(m + k, n + l),<label>(1)</label></formula><p>where * , K , {m, n}, and {k, l} represent the Conv operation, size of the kernel, first coordinate or origin of the image/patch, and element index of the kernel respectively. Hence, feature map dimension of the conv layer is given by (I s -K s + 2 × P)/S + 1, where I s , K s , P, and S denotes size of input image/path, filter size, number of zero-padded pixels, and stride rate respectively. The 2D Conv can be extended for a Conv3D as follows. Let the input path x as a volume of data. Then, the 3D Conv C 3D w.r.t. kernel ω, is computed as</p><formula xml:id="formula_2">C 3D (q, m, n) = T -1 t =0 K -1 k=0 K -1 l=0 ω(t, k, l) * x(q +t, m +k, n +l),<label>(2)</label></formula><p>where * , T , K , {q, m, n}, and {t, k, l} represent the Conv operation, temporal length of the data, size of the kernel, first coordinate or origin of the input patch, and element index of the kernel respectively.</p><p>Hence, the conventional 1D LSTMs take temporal dependency into consideration, but not the spatial dependency. However, in this work, the 2D LSTMs cover both the spatial and temporal relationships as they are integrated with 3D Conv. Figure <ref type="figure">6</ref> describes a standard LSTM unit, where X 1 , . . . , X t are the inputs, C t is the cell state, H t is the hidden state, Fig. <ref type="figure">5</ref>. A layer-wise schematic of the 3D CNN-LSTM model. It exploits autoencoder-like micro modules and slow-decoding strategy. Fig. <ref type="figure">6</ref>. A standard LSTM module with three gates. and i t , f t , and o t are the gates of a ConvLSTM block. If ' * ' and '•' denote the conv operator and Hadamard product, then computation of the ConvLSTM block can be derived as:</p><formula xml:id="formula_3">i t = σ (W xi * X t + W hi * H t -1 + b i ), (<label>3</label></formula><formula xml:id="formula_4">) f t = σ (W x f * X t + W h f * H t -1 + b f ), (<label>4</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">o t = σ (W xo * X t + W ho * H t -1 + b o ), (<label>5</label></formula><formula xml:id="formula_7">) C t = f t • C t -1 +i t • tanh(W xc * X t +W hc * H t -1 +b c ), (<label>6</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">H t = o t • tanh(C t ), (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where σ is the recurrent activator, W x_ and W h_ are the spatial dimension of conv kernels. In this case, σ is a hard sigmoi d function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transpose Convolution</head><p>The Conv3DT layers perform upsampling of 3D Conv such that spatial dimension of the output feature maps become as twice as the input without losing the connectivity pattern.</p><p>In contrast to spatial resizing (extrapolation), the transpose layer has trainable parameters. It is done by inserting zeros between consecutive neurons in the input receptive field, then sliding the conv kernel with unit strides <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Activation Functions</head><p>The activation functions improve NN's representation ability by introducing non-linear factors, since the linear representation of Conv operation faces its limits when it comes to deep architectures. The ReLU can be formally defined as <ref type="bibr" target="#b7">(8)</ref> when taken a case with K number of anchor vectors, denoted by w k ∈ R N , k = 1, 2, . . . , K . For a given input x, the correlations with a k and k = 1, 2, . . . , K , defines a nonlinear rectification to an output y = (y 1 , . . . , y K ) T , where</p><formula xml:id="formula_11">y k (x, a k ) = max(0, a T k x) ≡ ReLU(a T k x),<label>(8)</label></formula><p>i.e., it clips negative values to zero while keeping positive quantities intact. The benefit of ReLU is sparsity, overcoming vanishing gradient, and efficient computation than other activations. Sigmoid, on the other hand, has output in the range [0, 1] for an input x and it is defined by</p><formula xml:id="formula_12">f (x) = 1 1 + ex p(-x) . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>It befits a binary classifier, as used in this work and linear regression problems. Hard-Sigmoid is a linear piece-wise function that approximates the outputs as a linear interpolation between pair of cut-points. It is computationally very fast <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Batch Normalization</head><p>The BN operation can be mathematically formulated as follows. Let the output of a layer X ∈ R N,D , where N is the number of samples in the mini-batch and D is the number of hidden neurons, then normalized matrix X is given as</p><formula xml:id="formula_14">X = X -μ B σ 2 B + , (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where μ B , σ 2 B , and refer to the mean and variance of the mini-batch, and a small value of 0.001 to prevent division by zero, respectively. Then, the layer maintains its representational strength by testing the identity transform as</p><formula xml:id="formula_16">y = γ X + β, (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>where, β and γ are trainable parameters that are initialized with β = 0 and γ = 1, in this work. Note that, when β = μ B and γ = σ 2 B + it returns the previous layer's activation map. Employing BN has multifaceted benefits: i) reducing internal Covariate shift by keeping μ B and σ B close to 0 and 1. ii) Since the batch of examples given in the training are normalized, it increases the generalization of the model. iii) When the BN is located prior to non-linearity, it avoids an undesirable situation, where the training saturates areas of non-linearities, solving the issues of vanishing exploding gradients, and iv) It allows the training process with much higher learning rates without much attention to initialization <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Training Strategy 1) Exclusive Sets:</head><p>Experiments are carried on widely accepted video sequences from change detection 2014 <ref type="bibr" target="#b19">[20]</ref> benchmark database, a.k.a. CDnet. Table <ref type="table">II</ref> briefs the properties of the datasets. To form exclusive sets of training and test, the available samples with ground truths are divided such a way the training set takes first 70% of frames and the test set takes the rest. This approach is more appropriate than a random selection of frames used in <ref type="bibr" target="#b30">[31]</ref> for video FG segmentation. Because, an arbitrary choice of samples may pick a frame, I t for training set while picking a temporally closest frame, like I t +1 or I t -1 for test set. There can be Fig. <ref type="figure">7</ref>.</p><p>Sequence generation for the 3D CNN-LSTM model: 4D ∈ R b×H ×W ×D , 5D ∈ R b×t×H ×W ×D , where b, H, W, D, and t stand for input batch size, height, width, number of channels and number of look back frames, respectively. many such instances in random selection resulting in mere exclusiveness of training and test sets. Note that, in <ref type="bibr" target="#b0">[1]</ref>, 90% of the samples are selected for training and only 20 samples from the rest are considered for testing from each dataset. To meet the input layer requirement of the proposed model, the divided training and test sets, have to be rearranged to form a 5D data sequence as shown in Fig. <ref type="figure">7</ref>, where t, k refer the number of frames taken to represent the temporal domain and total number of sequential samples in the particular dataset. Accordingly, the same arrangement is done for the corresponding ground truths as well. Here, the batch size b, and the number of look back frames t are set to 16 and 4 respectively for all the video sequences. The selection of these values depends on the available GPU memory. In our case, the GPU is an NVIDIA GeForce GTX 1080 Ti with 11GB memory.</p><p>2) Training: The 3D CNN-LSTM is trained individually on each dataset with Adadelta optimizer that minimizes binary cross-entropy loss defined by Eqn. <ref type="bibr" target="#b11">(12)</ref>, where the base learning is set to 0.0002 with a scheduler that reduces the learning rate by factor of 0.8. <ref type="bibr" target="#b11">(12)</ref> where it takes two inputs; first one is the output from the final layer of the network with dimension of N × t × C × H × W , which maps the FG pixel probabilities pn = σ (x n ) ∈ [0, 1] using Sigmoid classifier, σ (.) defined earlier in Eqn. 9. And the second one is target p n ∈ [0, 1] with the same dimension as the first one, where N, t, C, H , and W represent the batch size, the number of frames in temporal axis and channels, height, and width respectively. In this case, p n is the normalized segmentation ground truth images.</p><formula xml:id="formula_18">E = -1 n N n=1 p n log pn + (1 -p n ) log(1 -pn ) ,</formula><p>3) Transfer Learning: To improve the network's trainability in short-span of epochs, it is necessary to have proper weight initialization. It can be achieved by transfer learning, where the model learns new task efficiently by using already learned parameters or knowledge <ref type="bibr" target="#b55">[56]</ref>. To this end, we incorporate intraclass domain transfer and fine-tuning following the dataset pairs given in Table <ref type="table" target="#tab_2">III</ref>. For instance, the model is trained with random initialization on TramStation then fine-tuned for Highway. Note that, since the network has few trainable parameters we fine-tune the entire layers with a smaller learning rate. We expect the pre-trained weights to be quite good already when compared to random initialization, so we do not like to distort them too quickly and too much. As a rule of thumb, we set the initial learning rate ten times smaller than the one used for training from scratch.</p><p>4) Environment: Python with Keras (Tensorflow backend) is used as a software paradigm. The network is trained on a GTX 1080 Ti 11 GiB with Intel(R) Core(TM) i7-6850K CPU @ 3.60 GHz, 64 GiB memory, and Ubuntu 64-bit OS. In average, the training takes about 1.5 to 2 hours depends on the properties of the video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Binary Foreground Mask</head><p>It is also crucial to create a binary mask that segments FG region from BG. We apply an empirically determined dataset-specific global threshold value ([0.05, 0.75]) to transform FG confidence maps generated during inferencing. Then to clean noisy artifacts, a neighborhood connectivity-based post-processing is carried out removing regions of 50 pixels or less. As the FG confidence map represents a bi-modal grayscale image, we also employ the Nobuyuki Otsu's clustering algorithm to choose an appropriate threshold adaptively. Otsu iteratively computes a threshold value, τ that lies inbetween two peaks of the intensity histogram of a bi-model image, whereby intraclass variances are minimum <ref type="bibr" target="#b56">[57]</ref>. The weighted sum of within-class variance is defined as</p><formula xml:id="formula_19">σ 2 ρ (τ ) = ρ 0 (τ )σ 2 0 (τ ) + ρ 1 (τ )σ 2 1 (τ ), (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where the weights ρ 0 and ρ 1 are the probabilities of BG and FG clustered by a threshold τ , and the variances of these two classes are σ 2 0 and σ 2 1 respectively. This binarization process is part of testing stage only as the numerical analysis is made on the binarized FG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP, RESULTS, AND DISCUSSION</head><p>This section provides an empirical evidence for the proposed 3D CNN-LSTM model as a performance comparison to the Vanilla model. Then, it extends the examination of the proposed model through comparisons to existing methods, including classical approaches and recent NN-based models. Some highlights of the compared methods are also provided on-the-fly.</p><p>The model is evaluated on sixteen video sequences from the benchmark change detection database <ref type="bibr" target="#b19">[20]</ref> that consists of both indoor and outdoor scenes. A succinct description of the datasets is given in Table <ref type="table">II</ref>. General nature of the datasets as follows: the baseline represents a mixture of mild challenges, like subtle background motion, isolated shadows, swaying tree branches, and natural illumination changes; the dynamic background includes scenes with strong (parasitic) BG motion, and shimmering water; the camera jitter contains outdoor videos captured by vibrating cameras due to high wind; and the shadow category comprises indoor video exhibiting strong as well as faint shadows, where the shadows are even cast by the moving FG objects on the scene; the low frame rate contains sequences recoded with low frame rate; The PTZ camera and nighttime categories contain surveillance videos shot with PTZ cameras and shot at night, respectively; and the intermittent object motion set contains videos containing BG objects moving away, abandoned objects, and objects stopping for a short while and then moving away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Matrix</head><p>The standard performance measure that evaluates the similarity between predicted FG and the ground truth segmentation. It is a weighted harmonic mean measure of recall and precision, i.e., a region of intersection taken as ratio of union of predicted and actual FG segments as:</p><formula xml:id="formula_21">FoM = 2 × (Precision × Recall) Precision + Recall × 100%,<label>(14)</label></formula><p>where [0 ≤ FoM ≤ 100], recall is the detection rate defined by T P/(T P + F N) and precision is the percentage of correct prediction compared to the total number of detections as positives, given by T P/(T P + F P), where T P, F N, and F P refer true positive, false negative, and false positive respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sanity Test: Vanilla Model vs 3D CNN-LSTM</head><p>To validate the performance gain of the proposed 3D CNN-LSTM model that exploits the micro-autoencoder and slow decoding blocks, firstly we carry out a comparative analysis between the Vanilla model (Fig. <ref type="figure" target="#fig_3">4</ref>) and the improved architecture (Fig. <ref type="figure">5</ref>). The sanity test is performed on a subset of video sequences from Table <ref type="table">II</ref>, including three categories: the baseline, dynamic background, and cast shadow. The sanity check results are tabularized in Table <ref type="table" target="#tab_3">IV</ref>.</p><p>The results show that the 3D CNN-LSTM records the best performance on all the datasets while there is an ≈ 4% improvement overall. Hence, the proposed 3D CNN-LSTM model has inferencing speed of ≈ 24 frames-per-second (FPS) which is 9 frames higher than the Vanilla model that only produces ≈ 15 FPS. Considering these results as an empirical foundation, extensive investigation is followed through on the proposed model with all sixteen video sequences described in Section III-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Analysis</head><p>A visual inspection is carried out by comparing the predicted FG regions with the ground truth segmentations. We limit the qualitative presentation with one sample per data  sequence as shown in Fig. <ref type="figure" target="#fig_4">8</ref> due to space constraints. The qualitative results show that the proposed model has tightly detected the FG and BG when compared to the ground truth segmentations. However, it is important to evaluate its performance across all the test frames of all the video sequences numerically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Analysis: FoM</head><p>The quantitative analysis is provided in Table <ref type="table" target="#tab_4">V</ref> as a comparison between the proposed model and existing methods ranging from traditional approaches to deep learning techniques. It shows that none of the algorithms output performs others on all the video sequences. However, the prosed 3D CNN-LSTM model exhibits a robustness with average FoM of ≈ 94%. The overall improvements of the proposed model are 14%, 6%, 7%, 7%, 8%, and 6% compared to PBAS <ref type="bibr" target="#b41">[42]</ref>, PAWCS <ref type="bibr" target="#b12">[13]</ref>, IUTIS-5 <ref type="bibr" target="#b57">[58]</ref>, MBS <ref type="bibr" target="#b58">[59]</ref>, DeepBS <ref type="bibr" target="#b30">[31]</ref>, and DBFCN <ref type="bibr" target="#b0">[1]</ref>, respectively.</p><p>Hence, It is noticed that there is not much significance gain between Global threshold and Otsu's algorithm to transform the Sigmoid confidence map generated by the 3D CNN-LSTM. It proves that the network generates very strong probability scores that have ignorable noise between FG and BG.</p><p>The key aspects of the compared existing methods are as follows. Hofmann et al. <ref type="bibr" target="#b41">[42]</ref> come up a Pixel-Based Adaptive Segmenter (PBAS) using local features, that generates a generic BG from a dictionary of recently observed pixel values in a non-parametric manner. Then, the FG region is separated based on a set threshold. Researchers, like Charles et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>   non-parametric consensus-based BG model that is then automatically tuned using pixel-level feedback loops. Meanwhile, Babaee et al. <ref type="bibr" target="#b30">[31]</ref> employ a conventional CNN, train the network with randomly selected video frames with ground-truth segmentations patch-wise, like in <ref type="bibr" target="#b27">[28]</ref>, and carry out spatialmedian filtering as the post-processing of the network outputs. Yang et al. <ref type="bibr" target="#b0">[1]</ref> follow the structure of the FCN described in <ref type="bibr" target="#b35">[36]</ref> with replacing the few standard Conv layers to atrous convolution branches that use different dilate to extract spatial information from extended neighborhoods of pixels. They also include a CRF-based refinement step. Although over the past two decades many algorithms have been proposed, none of them can be the ultimate model for video FG inferencing. Therefore, Bianco et al. <ref type="bibr" target="#b57">[58]</ref> explore a way of harnessing multiple state-of-the-art moving object detection algorithms to improve the FG segmentation. They obtain a solution tree through Genetic Programming (GP); however, this approach also cannot be acclaimed as a universal solution.</p><p>Similarly, Sajid and Cheung <ref type="bibr" target="#b58">[59]</ref> introduce a multi-modality framework that estimates multiple BG models and use them as Background Model Bank (BMB). Then, to segment the FG from the dynamic BG, they apply a spatial de-noising approach based on Mega-Pixel (MP) to pixel-level probability estimation using various color spaces and get multiple FG regions. Later, a fusion technique is employed to define a final FG. In summary, most of the state-of-the-art methodologies use patch-wise processing and multi-modality-based algorithms for BG establishment and a feedback-based approach as post processing to refine the primarily detected FG regions. Such setup ensues complex computations and higher processing time due to the time-consuming iterative pursuit of low-rank or sparse matrix. On the contrary, the proposed model processes the whole input image as a single entity during inferencing. Then, it refines the output by a non-iterative process, resulting ≈ 24 FPS for FG-BG inferencing. V. LIMITATIONS Although the proposed model focuses on FG-BG segmentation for static-camera recorded videos, this section conducts a failure analysis of the model with dynamic camera condition. Thus, this analysis is carried out on DAVIS-2016 <ref type="bibr" target="#b40">[41]</ref> validation dataset with the same domain specific supervised training method and configuration used for the experiments in Section IV. This experimental study includes the following twenty video sequences, namely Blackswan (BS), Bmx-trees (BT), Breakdance (BD), Camel (CA), Carroundabout (CR), Car-shadow (CS), Cows (CO), Dance-twirl (DT), Dog (DO), Drift-chicane (DC), Drift-straight (DS), Goat (GO), Horsejump-high (HH), Kite-surf (KS), Libby (LI), Motocross-jump (MJ), Paragliding-launch (PL), Parkour (PA), Scooter-black (SB), and Soapbox (SO). Figure <ref type="figure" target="#fig_6">9</ref> summarizes few visual results and Table VII lists the average performance of the 3D CNN-LSTM on each video sequence. Table VIII compares the mean average performance of the model across all the sequences with existing methods in terms of FoM. All the compared methods also apply domain-specific semi-supervised training strategy. The results of the existing methods are adopted from <ref type="bibr" target="#b68">[69]</ref>. Similar to the employed fine-tuning strategy for the experiments on CDnet database, the model's trainable parameters are initialized as given in Table <ref type="table" target="#tab_4">VI</ref>.  The results are encouraging values. However, when compared to the achievements on CDnet sequences there is a drop of ≈ 7% in overall mean average FoM. On CDnet the proposed model records overall performance of 94.02%, but on DAVIS-2016 it gets 87.20%. Two factors affect the performance of the 3D CNN-LSTM: i. Dynamic camera motion, and ii. The limited number of available frames (&lt; 100) in each video. Note that, in CDnet each sequence has hundreds of frames.</p><p>Hence, the proposed 3D CNN-LSTM has to be retrained for the cases of domain transfer. It is because, the temporal features will be different when the nature of the domain differs, like changes in the frame rate, the motion of the FG objects, motion in the BG, and dynamic cameras. From the extensive experiments, we found that when the model is trained on all the data across all datasets (in Table <ref type="table">II</ref>), the learned weights can be used as an optimal initial model weights for the new domain. Resulting in, a quicker fine tuning and better performance than train from scratch. The number of samples required in the fine-tuning process is subjected to the demand of the new task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This work excogitates a DL model for video fore-/background segmentation. Initially, a Conv-LSTM2D image to image encoder-decoder model is implemented. Then, using it as a base, a 3D CNN-LSTM model is achieved by optimizing number of trainable parameters while increasing the depth of the network via micro-autoencoders and slow decoding process with frequent residual feature forwarding. A sanity check is carried out to validate the improved model's performance over the base structure. The 3D CNN-LSTM captures short-and long-short-term spatiotemporal features through 3D convolutions and LSTM units collectively from a set of t frames before predicting the FG-BG segmentation of the current frame. In contrast to the conventional approaches, DL models do not require any feature engineering and manual parameter tuning as the network parameters are learned from exemplar FG-BG segmentations during training. Therefore, it is believed that the proposed 3D CNN-LSTM is a new addition to the state-of-the-art FG-BG segmentation algorithms.</p><p>The qualitative and quantitative analysis with sixteen benchmark video sequences demonstrates that the network is performant when dealing with FG-BG separation involving lighting variations, cast shadow, dynamic backgrounds, nighttime in indoor and outdoor environments. The results also show that our model superiorly performs most of the cases when compared with traditional and modern NN-based methods. However, this model lacks capability of handling moving camera scenarios. We leave this for the future work. The proposed 3D CNN-LSTM model is applicable to many computer visionbased intelligent systems not just limited to path segmentation for autonomous vehicles and MRI brain slice partitioning. Finally, it is understood that developing a robust FG-BG segmentation solution is still an intriguing task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Traffic flow identification: (a) Multi-channel [R,G,B] spatio-temporal input, (b) 3D CNN-LSTM, (c) FG score map, and (d) identified traffic flow (white -FG and dark -BG).</figDesc><graphic coords="1,312.11,196.85,250.70,70.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of the proposed 3D CNN-LSTM image-to-image network: 3D Conv -3D Convolution w/t down sampling, 3D ConvT -3D transpose conv (up-sampling), BN -Batch normalization, E(•) -binary cross-entropy error.</figDesc><graphic coords="2,50.15,58.49,511.58,129.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CNN feature flows: (a) ResNet flow, and (b) the residual feature mapping of our 3D CNN-LSTM FG segmenter.</figDesc><graphic coords="3,49.07,58.13,250.82,88.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. A layer-wise schematic of the Vanilla model inspired by the U-net<ref type="bibr" target="#b38">[39]</ref>. It exploits Conv-LSTM2D modules in an encoder-decoder fashion with residual feature concatenations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Qualitative results of the proposed 3D CNN-LSTM image to image encoder-decoder FG-BG segmenter: bright and dark pixels represent the FG and BG respectively. (a) Samples for Highway, Office, Pedestrians, PETS2006, Canoe, Boats, OverPass, and Fall datasets (from row 1 to 8). Where, from column 1 to 5: input frames, ground truths, Sigmoid confidence maps, binary FG-BG segmentations generated through G-th and O-th respectively. (b) Samples for Boulevard, CopyMachine, PeopleInShade, BusStation, TwoPositionPTZCam, Turnpike_5_fps, TramStation, and Sofa data sequences (from row 1 to 8). Where, column 1 to 5: input frames, ground truths, Sigmoid confidence maps, binary FG-BG segmentations generated through G-th and O-th respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>also exploit local features to model the BG. They adapt and integrate Local Binary Similarity Patterns (LBSP) as additional cues to pixel intensities in a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Qualitative results of the proposed 3D CNN-LSTM image to image encoder-decoder FG-BG segmenter on DAVIS 2016 dataset: bright and dark pixels represent FG and BG respectively. (a) Sample results. Row 1 to 10: Cows, Blackswan, Breakdance, Dance-twirl, Camel, Kite-surf, Car-roundabout, Car-shadow, Dog, and Drift-chicane. Column 1 to 5: input frames, ground truths, Sigmoid confidence maps, binary FG-BG segmentations generated through G-th and O-th, respectively. (b) Sample results. Row 1 to 10: Paragliding-launch, Scooter-black, Soapbox, Parkour, Drift-straight, Libby, Motocross-jump, Goat, Horsejump-high, and bmx-trees sequences. Column 1 to 5: input frames, ground truths, Sigmoid confidence maps, binary FG-BG segmentations generated through G-th and O-th, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DATASET SUMMARY</head><label>IISUMMARY</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>DATASET PAIRS USED FOR MODEL FINE-TUNING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SANITY</head><label>IV</label><figDesc>CHECK RESULTS OF THE PROPOSED 3D CNN-LSTM COMPARED TO THE VANILLA MODEL IN TERMS OF FOM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON IN TERMS OF FOM: GLOBAL-TH AND OTSU-TH STAND FOR THE TWO METHODS APPLIED TO TRANSFORM SIGMOID SCORES TO BINARY MASK. VALUES IN ARE THE BEST WHILE THE ONES IN ARE THE SECOND BEST FOM (NA -NOT AVAILABLE)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII F</head><label>VII</label><figDesc>-MEASURE OF THE 3D CNN-LSTM ON DAVIS-2016 BINARY FG-BG SEGMENTATION: G-TH AND O-TH STAND FOR THE TWO THRESHOLDS (TH.) APPLIED TO TRANSFORM SIGMOID SCORES TO BINARY MASKTABLE VIII PERFORMANCE COMPARISON ON DAVIS-2016 IN TERMS OF AVERAGE FOM: G-TH AND O-TH STAND FOR THE TWO THRESHOLDS APPLIED TO TRANSFORM SIGMOID SCORES TO BINARY MASK. VALUES IN ARE THE BEST WHILE THE ONES IN ARE THE SECOND BEST FOM</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Canada Research Chair Program and in part by the NSERC Discovery Grant. The Associate Editor for this paper was C. Guo.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep background modeling using fully convolutional network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="254" to="262" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An adaptive background modeling method for foreground segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road scene content analysis for driver assistance and autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Celenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3398" to="3407" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning shape priors for object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1870" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified threshold updating strategy for multivariate Gaussian mixture based moving object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thangarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. High Perform. Comput. Simulation (HPCS)</title>
		<meeting>Int. Conf. High Perform. Comput. Simulation (HPCS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="570" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and tracking in video collections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="3173" to="3181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Review of medical image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paramesran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity fusion for visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="363" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards meaningful human-robot collaboration on object placement</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RSS Workshop Planning Hum.-Robot Interact., Shared Autonomy Collaborative Robot</title>
		<meeting>RSS Workshop Planning Hum.-Robot Interact., Shared Autonomy Collaborative Robot</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NIC: A robust background extraction algorithm for foreground detection in dynamic scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh-The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le-Tien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1478" to="1490" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ViBe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SuBSENSE: A universal change detection method with local adaptive sensitivity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A self-adjusting approach to change detection based on background word consensus</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. Vis. (WCACV)</title>
		<meeting>IEEE Winter Conf. Appl. Comput. Vis. (WCACV)</meeting>
		<imprint>
			<date type="published" when="2015-01">Jan. 2015</date>
			<biblScope unit="page" from="990" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PIDbased regulation of background dynamics for foreground segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tiefenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="3282" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video foreground detection in non-static background using multidimensional color space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thangarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Euro. Conf. Comput. Vis. (ECCV)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusion-based foreground enhancement for background subtraction using multivariate multimodel Gaussian distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="page" from="414" to="431" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="246" to="252" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial mixture of gaussians for dynamic background modelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Adv. Video Signal Based Surveill</title>
		<meeting>IEEE Int. Conf. Adv. Video Signal Based Surveill</meeting>
		<imprint>
			<date type="published" when="2013-08">Aug. 2013</date>
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cdnet 2014: An expanded change detection benchmark dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orpca with mrf for robust foreground detection in highly dynamic backgrounds</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Asian Conf. Comput. Vis., Part III</title>
		<meeting>12th Asian Conf. Comput. Vis., Part III<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Bayesian computer vision system for modeling human interactions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="843" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subspace-based background subtraction applied to aeroacoustic wind tunnel testing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Bahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Horne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Aeroacoustics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="299" to="325" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Background subtraction based on modified online robust principal component analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1839" to="1852" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal background subtraction using word consensus models</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bergevin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4768" to="4781" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A codebook based background subtraction method for image defects detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Intell. Secur. (CIS)</title>
		<meeting>Int. Conf. Comput. Intell. Secur. (CIS)</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="704" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal context for codebook-based dynamic background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEU Int. J. Electron. Commun</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="739" to="747" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning driven blockwise moving object detection with binary scene modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="454" to="463" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked multilayer self-organizing map for background modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2841" to="2850" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel background subtraction approach based on multi layered self-organizing maps</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gemignani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2015-09">Sep. 2015</date>
			<biblScope unit="page" from="462" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network for video sequence background subtraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="635" to="649" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end video background subtraction with 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sakkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="23023" to="23041" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Double encodingslow decoding image to image cnn for foreground identification with application towards intelligent transportation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Inter. Conf. Green Comput</title>
		<meeting>IEEE Inter. Conf. Green Comput</meeting>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="395" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asymmetric mixture model with simultaneous feature selection and model detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="408" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online glocal transfer for automatic figure-ground segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kpalma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ronsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2109" to="2121" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected MRFS</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in MRI images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inter. Conf. Med. Image Comput</title>
		<meeting>Inter. Conf. Med. Image Comput<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Background segmentation with feedback: The pixel-based adaptive segmenter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiefenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
		<meeting>Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="38" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pérez-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="250" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Autom. Speech Recognit. Understand. (ASRU)</title>
		<meeting>IEEE Workshop Autom. Speech Recognit. Understand. (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machi. Learn. (ICML)</title>
		<meeting>Int. Conf. Machi. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Frontiers Handwriting Recognit. (ICFHR)</title>
		<meeting>Int. Conf. Frontiers Handwriting Recognit. (ICFHR)</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human-like hand reaching by motion prediction using long short-term memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inter. Conf. Social Robot</title>
		<meeting>Inter. Conf. Social Robot<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Online driver distraction detection using long short-term memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">New trend in video foreground detection using deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Midwest Symp. Circuits Syst. (MWSCAS)</title>
		<meeting>IEEE Int. Midwest Symp. Circuits Syst. (MWSCAS)</meeting>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1603.07285" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding convolutional neural networks with a mathematical model</title>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="406" to="413" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A late fusion approach for harnessing multi-CNN model high-level features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst., Man, Cybern. (SMC)</title>
		<meeting>IEEE Int. Conf. Syst., Man, Cybern. (SMC)</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="566" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combination of video change detection algorithms by genetic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="914" to="928" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Universal multimode background subtraction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3249" to="3260" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="2051" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SeamSeg: Video object segmentation using patch seams</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">JumpCut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="195" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3491" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="451" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">MaskRNN: Instance level video object segmentation</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural In. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="325" to="334" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
