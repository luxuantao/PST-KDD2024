<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time series forecasting by evolving artificial neural networks with genetic algorithms, differential evolution and estimation of distribution algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-10-14">14 October 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juan</forename><surname>Peralta Donate</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
							<email>xiaodong.li@rmit.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Germa ´n Gutie ´rrez Sa ´nchez •</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Araceli</forename><surname>Sanchis De Miguel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Donate</surname></persName>
						</author>
						<author>
							<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Sa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Á</forename><forename type="middle">A S</forename><surname>De Miguel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Group CAOS</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
								<address>
									<addrLine>Avenida de la Universidad 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganes</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">Royal Melbourne Institute of Techonology</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Time series forecasting by evolving artificial neural networks with genetic algorithms, differential evolution and estimation of distribution algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-10-14">14 October 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">08EEBF87E9BA524EDDE11184F586F2CB</idno>
					<idno type="DOI">10.1007/s00521-011-0741-0</idno>
					<note type="submission">Received: 14 February 2011 / Accepted: 9 September 2011 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Evolutionary computation</term>
					<term>Genetic algorithms</term>
					<term>Differential evolution</term>
					<term>Estimation of distribution algorithm</term>
					<term>Artificial neural networks</term>
					<term>Time series</term>
					<term>Forecasting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series forecasting is an important tool to support both individual and organizational decisions (e.g. planning production resources). In recent years, a large literature has evolved on the use of evolutionary artificial neural networks (EANN) in many forecasting applications. Evolving neural networks are particularly appealing because of their ability to model an unspecified nonlinear relationship between time series variables. In this work, two new approaches of a previous system, automatic design of artificial neural networks (ADANN) applied to forecast time series, are tackled. In ADANN, the automatic process to design artificial neural networks was carried out by a genetic algorithm (GA). This paper evaluates three methods to evolve neural networks architectures, one carried out with genetic algorithm, a second one carried out with differential evolution algorithm (DE) and the last one using estimation of distribution algorithms (EDA). A comparative study among these three methods with a set of referenced time series will be shown. In this paper, we also compare ADANN forecasting ability against a forecasting tool called Forecast Pro Ò (FP) software, using five benchmark time series. The object of this study is to try to improve the final forecasting getting an accurate system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Forecasting the future based on the observed past is an important tool to support individual and organizational decision making. Time series forecasting (TSF), which predicts the behavior of a given phenomenon based solely on the past values of the same event, has become increasingly used in areas such as agriculture, finance, management, production or sales.</p><p>Several TSF methods have been proposed, such as Holt-Winters (in the sixties) or the ARIMA methodology <ref type="bibr" target="#b0">[1]</ref> (in the seventies). More recently, several computational intelligence (CI) methods have been applied to TSF such as immune systems <ref type="bibr" target="#b1">[2]</ref>, support vector machines (SVM) <ref type="bibr" target="#b2">[3]</ref>, artificial neural networks (ANN) <ref type="bibr" target="#b3">[4]</ref>, fuzzy techniques <ref type="bibr" target="#b4">[5]</ref> or hybrid systems combining any of the previous ones with evolutionary search <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>In this paper, we focus on ANN <ref type="bibr" target="#b10">[11]</ref> which are flexible models that do not require a priori knowledge, are capable of nonlinear modeling and also often robust to noisy data. These properties of ANN make them a natural solution for TSF. In effect, ANN have been applied in real-world forecasting tasks, such as market predictions <ref type="bibr" target="#b7">[8]</ref>, meteorological <ref type="bibr" target="#b8">[9]</ref> and network traffic forecasting <ref type="bibr" target="#b9">[10]</ref>.</p><p>A crucial issue is the design of an appropriate ANN model for a particular time series <ref type="bibr" target="#b3">[4]</ref>. It will be necessary to set the type of ANN that will solve the forecasting task (Multilayer Perceptron), the learning algorithm used (Backpropagation) and which specific architecture (topology and connection weights values) will be better to forecast this specific time series. So, it has to be established the suitable value for each degree of freedom in the ANN <ref type="bibr" target="#b10">[11]</ref> (number of input nodes, number of outputs neurons, number of hidden layers, number of hidden neurons in each layer, the connections from one node to another, connection weights, etc.).</p><p>The goals of this paper are to develop an accurate automatic method, to design ANN and to forecast time series based on evolutionary computation (EC). This automatic process consists of an evolutionary search technique such as a GA that uses its exploitation and exploration properties to find a good solution. The chromosome codifies the set of parameters previously commented, and the fitness function will be minimum validation error obtained during the training process. On the other hand, two more EC methods will be used to carry out the evolutionary search, DE and EDA.</p><p>The paper is organized as follows. Section 2 reviews the related work about forecast task with ANN. Section 3 explains how our system designs ANN with GA, DE and EDA to forecast time series. In Sect. 4, experimental setup and results are shown. And finally, conclusions and future works are described in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Several authors have addressed forecasting tasks using ANN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. This reveals the full consideration of ANN (as a data driven learning machine) into forecasting theory <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Often, the process of designing the correct ANN model for TSF is based on trial and error heuristics. If manual design of ANN is carried out, several topologies (i.e. different number of inputs nodes and different number of hidden neurons) with different learning rates are trained. For each of them, training and validation errors are obtained, and one with better generalization capability is selected to forecast the future values. A better alternative is to use an automatic ANN design, where EC plays an important role, in what is known as evolutionary ANN (EANN). Several works have proposed EANN, such as <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>.</p><p>Some of them use direct encoding schemes (DES) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, and the others use Indirect Encoding Scheme (IES) <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. For DES, the chromosome contains information about parameters of the topology, architecture, learning parameters, etc. of the artificial neural network. In IES the chromosome contains the necessary information, so that a constructive method gives rise to an artificial neural network topology (or architecture).</p><p>Abraham <ref type="bibr" target="#b20">[21]</ref> shows an automatic framework for optimization ANN in an adaptive way, and Yao et al. <ref type="bibr" target="#b21">[22]</ref> try to spell out the future trends of the field. About the use of EANN in time series forecasting, many works have been carried out <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, Patuwo and Hu present a ''state of the art'' of ANN into forecasting task, and in <ref type="bibr" target="#b12">[13]</ref>, Crone proposes a stepwise selection of ANN models for time series forecasting.</p><p>Chen et al. <ref type="bibr" target="#b25">[26]</ref> propose local linear wavelet neural network to forecast time series. Rivas et al. <ref type="bibr" target="#b26">[27]</ref> use evolving RBF neural networks for time series forecasting. Few studies have been done using ANN and DE to generate a hybrid system to forecast time series <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Since EDA is a more recent technique, its use for EANN in TSF is scarce and within our knowledge, has only appeared very recently. Therefore, the main motivation of this paper is to obtain ANN models through a fully automatic process, due to not all users who need to deal with forecasting are ANN experts. They are usually people from industry or economics, not computer scientists.</p><p>3 Evolutionary design of artificial neural networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time series and ANN</head><p>The problem of forecasting time series with ANN <ref type="bibr" target="#b6">[7]</ref> is considered as obtaining the relationship from the value at period ''t'' (in this system, the resulting ANN will have only one output neuron) and the values from previous elements of the time series (t -1, t -2, …, tk) to obtain a function as it is shown in (1):</p><formula xml:id="formula_0">a t ¼ f a tÀ1 ; a tÀ2 ; . . .; a tÀk ð Þ ð<label>1Þ</label></formula><p>In order to obtain a single ANN to forecast time series values, an initial step has to be done with the original values of the time series, i.e. normalizing the data. The original values of the time series are normalized into the range [0,1] (leading to the N t values). Once the ANN gives the resulting values, the inverse process is carried out, rescaling them back to the original scale. Only one neuron was chosen at the output layer (i.e. 1 to N ahead forecasting) because if it was allowed several nodes at the output layer (t, t ? 1, …, t ? n), the forecasting task would be done for several output future values in a row (1 ahead forecast). This forecasting method would lead to forecast value t from tk, …, t -2, t -1 but also to forecast values t ? 1, t ? 2, …, t ? n from tk, …, t -2, t -1, so in every forecasting step, important previous data would be missing for t ? 1, t ? 2, …, t ? n. Due to this, it is considered better to use 1 to N ahead forecast.</p><p>Therefore, the time series will be transformed into a pattern set depending on the k input nodes of a particular ANN, and each pattern will consist of the following:</p><p>• k inputs values that correspond to k normalized previous values of period t: N t-1 ,N t-2 , …, N t-k . • One output value: N t (the desired target).</p><p>This patterns set will be used to train and validate each ANN generated during the evolutionary execution. Therefore, patterns set will be split into two subsets, training and validation. The complete patterns set is ordered into the same way the time series is. The first x% (where x is a parameter) from the total patterns set will generate the train patterns subset, and the validation subset will be obtained from the rest of the total patterns set. The test subset will be the future (and unknown) time series values that the user wants to forecast. An example of this process using an ANN with 3 input nodes (k = 3) can be seen at Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ADANN</head><p>The problem of designing ANN could be seen as a search problem into the space of all possible ANN. Then, that search can be done by a GA <ref type="bibr" target="#b29">[30]</ref> using exploitation and exploration.</p><p>Therefore, there are three crucial issues: (1) about the solution's space, which information of the net should be included into the chromosome; (2) how this information is codified into the chromosome, i.e. encoding schema; and (3) how each individual is evaluated and translated into the fitness function.</p><p>In this approach, it has been chosen multilayer perceptron (MLP) as computational model due to its approximation capability and inside this group, Full Connected MLP with only a hidden layer and Backpropagation (BP) as learning algorithm, according to <ref type="bibr" target="#b30">[31]</ref>.</p><p>As it was mentioned in Sect. 1, the design of the ANN will be done by setting the parameters of the ANN. In the case of MLP with only one hidden layer and BP, these parameters are: number of inputs nodes, number of hidden neurons, number of output neurons (only one, it is set by the forecasting problem), which is the connection pattern (how the nodes are connected), and the whole set of connection weights (implemented by the seed used to initialize the connection weights as it will be explained below).</p><p>For our approach <ref type="bibr" target="#b6">[7]</ref> to design ANN to forecast time series, a DES for Full Connected MLP has been considered. For this DES, the information placed into the chromosome will be two decimal digits, i.e. two genes, to codify the number of inputs nodes (i); other two for the number of hidden nodes (h); two more for the learning factor (a); and the last ten genes for the initialization seed value of the connection weights (s) (seed in SNNS <ref type="bibr" target="#b31">[32]</ref> is ''long int'' type, that is why it has been used 10 genes (decimal digits) to encode ''s''), as it can be seen in Fig. <ref type="figure" target="#fig_1">2</ref>. This way, the values of ''i,'' ''h,'' ''a'' and ''s'' are obtained from the chromosome as it can be seen in ( <ref type="formula" target="#formula_1">2</ref>). Max_inputs and max_hidden are parameters of the system. NTS represents the number of elements of the time series.</p><formula xml:id="formula_1">i ¼ NTS Á max inputs Á d 1 Á 10 þ d 2 ð Þ =100 ð Þ h ¼ NTS Á max hidden Á d 3 Á 10 þ d 4 ð Þ =100 ð Þ a ¼ d 5 Á 10 þ d 6 ð Þ =100 s ¼ X 16 j¼7 d j Á 10 16Àj<label>ð2Þ</label></formula><p>The search process (GA) will consist of the following steps: A schema of the whole search process can be seen at Fig. <ref type="figure">3</ref>.</p><p>The fitness value for each individual is the minimum validation error during the learning process (ANN training), according to <ref type="bibr" target="#b40">[43]</ref>.</p><p>Regarding the use of MSE in the fitness function, the rationale is to reduce extreme errors that may highly affect multistep ahead forecasts. Also, preliminary experiments (using only training data and a few datasets) have shown that the MSE fitness leads to better forecasts when compared with mean absolute error (MAE). To choose the genetic algorithm parameters, Goldberg's GA <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr">[34]</ref> were taken into account, apart of the experience obtained during the preliminary experiments. The GA parameters were set to population size, 50; maximum number of generations, 100; percentage of the best individual that stay unchangeable to the next generation (percentage of elitism), 10%; crossover: parents are split in one point randomly selected, offspring are the mixed of each part from parents; mutation probability will be one divided by the length of the chromosome (1/lengthchrom = 1/16 = 0.07), and it will be carried out for each gene of the chromosome.</p><p>At the end of the search process, the best individual from the last generation is used to forecast the future unknown values (t, t ? 1, …, t ? n) one by one using the k previous known values (tk, …, t -2, t -1). k is the number of inputs of the ANN obtained from the GA execution. To forecast several consecutive values (t, t ? 1, …, t ? n) every time a new value (t) is forecasted, it will be included in order into the previous known values set of the time series and used to forecast the next one, t ? 1 (1 to N ahead forecast).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differential evolution algorithm</head><p>Differential evolution algorithm (DE) is a stochastic nonlinear optimization algorithm by Storn and Price <ref type="bibr" target="#b33">[35]</ref> and belongs to the class of evolution strategy optimizers. DE looks for the global minimum of a multidimensional, multimodal function trying to obtain a good probability. Fig. <ref type="figure">3</ref> ANN design by GA schema DE community has been growing since the mid 1990s, and today more researchers are working on and with DE <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">36]</ref>.</p><p>The main idea of DE is a scheme for generating trial parameter vectors. DE differs from other evolutionary algorithms (EA) in their mechanism of generating offspring. In GA, an individual plays the role of a parent to generate an offspring. Nevertheless, DE adds the weighted difference between two vectors, i.e. chromosomes of the population, to a third vector. So, no separate probability distribution has to be used, which makes the scheme completely self-organizing.</p><p>In DE a population of solution vectors is successively updated by addition, subtraction and component swapping, until the population converges to an optimum. No derivatives are used, very few parameters are set and it is a simple and apparently very reliable method. That means DE does not require for the optimization problem to be differentiable as is required by classic optimization methods. DE can therefore also be used on optimization problems that are not even continuous and are noisy, change over time, etc.</p><p>Some studies using DE have been done in weather time series forecasting field <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. In this document, it is presented an automatic method to design ANN using advantages of DE (i.e. hybrid system), to forecast time series.</p><p>There are many schemes of generating individuals in DE. In this document, it is presented the most popular scheme, the DE/rand/1/bin, which is used in this research. Here, we have the process for a DE/rand/1/bin:</p><p>1. P initial B Generate and evaluate an initial population of x solutions. 2. Repeat for i = 0, 1, 2, …, until a stopping criterion is met 2.1. A target vector (X i ) and a base vector (X r0 ) are chosen. The second one randomly, where i, r 0 e {1, …, x}. 2.2. Two random different population members (X r1 , X r2 ) are also chosen, where r 1 , r 2 e {1, …, x}. 2.3. Compute the difference vector from X r1 and X r2 (i.e. X r1 -X r2 ). 2.4. Multiply ''2.3'' by the mutation factor F (parameter of the system). 2.5. V i B Add ''2.4'' to the base vector (X r0 ) to obtain a mutant vector. 2.6. U i B Crossover between target vector (X i ) and V i . 2.7. Selection is carried out between X i and U i .</p><p>To apply DE to our system, it was necessary to replace the GA, which is responsible for carrying out the global search into the hybrid system, by DE as it can be seen in <ref type="bibr" target="#b41">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Estimation distribution algorithm</head><p>Estimation of distribution algorithms (EDA) <ref type="bibr" target="#b39">[42]</ref> is an outgrowth of GA. In GA, a population of individual solutions of a problem is kept as part of the search for a better solution. This population is typically represented as an array of objects. Depending on the specification of the GA, the objects might be bit strings, real numbers, etc. In EDA, this representation of the population is replaced by a probability distribution over the choices available at each position in the vector that represents each individual of the population.</p><p>For example, in the case the population is represented by bit vectors of length 4, the EDA for these populations would be a single vector of four probabilities (p1, p2, p3 and p4) where each p is the probability of that position being any of the possible values. Using this probability vector, it is possible to generate an arbitrary number of new candidate solutions.</p><p>In EC, new solutions are often generated by combining and modifying existing ones in a stochastic way. Probability distribution of new solutions over the space of all possible ones is usually not specified. In EDA, a probability distribution is used to approximate a population and new solutions are obtained by sampling this distribution. This has several advantages; one of them is to avoid premature convergence and being a more compact representation.</p><p>Maybe because estimation distribution algorithm is a recent technique (came into use just a few years ago), it has been carried out few hybrid studies (i.e. ANN ? EDA) applied only to classification domains <ref type="bibr" target="#b35">[37]</ref>. In this paper, it is proposed a new hybrid method using advantages of EDA to design ANN to forecast time series. Besides, it is a totally automatic method.</p><p>There are different kinds of EDA, but for our approach, it has been chosen UMDA, with no dependencies between variables, according to <ref type="bibr" target="#b35">[37]</ref>. Here, we have the process for a general EDA: To apply EDA to our approach, it was necessary to replace the GA (explained in Sect. 3.2), which is responsible for carrying out the global search into the hybrid system, by EDA. It can be observed that the principal differences between GA and EDA consist of steps ''2.2'' and ''2.3,'' where instead of carrying out crossover and mutation, it is estimated the empirical probability of each individual and sampled the solutions. A more detailed explanation of the system using EDA is given in <ref type="bibr" target="#b42">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Time Series</head><p>The research presented in this paper was initially motivated by NN3 (2007) and NN5 (2008) time series competition <ref type="bibr" target="#b36">[38]</ref>. To compare the proposed TSF methods, we selected 5 benchmark time series. Four of them are series from the well-known Hyndman's Time Series Data Library (TSDL) <ref type="bibr" target="#b43">[46]</ref>.</p><p>These time series are named Passengers, Temperature, Dow-Jones and Quebec. Passengers time series represents the number of passengers of an international airline in thousands, measured monthly from January 1949 till December 1960. Temperature time series shows the mean monthly of air temperature measured at Nottingham Castle from 1920 till 1939; in this case. Dow-Jones is about the monthly closings of the Dow-Jones industrial index from August 1968 till August 1981. Quebec represents the number of births daily measured in Quebec from <ref type="bibr">January 1, 1977</ref><ref type="bibr">, till December 31, 1978</ref>. The last one called Mackey-Glass is based on the Mackey-Glass differential equation and is widely regarded as a benchmark for comparing the generalization ability of different methods. This series is a chaotic time series generated from a time-delay ordinary differential equation. This time series has been chosen in order to extend the experimental datasets by a different kind of a benchmark, i.e. by a time series that is not based on real-world data and that contains neither a trend nor a noise component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>The global performance of a forecasting model is evaluated by an accuracy measure, such as root mean squared error (RMSE) and symmetric mean absolute percentage error (SMAPE):</p><formula xml:id="formula_2">RMSE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 h X Tþh t¼Tþ1 e 2 t v u u t<label>ð3Þ</label></formula><formula xml:id="formula_3">SMAPE ¼ 1 h X Tþh t¼Tþ1 je t j ðjy t j þ jF t jÞ=2 Â 100%<label>ð4Þ</label></formula><p>Where e t = y t -F t for t = T ? 1, …, T ? h, ''T'' is the number of previous known elements, ''h'' denotes the forecasting horizon, y t represents the real value and F t the forecasted one.</p><p>In all these measures, lower values indicate better forecasts. Historically, the RMSE is a very popular metric within TSF.</p><p>SMAPE has the advantage of being scale independent, thus can be more easily used to compare methods across different series. Although the SMAPE was originally proposed in <ref type="bibr" target="#b37">[39]</ref>, (4) adopts the variant used in <ref type="bibr" target="#b38">[40]</ref> since it does not lead to negative values (ranging from 0 to 200%). SMAPE is also used in NN3, NN5 and NNGC1 <ref type="bibr" target="#b36">[38]</ref> forecasting competitions as evaluation error. For the forecasting comparison, we opted to compute SMAPE and RMSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evolutionary method results</head><p>Each evolutionary approach (i.e. GA, DE and EDA hybrid systems) was executed five times, with a stopping criterion of 100 and 200 generations, for all time series. We report the mean results of these five executions. To evaluate the error for each method, forecasted values were compared with real values, under the RMSE and SMAPE criteria <ref type="bibr">(3 and 4)</ref>.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, the results obtained for all time series and the average in generation number 100 are shown. On the other hand, it is shown the results obtained for all time series and the average in generation number 200 in Table <ref type="table" target="#tab_2">2</ref>.</p><p>As it can be observed in Table <ref type="table" target="#tab_1">1</ref>, applying DE instead of GA, we achieve better forecasting (SMAPE/RMSE) in two of the time series. On the problems of Mackey-Glass and Temperature, better SMAPE results are obtained with DE, and in Mackey-Glass case, the improvement is about 2.6%. Besides, DE and EDA obtain better average results than GA.</p><p>If the experiments are run over 200 generations (Table <ref type="table" target="#tab_2">2</ref>), it can be seen an important improvement in almost all the time series, where DE obtains a better forecast than GA in four of the five time series. Only in Quebec time series, GA is still better than DE although difference is not significant. A special consideration has to be taken on Mackey-Glass time series where the SMAPE error result is 3.74%, being the values forecasted by our approach almost identical to the real time series values.</p><p>Better results obtained by DE, compared with GA, after having run experiments 200 generations can be explained because in DE, more variation in population (because solutions still do not converge) leads to more varied search over solution space. That is why it can take more time to DE to arrive to a better solution.</p><p>As it can be seen in Table <ref type="table" target="#tab_1">1</ref>, EDA outperforms GA in two of the time series when the experiment has been run only 100 generations. Mackey-Glass and Temperature obtain better SMAPE results with EDA, and in Mackey-Glass case, the improvement is about 2.4%. Besides, EDA does not win in any time series, but in the average, it is the best method.</p><p>But if the experiments are run over 200 generations, it can be seen in Table <ref type="table" target="#tab_2">2</ref> an important improvement in almost all the time series, where EDA obtain a better forecast than GA in four of the five time series. Only in Passengers time series, GA is still better than EDA although both results are  To have a better idea how each method (i.e. GA, DE and EDA) has evolved during the 200 generations, it is shown in Fig. <ref type="figure" target="#fig_2">4</ref> the evolution of the fitness value for Mackey-Glass time series.</p><p>As it can be observed, a lower value means a better fitness. While GA stops decreasing its fitness value between generation 60 and 70, DE and EDA keeps on decreasing their values after generation 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ADANN versus Forecast pro Ò</head><p>As a baseline comparison, we choose the popular forecasting tool Forecast Pro Ò (FP) <ref type="bibr">[41]</ref>. In particular, we fed the tool with the in-samples of five datasets (i.e. Passengers, Temperature, Dow-Jones, Quebec and Mackey-Glass) and executed the full automatic feature of the tool to obtain the forecasts. The rationale is to use a popular benchmark that can easily be compared and that does not require expert model selection capabilities from the user.</p><p>Table <ref type="table" target="#tab_3">3</ref> shows the results obtained for each time series applying these two different methods, FP and ADANN using the EDA version with 200 generations. Error obtained by each method will be measured in SMAPE and RMSE as it has been done before.</p><p>Different information can be acquired from Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref>. First of all, Mackey-Glass forecast done by FP is not a good one, while ADANN obtains a SMAPE error of 1.8%, really close to the real values. Although FP outperforms ADANN in three of the five time series (Temperature, Dow-Jones and Quebec), results from both systems are really close. Besides, ADANN has a better average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future works</head><p>In this paper, we propose three methods to design ANN to forecast time series.</p><p>As they are totally automatic methods, any previous knowledge from the user is not required. The user does not have to be an expert in time series. The user just have to give the time series he wants to forecast and the number of future elements he wants to be forecasted to the system, and this method will give these forecasted values as result to the user.</p><p>The results of the experiments disclose that using DE and EDA, instead of GA, obtain different results depending on the number of generations they are executed. With only 100 generations, DE and EDA results do not improve too much compared to GA although both averages are better than GA. But if 200 generations are reached, it can be  observed a significant improvement, sometimes with a of 4.3% in the results, as it happens in Mackey-Glass time series using DE and a gain of 6.2% using EDA.</p><p>As it was commented before, obtaining better results by DE and EDA than with GA after having run the experiments 200 generations could be explained because in DE more variation in population (because solution has not converged yet) leads to more varied search over solution space. That is why it can take more time to DE to arrive to a better solution. When GA gets stuck in a local minimum, EDA does not and it keeps on looking for a better result if it is given more time, in this case, more generations to look for.</p><p>In general, DE and EDA need more generations to improve results better than GA, but the improvement obtained after 200 generations, make these two methods better options to use than GA in the global search for a hybrid system to design ANN to forecast time series.</p><p>ADANN, using its EDA and 200 generations version, outperforms FP forecasting tool in two of the five cases and obtains a better average.</p><p>Future works with additional time series, with similar characteristics to Quebec and Mackey-Glass, will allow us to obtain more accurate conclusions about the effect of using DE and EDA instead of GA. On the other hand, it would be really interesting to improve DE system with some ideas such as instead of using a random X r0 , use the best one (i.e. the one with the best fitness value); or instead of using single difference (i.e. X r1 -X r2 ), use more vectors for more variations, for example (X r1 -X r2 ? X r3 -X r4 ). Or to improve EDA system with some ideas such as using EDA with dependencies between its variables such as MIMIC (i.e. variables with order one dependencies) or even ''tree'' EDA, with no restriction on the numbers of dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 3 .</head><label>13</label><figDesc>Fig. 1 Process to obtain train and validation pattern subsets</figDesc><graphic coords="3,204.13,514.61,340.32,199.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Encoding practice used (decimal presentation) where d j is the n decimal digit (0-9)</figDesc><graphic coords="4,204.13,59.24,340.32,72.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 GA, DE and EDA fitness value during 200 generations for Mackey-Glass time series</figDesc><graphic coords="8,53.87,541.44,232.80,144.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>SMAPE and RMSE Passengers, Temperature, Dow-</cell><cell>100 Generations</cell><cell>GA</cell><cell>DE</cell><cell>EDA</cell></row><row><cell>Jones, Quebec and Mackey-Glass with GA, DE and DEA for 100 generations (best values in bold)</cell><cell>Passengers SMAPE (%) RMSE</cell><cell>3.18 0.24 3 10 21</cell><cell>3.36 0.25 9 10 -1</cell><cell>3.57 0.26 9 10 -1</cell></row><row><cell></cell><cell>Temperature</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>4.31</cell><cell>3.91</cell><cell>3.98</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.59 9 10 -1</cell><cell>0.54 3 10 21</cell><cell>0.55 9 10 -1</cell></row><row><cell></cell><cell>Dow-Jones</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>6.66</cell><cell>8.19</cell><cell>6.81</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.14 3 10 0</cell><cell>0.17 9 10 0</cell><cell>0.14 3 10 0</cell></row><row><cell></cell><cell>Quebec</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>12.64</cell><cell>13.74</cell><cell>13.27</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.15 3 10 0</cell><cell>0.16 9 10 0</cell><cell>0.15 3 10 0</cell></row><row><cell></cell><cell>Mackey-Glass</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>8.67</cell><cell>5.99</cell><cell>6.27</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.60 9 10 -1</cell><cell>0.41 3 10 21</cell><cell>0.43 9 10 -1</cell></row><row><cell></cell><cell>Average</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>7.09</cell><cell>7.04</cell><cell>6.78</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.86 9 10 -1</cell><cell>0.90 9 10 -1</cell><cell>0.82 3 10 21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Glass time series where the SMAPE error result is 1.8%, being the values forecasted by our method almost identical to the real time series values. On the other hand, EDA average with 200 generations is almost 1.7% better than GA average. Good results obtained by EDA compared with GA after having run experiments 200 generations could be explained because EDA, unlike GA, avoid premature convergence. So, when GA gets stock in a local minimum value, EDA does not and it keeps on looking for a better result if it has more time, in this case, more generations to look for. In general, DE and EDA need more generations to improve results, but the improvements obtained after 200 generations, make these two methods a good option to be used as a global search in a hybrid system to design ANN to forecast time series.</figDesc><table><row><cell>SMAPE and RMSE Passengers, Temperature, Dow-</cell><cell>200 Generations</cell><cell>GA</cell><cell>DE</cell><cell>EDA</cell></row><row><cell>Jones, Quebec and Mackey-Glass with GA, DE and DEA for 200 generations (best values in bold)</cell><cell>Passengers SMAPE (%) RMSE</cell><cell>3.15 0.23 3 10 21</cell><cell>3.12 0.23 3 10 21</cell><cell>3.22 0.24 9 10 -1</cell></row><row><cell></cell><cell>Temperature</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>4.24</cell><cell>3.91</cell><cell>3.94</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.58 9 10 -1</cell><cell>0.54 3 10 21</cell><cell>0.56 9 10 -1</cell></row><row><cell></cell><cell>Dow-Jones</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>6.31</cell><cell>5.81</cell><cell>5.26</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.13 9 10 0</cell><cell>0.12 9 10 0</cell><cell>0.11 3 10 0</cell></row><row><cell></cell><cell>Quebec</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>12.12</cell><cell>13.68</cell><cell>10.96</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.14 9 10 0</cell><cell>0.16 9 10 0</cell><cell>0.13 3 10 0</cell></row><row><cell></cell><cell>Mackey-Glass</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>8.04</cell><cell>3.74</cell><cell>1.81</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.55 9 10 -1</cell><cell>0.25 9 10 -1</cell><cell>0.12 3 10 21</cell></row><row><cell></cell><cell>Average</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SMAPE (%)</cell><cell>6.77</cell><cell>6.05</cell><cell>5.04</cell></row><row><cell></cell><cell>RMSE</cell><cell>0.81 9 10 -1</cell><cell>0.76 9 10 -1</cell><cell>0.66 3 10 21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>SMAPE and RMSE Passengers, Temperature, Dow-Jones, Quebec and Mackey-Glass with ADANN and Forecast Pro Ò</figDesc><table><row><cell></cell><cell>ADANN</cell><cell>Forecast Pro Ò</cell></row><row><cell>Passengers</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>3.22</cell><cell>4.51</cell></row><row><cell>RMSE</cell><cell>0.24 3 10 21</cell><cell>0.27 9 10 -1</cell></row><row><cell>Temperature</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>3.94</cell><cell>3.42</cell></row><row><cell>RMSE</cell><cell>0.56 9 10 -1</cell><cell>0.49 3 10 21</cell></row><row><cell>Dow-Jones</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>5.26</cell><cell>4.78</cell></row><row><cell>RMSE</cell><cell>0.11 3 10 0</cell><cell>0.11 3 10 0</cell></row><row><cell>Quebec</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>10.96</cell><cell>10.37</cell></row><row><cell>RMSE</cell><cell>0.13 9 10 0</cell><cell>0.12 3 10 0</cell></row><row><cell>Mackey-Glass</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>1.81</cell><cell>26.2</cell></row><row><cell>RMSE</cell><cell>0.12 3 10 21</cell><cell>0.19 9 10 0</cell></row><row><cell>Average</cell><cell></cell><cell></cell></row><row><cell>SMAPE (%)</cell><cell>5.04</cell><cell>9.85</cell></row><row><cell>RMSE</cell><cell>0.66 3 10 21</cell><cell>0.99 9 10 -1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Neural Comput &amp; Applic (2013) 22:11-20</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The research reported here has been supported by the Spanish Ministry of Science and Innovation under project TRA2007-67374-C02-02. The authors want to thank specially Ramon Sagarna for introducing the subject of EDA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Forecasting methods and applications, 2nd edn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wheelwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Wiley, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The application of antigenic search techniques to time series forecasting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preparation of papers for international journal of automation and computing 13</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for time lag selection to forecast seasonal time series using neural networks and support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2010)</title>
		<meeting>the IEEE International Joint Conference on Neural Networks (IJCNN 2010)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature selection for time series prediction a combined filter and wrapper approach for neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Crone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kourentzes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1923" to="1936" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vavr ˇı ´c ˇkova ´L (2011) A linguistic approach to time series modeling with help of the F-transform</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dvor</forename><surname>ˇa ´k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavliska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.fss.2011.02.017</idno>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dens: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adann: automatic design of artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 GECCO conference companion on Genetic and evolutionary computation, GECCO&apos;08</title>
		<meeting>the 2008 GECCO conference companion on Genetic and evolutionary computation, GECCO&apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1863" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fusion model of HMM, ANN and GA for stock market forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="171" to="180" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An ensemble of neural networks for weather forecasting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Maqsood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A connectionist system for medium term horizon time series prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fessant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc int workshop application neural networks to telecoms</title>
		<meeting>int workshop application neural networks to telecoms</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural networks: a comprehensive foundation, 2nd edn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new intelligent system methodology for time series forecasting with artificial neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Adeodato</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-008-9085-x</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Process Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="129" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stepwise selection of artificial neural networks models for time series prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Crone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>United Kingdom</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Management Science Lancaster University Management School Lancaster</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks: the state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Patuwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Forecast</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolving time series forecasting ARMA models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:HEUR.0000034714.09838.1e</idno>
	</analytic>
	<monogr>
		<title level="j">J Heuristics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic Node Creation in Backpropagation Networks ICS</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Report</title>
		<imprint>
			<biblScope unit="volume">8901</biblScope>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
		<respStmt>
			<orgName>The Institute for Cognitive Science, University of California, San Diego (Saiensu-sh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolving neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Porto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Cybern</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="487" to="493" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Genetic synthesis of Boolean neural networks with a cell rewriting developmental process</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gruau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural Networks</title>
		<meeting>COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural Networks<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new evolutionary system for evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="694" to="713" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing Journal</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="567" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Artificial Neural Networks in Real-Life Applications. chapter III</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
		<editor>Rubun ˜al J, Dorado J</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Idea Group Publishing</publisher>
			<biblScope unit="page" from="47" to="70" />
			<pubPlace>Hershey</pubPlace>
		</imprint>
	</monogr>
	<note>Time series forecasting by evolutionary neural networks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolving the neural network model for forecasting air pollution time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Niska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hiltunena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karppinenb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruuskanena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolehmainena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng Appl Artif Intell</title>
		<idno type="ISSN">0952-1976</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="167" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evolutionary artificial neural networks for hydrological systems forecasting</title>
		<author>
			<persName><forename type="first">Y-H</forename><surname>Chena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F-J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Hydrol</title>
		<idno type="ISSN">0022-1694</idno>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Time-series prediction using a local linear wavelet neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="449" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evolving RBF neural networks for time-series forecasting with EvRBF</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Merelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Castellano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Sci</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="207" to="220" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hybrid differential evolutionary system for financial time series forecasting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Arau ´jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Vasconselos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ferreria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>IEEE congress on evolutionary computation</publisher>
			<biblScope unit="page" from="4329" to="4336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural networks training based on differential evolution algorithm compared with other architectures for weather forecasting 34</title>
		<author>
			<persName><forename type="first">Abdul-Kader</forename><surname>Hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Sci Netw Secur</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Evolutionary Computation: Toward a New Philosophy of Machine Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Wiley-IEEE Press</publisher>
			<pubPlace>New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approximation by superposition of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Control Signals Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Dr</forename><surname>Prof</surname></persName>
		</author>
		<author>
			<persName><surname>Andreas Zell</surname></persName>
		</author>
		<ptr target="http://www-ra.informatik.uni-tuebingen.de/SNNS/" />
		<imprint/>
		<respStmt>
			<orgName>WSI Computer Science Department, Computer Arquitecture, Software, Artif Neural Netw</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization, and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley Professional</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Global Opt</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Choosing leaders for multiobjective PSO algorithms using differential evolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wickramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on simulated evolution and learning (SEAL&apos;08)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the seventh international conference on simulated evolution and learning (SEAL&apos;08)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5361</biblScope>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adjusting weights in artificial neural networks using evolutionary algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sagarna</surname></persName>
		</author>
		<author>
			<persName><surname>Larran ˜aga P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Estimation of distribution algorithms. A new tool for evolutionary computation</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Larran ˜aga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="http://www.neural-forecastingcompetition.com" />
		<title level="m">Time Series Forecasting Competition for Neural Networks and Computational Intelligence</title>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Long-range forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A new Bayesian formulation for Holt&apos;s exponential smoothing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Andrawis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="218" to="234" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Estimation of Distribution Algorithms. A new tool for evolutionary computation (genetic algorithms and evolutionary computation)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Larran ˜aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Using genetic search to exploit the emergent behavior of neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Eshelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="244" to="248" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Time series forecasting by evolving artificial neural networks using genetic algorithms and differential evolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 WCCI conference, IJCNN-WCCI&apos;10</title>
		<meeting>the 2010 WCCI conference, IJCNN-WCCI&apos;10<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Time series forecasting by evolving artificial neural networks using genetic algorithms and estimation of distribution algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 WCCI conference, IJCNN-WCCI&apos;10</title>
		<meeting>the 2010 WCCI conference, IJCNN-WCCI&apos;10<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<ptr target="http://robjhyndman.com/TSDL/" />
		<title level="m">Hyndman RJ Time series data library</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
