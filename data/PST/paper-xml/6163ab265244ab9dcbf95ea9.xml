<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ranking Cost: Building An Efficient and Scalable Circuit Routing Planner with Evolution-Based Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin158@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
							<email>haojianye@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>tingchen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ranking Cost: Building An Efficient and Scalable Circuit Routing Planner with Evolution-Based Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Circuit routing has been a historically challenging problem in designing electronic systems such as very large-scale integration (VLSI) and printed circuit boards (PCBs). The main challenge is that connecting a large number of electronic components under specific design rules involves a very large search space. Early solutions are typically designed with hard-coded heuristics, which suffer from problems of non-optimal solutions and lack of flexibility for new design needs. Although a few learning-based methods have been proposed recently, they are typically cumbersome and hard to extend to large-scale applications. In this work, we propose a new algorithm for circuit routing, named as Ranking Cost, which innovatively combines search-based methods (i.e., A* algorithm) and learning-based methods (i.e., Evolution Strategies) to form an efficient and trainable router. In our method, we introduce a new set of variables called cost maps, which can help the A* router to find out proper paths to achieve the global objective. We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method. Our algorithm is trained in an end-to-end manner and does not use any artificial data or human demonstration. In the experiments, we compare with the sequential A* algorithm and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates and better scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As described in Moore's Law <ref type="bibr" target="#b5">[Schaller, 1997]</ref>, the number of transistors in a dense integrated circuit (IC) increases exponentially over time and the complexity of chips becomes higher and higher. Such high complexity makes the IC design a time-consuming and error-prone work. Thus more capable automatic design systems, such as electronic design automation (EDA) tools, are needed to improve the performance. In the flow of IC designs, we need to find proper paths to place wires which connect electronic components on ICs, and these wires need to achieve expected connectivity under certain constraints. One of the most important constraints is that wires on the same layout should not intersect. Besides, to reduce the signal propagation delay, the wire-length should be minimized. This is a critical and challenging stage in the IC design flow <ref type="bibr">[Hu and Sapatnekar, 2001]</ref>, known as circuit routing, which has been studied by lots of researchers <ref type="bibr" target="#b3">[Kramer, 1984;</ref><ref type="bibr">He and Bao, 2020]</ref>.</p><p>Circuit routing involves a large number of nets (a net is a set of vertices or pins with the same electrical property) to be routed, which is computationally expensive and makes manual design extremely time-consuming <ref type="bibr" target="#b1">[Coombs and Holden, 2001]</ref>. Even under the simplest setting, where only two pairs of pins need to be routed, it is an NP-complete problem <ref type="bibr" target="#b3">[Kramer, 1984]</ref>. Although lots of circuit routing algorithms have been proposed <ref type="bibr">[Zhang, 2016]</ref>, there remain two major challenges: (1) Early solutions <ref type="bibr">[Hu and Sapatnekar, 2001]</ref> are typically designed with hard-coded heuristics, which suffer from problems of non-optimal solutions <ref type="bibr">[Zhang, 2016]</ref> and lack of flexibility over new design needs. Therefore, a more powerful routing method that does not depend on domain knowledge is highly desired.</p><p>(2) Although a few learning-based methods have been proposed <ref type="bibr" target="#b3">[Liao et al., 2020;</ref><ref type="bibr">He and Bao, 2020]</ref> recently, their methods are hard to extend to large-scale applications. In real settings, there are lots of components and nets on a single chip, which shows greater demand for the scalability of routing algorithms.</p><p>To relieve these problems, we propose a novel algorithm, denoted as Ranking Cost (RC), for circuit routing. We innovatively combine search-based methods (i.e., A* algorithm, the most commonly used algorithm in circuit routing tasks) and learning-based methods (i.e., Evolution Strategies <ref type="bibr" target="#b4">[Salimans et al., 2017]</ref>, a powerful black-box optimization method) to form a trainable router with proper parametrization. Our method is flexible for integrating new constraints and rules so long as they can be merged into a global objective function. Moreover, our method is an onestage algorithm, which optimizes the global objective function directly. In our method, we introduce a new set of variables called cost maps, which can help the A* routers to find out proper paths to achieve the global objective. We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method. In the experiments, we compare our method with the commonly used A* method and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates. Our ablation study also shows that the trained cost maps can capture the global information and lead to a reasonable routing solution.</p><p>We also show that our method is scalable to larger maps. However, recent learning-based methods <ref type="bibr" target="#b3">[Liao et al., 2020;</ref><ref type="bibr">He and Bao, 2020]</ref> only conduct experiments on small maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We summarize the related work on circuit routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heuristic Algorithms for Circuit Rouging</head><p>The routing problem can be heuristically separated into two stages, the first being the global routing step <ref type="bibr" target="#b4">[Mo et al., 2001;</ref><ref type="bibr" target="#b1">Cho et al., 2007]</ref>, followed by detailed routing <ref type="bibr" target="#b2">[Ho et al., 1991;</ref><ref type="bibr">Mandal et al., 2020;</ref><ref type="bibr" target="#b0">Chen and Chang, 2009</ref>]. On the one hand, there are multiple heuristic-based approaches for global routing including regionwise routing <ref type="bibr">[Hu and Sapatnekar, 2001]</ref>, force-directed routing <ref type="bibr" target="#b4">[Mo et al., 2001]</ref>, and rip-up and reroute <ref type="bibr" target="#b1">[Cho et al., 2007]</ref>. Besides, <ref type="bibr" target="#b6">[Shi et al., 2016]</ref> proposes a method to improve the distribution of congestion in global routing, and <ref type="bibr" target="#b5">[Shi and Davoodi, 2017]</ref> propose a framework to analyze local congestion for global routing. On the other hand, the most commonly used detailed routing algorithms are channel routing and its variants <ref type="bibr" target="#b2">[Ho et al., 1991;</ref><ref type="bibr">Mandal et al., 2020]</ref>, which decompose the routing region into routing channels and generate wires in these channels <ref type="bibr" target="#b0">[Chen and Chang, 2009]</ref>. One main issue of the two-stage methods is that these two stages do not always co-ordinate well <ref type="bibr" target="#b5">[Shi and Davoodi, 2017]</ref>, which results in enormous difficulty in joint optimization. Instead, our method is a one-stage algorithm and new design constraints can be simply involved in the objective function without changing the algorithm itself.</p><p>A more straightforward strategy for circuit routing is to select a specific order and then route nets sequentially, e.g., sequential A* algorithm and Lees algorithm <ref type="bibr" target="#b2">[Huang et al., 2014;</ref><ref type="bibr">Malavasi and Sangiovanni-Vincentelli, 1993]</ref>. The major advantage of this type of approaches is that the congestion information for previously routed nets can be taken into consideration while routing the current one. However, the drawback of these sequential approaches is that the quality of the solution is very sensitive to the orders <ref type="bibr">[Zhang, 2016]</ref>. Moreover, earlier routed paths only focus on finding their own best solutions and are impossible to take into account the situation of subsequent paths. Such greedy strategies may make a solvable circuit routing problem insolvable. Figure <ref type="figure" target="#fig_0">1</ref> shows an example that the sequential A* algorithm will fail to handle. In this example, there are two pairs of points to be routed, i.e., we should connect start vertices S i and end vertices E i , i ∈ {1, 2}, respectively. If we connect the pair (S 1 , E 1 ) first in its shortest path, it will make (S 2 , E 2 ) disconnected as shown in Figure <ref type="figure" target="#fig_0">1(d)</ref>, and vice versa (Figure <ref type="figure" target="#fig_0">1(e)</ref>). But this case can be solved easily by our algorithm as shown in Figure <ref type="figure" target="#fig_0">1</ref>(f), which means that our algorithm can take into account the global information when generating each path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Notations</head><p>In this section, we formalize the circuit routing problem first and then give a brief introduction to the OpenAI-ES algorithm <ref type="bibr" target="#b4">[Salimans et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Circuit Routing</head><p>Circuit routing is a path search problem, where the goal is to find non-intersecting paths that connect an arbitrary number of pairs of vertices. It can be formalized as a grid graph G = (V, E), where each vertex v i ∈ V represents an intersection on the grid, and each edge e ij ∈ E represents the path between v i and its 1-hop neighbor v j ∈ V . And the non-routable obstructive vertices form an obstacle set, denoted as O ⊂ V . In circuit routing, a net N = {v n1 , v n2 , ...} ⊂ V is a set of vertices that need to be connected and a multi-vertex net can be decomposed into multiple two-vertex nets via a minimum spanning tree (MST) or a rectilinear Steiner tree (RST) <ref type="bibr" target="#b1">[de Vincente et al., 1998;</ref><ref type="bibr">Hu and Sapatnekar, 2001]</ref>. Following [He and Bao, 2020], we simplify this problem by letting each net only contain two vertices, i.e, a two-vertex net is defined as N = {v s , v e } with a start vertex v s and an end vertex v e . The net should be connected by a path</p><formula xml:id="formula_0">P = [v 1 , v 2 , ..., v n ] with v s = v 1 , v e = v n</formula><p>and P ∩O = ∅. We use |P | to represent the length of the path P . Given a set N of nets {N i }, we need to find a set P of paths {P i } that connect these nets. A reasonable routing plan for a given set of nets is that all the paths do not share any vertex. In some cases, the routing problem does not have a solution because such a non-intersecting set P does not exist. In circuit routing, the commonly used objective is minimizing the total length of paths. Given a solution P, the total length of paths is defined as L = P ∈P |P |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evolution Strategies</head><p>To optimize the parameters in our algorithm, we need a powerful black-box optimization method. Actually, evolution Strategies (ES) is a class of black-box optimization methods <ref type="bibr">[Rechenberg, 1994;</ref><ref type="bibr">Hansen and Ostermeier, 2001]</ref>. Recently, an evolution strategy variant, referred to as the OpenAI-ES <ref type="bibr" target="#b4">[Salimans et al., 2017]</ref>, has attracted attention because it could rival the performance of modern deep reinforcement learning methods on multiple control tasks. In the OpenAI-ES, there is a reward function, denoted as F (ψ), where ψ is a solution vector, sampled from a probability distribution function ψ ∼ p θ (ψ). The goal is to maximize the expected value J(θ):</p><formula xml:id="formula_1">max θ J(θ) = E ψ∼p θ [F (ψ)].</formula><p>(1)</p><p>We can use the gradient ascent to optimize θ:</p><formula xml:id="formula_2">θ ← θ + α∇ θ J(θ), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where α is the learning rate. In the OpenAI-ES, a score function estimator is used to calculate the gradient, which is similar to REINFORCE <ref type="bibr" target="#b7">[Williams, 1992]</ref>:</p><formula xml:id="formula_4">∇ θ J(θ) = ∇ θ E ψ∼p θ [F (ψ)] = E ψ∼p θ [F (ψ)∇ θ log p θ (ψ)].<label>(3)</label></formula><p>Usually, p θ is an isotropic multivariate Gaussian with mean θ and fixed covariance σ 2 I. And the expected value can be rewritten as</p><formula xml:id="formula_5">E ψ∼p θ [F (ψ)] = E ϵ∼N (0,I) [F (θ +σϵ)].</formula><p>Thus the gradient estimator changes to:</p><formula xml:id="formula_6">∇ θ J(θ) = ∇ θ E ϵ∼N (0,I) [F (θ + σϵ)] = 1 σ E ϵ∼N (0,I) [F (θ + σϵ)ϵ],<label>(4)</label></formula><p>where ϵ is sampled from a standard normal distribution. Once we form an objective function J(θ), the gradient ∇ θ J(θ) can be approximated via Eq. ( <ref type="formula" target="#formula_6">4</ref>) and parameters θ can be updated via Eq. ( <ref type="formula" target="#formula_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we first introduce the Ranking Learning algorithm and then the Cost-Map Learning algorithm. At last, we give out our final algorithm (Ranking Cost) and its training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequential A* with Ranking Learning</head><p>The A* algorithm can be used to find the shortest path in a two-vertex net N = {v s , v e }, which is considered more efficient than breadth-first search. A* finds a path between a start vertex v s and an end vertex v e , with a priority function defined as:</p><formula xml:id="formula_7">s(v) = g(v) + h(v),<label>(5)</label></formula><p>where v is a vertex in the graph, g(v) represents the total length from v s to the current vertex v, and h(x) represents the future heuristic cost from v to the end vertex v e . The h(x) is often set to the Euclidean distance or Manhattan distance. In a circuit routing problem, there are k nets to be routed.</p><p>Given a specific ranking order R, we can apply the A* algorithm to each net sequentially. For example, if there are 3 nets {N 1 , N 2 , N 3 } to be routed and a ranking order R = (2, 1, 3) is given. We can apply A* to these 3 nets following the order N 2 → N 1 → N 3 . After the routing procedure, we can get a total path length from this ranking order, denoted as L(R). Different ranking orders will result in different total lengths. We can go through all the ranking orders and get their corresponding total lengths. The ranking order with minimum total length can be used as the final solution. However, this is unacceptable in time complexity because a routing problem with k nets has O(k!) different ranking orders.</p><p>To relieve the problem of combinatorial explosion of ranking orders, we propose a method to learn the ranking order. We define a k-dimension ranking parameter θ r = (β 1 , β 2 , ..., β k ). And the ranking order is determined completely by the ranking parameter. More concretely, the order of values in θ r is exactly the routing order, and the net with the largest value will be routed first and the rest may be deduced by analogy. For example, given a ranking parameter θ r = (0.5, 0.2, 0.4), we have β 1 &gt; β 3 &gt; β 2 and the corresponding ranking order will be R θr = (1, 3, 2). And the ranking parameter will determine the final routing result and also the total path length. We define a reward function over the ranking parameter:</p><formula xml:id="formula_8">F (θ r ) = −L(R θr ),<label>(6)</label></formula><p>where L(R θr ) is the total length when taking the order R θr , and less length leads to larger reward. As described in Section 3.2, we define the expected value over F (•) as:</p><formula xml:id="formula_9">J(θ r ) = E ψ∼p θr F (ψ) = E ϵ∼N (0,I) F (θ r + σϵ).<label>(7)</label></formula><p>The gradient ∇ θr J(θ r ) can be estimated via Eq. ( <ref type="formula" target="#formula_6">4</ref>), and θ r can be updated via Eq. (2). At every training step, we sample a fixed number of noise vectors from Gaussian, add them to the ranking parameter and then evaluate all the new ranking parameters. The reward function F (•) will return a group of scalar rewards and the ranking parameter will be updated with these rewards. In this way, we can train a ranking parameter θ r to approach the minimal total length L.</p><p>As mentioned in Section 2.1, a sequential routing algorithm may make a solvable circuit routing problem insolvable. And even a method with a learned ranking order will still suffer from this problem. In the following section, we will introduce how our method could overcome this problem by coupling the A* algorithm with cost maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Circuit Routing with Cost Maps</head><p>As introduced in the previous section, g(v) in Eq. ( <ref type="formula" target="#formula_7">5</ref>) represents the total length from start vertex v s to current vertex v, thus the A* algorithm can only search for the shortest path of the current net and it cannot take into account the information of following paths. Our goal is to add global information to the A* routers so that these routers can cooperate to achieve their common objective. First, for each net N i , i ∈ {1, ..., k}, we define its cost map as C i = (c 1 , c 2 , ..., c m ) ∈ R m , where m = |V | is the number of vertices in graph G. Therefore, there is a total of k cost maps, and all the cost maps can be learned with our algorithm. We apply a cost-map function C(i, v) = c, c ∈ C i to simplify the notation. In this way, we can reformulate the A* algorithm by adding the cost maps:</p><formula xml:id="formula_10">s(v) = g(v) + k i=1 C(i, v) + h(v),<label>(8)</label></formula><p>where g(v) and h(v) are with the same definitions as in Eq. ( <ref type="formula" target="#formula_7">5</ref>). The cost-map enhanced A* algorithm reveals two key points: (1) When removing g(v) and h(v) from Eq. ( <ref type="formula" target="#formula_10">8</ref>), it will degenerate into a pure learning-based algorithm and the path will be determined completely by the cost function.</p><p>(2) When removing the cost functions or set them to zero (i.e., C(i, v) = 0), it is a pure search-based algorithm. Hence, it is a method which combines the searched-based algorithm and the learning-based algorithm and could take advantage of both sides. Firstly, the A* search makes our method easier to find a connected path in a complex environment, while it is hard for learning-based methods to find a connected path <ref type="bibr" target="#b6">[Tamar et al., 2016]</ref>. Second, the global information can be merged into cost maps and the global objective can be approached by tuning the cost maps.</p><p>To learn the cost maps, we first define a cost-map parameter θ c ∈ R k×m and the values in cost maps are defined as c j = max(0, θ c [i, j]), c j ∈ C i . Given a ranking order R and cost maps, we can route the nets sequentially using the costmap enhanced A* and a total path length L will be obtained from the routing result. Similar to Eq. ( <ref type="formula" target="#formula_8">6</ref>), we define a reward function over the cost-map parameter and also its the expected value:</p><formula xml:id="formula_11">F (θ c ) = −L(R, C θc ), J(θ c ) = E ψ∼p θc F (ψ) = E ϵ∼N (0,I) F (θ c + σϵ),<label>(9)</label></formula><p>where C θc presents the cost maps derived from θ c , and θ c can be updated via Eq. (2). By iteratively executing the A* search and cost-map learning step, our method could solve some hard problems that a sequential routing algorithm can not solve as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In the next section, we will introduce how cost-map learning can couple with ranking learning and how to train the algorithm efficiently in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ranking Cost Algorithm</head><p>Our final algorithm, denoted as Ranking Cost (RC), can learn the ranking parameter θ r and cost-map parameter θ c jointly. In Eq. ( <ref type="formula" target="#formula_10">8</ref>), all the cost maps are used when calculating the priority function and the ranking order has no impact on the cost maps. To learn the cost-map with ranking order, we change the priority function to:</p><formula xml:id="formula_12">s j (v) = g j (v) + k i=j+1 C(i, v) + h j (v),<label>(10)</label></formula><p>where s j (v) is the priority function for the net whose ranking order is the j-th. This modification is reasonable because the earlier routed nets will use more cost maps and observe more global information. For example, when routing the first net, its ranking index is j = 1 and it will use all the rest k − 1 cost maps. As a result, the first routed path focuses more on its impact on other unrouted nets. When routing the last net, its ranking index is j = k and no cost map will be used. As a result, the last net is routed via a normal A* algorithm and the searched solution is the shortest path at the current situation. As all the previous nets have been routed, the best way to route the last net is just to find out the shortest path. Our RC algorithm achieves this naturally and makes it flexible for arbitrary ranking orders. When the ranking order changes, the order and the usage of cost maps change accordingly. In this way, the reward function can be defined as:</p><formula xml:id="formula_13">F (θ r , θ c ) = −L(R θr , C θc ),<label>(11)</label></formula><p>and both θ r and θ c can be updated via the OpenAI-ES algorithm. Actually, the reward function F can be integrated with arbitrary metrics, such as the signal latency. Thus the cost maps are free to adapt to new constraints and design rules.</p><p>In the OpenAI-ES algorithm, we will sample a fixed number of Gaussian noises and add them to the original parameters to form new parameters. These new parameters are then fed into n evaluators (or workers). The evaluator has an independent environment and executes the algorithm based on received parameters. Finally, each evaluator will return a scalar reward. To stabilize the training process, we normalize collected rewards {r 1 , ..., r n } from all the evaluators:</p><formula xml:id="formula_14">r i = (r i − r mean )/r std ,<label>(12)</label></formula><p>where r mean and r std are the mean and standard deviation of all the rewards. To further improve the final results, we add a postprocessing step to the solution obtained from the Ranking Cost algorithm. In the post-processing step, we will choose a connected path and re-plan it using the canonical A* algorithm while keeping other paths fixed. If we find a shorter path during the re-planning, we will update the current path to the new one. This post-processing step will be iteratively applied on each path until all the paths can not be updated. In practice, all the evaluators can execute in parallel as proposed in <ref type="bibr" target="#b4">[Salimans et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we will compare our proposed method with some other baselines. We will also study how ranking learning and cost maps work in our algorithm. Finally, we further show the scalability of our algorithm for larger applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods Evaluation</head><p>To evaluate our algorithm, we build a grid map simulator as the test environment. We construct 300 maps with three different sizes: 16 × 16, 32 × 32 and 64 × 64. These maps are split into two types, i.e., a simple one without obstacle and a more complex one with obstacles. Figure <ref type="figure" target="#fig_0">1(a)</ref> and Figure <ref type="figure" target="#fig_0">1(b)</ref> show some of the maps used in our experiments. Algorithms will be evaluated on these maps, and the success rates and the average of total lengths will be reported. Our experiments are performed on a desktop machine with 128 GB RAM and one 64-core CPU. More details about the maps can be found in the supplementary material.  <ref type="table">1</ref>: Evaluation results of different algorithms over 6 seeds. Two values and their corresponding standard deviations are reported. The first value is the success rate (the higher the better) and the second value in the bracket is the common average length (the lower the better).</p><p>Our methods achieve the best performance on most of the maps. The comparison between CML and RC shows that the ranking parameter learning can improve the performance of our algorithm.</p><p>Seq  The VIN is a learning-based approach for routing on grid maps. VINs use the value iteration from reinforcement learning and can achieve better performance compared with supervised models. We apply VINs to the circuit routing problem by sequentially executing it on each net. Similar to the sequential A* algorithm, we randomly sample five different ranking orders and the best score of five runs will be reported.</p><p>Cost-Map Learning (CML): We construct a Cost-Map Learning algorithm from the Ranking Cost. In the CML, the ranking parameter will not be trained (the ranking order is fixed) and only the cost-map will be trained.</p><p>We also implement two versions of the Ranking Algorithm, i.e., the version without post-processing step is denoted as Ranking Cost I and the version with post-processing step is denoted as Ranking Cost II. Table <ref type="table">1</ref> shows the evaluation results of different algorithms on all the maps over 6 seeds. Two values and their corresponding standard deviations are reported for each algorithm. The first value is the success rate (the higher the better) and the second value in the bracket is the common average length (the lower the better). Our final algorithm (Ranking Cost) achieves the best performance on most of the maps. On the most complex maps (with size 64 × 64, obstacles and 10 pairs of vertices), Ranking Cost outperforms other baselines by a large margin, which implies that our method has greater advantage on handling larger maps. The comparison between Seq A* (200) and Ranking Cost shows that our method can achieve the performance which the fully sampled A* will never achieve. Taking maps with the size of 16x16 (4 pairs) for example, there are only 4! = 24 orders, and we sampled all the possible orders for the A* since our maximum sampling number for is 200. But the final connecting rate of the A* is 96%. However, our RC algorithm can reach 100% connecting rate. Even given the full sampling, the A* will fail in many cases. Moreover, the comparison between Cost-Map Learning and Ranking Cost shows that the ranking parameter learning can further improve the performance of our algorithm. Table <ref type="table" target="#tab_2">2</ref> shows the wallclock time of different algorithms. The comparison between Ranking Cost I and Ranking Cost II shows that our post-processing step can help to reduce the total connected length with insignificant time-consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In this section, we study how each part of our algorithm works and the scalability of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Ranking Orders</head><p>To show the impact of ranking learning, we increase the sample number of orders for sequential A* and use the Ranking Learning algorithm described in Section 4.1 as a comparison. Figure <ref type="figure">2</ref>(a) shows the curve of the changes in success rates with different order sample numbers. The result shows that we can improve the success rate by increasing the order sample number, but it is hard to further improve the performance by linearly increasing the sample number as the order complexity is O(k!), where k is the number of pairs (or nets). However, our learned ranking order can achieve much better performance than randomly sampled orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Cost Maps</head><p>To study the impact of cost maps, we fix the ranking order and only train the cost-map parameter. We use the example map showed in Figure <ref type="figure" target="#fig_0">1</ref>(f) and the ranking order is fixed as R = (1, 2). Because the ranking order is fixed and there are only two cost maps, only the second cost map will be used when routing the first pair (i.e., (S 1 , E 1 )). Finally, we visualize the trained cost maps as shown in Figure <ref type="figure">2</ref>(b). There are two larger cost values (the yellow grids) in the cost map, which will give out a large cost if the path chooses to go through here. As a result, the first pair ((S 1 , E 1 )) will take the path as shown in Figure <ref type="figure" target="#fig_0">1</ref>(f) instead of taking its shortest path which will block the second pair((S 2 , E 2 )). It shows that our cost maps can capture global information and guide routers to approach global optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability of Ranking Cost Algorithm</head><p>In <ref type="bibr" target="#b3">[Liao et al., 2020]</ref>, their algorithm is only evaluated on the maps with the size of 8 × 8. In [He and Bao, 2020], their algorithm is evaluated on the maps with the size of 30 × 30 and with only 5 pairs. The search space is huge in their methods, which prevents them from applying to larger maps.</p><p>To test the scalability of our algorithm, we apply our algorithm on larger maps with the size of 150 × 150 and with 15 pairs. Our algorithm solves such large maps within 3 min and Figure <ref type="figure">2</ref>(c) shows the routing result of our method without post-processing step, which implies that our method could be adapted to larger scales of applications. Figure <ref type="figure">2</ref> <ref type="bibr">(d)</ref> shows the routing result of the Ranking Cost algorithm with the post-processing step, which implies that the post-processing step could improve the final planning result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we propose a novel algorithm, called Ranking Cost, to solve the historically challenging circuit routing problem. In our method, we innovatively combine searchbased algorithms and learning-based algorithms to form an efficient and trainable router under a proper parameterization.</p><p>Our method is a one-stage circuit routing algorithm which can optimize the global objective function directly, and it is easy to implement and flexible for new design rules and constraints. Experimental results show that our method is powerful and scalable to more complex tasks. In the future, we will try to explore faster optimizer for the Ranking Cost parameters and extend our algorithm to broader applications, such as pedestrian path prediction and robot navigation. Lastly, we do not believe our work has broader negative societal impacts, as we focus on developing circuit routing algorithms with evolution-based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Yellow vertices are the start vertices, green vertices are the end vertices and black blocks are obstacles where paths can not pass through. (a) Examples of maps without obstacle. (b) Examples of maps with obstacles. (c) Circuit routing results derived by our algorithm. In (d)-(f), (Si, Ei), i ∈ {1, 2} are two pairs of vertices which need to be connected. (d) and (e) show that sequential A* algorithm will greedily find the shortest path of one pair, and it fails to obtain a global solution. (f) shows the solution found by our algorithm, which can connect the two pairs successfully.</figDesc><graphic url="image-4.png" coords="2,57.43,162.76,72.90,55.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: (a) The curve of success rates and sample numbers and the experiment is conducted on maps with size of 64×64 and with obstacles. Our learned ranking order achieves better performance than randomly sampled orders. (b) Trained cost map of the example from Figure 1(f).There are two larger cost values (the yellow grids) in the cost map, which will give out a large cost if the path chooses to go through here. As a result, the first pair ((S1, E1)) will not take its shortest path which will block the second pair((S2, E2)). It shows that cost maps can capture global information and guide routers to complete the task successfully. (c) An example of routing results of the Ranking Cost algorithm on a large map with the size of 150 × 150 and with 15 pairs, which implies that our method could be adapted to larger scales of applications. (d) An example shows the routing result of the Ranking Cost algorithm with the post-processing step, which implies that the post-processing step could improve the final planning result.</figDesc><graphic url="image-9.png" coords="6,308.49,63.96,115.92,115.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The wallclock time of different algorithms. The Ranking Cost algorithm takes more running time, since it involves a learning procedure when solving each task. But the time consumption is acceptable, i.e., our algorithm takes less than one minute to get the solution, and it is worthy to sacrifice more time to earn better solutions.</figDesc><table><row><cell>specific ranking order and different orders will lead to</cell></row><row><cell>different results. In the experiments, we randomly sample 5</cell></row><row><cell>different ranking orders and run the algorithm 5 times. The</cell></row><row><cell>best score of 5 runs will be reported. To build a stronger</cell></row><row><cell>baseline, we further randomly sample 200 different ranking</cell></row><row><cell>orders and the best score of 200 runs will be reported.</cell></row><row><cell>Value Iteration Networks (VINs) [Tamar et al., 2016]:</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global and detailed routing</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chang ; Huang-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao-Wen</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Design Automation</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="687" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nikolaus Hansen and Andreas Ostermeier. Completely derandomized selfadaptation in evolution strategies</title>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13607</idno>
	</analytic>
	<monogr>
		<title level="m">Youbiao He and Forrest Sheng Bao. Circuit routing using monte carlo tree search and deep neural networks</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Hansen and Ostermeier</publisher>
			<date type="published" when="1998">2007. 2007. 2001. 2001. 1998. 1998. 2001. 2001. 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="159" to="195" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2007 IEEE/ACM International Conference on Computer-Aided Design</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jiang Hu and Sachin S Sapatnekar. A survey on multi-net global routing for integrated circuits</title>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A general greedy channel routing algorithm. IEEE transactions on computer-aided design of integrated circuits and systems</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991">1991. 1991. 2001. 2001. 2014. 2014</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>2014 ACM/IEEE International Workshop on System Level Interconnect Prediction (SLIP)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Malavasi and Sangiovanni-Vincentelli, 1993] Enrico Malavasi and Alberto Sangiovanni-Vincentelli. Area routing for analog layout</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">R</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Advancement in Communication Circuits and Systems</title>
				<editor>
			<persName><surname>Mandal</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984">1984. 1984. 2020. 2020. 1993. 2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
		</imprint>
	</monogr>
	<note>Advances in computing research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evolutionsstrategie: Optimierung technischer systeme nach prinzipien der biologischen evolution. frommann-holzbog, stuttgart</title>
		<author>
			<persName><surname>Mo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
	</analytic>
	<monogr>
		<title level="m">Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning</title>
				<meeting><address><addrLine>Jonathan Ho, Xi Chen</addrLine></address></meeting>
		<imprint>
			<publisher>Tim Salimans</publisher>
			<date type="published" when="1973">2001. 2001. 2001. 1973. 1994. 2017. 2017</date>
			<biblScope unit="page" from="404" to="407" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE/ACM International Conference on Computer Aided Design. IC-CAD</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trapl: Track planning of local congestion for global routing</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Schaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daohang</forename><surname>Schaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azadeh</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Davoodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference 2017</title>
				<meeting>the 54th Annual Design Automation Conference 2017</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997. 2017</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Moore&apos;s law: past, present and future</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A procedure for improving the distribution of congestion in global routing</title>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
				<editor>
			<persName><forename type="first">Yi</forename><surname>Tamar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Garrett</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Thomas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pieter</forename><surname>Levine</surname></persName>
		</editor>
		<editor>
			<persName><surname>Abbeel</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016. 2016. 2016</date>
			<biblScope unit="page" from="2154" to="2162" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992. 1992. 2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Ran Zhang. A Study of Routing Algorithms for PCB Design</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
