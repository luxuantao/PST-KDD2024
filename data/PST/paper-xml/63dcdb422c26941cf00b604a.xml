<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2023 TRANSFER NAS WITH META-LEARNED BAYESIAN SURROGATES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2023 TRANSFER NAS WITH META-LEARNED BAYESIAN SURROGATES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While neural architecture search (NAS) is an intensely-researched area, approaches typically still suffer from either (i) high computational costs or (ii) lack of robustness across datasets and experiments. Furthermore, most methods start searching for an optimal architecture from scratch, ignoring prior knowledge. This is in contrast to the manual design process by researchers and engineers that leverage previous deep learning experiences by, e.g., transferring architectures from previously solved, related problems. We propose to adopt this human design strategy and introduce a novel surrogate for NAS, that is meta-learned across prior architecture evaluations across different datasets. We utilize Bayesian Optimization (BO) with deep-kernel Gaussian Processes, graph neural networks for obtaining architecture embeddings and a transformer-based dataset encoder. As a result, our method consistently achieves state-of-the-art results on six computer vision datasets, while being as fast as one-shot NAS methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While deep learning has removed the need for manual feature engineering, it has shifted this manual work to the meta-level, introducing the need for manual architecture engineering. The natural next step is to also remove the need to manually define the architecture. This is the problem tackled by the field of neural architecture search (NAS).</p><p>Even though NAS is an intensely-researched area, there is still no NAS method that is both generally robust and efficient. Blackbox optimization methods, such as reinforcement learning <ref type="bibr" target="#b59">(Zoph &amp; Le, 2017)</ref>, evolutionary algorithms <ref type="bibr" target="#b31">(Real et al., 2019)</ref>, and Bayesian optimization <ref type="bibr" target="#b34">(Ru et al., 2021;</ref><ref type="bibr" target="#b48">White et al., 2021)</ref> work reliably but are slow. On the other hand, one-shot methods <ref type="bibr" target="#b25">(Liu et al., 2019;</ref><ref type="bibr">Dong &amp; Yang, 2019b)</ref> often have problems with robustness <ref type="bibr" target="#b57">(Zela et al., 2020)</ref>, and the newest trend of zero-cost proxies often does not provide more information about an architecture's performance than simple statistics, such as the architecture's number of parameters <ref type="bibr" target="#b49">(White et al., 2022</ref>).</p><p>An understudied path towards efficiency in NAS is to transfer information across datasets. This idea is naturally motivated by how researchers and engineers tackle new deep learning problems: they leverage the knowledge they obtained from previous experimentation and, e.g., re-use architectures designed for one task and apply or adapt them to a novel task. While a few NAS approaches in this direction exist <ref type="bibr" target="#b55">(Wong et al., 2018;</ref><ref type="bibr" target="#b23">Lian et al., 2020;</ref><ref type="bibr" target="#b11">Elsken et al., 2020;</ref><ref type="bibr" target="#b51">Wistuba, 2021;</ref><ref type="bibr" target="#b21">Lee et al., 2021;</ref><ref type="bibr" target="#b34">Ru et al., 2021)</ref>, they typically come with one or more of the following limitations: (i) they are only applicable to settings with little data, (ii) they only explore a fairly limited search space or even can just choose from a handful of pre-selected architecture, or (iii) they can not adapt to data seen at test-time. One approach to obtain efficient NAS methods that has been overlooked in the literature so far is to exploit the common formulation of NAS as a hyperparameter optimization (HPO) problem <ref type="bibr" target="#b1">(Bergstra et al., 2013;</ref><ref type="bibr" target="#b5">Domhan et al., 2015;</ref><ref type="bibr">Awad et al., 2021)</ref> and draw on the extensive literature on transfer HPO <ref type="bibr" target="#b53">(Wistuba et al., 2016;</ref><ref type="bibr">Feurer et al., 2018a;</ref><ref type="bibr" target="#b29">Perrone &amp; Shen, 2019;</ref><ref type="bibr" target="#b35">Salinas et al., 2020;</ref><ref type="bibr" target="#b52">Wistuba &amp; Grabocka, 2021)</ref>. In contrast to standard transfer HPO methods that meta-learn parametric surrogates from a pool of source datasets <ref type="bibr" target="#b53">(Wistuba et al., 2016;</ref><ref type="bibr">Feurer et al., 2018a;</ref><ref type="bibr" target="#b52">Wistuba &amp; Grabocka, 2021)</ref>, in this work we explore the direction of meta learning surrogates by contextualizing them on the dataset characteristics (a.k.a. meta-features) <ref type="bibr" target="#b45">(Vanschoren, 2018;</ref><ref type="bibr">Jomaa et al., 2021a;</ref><ref type="bibr" target="#b33">Rivolli et al., 2022)</ref>.</p><p>In this work, we present an efficient Bayesian Optimization (BO) method with a novel deep-kernel surrogate that yields a new NAS method which combines the best of both worlds: the reliability of blackbox optimization at a computational cost in the same order of magnitude as one-shot approaches. Concretely, we propose a BO method for NAS that leverages dataset-contextualized surrogates for transfer learning. Following <ref type="bibr" target="#b21">Lee et al. (2021)</ref>, we use a graph encoder <ref type="bibr" target="#b58">(Zhang et al., 2019)</ref> to encode neural architectures and an attention-based dataset encoder <ref type="bibr" target="#b22">(Lee et al., 2019)</ref> to obtain context features. We then use deep kernel learning <ref type="bibr" target="#b50">(Wilson et al., 2016)</ref> to obtain meta-learned kernels for the joint space of architectures and datasets, allowing us to use the full power of BO for efficient NAS. This approach solves two key issues of <ref type="bibr" target="#b21">Lee et al. (2021)</ref>, which is closest to our work: (i) a lack of trading-off exploration vs. exploitation and (ii) the lack of exploiting new function evaluations on a test task, blindly following what has been observed during meta training. As a result, our surrogates are optimized for efficiently transferring architectures for a new target dataset based on its meta-features. To sum up, our contributions are as follows:</p><p>? Inspired by manual architecture design, we treat NAS as a transfer or few-shot learning problem. We leverage ideas from transfer HPO to meta-learn a kernel for Bayesian Optimization, which encodes both architecture and dataset information.</p><p>? We are the first to combine deep-kernel Gaussian Processes (GPs) with a graph neural network encoder, a transformer-based dataset encoder, the first to apply BO with deep GPs to NAS, and the first to do all of this in a transfer NAS setting.</p><p>? Our resulting method outperforms both state-of-the-art blackbox NAS methods as well as state-of-the-art one-shot methods across six computer vision benchmarks.</p><p>To foster reproducibility, we make our code available at https://anonymous.4open. science/r/TNAS-DCS-CC08. We address the points in the "NAS Best Practices Checklist" in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>NAS is an intensely-researched field, with over 1000 papers published in the last two years alone<ref type="foot" target="#foot_0">1</ref> . We therefore limit our discussion of NAS to the most related fields of Bayesian optimization for NAS and meta learning approaches for NAS. For a full discussion of the NAS literature, we refer the interested readers to a series of surveys by <ref type="bibr" target="#b10">Elsken et al. (2019)</ref>, <ref type="bibr" target="#b54">Wistuba et al. (2019)</ref> and <ref type="bibr" target="#b32">Ren et al. (2020)</ref>, and for an introduction to BO to <ref type="bibr" target="#b36">Shahriari et al. (2016)</ref>; <ref type="bibr" target="#b17">Hutter et al. (2019)</ref>.</p><p>Bayesian optimization (BO) for NAS. As BO is commonly used in hyperparameter optimization (HPO), one can simply treat architectural choices as categorical hyperparameters and re-use, e.g., tree-based HPO methods that can natively handle categorical choices well <ref type="bibr" target="#b1">(Bergstra et al., 2013;</ref><ref type="bibr" target="#b5">Domhan et al., 2015;</ref><ref type="bibr" target="#b12">Falkner et al., 2018)</ref>. While Gaussian Processes (GPs) are more typically applied to continuous hyperparameters, they can also be used for NAS by creating an appropriate kernel; such kernels for GP-based BO can be manually engineered <ref type="bibr" target="#b42">(Swersky et al., 2013;</ref><ref type="bibr" target="#b20">Kandasamy et al., 2018;</ref><ref type="bibr" target="#b34">Ru et al., 2021)</ref>. A recent alternative is to exploit (Bayesian) neural networks for BO <ref type="bibr" target="#b40">(Snoek et al., 2015;</ref><ref type="bibr" target="#b41">Springenberg et al., 2016;</ref><ref type="bibr" target="#b48">White et al., 2021)</ref>. However, while these neural networks are very expressive, they require more data to fit well than GPs and thus are outperformed by GP-based approaches when only a few function evaluations can be afforded. In this work, we combine the sample efficiency of GPs and the expressive power of neural networks, by using deep GPs combined with a graph neural network encoder.</p><p>Meta learning for NAS. To mitigate the computational infeasibility of starting NAS methods from scratch for each new task, several approaches have been proposed along the lines of meta and transfer learning. Most of these warm-start the weights of architectures in a target task <ref type="bibr" target="#b55">Wong et al. (2018)</ref>; <ref type="bibr" target="#b23">Lian et al. (2020)</ref>; <ref type="bibr" target="#b11">Elsken et al. (2020)</ref>; <ref type="bibr" target="#b51">Wistuba (2021)</ref>. <ref type="bibr" target="#b34">Ru et al. (2021)</ref> extracts architectural motifs that can be reused on other datasets. Most related to our work is MetaD2A <ref type="bibr" target="#b21">(Lee et al., 2021)</ref>, where the authors propose to generate candidate architectures and rank them conditioned directly on a task, utilizing a meta-feature extractor <ref type="bibr" target="#b22">(Lee et al., 2019)</ref>. However, there are two key differences in our work: (i) the performance predictor of MetaD2A is not probabilistic and thus can not naturally tradeoff exploration vs. exploitation but rather only exploits what has been observed during meta-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset encoding</head><p>Architecture encoding GP Bayesian optimization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint encoding Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-learned</head><p>Figure <ref type="figure">1</ref>: Illustration of TNAS . We employ a GNN ? to encode architectures, a transformer ? to encode a dataset, and an MLP ? to merge the encodings. This joint encoding is then fed into a GP surrogate used within BO. All encodings are meta-learned.</p><p>(ii) During the meta testing phase, MetaD2A simply proposes N architectures for a new task, trains the top-K (as estimated by the performance predictor) out of N architectures and returns the best. In particular, the performance of the K evaluated architectures is not used as feedback for MetaD2A and thus the method never adapts to function evaluations on the test task, blindly following what has been observed during meta training. This can cause problems for new tasks which are poorly correlated with the meta training data. And, indeed, MetaD2A stagnates on several of the datasets in our experimental analysis. In contrast, our approach, dubbed TNAS (transferrable NAS), uses the function evaluations from meta testing to update the surrogate employed within our BO framework, thus allowing to adapt to the meta testing scenario.</p><p>Meta &amp; transfer learning for HPO There are many approaches to achieve meta or transfer learning in HPO, see, e.g., the survey by <ref type="bibr" target="#b44">Vanschoren (2019)</ref> or <ref type="bibr">Feurer et al. (2018b, Section 7)</ref>. One particularly promising approach is to employ Deep Kernel <ref type="bibr">Learning (Wilson et al., 2016)</ref> which strives to learn the kernel function by using a neural network to transform the input to a latent representation, which is then used in a kernel function. <ref type="bibr" target="#b52">Wistuba &amp; Grabocka (2021)</ref> and Jomaa et al. (2021b) utilized a deep kernel for transfer learning in HPO. While numerical hyperparameters can be encoded using an MLP, we adapt this approach to NAS by using a graph neural network to encode architectures as inputs; we also extend it by encoding datasets into a latent embedding and learning a deep kernel that spans the combined space of architectures and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>We consider the following problem: given a history of Q datasets, where for each dataset D (q)  we have already evaluated a set of neural network architectures x</p><formula xml:id="formula_0">(q) 1 , . . . , x<label>(q)</label></formula><p>n with corresponding performance (e.g., accuracy) y</p><formula xml:id="formula_1">(q) 1 , . . . , y (q)</formula><p>n . For a new dataset D (new) , we want to quickly discover an optimal architecture by leveraging information from the history of datasets. We build upon the state-of-the-art few-shot Bayesian optimization (BO) framework by <ref type="bibr" target="#b52">Wistuba &amp; Grabocka (2021)</ref>, which was proposed to address a similar problem: transferring optimal hyperparameter configurations across datasets. The authors propose to learn a deep kernel across tasks, which is then used for a Gaussian process (GP) surrogate in the typical BO setup.</p><p>However, while hyperparameter configurations can typically be presented by an N-dimensional vector, it is less clear how to represent neural network architectures. Simply representing architectures as vectors and plugging them into an off-the-shelf GP kernel is likely sub-optimal. In fact, <ref type="bibr" target="#b47">White et al. (2020)</ref> have shown that the type of architecture representation substantially impacts the performance of a downstream NAS algorithm. To address this issue, we employ graph neural networks (GNNs) to obtain a learnable representation of neural networks. GNNs are a common choice in NAS as neural networks architectures can be naturally represented as graphs <ref type="bibr" target="#b38">(Siems et al., 2020;</ref><ref type="bibr" target="#b47">White et al., 2020;</ref><ref type="bibr" target="#b46">Wen et al., 2020;</ref><ref type="bibr" target="#b9">Dudziak et al., 2020)</ref>. Furthermore, while we could directly feed this architecture encoding into the GP's kernel, we argue that the kernel should also be conditioned on the characteristics of a dataset in order to meaningfully asses (dis-)similarities between architectures. To motivate this, consider the question "What makes two architectures similar?". We argue that two architectures are not similar only because they share some similar sub-graph components (which will be represented by the GNN encoding), but also because they achieve similar performance on the target dataset. Following this line of reasoning, we condition the deep kernel on the characteristics (meta-features) of a dataset <ref type="bibr" target="#b45">(Vanschoren, 2018)</ref>. In a similar fashion as for the architecture encoding, we again use a learnable representation of datasets via employing a set transformer, as also done by <ref type="bibr" target="#b22">(Lee et al., 2019)</ref>. The architecture and dataset encoding are then processed by a fully-connected neural network, whose output serves as the input for an off-the-shelf kernel function, e.g., a Mat?rn kernel, which is finally used to compute the distance of two (architecture, dataset) datapoints. This results in an end-to-end learnable encoding of the problem, and the parameters of the GNN, transformer and fully-connected neural network are meta-learned in a similar fashion as in <ref type="bibr" target="#b52">Wistuba &amp; Grabocka (2021)</ref>. We refer to Figure <ref type="figure">1</ref> for an overview of our framework.</p><p>In the following subsection, we discuss the details of the different components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BAYESIAN OPTIMIZATION WITH DEEP KERNEL GAUSSIAN PROCESSES</head><p>We start by introducing Gaussian Processes, which represents the surrogate of our method within Bayesian optimization (BO). In a typical hyperparameter optimization (HPO) setup, the inputs x ? X represent hyperparameter configurations, and the target y ? Y denotes the performance of a machine learning method when trained with the hyperparameter configuration x. Consider the training</p><formula xml:id="formula_2">D = {(x i , y i )} n i=1 and testing D * = {(x * i , y * i )} n * i=1</formula><p>splits of a dataset of evaluated hyperparameters. In that context, GPs are non-parametric models that assume a prior over functions, and approximate the target y ? Y ? R + given the features x ? X ? R L . The estimation of the target variable y * for the test instances x * is also jointly Gaussian as y y * ? N 0,</p><formula xml:id="formula_3">K(x, x) K(x, x * ) K(x, x * ) T K(x * , x * )</formula><p>. Each respective block of the covariance matrix is the result of applying a kernel function k : X ? X ? R + on pairs of instances, e.g., K(x, x * ) i,j := k(x i , x * j ). The estimated posterior mean and covariance of GPs <ref type="bibr" target="#b30">(Rasmussen &amp; Williams, 2006)</ref> for the target y * of the test instances x * is given as follows:</p><formula xml:id="formula_4">E[y * | x * , x, y] = K(x * , x)K(x, x) -1 y (1) cov[y * | x * , x] = K(x * , x * ) -K(x, x * ) T K(x, x) -1 K(x, x * ).<label>(2)</label></formula><p>We refer to, e.g., <ref type="bibr" target="#b26">Murphy (2012)</ref> for the derivation. GPs are lazy models that rely on the similarity of the test instances to the training instances via kernel functions k, such as the Mat?rn kernel.</p><p>Unfortunately, typical kernels used with GPs are designed manually and rely on sub-optimal assumptions <ref type="bibr" target="#b4">(Cowen-Rivers et al., 2020)</ref>, which deteriorates the GP's performance. A promising direction for designing powerful and efficient kernel functions that adapt to a learning task is Deep Kernel Learning <ref type="bibr" target="#b50">(Wilson et al., 2016)</ref>, where kernels are represented as trainable neural networks. A mapping ? : X ? R L projects the features to a latent representation, where similar instances are co-located.</p><p>The embedding ? for the deep kernel of our GPs is a fully-connected neural network, that takes as input the encoding of the architecture ? as well as the dataset encoding ?. In detail, the L-dimensional architecture encoding ? is fused with the K-dimensional dataset encoding ? and processed through a fully connected neural network ? : R K+L ? R M , where the last layer has M neurons. We re-use both the GNN-based architecture encoding and the transformer-based dataset encoding from <ref type="bibr" target="#b21">Lee et al. (2021)</ref>, thus we only briefly describe it below and refer to <ref type="bibr" target="#b21">Lee et al. (2021)</ref> for details.</p><p>The architecture encoding ? consists of a directed acyclic graph encoder <ref type="bibr" target="#b58">(Zhang et al., 2019)</ref> to obtain the encoding for the architectures. By using one GRU cell to traverse the topological order of the DAG in the direction from the input to the output, and another GRU cell to pass through the DAG in the backward direction, we obtain latent representations of the graph which are then put through a fully-connected neural network layer to obtain the encoding for the architecture.</p><p>The dataset encoding ? consists of two stacked Set-Transformer <ref type="bibr" target="#b22">(Lee et al., 2019)</ref> architectures.</p><p>The first Set-Transformer layer captures the interaction between randomly sampled data points of the same class, whereas the second one captures the interactions between the different classes of the dataset. The resulting output of the second Set-Transformer layer represents the dataset encoding.</p><p>We tuned the dimensionality of the embedding of the dataset encoder and graph encoder, as well as the architecture of the feed-forward neural network of our method using the multi-fidelity Bayesian optimization method BOHB <ref type="bibr" target="#b12">(Falkner et al., 2018)</ref> on the meta-training dataset; please refer to Appendix A for details.</p><p>Putting it all together, the kernel/similarity between two architectures, specifically x evaluated on dataset D, and x ? evaluated on dataset D ? , is:</p><formula xml:id="formula_5">k (x, D, x ? , D ? ; w) = k ? ?(x; w (?) ), ? D; w (?) ; w (?) , ? ?(x ? ; w (?) ), ? D ? ; w (?) ; w (?) ; w (k)</formula><p>with w (?) being the parameters of the neural network ?, w (?) the parameters of the architecture encoding ?, w (?) the parameters of the dataset encoding ?, and w (k) additional parameters of the kernel function. We denote the cumulative parameters as w := w (?) , w (?) , w (?) , w (k) . All parameters are jointly meta-learned to maximize the marginal likelihood <ref type="bibr" target="#b52">(Wistuba &amp; Grabocka, 2021)</ref>, as will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">META-LEARNING DEEP-KERNEL GP SURROGATES</head><p>Recall that we assume we are given a set of Q datasets, where on each dataset D q we have N q ? N + evaluated architectures. We denote the n-th architecture evaluated on the q-th dataset as x q,n and its validation accuracy as y q,n . The meta-dataset of all the evaluations on all the datasets is defined as Algorithm 1 Meta-learning our deep-kernel GPs 1: Require: meta-dataset M; learning rates ? SGD , ? REP ; inner update steps v. 2: while not converged do 3: Sample mini-batch from M:</p><formula xml:id="formula_6">M := Q q=1 Nq n=1 {(x q,n , y q,n , D q )}. By x := (x 1,1 , . . . , x Q,Nq ), y := (y 1,1 , . . . , y Q,Nq</formula><p>x</p><formula xml:id="formula_7">= [x 1 , . . . , x k ], y = [y 1 , . . . , y k ], D = [D 1 , . . . , D k ] 4: L(w) = y T K -1 (x, D; w)y + log |K(x, D; w)| 5: w ? ? w 6: for j = 1 to v do 7: w ? ? w ? -? SGD ? w ? L(w ? ) 8: Update w ? w -? REP (w -w ? )</formula><p>In practice, we resort to sampling mini-batches and employ stochastic gradient descent, following established practices <ref type="bibr" target="#b50">(Wilson et al., 2016;</ref><ref type="bibr" target="#b52">Wistuba &amp; Grabocka, 2021;</ref><ref type="bibr" target="#b28">Patacchiola et al., 2020)</ref>. As all components of our method are differentiable, our approach is end-to-end differentiable.</p><p>For updating the meta-parameters w, we use the meta-learning algorithm REPTILE <ref type="bibr" target="#b27">(Nichol et al., 2018)</ref>, due to its simplicity compared to, e.g., MAML <ref type="bibr" target="#b15">(Finn et al., 2017)</ref>. Our method's pseudocode is shown in Algorithm 1. The procedure samples a mini-batch of (architecture, dataset) pairs with corresponding validation accuracy (line 3). We sample datasets and architectures uniformly at random from the meta-training dataset. Then we fit the surrogate to estimate the accuracies y of architectures x on a dataset D using SGD by minimizing Equation 3 (lines 4-7). Finally, we use REPTILE <ref type="bibr" target="#b27">(Nichol et al., 2018)</ref> to update w (line 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">META-TESTING</head><p>Once the optimal w are found, we plug in the meta-learned kernel for a GP, with the posterior and use vanilla Bayesian Optimization (BO) to quickly identify the optimal configuration in the new response surface. Note that by employing BO in the meta-testing phase, TNAS adapts to the test task by updating the posterior of the GP surrogate with function evaluations from the test task (remember equations 1, 2). We offer a more detailed description of the BO loop in Appendix C. For each of these datasets, the accuracy of one (different) architecture from the NAS-Bench-201 search space is given. For the evaluation of our method and the baselines, we use six popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN, Aircraft, Oxford IIT Pets, and MNIST. For CIFAR-10 and CIFAR-100, we query the performances of architectures from the NAS-Bench-201 benchmark, whereas for the other four datasets we train the suggested architectures from scratch using the NAS-Bench-201 pipeline (as these are not available in the benchmark). We ran three trials for each experiment and report the mean and standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BASELINES</head><p>Classic HPO. A simple baseline is Random Search (RS) <ref type="bibr" target="#b2">(Bergstra &amp; Bengio, 2012)</ref>. RS samples architectures uniformly at random from the search space and returns the top preforming one. Another simple yet powerful baseline is Bayesian Optimization with a vanilla GP surrogate <ref type="bibr" target="#b39">(Snoek et al., 2012)</ref>. We use the Mat?rn 5/2 kernel and rely on the GPytorch <ref type="bibr" target="#b16">(Gardner et al., 2018)</ref> implementation. We tried both Expected Improvement (EI) and Upper Confidence Bound (UCB) as acquisition functions, with UCB performing better in our experiments. We also compare to HEBO <ref type="bibr" target="#b4">(Cowen-Rivers et al., 2020)</ref>, a black-box HPO method that performs input and output warping to mitigate the effects of heteroscedasticity and non-stationarity on HPO problems. HEBO won the 2020 NeurIPS blackbox optimization challenge <ref type="bibr">(Turner et al., 2021)</ref>. We use the implementation provided by the authors.</p><p>HPO for NAS. <ref type="bibr" target="#b48">White et al. (2021)</ref> proposed a BO method for NAS that uses an ensemble of fullyconnected neural networks as a surrogate, named BANANAS. Moreover, BANANAS uses a path encoding for the neural architectures, which serves as an input to the ensemble. When applied to a new test task, BANANAS starts the neural architecture search from scratch. NASBOWL <ref type="bibr" target="#b34">(Ru et al., 2021</ref>) is a GP-based BO method for NAS and utilizes the Weisfeiler-Lehman kernel <ref type="bibr" target="#b37">(Shervashidze et al., 2011)</ref>. We use the implementations provided by the authors for BANANAS and NASBOWL.</p><p>State-of-the-art in NAS. One-shot methods have recently shown strong empirical performance for NAS. We compare to GDAS <ref type="bibr">(Dong &amp; Yang, 2019b)</ref>, SETN <ref type="bibr">(Dong &amp; Yang, 2019a)</ref>, PC-DARTS (Xu  <ref type="bibr">et al., 2020)</ref> and DrNAS <ref type="bibr" target="#b3">(Chen et al., 2021)</ref>. For these methods, we compare to published results from the literature for six computer vision datasets.</p><p>Transfer NAS. We compare to the most related transfer NAS method, MetaD2A <ref type="bibr" target="#b21">(Lee et al., 2021)</ref>, which is a transfer learning NAS method with a dataset-contextualized neural network generator and performance predictor. The neural network generator and performance predictor are meta-trained on the same source datasets that we also use. When applied to a test task, MetaD2A generates 500 candidate architectures conditioned on the test dataset and then selects the top architectures based on its performance predictor. We use the implementation provided by MetaD2A's authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESEARCH HYPOTHESES AND EXPERIMENTAL RESULTS</head><p>Our experiments are designed to validate the following research hypotheses for our approach, dubbed TNAS :</p><p>Hypothesis 1: TNAS is more efficient than classical HPO methods applied to NAS as well as HPO methods specifically adapted to NAS and outperforms them in terms of anytime-performance, while achieving strong final performance.</p><p>Hypothesis 2: TNAS is competitive with one-shot approaches in terms of runtime.</p><p>In summary, we validate our claim from the introduction:</p><p>Hypothesis 3: TNAS achieves the consistency of blackbox optimization algorithms (such as classical HPO methods) while being as efficient as one-shot methods.</p><p>Results for Hypothesis 1. In Figure <ref type="figure" target="#fig_1">2</ref>, we compare the performance of TNAS, with several HPO baselines. For all methods, we use the 5 top-performing architectures from the meta-training dataset as a starting point. In that sense, all these baselines are "transfer learning" by being initialized with the best architectures on the meta-training dataset. On all of the datasets except CIFAR100, TNAS finds top-performing architectures faster than all the baselines and achieves stronger anytime performance. Furthermore, on all benchmarks, TNAS eventually performs best.</p><p>Results for Hypothesis 2. We demonstrate that with our meta-learned deep kernel within Bayesian Optimization, the search time can be significantly reduced, to the same order as one-shot approaches.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the performance of both TNAS and MetaD2A compared to state-of-the-art NAS  methods. Except for CIFAR10 and CIFAR100, TNAS clearly outperforms the NAS baselines by the time the one-shot approaches finish the search. On CIFAR10, TNAS achieves similar performance as the baselines, while it is slightly inferior on CIFAR100 for some baselines and only overtakes them given more time. We refer to Table <ref type="table" target="#tab_1">3</ref> in the appendix for concrete numbers. We can furthermore observe the drawbacks of MetaD2A discussed earlier: (i) MetaD2A does not trade-off exploration vs. exploitation but rather only exploits what has been observed during meta-training and (ii) MetaD2A does not use evaluations on the new target dataset as feedback and never adapts. As a result, MetaD2A stagnates on several of the datasets.</p><p>Results for Hypothesis 3. In Figure <ref type="figure" target="#fig_4">4</ref>, we show that TNAS consistently achieves strong results, while the existing state-of-the-art NAS baselines have much higher variance -their ranking changes across benchmarks. Furthermore, the figure shows how the ranking evolves over the course of running of the methods (by means of runtime in GPU hours). The analysis indicates that TNAS consistently achieves the best performance, for both small and large computational budgets, in particular also when compared to MetaD2A. We empirically analyse our design choices, namely the graph and dataset encoder and demonstrate a lift in performance compared to ceteris paribus ablations that do not employ these designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ABLATING OUR DESIGN CHOICES</head><p>Concretely, we ablate our method's components and test them on CIFAR-10 and CIFAR-100, reporting the result in Figure <ref type="figure" target="#fig_5">5</ref>. TNAS (no Set Encoder) shows the performance of our method using a graph encoder, but no set-encoder (i.e., without using any dataset meta-features). TNAS (no Graph Encoder) shows the performance of our method using the dataset encoding in combination with a matrix encoding for the architectures (i.e., no graph encoder). TNAS outperforms the other variations; thus, we conclude that using both the learnable dataset meta-features and a graph neural network encoding is the most robust surrogate design. This finding validates the design choices of our method. Figure <ref type="figure">6</ref>: Ablation of the initial design and the initialization of our method on CIFAR-10 (top row) and CIFAR-100 (bottom row). Left: using up to 5 random architectures; middle: using up to 5 top architectures from the meta-training set; right: ablating the comparison of (i) a meta-learned surrogate and a randomly initialized one with (ii) a random initial design and an initial design from the top 5 architectures from the meta-training set. We also empirically evaluated (i) whether there is actually a benefit in meta-learning the surrogate, and (ii) our method's performance with different initial architectures. Remember that as an initial design for our method, we used the top-5 performing architectures from the meta-training dataset. As alternatives, we consider values other than 5 and also start from randomly sampled architectures. We also turn-off meta-learning. The plots for these experiments are shown in Figure <ref type="figure">6</ref>, evaluated on CIFAR-10 and CIFAR-100. The results suggest that both design choices are beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>NAS is an intensely researched task and in essence is an instance of the hyperparameter optimization (HPO) problem. In this work, we exploited this relationship and, motivated by state-of-the-art transfer HPO methods, adapted deep Gaussian Process (GP) surrogates to capture architecture representations. Furthermore, we proposed a novel conditioning of the deep GP on dataset meta-features to enable transferring well-performing architectures from source datasets with similar meta-features. We empirically motivated the impact of each component of our proposed method through extensive ablations. In addition, we showed that our novel deep GPs with dataset meta-features and architecture encodings achieve the highest accuracy on six computer vision datasets compared to a broad range of HPO methods, BO methods for NAS, and one-shot NAS methods. Lastly, we demonstrated that proxy architecture evaluations allow our method to discover more accurate architectures within the same search time one-shot NAS methods require.</p><p>Future work. A deep surrogate in principle allows us to capture the interaction between architectures and hyperparameter configurations. The NAS literature at the moment underexplores the impact of hyperparameters on the performance of an architecture. In fact, it is commonly known that the same architecture would perform differently if the training pipeline is altered, for example by changing the learning rate, the number of epochs, or the degree of regularization. Our previously-defined surrogate can be trivially extended to model the interaction of architecture embeddings, the dataset meta-features and hyperparameter configurations. We have not explored this direction empirically due to the lack of available NAS meta-datasets that vary both architectures and hyperparameters but would like to do in the future.</p><p>Reproducibility Statement. To foster reproducibility, we make our code available at https: //anonymous.4open.science/r/TNAS-DCS-CC08. We give details on our experimental protocol in the "NAS Best Practices Checklist" in Appendix F.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) and D := (D 1 , . . . , D Q ) we denote vectors containing all the architectures, accuracies and datasets. The parameters w of the deep kernel are optimized jointly by maximizing the log marginal likelihood of the GP surrogate on the meta training dataset: arg max w log p (y | x, D ; w) (3) ? arg min w y T K -1 (x, D; w) y + log |K(x, D; w)|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing TNAS to random search (RS) and the four different Bayesian optimization methods on six image datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of TNAS to state-of-the-art NAS methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ranking (averaged across benchmarks) over the course of runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Consistency of TNAS compared to baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation of the components of TNAS .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Figure 7: Comparison of TNAS to MetaD2A on the MobileNetV3 search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our work to Lee et al. (2021). E COMPARISON OF TNAS TO SOTA NAS METHODS AND MOBILENET V3 SPACE</figDesc><table><row><cell></cell><cell>predictor</cell><cell>candidate architecture generation</cell><cell>test-time adaptation</cell></row><row><cell>Lee et al. (2021)</cell><cell>(dataset, architecture) encoding, MLP (deterministic prediction)</cell><cell>GNN-based architecture generator, select top-K based on predictor</cell><cell>no</cell></row><row><cell>Ours</cell><cell>(dataset, architecture) encoding, MLP, GP (probabilistic prediction)</cell><cell>BO, maximize acquisition function</cell><cell>yes, adapt GP predictor w.r.t. new candidate evaluations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of our method (TNAS ) in terms of time and accuracy compared to state-of-the-art NAS methods on the NASBench201 search space.</figDesc><table><row><cell>Data</cell><cell cols="2">Method GPU days Accuracy</cell></row><row><cell></cell><cell>SETN</cell><cell>0.40 87.64?0.00</cell></row><row><cell></cell><cell>GDAS</cell><cell>0.34 93.61?0.09</cell></row><row><cell>CIFAR-10</cell><cell cols="2">PC-DARTS 0.17 93.66?0.17 DrNAS 0.30 94.36?0.00</cell></row><row><cell></cell><cell>arch2vec</cell><cell>1.38 91.41?0.22</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>2.08 94.37?0.00</cell></row><row><cell></cell><cell>TNAS</cell><cell>2.18 94.37?0.00</cell></row><row><cell></cell><cell>SETN</cell><cell>0.73 59.09?0.24</cell></row><row><cell></cell><cell>GDAS</cell><cell>0.64 70.70?0.30</cell></row><row><cell>CIFAR-100</cell><cell cols="2">PC-DARTS 0.28 66.64?2.34 DrNAS 0.45 73.51?0.00</cell></row><row><cell></cell><cell>arch2vec</cell><cell>1.38 73.35?0.32</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>2.08 73.51?0.15</cell></row><row><cell></cell><cell>TNAS</cell><cell>2.18 73.51?0.00</cell></row><row><cell></cell><cell>SETN</cell><cell>0.87 99.69?0.04</cell></row><row><cell></cell><cell>GDAS</cell><cell>0.76 99.64?0.04</cell></row><row><cell>MNIST</cell><cell cols="2">PC-DARTS 0.35 99.66?0.04 DrNAS 0.57 99.59?0.02</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>0.83 99.71?0.02</cell></row><row><cell></cell><cell>TNAS</cell><cell>0.89 99.78?0.00</cell></row><row><cell></cell><cell>SETN</cell><cell>0.46 44.84 ?3.96</cell></row><row><cell></cell><cell>GDAS</cell><cell>0.46 53.52?0.48</cell></row><row><cell>Aircraft</cell><cell cols="2">PC-DARTS 0.29 26.33?3.40 DrNAS 0.65 46.08?7.00</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>0.83 57.71?0.72</cell></row><row><cell></cell><cell>TNAS</cell><cell>0.89 59.51?0.0</cell></row><row><cell></cell><cell>SETN</cell><cell>0.35 25.17?1.68</cell></row><row><cell></cell><cell>GDAS</cell><cell>0.33 24.02?2.75</cell></row><row><cell>Pets</cell><cell cols="2">PC-DARTS 0.28 25.31?1.38 DrNAS 0.31 26.73?2.61</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>0.83 39.04?0.72</cell></row><row><cell></cell><cell>TNAS</cell><cell>0.89 43.24?0.0</cell></row><row><cell></cell><cell>SETN</cell><cell>1.61 96.02?0.04</cell></row><row><cell></cell><cell>GDAS</cell><cell>1.46 95.57?0.04</cell></row><row><cell>SVHN</cell><cell cols="2">PC-DARTS 0.99 95.40?0.04 DrNAS 1.24 96.30?0.02</cell></row><row><cell></cell><cell>MetaD2A</cell><cell>1.08 96.44?0.05</cell></row><row><cell></cell><cell>TNAS</cell><cell>1.18 96.57?0.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See list at: https://www.automl.org/automl/literature-on-neural-architecture-search</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETER OPTIMIZATION FOR GNN AND SET TRANSFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARCHITECTURES</head><p>We tuned the dimensionality of the embedding of the dataset encoder and graph encoder (Embedding dims.), the architecture of the feed-forward neural network of our method (Num. layers, Num. units in layer 1, Num. units in layer 2, Num. units in layer 3, Num. units in layer 4), and the learning rate of the joint meta-training using BOHB <ref type="bibr" target="#b12">(Falkner et al., 2018)</ref>  [10 -6 , 10 -1 ] (log space)</p><p>Table <ref type="table">1</ref>: The hyperparameter space for tuning our method with BOHB <ref type="bibr" target="#b12">(Falkner et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ARCHITECTURE ENCODING IN TNAS</head><p>To encode the architectures, we use a directed acyclic graph encoder <ref type="bibr" target="#b58">Zhang et al. (2019)</ref>. It takes architectures represented as a graph as input, and provides as output vector representations for those architectures. Following <ref type="bibr" target="#b58">Zhang et al. (2019)</ref>, the graph encoder consists of two GRU cells and a fully-connected neural network layer. Each GRU cell traverses the graph representation of the architecture in opposite directions to the other. The output of both cells is then concatenated and input through the fully-connected layer, which outputs the vector representation that will be used as an encoding for the architecture. We meta-train the graph encoder, as well as the dataset encoder, jointly with the embedding of the deep kernel during meta-training to maximize the log marginal likelihood of the GP surrogate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BAYESIAN OPTIMIZATION WITH TNAS</head><p>Once we meta-train our surrogate on the meta-training set using Algorithm 1, we use it at meta-testing time in a Bayesian optimization (BO) loop. First, we evaluate the top-5 architectures from the meta-training set on the test dataset. Once the GP is fit to these architectures and their respective accuracies, we use EI as the acquisition function to find the next architecture to evaluate. In the NASBench201 search space, we repeat this BO loop for a total of 100 evaluations for CIFAR10 and CIFAR100, whereas for SVHN, Aircraft, Pets, and MNIST, we do 40 evaluations. In the MobilenetV3 search space we do 50 evaluations for each of the datasets.</p><p>For maximizing the acquisition function in order to find the next candidate architecture, we either exhaustively evaluate the entire search space (in the case of small search spaces such as NAS-Bench-201), or simply sample K architectures randomly and pick the best (in the case of the MobileNet V3 space, K = 10 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D COMPARISON OF TNAS AND METAD2A</head><p>We refer to Table <ref type="table">2</ref> for a comparison of the key ingredients of <ref type="bibr" target="#b21">Lee et al. (2021)</ref> and our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F NAS BEST PRACTICE CHECKLIST</head><p>We now describe how we addressed the individual points of the NAS best practice checklist <ref type="bibr" target="#b24">(Lindauer &amp; Hutter, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Best Practices for Releasing Code</head><p>For all experiments you report:</p><p>(a) Did you release code for the training pipeline used to evaluate the final architectures?</p><p>The code for the training pipeline for the architectures can be found in the repo we provide. (b) Did you release code for the search space? We used the NAS-Bench-201 search space in our experiments, the description and code for which is publicly available. (c) Did you release the hyperparameters used for the final evaluation pipeline, as well as random seeds? We the NAS-Bench-201 pipeline and hyperparameters as our final evaluation pipeline. We release it as well as the random seeds in the repo we provide. (d) Did you release code for your NAS method? The code for our NAS method can be found in https://anonymous.4open.science/r/TNAS-DCS-CC08. (e) Did you release hyperparameters for your NAS method, as well as random seeds? The hyperparameters for our NAS method, as well as random seeds for the experiments can be found in the repo we provide.  <ref type="formula">2018</ref>) to tune the hyperparameters of our method. We ran BOHB with three different random seeds for 24 hours. (b) Did you report the time for the entire end-to-end NAS method (rather than, e.g., only for the search phase)? Yes. (c) Did you report all the details of your experimental setup? The details of our experimental setup can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Best practices for comparing NAS methods</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dehb: Evolutionary hyberband for scalable, robust and efficient hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Noor</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeratyoy</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI&apos;21)</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dr{nas}: Dirichlet neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9FWas6YbmB3" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Alexander I Cowen-Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tutunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Jianye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Bou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03826</idno>
		<title level="m">Heteroscedastic evolutionary bayesian optimisation</title>
		<meeting><address><addrLine>Hebo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>winning submission to the NeurIPS 2020 Black Box Optimisation Challenge</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxyZkBKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Brp-nas: Prediction-based nas using gcns</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Royson</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lane</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/768" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10480" to="10490" />
		</imprint>
	</monogr>
	<note>e78024aa8fdb9b8fe87be86f64745-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning of neural architectures for few-shot learning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Staffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01238</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01238" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12362" to="12372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BOHB: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML&apos;18)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning (ICML&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scalable meta-learning for bayesian optimization</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Bakshy</surname></persName>
		</author>
		<idno>CoRR, abs/1802.02219</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Practical transfer learning for bayesian optimization</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Bakshy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.02219" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration</title>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="7587" to="7597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automated Machine Learning: Methods, Systems, Challenges</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://automl.org/book" />
		<editor>L. Kotthoff, and J. Vanschoren</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dataset2vec: learning dataset metafeatures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Jomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josif</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><surname>Grabocka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="964" to="985" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning for bayesian hpo with end-to-end landmark meta-features</title>
		<author>
			<persName><forename type="first">Jomaa</forename><surname>Hadi Samer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pineda Arango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid neural architecture search by learning to generate graphs from datasets</title>
		<author>
			<persName><forename type="first">Hayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyoung</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkQuFUmUOg3" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards fast adaptation of neural architectures with meta learning</title>
		<author>
			<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yintao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1eowANFvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian meta-learning for the few-shot setting via deep kernels</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning search spaces for bayesian optimization: Another view of hyperparameter transfer learning</title>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huibin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="12751" to="12761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning. Adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Pengzhen</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02903</idno>
		<title level="m">A comprehensive survey of neural architecture search: Challenges and solutions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Rivolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Lu?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P L F</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><surname>De Carvalho</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.108101</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0950705121011631" />
	</analytic>
	<monogr>
		<title level="j">Meta-features for meta-learning. Knowledge-Based Systems</title>
		<idno type="ISSN">0950-7051</idno>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page">108101</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels</title>
		<author>
			<persName><forename type="first">Binxin</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A quantile-based approach for hyperparameter transfer learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huibin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Valerio Perrone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Virtual Event</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="page" from="8438" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of Bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="175" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nasbench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 4th Workshop on Meta-Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Mostofa</forename><surname>Ali Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015. 2015</date>
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Advances in Neural Information Processing Systems (NeurIPS&apos;16)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting>the 29th International Conference on Advances in Neural Information Processing Systems (NeurIPS&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Raiders of the lost architecture: Kernels for Bayesian optimization in conditional parameter spaces</title>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Optimization in Theory and Practice</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>BayesOpt&apos;13</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10201[cs.LG</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Meta-learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<ptr target="http://automl.org/book" />
		<editor>Hutter et al.</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Meta-learning: A survey</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno>CoRR, abs/1810.03548</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural predictor for neural architecture search</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A study on encodings for neural architecture search</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20309" to="20319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A deeper look at zero-cost proxies for lightweight nas</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renbo</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shital</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/" />
	</analytic>
	<monogr>
		<title level="m">ICLR Blog Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xfernas: Transfer neural architecture search</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jefrey</forename><surname>Lijffijt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="247" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Few-shot bayesian optimization with deep kernel surrogates</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Two-stage transfer surrogate model for automatic hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -European Conference, ECML PKDD 2016</title>
		<meeting><address><addrLine>Riva del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">September 19-23, 2016. 2016</date>
			<biblScope unit="page" from="199" to="214" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejaswini</forename><surname>Pedapati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01392</idno>
		<title level="m">A survey on neural architecture search</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transfer learning with neural automl</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>f45e6734c35733d24299d3f4-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlS634tPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;20)</title>
		<meeting>the International Conference on Learning Representations (ICLR&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Published online: iclr.cc</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
