<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Recurrent Neural Networks for Session-based Recommendations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-16">16 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><forename type="middle">Kiam</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>liuyong@ihpc.a-star.edu.sg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<addrLine>A*STAR 1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Recurrent Neural Networks for Session-based Recommendations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-16">16 Sep 2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2988450.2988452</idno>
					<idno type="arXiv">arXiv:1606.08117v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Computing methodologies → Supervised learning</term>
					<term>Neural networks</term>
					<term>•Information systems → Recommender systems</term>
					<term>Recurrent neural networks</term>
					<term>Recommender systems</term>
					<term>Sessionbased recommendations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNNbased models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Users of e-commerce websites are often inundated by the huge number of items available for sale. Recommender systems can be used to enhance user experience by making personalized and useful recommendations for each user. For example, the system could automatically display items of interest, or suggest new discounts relevant to each user. In order to personalize recommendations, traditional recommender systems often need to build up a user profile. Collaborative filtering approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> can define useruser similarity based on their history of purchases, or they could rely on matrix factorization to build latent factor vectors for each user. Crucially, these approaches require the user to be identified when making recommendations. This may not always be possible: new users to the site will not have any profile, or users may not be logged in, or they may have deleted their tracking information. This leads to the problem of cold-start for recommendation methods that require user history.</p><p>An alternative to relying on historical data is to make session-based recommendations <ref type="bibr" target="#b22">[23]</ref>. In this setting, the recommender system makes recommendations based only on the behaviour of users in the current browsing session. This avoids the aforementioned cold-start issue but we must ensure that the system remains accurate and responsive (i.e. the predictions do not take too long to make). Recurrent Neural Networks (RNNs) were recently proposed in <ref type="bibr" target="#b9">[10]</ref> for the session-based recommendation task. The authors showed significant improvements over traditional session-based recommendation models using an RNN. The proposed model utilizes session-parallel mini-batch training, and also employs ranking-based loss functions for learning the model.</p><p>In this work, we further study the application of RNNs for session-based recommendations. In particular, we examine and adapt various techniques from the literature for this task. These include:</p><p>• Data augmentation via sequence preprocessing and embedding dropout to enhance training and reduce overfitting.</p><p>• Model pre-training to account for temporal shifts in the data distribution.</p><p>• Distillation using privileged information to learn from small datasets.</p><p>Additionally, we propose a novel alternative model that reduces the time and space requirements for predictions by predicting item embeddings directly. This makes RNNs more readily deployable in real-time settings.</p><p>Our proposed techniques were evaluated on the RecSys Challenge 2015 data set. The effectiveness of our data augmentation strategy is evidenced by relative model performance improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank @20 (MRR@20) metrics respectively. We also showed that distillation could be successfully applied for performance gains on small datasets. Finally, our novel item embedding output approach significantly reduces the time and space requirements of the RNN model.</p><p>We start with a discussion of related work in Section 2. Then, we present the details of our improved RNN models in Section 3, and our experiments on the models in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Matrix factorization and neighbourhood-based methods are widely utilized for recommender systems in the literature. Matrix factorization methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> are based on the sparse user-item interaction matrix, where the recommendation problem is formulated as a matrix completion task. After decomposing the matrix, each user and item is represented by a latent factor vector. The missing value of the user-item matrix can then be filled by multiplying the appropriate user and item vectors. Since this requires us to identify both the user and item vectors, matrix factorization methods are not directly suitable for session-based recommendations where the users are unknown. One way to solve this cold-start problem is to use pairwise preference regression <ref type="bibr" target="#b19">[20]</ref>. Neighbourhood based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref> utilize the similarities between item and user purchase history; they can be applied to session-based recommendations by comparing session similarity.</p><p>Deep learning has recently been applied very successfully in areas such as image recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>, speech recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref> and natural language processing <ref type="bibr" target="#b23">[24]</ref>. Deep models can be trained to learn discriminative representmations from unstructured data such as images and speech signals. They have also been used for collaborative filtering <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, RNNs were proposed for session-based recommendations. The authors compared RNNs (with several customized ranking losses) to existing methods for sessionbased predictions and found that RNN-based models performed 20% to 30% better than the baselines. Our work is closely related, and we study extensions to their RNN models. In <ref type="bibr" target="#b30">[31]</ref>, the authors also use RNNs for click sequence prediction; they consider historical user behaviours as well as hand engineered features for each user and item. In this work, we rely entirely on automatically learned feature representations.</p><p>Many approaches have been proposed to improve the predictive performance of trained deep neural networks. Popular approaches include data augmentation <ref type="bibr" target="#b15">[16]</ref>, dropout <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6]</ref>, batch normalization <ref type="bibr" target="#b11">[12]</ref> and residual connections <ref type="bibr" target="#b8">[9]</ref>. We seek to apply some of these methods to enhance the training of our recommendation RNNs.</p><p>The learning using privileged information (LUPI) framework <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> was proposed to utilize the additional feature representations that are only available during training but not during testing. When there is a limited amount of training data, the use of such information has been found to be helpful <ref type="bibr" target="#b26">[27]</ref>. In the generalized distillation approach <ref type="bibr" target="#b6">[7]</ref>, a student model learns from soft labels provided by a teacher model. If we train the teacher model on the privileged dataset, then this approach can be applied to LUPI. In this work, we propose the use of this framework for the click sequence prediction by using the future portion of each click sequence as a form of privileged information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APPROACHES</head><p>In this section, we explain the use of RNNs for the sessionbased recommendation problem (3.1). This is followed by our proposed data augmentation methods (3.2), our ap- proach to handling temporal shifts (3.3), an explanation of the application of LUPI (3.4), and finally, an alternative model based on embeddings to trade model accuracy for speed and memory requirements (3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RNNs for session-based recommendations</head><p>The session-based recommendation problem can be formulated as a sequence-based prediction problem as follows.</p><formula xml:id="formula_0">Let [x1, x2, . . . , xn−1, xn] be a click session, where xi ∈ R (1 ≤ i ≤ n) is the index of one clicked item out of a total number of m items.</formula><p>We seek a model M such that for any given prefix clicksequence of the session, x = [x1, x2, . . . , xr−1, xr], 1 ≤ r &lt; n, we get the output y = M (x), where y = [y1, . . . , ym] ∈ R m . We view y as a ranking over all the next items that can occur in that session, where yi corresponds to the score of item i. Since we typically need to make more than one recommendation for the user to choose from, the top-k items (as ranked by y) are recommended.</p><p>In most of our models, we use a classification-based output, where y corresponds to a probability distribution over the items. Let xr+1 be the next click of the click sequence x; we can represent it with an m-dimensional 1-HOT encoded vector V (x) ∈ R m . The model can be tuned by minimizing a chosen loss function e.g. the cross entropy loss, L(M (x), V (xr+1)). Other outputs are possible: the models in <ref type="bibr" target="#b9">[10]</ref> output ranking scores for each item, and they are trained with ranking losses.</p><p>We follow the generic structure of the RNN model shown in Figure <ref type="figure" target="#fig_0">1</ref>. For the recurrent layers, we use the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref> as it was found in <ref type="bibr" target="#b9">[10]</ref> that they outperformed the Long-term Short Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> units. However, we do not utilize the stateful RNN training procedure, where the models are trained in a session-parallel, sequence-to-sequence manner. Instead, our networks process each sequence [x1, x2, . . . , xr] separately, and are trained to predict the next item, xr+1, in that sequence. We also represent all our input using trainable embeddings. Our networks can be trained using standard mini-batch gradient descent on the cross-entropy loss via Backpropagation-Through-Time (BPTT) for a fixed number of time steps. This training procedure is visualized in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data augmentation</head><p>Click sessions often vary in length: some users may take a long time before finding their desired item, while others find it with just a few clicks. One aim of the recommender system should be to provide accurate predictions regardless of the current session length. Data augmentation techniques have been widely used to enhance image-based models <ref type="bibr" target="#b15">[16]</ref>. Here, we propose two methods to augment click sequences.</p><p>The first is an application of the sequence preprocessing method proposed in <ref type="bibr" target="#b4">[5]</ref> Embedding dropout is a form of regularization applied to input sequences <ref type="bibr" target="#b5">[6]</ref>. Applying it to a click sequence is equivalent to a preprocessing step that randomly deletes clicks at random. Intuitively this makes our model less sensitive to noisy clicks, e.g. where users may have accidentally clicked on items that are not of interest. Hence, it makes the model less likely to over-fit to specific noisy sequences. It can also be viewed as a form of data augmentation, where shorter, pruned sequences are being generated for model training.</p><p>We apply both methods to all our models, and a graphical example is shown in Figure <ref type="figure" target="#fig_3">3</ref>. Note that different clicks are dropped in each sequence for every training epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adapting to temporal changes</head><p>A key assumption of many machine learning models is that the input is independent and identically distributed. This is not strictly true in the item recommendation setting since new products will only appear in sessions collected after that product is released, and user behaviour/preferences may also shift over time. Moreover, the purpose of the recommender system is to make prediction on new sequences, i.e. those arising from recent user behaviours. Learning a recommendation model on the entire dataset may, therefore, lead to worse performance because the model ends up focusing on some out-of-date properties that are irrelevant to the latest sequences. One way to handle this is to define a temporal threshold, and discard click sequences that are older than the threshold when building the model. However, this reduces the amount of training data available for our models to learn from. We propose a simple solution to get the best of both worlds via pre-training. We first train a model on the entire dataset. The trained model is then used to initialize a new model, which is only trained using only a more recent subset of the data, e.g. the last month worth of data out of a year of click sequences. This allows the model to have the benefit of a good initialization using large amounts of data, and yet is focused on more recent click-sequences. In this way, it resembles the fine-tuning process used in training of imagebased networks <ref type="bibr" target="#b1">[2]</ref>, where the models are typically initialized by pre-training on ImageNet (a large image classification dataset) before the weights are fine-tuned on a smaller image dataset in the desired domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Use of privileged information</head><p>The item sequence clicked by users after an item may also contain information about that item (highlighted in Figure <ref type="figure" target="#fig_3">3</ref>). This information cannot be used for making predictions since we cannot view the future sequences when making recommendations. We can, however, utilize these future sequences as privileged information <ref type="bibr" target="#b26">[27]</ref> in order to provide soft labels for regularizing and training our models. We use the generalized distillation framework <ref type="bibr" target="#b16">[17]</ref> for this purpose.</p><p>Formally, given a sequence [x1, x2, . . . , xr] with label xr+1 from a session, we define the privileged sequence as x * = [xn, xn−1, . . . , xr+2] where n is the length of the original session before our preprocessing. The privileged sequence is simply the reversed, future sequence that occurs after the r th item. We can now train a teacher model on the privileged sequences x * , with the same label, xr+1.</p><p>Next, we tune our student model M (x) by minimizing a loss function<ref type="foot" target="#foot_0">1</ref> of the form:</p><formula xml:id="formula_1">(1 − λ) * L(M (x), V (xn)) + λ * L(M (x), M * (x * ))</formula><p>, where λ ∈ [0, 1] is a tradeoff parameter between the two sets of labels. This allows M to learn from both the real labels, as well as the labels predicted by its teacher, M * . This learning procedure is useful when the amount of training data available is small, which may be the case for a new, small scale website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Output embeddings for faster predictions</head><p>An issue with the models we have described thus far is the size of the output layer. The output layer 2 is typically fully connected to the previous hidden layer -this means that the number of parameters to be tuned in these two layers alone is H * N where H is the number of nodes in the hidden layer and N is the number of candidate items for prediction. Besides the memory requirements, this also makes prediction slower since the model has to perform an additional large matrix multiplication.</p><p>A similar problem has also been studied in natural language processing, where the output vocabulary can be huge. Typical approaches include the use of a hierarchical softmax layer <ref type="bibr" target="#b18">[19]</ref>, and sampling only the most frequent items. The hierarchical softmax approach does not apply directly in our case, since we are required to make a top-k prediction, rather than just a top-1 prediction.</p><p>We instead view item embeddings as a projection of the items from a 1-HOT encoded space of dimension N onto a lower dimensional space. Using this point of view, we propose to train the model to predict the embedding of the next item directly. The model is tuned using the cosine loss between the embedding of the true output and the predicted embedding. This approach is inspired by the distributed representations of words <ref type="bibr" target="#b17">[18]</ref>, where similar words have embeddings that are closer in cosine distance. We expect, similarly, that the items which a user is likely to click after a given sequence should be close in the item embedding space. Using this type of output reduces the number of parameters in the final layers to H * D, where D is the dimensionality of the embedding. A drawback of this approach is that it requires a good quality embedding for each item. One way to obtain such an embedding is to extract and re-use the trained item embedding from the models described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate our proposed extensions to the basic RNN model on the RecSys Challenge 2015 dataset. The dataset is split following <ref type="bibr" target="#b9">[10]</ref>, where sessions in the last day are placed in the test set, and everything else is placed in the training set. This yields 7966257 sessions in the training set, 15234 sessions in the test set, and 37483 candidate items for prediction. We have 23670981 training sequences after preprocessing the sessions. To better evaluate some of our models, e.g. privileged information and pre-training, we sort the training sequences by time and report our results on models trained on more recent fractions ( 1 256 , 1 64 , 1 16 , 1 4 , 1 1 ) of the training sequences as well.</p><p>We also follow the evaluation procedure of <ref type="bibr" target="#b9">[10]</ref>; each session is input item-by-item to the model, and we calculate the model's ranking of the next item in the session. The rameter T which can be used to control the softness of the softmax labels. 2 Although we focus on the softmax activation function, this also applies for ranking-based outputs. evaluation metrics used were Recall@20 and Mean Reciprocal Rank (MRR)@20. These metrics are designed for the recommendation setting, as we usually want to make multiple recommendations for each user. For M1-M3, we take the top 20 most probable items directly from the softmax outputs. For M4, we compute the cosine distance of the model output against the embedding of items, and take the top 20 closest items 3 . Finally, we also report the model size and batch prediction times for each model. These are important considerations if the model is going to be deployed in a real recommender system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>All our models used 50-dimensional embeddings for the items, with 25% embedding dropout. Optimization was done using Adam <ref type="bibr" target="#b12">[13]</ref>, with mini-batch size fixed at 512. We truncated BPTT using a fixed window of 19 time-steps since 99% of the original training sessions had lengths less than or equal to 19. Sequences shorter than 19 items were padded with zeros for simplicity, the RNN ignores these zeroes. The number of epochs was set by early stopping using 10% of the training data as the validation set for each model. We used one recurrent (GRU) layer in all our models as we found that additional layers did not improve performance. The GRU was set at 100 and 1000 hidden units for each model. The models are defined and trained in Keras <ref type="bibr" target="#b2">[3]</ref> and Theano <ref type="bibr" target="#b25">[26]</ref> on a GeForce GTX Titan Black GPU. The specifics of each model (along with their labels) are as follows:</p><p>M1 The RNN model with softmax outputs, sequence preprocessing and embedding dropout. The recurrent layer is fully connected to the output layer.</p><p>M2 M1, but additionally re-tuned on more recent fractions of the training dataset.</p><p>M3 An M1 model trained on the privileged information (future sequences) available in each data fraction. This is used to provide soft labels for another M1 model with parameters T = 1 and λ = 0.2. We did not extensively tune these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M4</head><p>The output of this model predicts an item embedding directly. We added a fully connected hidden layer between the recurrent and output layers as we found that this improved the model's stability. We used the embeddings trained on the full training dataset in M1 for these models.</p><p>B This refers to the best results reported in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>The performance of each model on the evaluation metrics is summarized in Figure <ref type="figure" target="#fig_4">4</ref>. Overall, M1 and M2 yielded strong performance gains over the reported baseline RNN models. From the results of M1, we also see that training with the entire dataset yields slightly poorer results than training it on more recent fractions of the dataset. This indicates that our recommendation models do need to account for changing user behaviour over time. Our best performing models are reported in Table <ref type="table" target="#tab_0">1</ref>. We also list the baseline results reported in <ref type="bibr" target="#b9">[10]</ref>, including their best RNN based models (i.e., TOP1 and BPR) and two traditional algorithms 3 This can be efficiently computed on the GPU as an additional Theano expression. Model Type (GRU Size) Recall@20 MRR@20 S-POP (-) <ref type="bibr" target="#b9">[10]</ref> 0.2672 0.1775 Item-KNN (-) <ref type="bibr" target="#b9">[10]</ref> 0.5065 0.2048 TOP1 (1000) <ref type="bibr" target="#b9">[10]</ref> 0.6206 0.2693 BPR (1000) <ref type="bibr" target="#b9">[10]</ref> 0 (i.e., S-POP and Item-KNN). Surprisingly, moving from a GRU of 100 to GRU of 1000 did not significantly improve the performance of our models (M1-M3).</p><p>We found that the privileged information model (M3) takes an extremely long time to train; we omitted results for M3 with GRU size 1000 as it could not be trained in reasonable time. We believe the main reason for the drastic increase in training time was the need to (1) compute the soft labels, and (2) compute a corresponding cross-entropy loss against these labels for every mini-batch. This scales very poorly when the number of possible labels is large, as is the case here. Nevertheless, M3 yielded modest performance gains over M1 on the smallest dataset sizes. This is consistent with the use of privileged information in <ref type="bibr" target="#b16">[17]</ref>, and suggests that it might be useful in settings where little data is available.</p><p>Finally, M4 performs poorly compared to our other models in terms of predictive accuracy (although it still improves over the baseline). We may be able to further improve the accuracy in M4 if better quality embeddings were available as targets. We did not, for example, used any additional information of the items, e.g. category or brand, that will be available in an online store.</p><p>On the other hand, the batch prediction time and model sizes are shown in Table <ref type="table">2</ref>. Predictions can be made in M4 using only about 60% of the prediction time of classificationbased models (M1-M3). M4 also has much fewer parameters, and therefore, requires less memory. Together, these are steps towards making RNN models deployable in real recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have presented, and empirically evaluated, several proposed extensions to a basic RNN model. We showed that it is possible to enhance the performance of recurrent models for session-based recommender systems by using proper data augmentation techniques, and accounting for temporal shifts in user behaviour. Directions for future work include exploring the tradeoffs of the embedding-based model, and using known features of the items in our the models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generic structure of the network used in our models. The output layer can either use a softmax or linear activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training procedure for a single sequence in our RNN. Gradients are backpropagated along the grey arrows. The input item sequence (in blue) and target output (in orange) are typically provided in mini-batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. All prefixes of the original input sessions are treated as new training sequences. Given an input training session [x1, x2, . . . , xn], we generate the sequences and corresponding labels ([x1], V (x2)), ([x1, x2], V (x3)), . . . , ([x1, x2, . . . , xn−1], V (xn)) for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example application of our preprocessing step on a session with four clicks. The items in orange are output labels corresponding to their respective training sequences (in grey), and the items with a dotted outline are randomly dropped during training. The privileged information for each preprocessed sequence is coloured in blue, they are not used in the standard training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Plots of both evaluation metrics on models with GRU size 100 (left) and 1000 (right). The x-axis is logarithmic in dataset fraction, the rightmost point corresponds to the full dataset. M2 does not apply to the full dataset, and results for M3 on the larger GRU size were omitted.</figDesc><graphic url="image-1.png" coords="5,40.58,30.67,528.54,264.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Best performing models in our experiments compared against various baselines.</figDesc><table><row><cell></cell><cell>.6322</cell><cell>0.2467</cell></row><row><cell>M4 (1000)</cell><cell>0.6676</cell><cell>0.2847</cell></row><row><cell>M2 (100)</cell><cell>0.7129</cell><cell>0.3091</cell></row><row><cell cols="3">M (GRU Size) Prediction time (s) Parameters</cell></row><row><cell>M1 -M3 (100)</cell><cell>0.665 (± 0.023)</cell><cell>5705384</cell></row><row><cell>M4 (100)</cell><cell>0.366 (± 0.022)</cell><cell>1950150</cell></row><row><cell>M1 -M3 (1000)</cell><cell>0.824 (± 0.025)</cell><cell>42548684</cell></row><row><cell>M4 (1000)</cell><cell>0.485 (± 0.022)</cell><cell>7133250</cell></row><row><cell cols="3">Table 2: Average batch prediction time in seconds</cell></row><row><cell cols="3">and memory requirements for each proposed model</cell></row><row><cell cols="3">(M) at the prediction phase. Prediction times for</cell></row><row><cell cols="3">M4 includes computing the cosine distance of the</cell></row><row><cell cols="3">predicted embedding against each item's embed-</cell></row><row><cell>ding.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The original presentation also includes a temperature pa-</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<idno>CoRR, abs/1512.02595</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Baidu Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Artificial neural networks applied to taxi destination prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Brébisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1508.00021</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>CoRR, abs/1512.05287</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jeff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03643</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-26">2013. May 26-31, 2013. 2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
				<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">August 24-27, 2008. 2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pairwise preference regression for cold-start recommendation</title>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Conference on Recommender Systems, RecSys &apos;09</title>
				<meeting>the Third ACM Conference on Recommender Systems, RecSys &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
				<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International World Wide Web Conference</title>
				<meeting>the Tenth International World Wide Web Conference<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">May 1-5, 2001. 2001</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recommender systems in e-commerce</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Conference on Electronic Commerce, EC &apos;99</title>
				<meeting>the 1st ACM Conference on Electronic Commerce, EC &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="158" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
				<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>prints, abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="544" to="557" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum margin matrix factorization for collaborative ranking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simple and efficient learning using privileged information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1604.01518</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequential click prediction for sponsored search with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1369" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transfer hashing with privileged information</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
