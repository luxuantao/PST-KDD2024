<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Actions: Discriminative Models for Contextual Group Activities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>yangwang@uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weilong</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Actions: Discriminative Models for Contextual Group Activities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A996757365796EAA483BF3349D0C43C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Look at the two persons in Fig. <ref type="figure" target="#fig_1">1</ref>(a), can you tell they are doing two different actions? Once the entire contexts of these two images are revealed (Fig. <ref type="figure" target="#fig_1">1(b)</ref>) and we observe the interaction of the person with other persons in the group, it is immediately clear that the first person is queuing, while the second person is talking. In this paper, we argue that actions of individual humans often cannot be inferred alone. We instead focus on developing methods for recognizing group activities by modeling the collective behaviors of individuals in the group.</p><p>Before we proceed, we first clarify some terminology used throughout the rest of the paper. We use action to denote a simple, atomic movement performed by a single person. We use activity to refer to a more complex scenario that involves a group of people. Consider the examples in Fig. <ref type="figure" target="#fig_1">1(b)</ref>, each frame describes a group activity: queuing and talking, while each person in a frame performs a lower level action: talking and facing right, talking and facing left, etc.</p><p>Our proposed approach is based on exploiting two types of contextual information in group activities. First, the activity of a group and the collective actions of all the individuals serve as context (we call it the group-person interaction) for each other, hence should be modeled jointly in a unified framework. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, knowing the group activity (queuing or talking) helps disambiguate individual human actions which are otherwise hard to recognize. Similarly, knowing most of the persons in the scene are talking (whether facing right or left) allows us to infer the overall group activity (i.e. talking). Second, the action of an individual can also benefit from knowing the actions of other surrounding persons (which we call the person-person interaction). For example, consider Fig. <ref type="figure" target="#fig_1">1(c</ref>). The fact that the first two persons are facing the same direction provides a strong cue that  both of them are queuing. Similarly, the fact that the last two persons are facing each other indicates they are more likely to be talking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work:</head><p>Using context to aid visual recognition has received much attention recently. Most of the work on context is in scene and object recognition. For example, work has been done on exploiting contextual information between scenes and objects <ref type="bibr" target="#b12">[13]</ref>, objects and objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, objects and so-called "stuff" (amorphous spatial extent, e.g. trees, sky) <ref type="bibr" target="#b10">[11]</ref>, etc.</p><p>Most of the previous work in human action recognition focuses on recognizing actions performed by a single person in a video (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>). In this setting, there has been work on exploiting contexts provided by scenes <ref type="bibr" target="#b11">[12]</ref> or objects <ref type="bibr" target="#b9">[10]</ref> to help action recognition. In still image action recognition, object-action context <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> is a popular type of context used for human-object interaction. The work in <ref type="bibr" target="#b2">[3]</ref> is the closest to ours. In that work, person-person context is exploited by a new feature descriptor extracted from a person and its surrounding area.</p><p>Our model is directly inspired by some recent work on learning discriminative models that allow the use of latent variables <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, particularly when the latent variables have complex structures. These models have been successfully applied in many applications in computer vision, e.g. object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, action recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, human-object interaction <ref type="bibr" target="#b5">[6]</ref>, objects and attributes <ref type="bibr" target="#b20">[21]</ref>, human poses and actions <ref type="bibr" target="#b21">[22]</ref>, image region and tag correspondence <ref type="bibr" target="#b19">[20]</ref>, etc. So far only applications where the structures of latent variables are fixed have been considered, e.g. a tree-structure in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. However in our applications, the structures of latent variables are not fixed and have to be inferred automatically.</p><p>Our contributions: In this paper, we develop a discriminative model for recognizing group activities. We highlight the main contributions of our model. (1) Group activity: most of the work in human activity understanding focuses on single-person action recognition. Instead, we present a model for group activities that dynamically decides on interactions among group members. (2) Group-person and person-person interaction: although contextual information has been exploited for visual recognition problems, ours introduces two new types of contextual information that have not been explored before. (3) Adaptive structures: the person-person interaction poses a challenging problem for both learning and inference. If we naively consider the interaction between every pair of persons, the model might try to enforce two persons to have take certain pairs of labels even though these two persons have nothing to do with each other. In addition, selecting a subset of connections allows one to remove "clutter" in the form of people performing irrelevant actions. Ideally, we would like to consider only those person-person interactions that are strong. To this end, we propose to use adaptive structures that automatically decide on whether the interaction of two persons should be considered. Our experimental results show that our adaptive structures significantly outperform other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contextual Representation of Group Activities</head><p>Our goal is to learn a model that jointly captures the group activity, the individual person actions, and the interactions among them. We introduce two new types of contextual information, group-person  interaction and person-person interaction. Group-person interaction represents the co-occurrence between the activity of a group and the actions of all the individuals. Person-person interaction indicates that the action of an individual can benefit from knowing the actions of other people in the same scene. We present a graphical model representing all the information in a unified framework.</p><p>One important difference between our model and previous work is that in addition to learning the parameters in the graphical model, we also automatically infer the graph structures (see Sec. 3).</p><p>We assume an image has been pre-processed (i.e. by running a person detector) so the persons in the image have been found. On the training data, each image is associated with a group activity label, and each person in the image is associated with an action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Formulation</head><p>A graphical representation of the model is shown in Fig. <ref type="figure" target="#fig_3">2</ref>. We now describe how we model an image I. Let I 1 , I 2 , . . . , I m be the set of persons found in the image I, we extract features x from the image I in the form of x = (x 0 , x 1 , . . . , x m ), where x 0 is the aggregation of feature descriptors of all the persons in the image (we call it root feature vector), and x i (i = 1, 2, . . . , m) is the feature vector extracted from the person I i . We denote the collective actions of all the persons in the image as h = (h 1 , h 2 , . . . , h m ), where h i ∈ H is the action label of the person I i and H is the set of all possible action labels. The image I is associated with a group activity label y ∈ Y, where Y is the set of all possible activity labels.</p><p>We assume there are connections between some pairs of action labels (h j , h k ). Intuitively speaking, this allows the model to capture important correlations between action labels. We use an undirected graph G = (V, E) to represent (h 1 , h 2 , . . . , h m ), where a vertex v i ∈ V corresponds to the action label h i , and an edge (v j , v k ) ∈ E corresponds to the interactions between h j and h k .</p><p>We use f w (x, h, y; G) to denote the compatibility of the image feature x, the collective action labels h, the group activity label y, and the graph G = (V, E). We assume f w (x, h, y; G) is parameterized by w and is defined as follows:</p><formula xml:id="formula_0">f w (x, h, y; G) = w Ψ(y, h, x; G) (1a) = w 0 φ 0 (y, x 0 ) + j∈V w 1 φ 1 (x j , h j ) + j∈V w 2 φ 2 (y, h j ) + j,k∈E w 3 φ 3 (y, h j , h k ) (1b)</formula><p>The model parameters w are simply the combination of four parts, w = {w 1 , w 2 , w 3 , w 4 }. The details of the potential functions in Eq. 1 are described in the following:</p><p>Image-Action Potential w 1 φ 1 (x j , h j ): This potential function models the compatibility between the j-th person's action label h j and its image feature x j . It is parameterized as:</p><formula xml:id="formula_1">w 1 φ 1 (x j , h j ) = b∈H w 1b 1(h j = b) • x j (2)</formula><p>where x j is the feature vector extracted from the j-th person and we use 1(•) to denote the indicator function. The parameter w 1 is simply the concatenation of w 1b for all b ∈ H.</p><p>Action-Activity Potential w 2 φ 2 (y, h j ): This potential function models the compatibility between the group activity label y and the j-th person's action label h j . It is parameterized as:</p><formula xml:id="formula_2">w 2 φ 2 (y, h j ) = a∈Y b∈H w 2ab • 1(y = a) • 1(h j = b)<label>(3)</label></formula><p>Action-Action Potential w 3 φ 3 (y, h j , h k ): This potential function models the compatibility between a pair of individuals' action labels (h j , h k ) under the group activity label y, where (j, k) ∈ E corresponds to an edge in the graph. It is parameterized as:</p><formula xml:id="formula_3">w 3 φ 3 (y, h j , h k ) = a∈Y b∈H c∈H w 3abc • 1(y = a) • 1(h j = b) • 1(h k = c)<label>(4)</label></formula><p>Image-Activity Potential w 0 φ 0 (y, x 0 ): This potential function is a root model which measures the compatibility between the activity label y and the root feature vector x 0 of the whole image. It is parameterized as:</p><formula xml:id="formula_4">w 0 φ 0 (y, x 0 ) = a∈Y w 0a 1(y = a) • x 0<label>(5)</label></formula><p>The parameter w 0a can be interpreted as a root filter that measures the compatibility of the class label a and the root feature vector x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning and Inference</head><p>We now describe how to infer the label given the model parameters (Sec. 3.1), and how to learn the model parameters from a set of training data (Sec. 3.2). If the graph structure G is known and fixed, we can apply standard learning and inference techniques of latent SVMs. For our application, a good graph structure turns out to be crucial, since it determines which person interacts (i.e. provides action context) with another person. The interaction of individuals turns out to be important for group activity recognition, and fixing the interaction (i.e. graph structure) using heuristics does not work well. We will demonstrate this experimentally in Sec. 4. We instead develop our own inference and learning algorithms that automatically infer the best graph structure from a particular set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference</head><p>Given the model parameters w, the inference problem is to find the best group activity label y * for a new image x. Inspired by the latent SVM <ref type="bibr" target="#b7">[8]</ref>, we define the following function to score an image x and a group activity label y:</p><formula xml:id="formula_5">F w (x, y) = max Gy max hy f w (x, h y , y; G y ) = max Gy max hy w Ψ(x, h y , y; G y )<label>(6)</label></formula><p>We use the subscript y in the notations h y and G y to emphasize that we are now fixing on a particular activity label y. The group activity label of the image x can be inferred as: y * = arg max y F w (x, y).</p><p>Since we can enumerate all the possible y ∈ Y and predict the activity label y * of x, the main difficulty of solving the inference problem is the maximization over G y and h y according to Eq. 6.</p><p>Note that in Eq. 6, we explicitly maximize over the graph G. This is very different from previous work which typically assumes the graph structure is fixed.</p><p>The optimization problem in Eq. 6 is in general NP-hard since it involves a combinatorial search. We instead use an coordinate ascent style algorithm to approximately solve Eq. 6 by iterating the following two steps:</p><p>1. Holding the graph structure G y fixed, optimize the action labels h y for the x, y pair:</p><formula xml:id="formula_6">h y = arg max h w Ψ(x, h , y; G y )<label>(7)</label></formula><p>2. Holding h y fixed, optimize graph structure G y for the x, y pair:</p><formula xml:id="formula_7">G y = arg max G w Ψ(x, h y , y; G )<label>(8)</label></formula><p>The problem in Eq. 7 is a standard max-inference problem in an undirected graphical model. Here we use loopy belief propagation to approximately solve it. The problem in Eq. 8 is still an NP-hard problem since it involves enumerating all the possible graph structures. Even if we can enumerate all the graph structures, we might want to restrict ourselves to a subset of graph structures that will lead to efficient inference (e.g. when using loopy BP in Eq. 7). One obvious choice is to restrict G to be a tree-structured graph, since loopy BP is exact and tractable for tree structured models. However, as we will demonstrate in Sec. 4, the tree-structured graph built from simple heuristic (e.g. minimum spanning tree) does not work that well. Another choice is to choose graph structures that are "sparse", since sparse graphs tend to have fewer cycles, and loopy BP tends to be efficient in graphs with fewer cycles. In this paper, we enforce the graph sparsity by setting a threshold d on the maximum degree of any vertex in the graph. When h y is fixed, we can formulate an integer linear program (ILP) to find the optimal graph structure (Eq. 8) with the additional constraint that the maximum vertex degree is at most d. Let z jk = 1 indicate that the edge (j, k) is included in the graph, and 0 otherwise. The ILP can be written as:</p><formula xml:id="formula_8">max z j∈V k∈V z jk ψ jk , s.t. j∈V z jk ≤ d, k∈V z jk ≤ d, z jk = z kj , z jk ∈ {0, 1}, ∀j, k<label>(9)</label></formula><p>where we use ψ jk to collectively represent the summation of all the pairwise potential functions in Eq. 1 for the pairs of vertices (j, k). Of course, the optimization problem in Eq. 9 is still hard due to the integral constraint z jk ∈ {0, 1}. But we can relax the value of z jk to a real value in the range of [0, 1]. The solution of the LP relaxation might have fractional numbers. To get integral solutions, we simply round them to the closest integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>Given a set of N training examples x n , h n , y n (n = 1, 2, . . . , N ), we would like to train the model parameter w that tends to produce the correct group activity y for a new test image x. Note that the action labels h are observed on training data, but the graph structure G (or equivalently the variables z) are unobserved and will be automatically inferred. A natural way of learning the model is to adopt the latent SVM formulation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> as follows: min w,ξ≥0,Gy</p><formula xml:id="formula_9">1 2 ||w|| 2 + C N n=1 ξ n (10a) s.t. max G y n f w (x n , h n , y n ; G y n ) -max Gy max hy f w (x n , h y , y; G y ) ≥ ∆(y, y n ) -ξ n , ∀n, ∀y<label>(10b)</label></formula><p>where ∆(y, y n ) is a loss function measuring the cost incurred by predicting y when the groundtruth label is y n . In standard multi-class classification problems, we typically use the 0-1 loss ∆ 0/1 defined as:</p><formula xml:id="formula_10">∆ 0/1 (y, y n ) = 1 if y = y n 0 otherwise (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>The constrained optimization problem in Eq. 10 can be equivalently written as an unconstrained problem:</p><formula xml:id="formula_12">min w,ξ 1 2 ||w|| 2 + C N n=1 (L n -R n ) (<label>12a</label></formula><formula xml:id="formula_13">)</formula><p>where</p><formula xml:id="formula_14">L n = max y max hy max Gy (∆(y, y n ) + f w (x n , h y , y; G y )), R n = max G y n f w (x n , h n , y n ; G y n )(12b)</formula><p>We use the non-convex bundle optimization in <ref type="bibr" target="#b6">[7]</ref> to solve Eq. 12. In a nutshell, the algorithm iteratively builds an increasingly accurate piecewise quadratic approximation to the objective function. During each iteration, a new linear cutting plane is found via a subgradient of the objective function and added to the piecewise quadratic approximation. Now the key issue is to compute two subgradients ∂ w L n and ∂ w R n for a particular w, which we describe in detail below.</p><p>First we describe how to compute ∂ w L n . Let (y * , h * , G * ) be the solution to the following optimization problem: max Then it is easy to show that the subgradient ∂ w L n can be calculated as</p><formula xml:id="formula_15">y max h max G ∆(y, y n ) + f w (x n , h, y; G) (<label>13</label></formula><formula xml:id="formula_16">) (a) (b) (c) (d)</formula><formula xml:id="formula_17">∂ w L n = Ψ(x n , y * , h * ; G * ).</formula><p>The inference problem in Eq. 13 is similar to the inference problem in Eq. 6, except for an additional term ∆(y, y n ). Since the number of possible choices of y is small (e.g.|Y| = 5) in our case), we can enumerate all possible y ∈ Y and solve the inference problem in Eq. 6 for each fixed y.</p><p>Now we describe how to compute ∂ w R n , let Ĝ be the solution to the following optimization problem:</p><formula xml:id="formula_18">max G f w (x n , h n , y n ; G )<label>(14)</label></formula><p>Then we can show that the subgradient ∂ w R n can be calculated as ∂ w R n = Ψ(x n , y n , h n ; Ĝ). The problem in Eq. 14 can be approximately solved using the LP relaxation of Eq. 9. Using the two subgradients ∂ w L n and ∂ w R n , we can optimize Eq. 10 using the algorithm in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate our model on the collective activity dataset introduced in <ref type="bibr" target="#b2">[3]</ref>. This dataset contains 44 video clips acquired using low resolution hand held cameras. In the original dataset, all the persons in every tenth frame of the videos are assigned one of the following five categories: crossing, waiting, queuing, walking and talking, and one of the following eight pose categories: right, frontright, front, front-left, left, back-left, back and back-right. Based on the original dataset, we define five activity categories including crossing, waiting, queuing, walking and talking. We define forty action labels by combining the pose and activity information, i.e. the action labels include crossing and facing right, crossing and facing front-right, etc. We assign each frame into one of the five activity categories, by taking the majority of actions of persons (ignoring their pose categories) in that frame. We select one fourth of the video clips from each activity category to form the test set, and the rest of the video clips are used for training.</p><p>Rather than directly using certain raw features (e.g. the HOG descriptor <ref type="bibr" target="#b3">[4]</ref>) as the feature vector x i in our framework, we train a 40-class SVM classifier based on the HOG descriptor of each individual and their associated action labels. In the end, each feature vector x i is represented as a 40-dimensional vector, where the k-th entry of this vector is the score of classifying this instance to the k-th class returned by the SVM classifier. The root feature vector x 0 of an image is also represented as a 40-dimensional vector, which is obtained by taking an average over all the feature vectors x i (i = 1, 2, ..., m) in the same image.   Importance of group-person interaction: The best result of the baselines comes from no connection between any pair of nodes, which clearly outperforms global bag-of-words. It demonstrates the effectiveness of modeling group-person interaction, i.e. connection between y and h in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of adaptive structures of person-person interaction:</head><p>In Table <ref type="table" target="#tab_0">1</ref>, the pre-defined structures such as the minimum spanning tree and the ε-neighborhood graph do not perform as well as the one without person-person interaction. We believe this is because those pre-defined structures are all based on heuristics and are not properly integrated with the learning algorithm. As a result, they can create interactions that do not help (and sometimes even hurt) the performance. However, if we consider the graph structure as part of our model and directly infer it using our learning algorithm, we can make sure that the obtained structures are those useful for differentiating various activities.</p><p>Evidence for this is provided by the big jump in terms of the performance by our approach.</p><p>We visualize the classification results and the learned structure of person-person interaction of our model in Fig. <ref type="figure" target="#fig_9">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a discriminative model for group activity recognition which jointly captures the group activity, the individual person actions, and the interactions among them. We have exploited two new types of contextual information: group-person interaction and person-person interaction.</p><p>We also introduce an adaptive structures algorithm that automatically infers the optimal structure of person-person interaction in a latent SVM framework. Our experimental results demonstrate that our proposed model outperforms other baseline methods.  We can also take a closer look at the weights within actions of crossing, as shown in (f). we can see that within the crossing category, the model favors seeing the same pose together, indicated by the light regions along the diagonal. It also favors some opposite poses, e.g. back-right with front-left. These make sense since people always cross street in either the same or the opposite directions.</p><p>Crossing Waiting Queuing Walking Talking </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Role of context in group activities. It is often hard to distinguish actions from each individual person alone (a). However, if we look at the whole scene (b), we can easily recognize the activity of the group and the action of each individual. In this paper, we operationalize on this intuition and introduce a model for recognizing group activities by jointly consider the group activity, the action of each individual, and the interaction among certain pairs of individual actions (c).</figDesc><graphic coords="2,183.58,81.86,189.42,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical illustration of the model in (a). The edges represented by dashed lines indicate the connections are latent. Different types of potentials are denoted by lines with different colors in the example shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different structures of person-person interaction. Each node here represents a person in a frame. Solid lines represent connections that can be obtained from heuristics. Dashed lines represent latent connections that will be inferred by our algorithm. (a) No connection between any pair of nodes; (b) Nodes are connected by a minimum spanning tree; (c) Any two nodes within a Euclidean distance ε are connected (which we call ε-neighborhood graph); (d) Connections are obtained by adaptive structures. Note that (d) is the structure of person-person interaction of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Results and Analysis: In order to comprehensively evaluate the performance of the proposed model, we compare it with several baseline methods. The first baseline (which we call global bag-ofwords) is a SVM model with linear kernel based on the global feature vector x 0 with a bag-of-words style representation. The other baselines are within our proposed framework, with various ways of setting the structures of the person-person interaction. The structures we have considered are illustrated in Fig.3(a)-(c), including (a) no pairwise connection; (b) minimum spanning tree; (c) graph obtained by connecting any two vertices within a Euclidean distance ε (ε-neighborhood graph) with ε = 100, 200, 300. Note that in our proposed model the person-person interactions are latent (shown in Fig. 3(d)) and learned automatically. The performance of different structures of person-person in-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Confusion matrices for activity classification: (a) global bag-of-words (b) our approach. Rows are ground-truths, and columns are predictions. Each row is normalized to sum to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the weights across pairs of action classes for each of the five activity classes. Light cells indicate large values of weights. Consider the example (a), under the activity label crossing, the model favors seeing actions of crossing with different poses together (indicated by the area bounded by the red box).We can also take a closer look at the weights within actions of crossing, as shown in (f). we can see that within the crossing category, the model favors seeing the same pose together, indicated by the light regions along the diagonal. It also favors some opposite poses, e.g. back-right with front-left. These make sense since people always cross street in either the same or the opposite directions.</figDesc><graphic coords="8,113.98,211.04,113.38,85.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (Best viewed in color) Visualization of the classification results and the learned structure of personperson interaction. The top row shows correct classification examples and the bottom row shows incorrect examples. The labels C, S, Q, W, T indicate crossing, waiting, queuing, walking and talking respectively. The labels R, FR, F, FL, L, BL, B, BR indicate right, front-right, front, front-left, left, back-left, back and back-right respectively. The yellow lines represent the learned structure of person-person interaction, from which some important interactions for each activity can be obtained, e.g. a chain structure which connects persons facing the same direction is "important" for the queuing activity.</figDesc><graphic coords="8,108.00,544.00,79.20,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of activity classification accuracies of different methods. We report both the overall and mean per-class accuracies due to the class imbalance. The first result (global bag-of-words) is tested in the multi-class SVM framework, while the other results are in the framework of our proposed model but with different structures of person-person interaction. The structures are visualized in Fig.3.teraction are evaluated and compared. We summarize the comparison in Table1. Since the test set is imbalanced, e.g. the number of crossing examples is more than twice that of the queuing or talking examples, we report both overall and mean per-class accuracies. As we can see, for both overall and mean per-class accuracies, our method achieves the best performance. The proposed model significantly outperforms global bag-of-words. The confusion matrices of our method and the baseline global bag-of-words are shown in Fig.4. There are several important conclusions we can draw from these experimental results:</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What are they doing? : Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative models for static human-object interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Structured Models in Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large margin training for hidden markov models with partially observed states</title>
		<author>
			<persName><forename type="first">T.-M.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selection and context for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using the forest to see the trees: A graphicsl model relating features, objects, and scenes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured output regression for detection with partial truncation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative latent model of image region and object tag correspondence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A discriminative latent model of object classes and attributes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grouplet: a structured image representation for recognizing human and object interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
