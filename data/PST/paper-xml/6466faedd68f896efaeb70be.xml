<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge Directionality Improves Learning on Heterophilic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bertrand</forename><surname>Charpentier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franscesco</forename><surname>Di</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Edge Directionality Improves Learning on Heterophilic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outgoing edges. We prove that Dir-GNN matches the expressivity of the Directed Weisfeiler-Lehman test, exceeding that of conventional MPNNs. In extensive experiments, we validate that while our framework leaves performance unchanged on homophilic datasets, it leads to large gains over base models such as GCN, GAT and GraphSage on heterophilic benchmarks, outperforming much more complex methods and achieving new state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have demonstrated remarkable success across a wide range of problems and fields <ref type="bibr" target="#b51">[52]</ref>. Most GNN models, however, assume that the input graph is undirected <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47]</ref>, despite the fact that many real-world networks, such as citation and social networks, are inherently directed. Applying GNNs to directed graphs often involves either converting them to undirected graphs or only propagating information over incoming (or outgoing) edges, both of which may discard valuable information crucial for downstream tasks.</p><p>We believe the dominance of undirected graphs in the field is rooted in two "original sins" of GNNs. First, undirected graphs have symmetric Laplacians which admit orthogonal eigendecomposition. Orthogonal Laplacian eigenvectors act as a natural generalization of the Fourier transform and allow to express graph convolution operations in the Fourier domain. Since some of the early graph neural networks originated from the field of graph signal processing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>, the undirected graph assumption was necessary for spectral GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref> to be properly defined. With the emergence of spatial GNNs, unified with the message-passing framework (MPNNs <ref type="bibr" target="#b14">[15]</ref>), this assumption was not strictly required anymore, as MPNNs can easily be applied to directed graphs by propagating over the directed adjacency, resulting however in information being propagated only in a single direction, Preprint. Under review. Figure <ref type="figure">1</ref>: Extending popular GNN architectures with our Dir-GNN framework to incorporate edgedirectionality information brings large gains (10% to 15%) on heterophilic datasets (left), while leaving performance mostly unchanged on homophilic datasets (right). The plots illustrate the average performance over all datasets, while the full results are presented in Sec. <ref type="bibr" target="#b6">7</ref>.</p><p>at the risk of discarding useful information from the opposite one. However, early works empirically observed that making the graph undirected consistently leads to better performance on established node-classification benchmarks, which historically have been mainly homophilic graphs such as Cora and Pubmed <ref type="bibr" target="#b40">[41]</ref>, where neighbors tend to share the same label. Consequently, converting input graphs to undirected ones has become a standard part of the dataset preprocessing pipeline, to the extent that the popular GNN library PyTorch-Geometric <ref type="bibr" target="#b13">[14]</ref> includes a general utility function that automatically makes graphs undirected when loading datasets <ref type="foot" target="#foot_0">1</ref> .</p><p>The key observation in this paper is that while accounting for edge directionality indeed does not help in homophilic graphs, it can bring extensive gains in heterophilic settings (Fig. <ref type="figure">1</ref>), where neighbors tend to have different labels. In the rest of the paper, we study why and how to use directionality to improve learning on heterophilic graphs.</p><p>Contributions. Our contributions are the following: ? We show that considering the directionality of a graph substantially increases its effective homophily in heterophilic settings, with negligible or no impact on homophilic settings (see Sec. 3). ? We propose a novel and generic Directed Graph Neural Network framework (Dir-GNN) which extends any MPNN to work on directed graphs by performing separate aggregations of the incoming and outgoing edges. Moreover, we show that Dir-GNN leads to more homophilic aggregations compared to their undirected counterparts (see Sec. 4). ? Our theoretical analysis establishes that Dir-GNN is as expressive as the Directed Weisfeiler-Lehman test, while being strictly more expressive than MPNNs (see Sec. 4.1). ? We empirically validate that augmenting popular GNN architectures with the Dir-GNN framework yields large improvements on heterophilic benchmarks, achieving state-of-the-art results and outperforming even more complex methods specifically designed for such settings. Moreover, this enhancement does not negatively impact the performance on homophilic benchmarks (see Sec. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We consider a directed graph G = (V, E) with a node set V of n nodes, and an edge set E of m edges. We define its respective directed adjacency matrix A ? {0, 1} n?n where a ij = 1 if (i, j) ? E and zero otherwise, its respective undirected adjacency matrix A u where (a u ) ij = 1 if (i, j) ? E or (j, i) ? E and zero otherwise. In this paper, we focus on the task of (semi-supervised) node classification on an attributed graph G with node features arranged into the n ? d matrix X and node labels y i ? {1, ..., C}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Homophily and Heterophily in Undirected Graphs</head><p>In this Section, we first review the heterophily metrics for undirected graphs, while in Sec. <ref type="bibr" target="#b2">3</ref> we propose heterophily metrics adapted to directed graphs. Most GNNs are based on the homophily assumption for undirected graphs, i.e., that neighboring nodes tend to share the same labels. While a reasonable assumption in some settings, it turns out not to be true in many important applications such as gender classification on social networks or fraudster detection on e-commerce networks.</p><p>Homophily metrics. Several metrics have been proposed to measure homophily on undirected graphs. Node homophily is defined as</p><formula xml:id="formula_0">h = 1 |V | i?V j:(i,j)?E I[y i = y j ] d i<label>(1)</label></formula><p>where I[y i = y j ] is the indicator function with value 1 if y i = y j or zero otherwise. Intuitively, node homophily measures the proportion of neighbors with the same label as the node itself, averaged across all nodes. However, since heterophily is a complex phenomenon which is hard to capture with only a single scalar, a better representation of a graph's homophily is the C ? C class compatibility matrix H <ref type="bibr" target="#b52">[53]</ref>, capturing the fraction of edges from nodes with label k to nodes with label l:</p><formula xml:id="formula_1">h kl = |(i, j) ? E : y i = k ? y j = l| |(i, j) ? E : y i = k| .</formula><p>Homophilic datasets are expected to have most of the mass of their compatibility matrices concentrated in the diagonal, as most of the edges are between nodes of the same class (e.g. see Citeseer-Full in Fig. <ref type="figure" target="#fig_2">4</ref>). Conversely, heterophilic datasets will have most of the mass away from the diagonal of the compatibility matrix (e.g. see Chameleon in Fig. <ref type="figure" target="#fig_2">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Message Passing Neural Network</head><p>In this Section, we first review the Message Passing Neural Network (MPNN) paradigm for undirected graphs, while in Sec. 4 we extend this formalism to directed graphs. An MPNN is a parametric model which iteratively applies aggregation maps AGG (k) and combination maps COM (k) to compute embeddings x</p><p>i for node i based on messages m</p><p>i containing information on its neighbors. Namely, the k-th layer of an MPNN is given by</p><formula xml:id="formula_4">m (k) i = AGG (k) { {(x (k-1) j , x (k-1) i ) : (i, j) ? E} } x (k) i = COM (k) x (k-1) i , m (k) i (2)</formula><p>where { {?} } is a multi-set. The aggregation maps AGG (k) and the combination maps COM (k) are learnable (usually a small neural network) and their different implementations result in specific architectures (e.g. graph convolutional neural networks (GCN) use linear aggregation, graph attention networks (GAT) use attentional layers, etc.). After the last layer K, the node representation x (K) i is mapped into the C-probability simplex via a (learnable) decoding step, often given by an MLP. Independent of the choice of AGG and COM, all MPNNs only send messages along the edges of the graph, which make them particularly suitable for tasks where edges do encode a notion of similarityas it is the case when adjacent nodes in the graph often share the same label (homophilic). Conversely, MPNNs tend to struggle in the scenario where they need to separate a node embedding from that of its neighbours <ref type="bibr" target="#b33">[34]</ref>, often a challenging problem that has gained attention in the community and which we discuss in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Heterophily in Directed Graphs</head><p>In this Section, we discuss how accounting for directionality can be particularly helpful for dealing with heterophilic graphs. By leveraging the directionality information, we argue that even standard MPNNs that are traditionally thought to struggle in the heterophilic regime, can in fact perform extremely well.</p><p>Weighted homophily metrics. First, we extend the homophily metrics introduced in Section 2.1 to account for directed edges and higher-order neighborhoods. Given a possibly directed and weighted n ? n message-passing matrix S, we define the weighted node homophily as ). The last column reports the gain in effective homophily obtained by using the directed graph as opposed to the undirected graph.</p><formula xml:id="formula_5">h(S) = 1 |V | i?V j?V s ij I[y i = y j ] j?V s ij (3) Au A 2 u h (eff) u A A A A AA h (eff) d h (eff</formula><p>Accordingly, by taking S = A and S = A respectively, we can compute the node homophily based on outgoing or incoming edges. Similarly, we can also take S to be any weighted 2-hop matrix associated with a directed graph (see details below) and compute its node homophily.</p><p>We can also extend the construction to edge-computations by defining the C ? C weighted compatibility matrix H(S) of a message-passing matrix S as</p><formula xml:id="formula_6">h kl (S) = i,j?V :yi=k?yj =l s ij i,j?V :yi=k s ij<label>(4)</label></formula><p>As above, one can take S = A or S = A to derive the compatibility matrix associated with the out and in-edges, respectively.</p><p>Effective homophily. Stacking multiple layers of a GNN effectively corresponds to taking powers of diffusion matrices, resulting in message propagation over higher-order hops. If higher-order hops exhibit increased homophily, exploring the graph through layers can prove beneficial for the task. Zhu et al. <ref type="bibr" target="#b52">[53]</ref> noted that for heterophilic graphs, the 2-hop tends to be more homophilic than the 1-hop. Consequently, we introduce the concept of effective homophily as the maximum weighted node homophily observable at any hop of the graph.</p><p>For directed graphs, there exists an exponential number of k-hops. For instance, four 2-hop matrices can be considered: the squared operators A<ref type="foot" target="#foot_1">2</ref> and (A ) 2 , which correspond to following the same forward or backward edge direction twice, as well as the non-squared operators AA and A A, representing the forward/backward and backward/forward edge directions. Given a graph G, we define its effective homophily as follows:</p><formula xml:id="formula_7">h (eff) = max k?1 max C?B k , h(C)<label>(5)</label></formula><p>where B k denotes the set of all k-hop matrices for a graph. If G is undirected, B k contains only A k .</p><p>In our empirical analysis, we will focus on the 2-hop matrices, as computing higher-order k-hop matrices becomes intractable for all but the smallest graphs 2 .</p><p>Leveraging directionality to enhance effective homophily. We observe that converting a graph from directed to undirected results in lower effective homophily for heterophilic graphs, while the impact on homophilic graphs is negligible (refer to the last column of Tab. 1). Specifically, AA and A A emerge as the most homophilic diffusion matrices for heterophilic graphs. In fact, the average relative gain of effective homophily, h</p><p>gain , when using directed graphs compared to undirected ones is only around 3% for homophilic datasets, while it is almost 30% for heterophilic datasets. We further validate this observation on synthetic directed graphs exhibiting various levels of homophily, generated through a modified preferential attachment process (see Appendix D.1 for more details). Fig. <ref type="figure">2a</ref> displays the results: the directed graph consistently demonstrates higher effective homophily compared to its undirected counterpart, with the gap being particularly prominent for lower node homophily levels. The minimal effective homophily gain on homophilic datasets further substantiates the traditional practice of using undirected graphs for benchmarking GNNs, as the datasets were predominantly homophilic until recently. Real-world example. We illustrate the concept of effective homophily in heterophilic directed graphs by taking the concrete task of predicting the publication year of papers based on a directed citation network such as Arxiv-Year <ref type="bibr" target="#b25">[26]</ref>. In this case, the different 2-hop neighbourhoods have very different semantics: the diffusion operator (A 2 ) i represents papers that are cited by those papers that paper i cites. As these 2-hop neighboring papers were published further in the past relative to paper i, they do not offer much information about the publication year of i. On the other hand, (A A) i represents papers that share citations with paper i. Papers cited by the same sources are more likely to have been published in the same period, so the diffusion operator A A is expected to be more homophilic. The undirected 2-hop operator</p><formula xml:id="formula_9">A 2 u = ( 1 2 (A + A )) 2 = 1 4 (A 2 + (A ) 2 + AA + AA )</formula><p>is the average of the four directed 2-hops. Therefore, the highly homophilic matrix A A is diluted by the inclusion of (A 2 ), leading to a less homophilic operator overall.</p><p>Harmless heterophily through directions. Ma et al. showed that heterophily is not necessarily harmful for GNNs, as long as nodes with the same label share similar neighborhood patterns, and different classes have distinguishable patterns. We find that some directed datasets, such as Arxiv-Year and Snap-Patents, show this form of harmless heterophily when treated as directed, and instead manifest harmful heterophily when made undirected (see Fig. <ref type="figure" target="#fig_4">5</ref> in the Appendix). This suggests that using directionality can be beneficial also when using only one layer, as we confirm empirically (see Fig. <ref type="figure">9</ref> in the Appendix).</p><p>Toy example. We further illustrate the concepts presented in this Section with the toy example in Fig. <ref type="figure">3</ref>, which shows a directed graph with three classes (blue, orange, green). Despite the graph being maximally heterophilic, it presents harmless heterophily, since different classes have very different neighborhood patterns that can be observed clearly from the compatibility matrix of A (b). When the graph is made undirected (c), we are corrupting this information, and the classes become less distinguishable, making the task harder. We also note that both A A and AA presents perfect homophily (d), while A 2 u does not, in line with the discussion in previous paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Directed Graph Neural Network</head><p>In this Section, we extend the class of MPNNs to directed graphs, and refer to such generalization as Directed Graph Neural Network (Dir-GNN). We follow the scheme in Eq. ( <ref type="formula">2</ref>), meaning that the update of a node feature is the result of a combination of its previous state and an aggregation over its neighbours. Crucially though, the characterization of neighbours now has to account for the edgedirectionality. Accordingly, given a node i ? V , we separate aggregations over the in-neighbours (j ? i) and the out-neighbours (i ? j) respectively:</p><formula xml:id="formula_10">m (k) i,? = AGG (k) ? { {(x (k-1) j , x (k-1) i ) : (j, i) ? E} } m (k) i,? = AGG (k) ? { {(x (k-1) j , x (k-1) i ) : (i, j) ? E} } x (k) i = COM (k) x (k-1) i , m (k) i,? , m (k) i,? .<label>(6)</label></formula><p>The idea behind our framework is that given any MPNN, we can readily adapt it to the directed case by choosing how to aggregate over both directions. For this purpose, we replace the neighbour aggregator AGG (k) with two separate in-and out-aggregators AGG (k) ? and AGG (k) ? , which can have independent sets of parameters for the two aggregation phases and, possibly, different normalizations as discussed below -however the functional expression in both cases stays the same. As we will show, accounting for both directions separately is fundamental for both the expressivity of the model (see Sec. 4.1) and its empirical performance (see Sec. 6).</p><p>Extension of common architectures. To make our discussion more explicit, we describe extensions of popular MPNNs where the aggregation map is computed by m (k) i = (Sx (k-1) ) i , where S ? R n?n is a message-passing matrix. In GCN <ref type="bibr" target="#b21">[22]</ref>,</p><formula xml:id="formula_11">S = D -1/2 A u D -1/2</formula><p>, where D is the degree matrix of the undirected graph. In the case of directed graphs, two message-passing matrices S ? and S ? are required for in-and out-neighbours respectively. Additionally, the normalization is slightly more subtle, since we now have two different diagonal degree matrices D ? and D ? containing the in-degrees and out-degrees respectively. Accordingly, we propose a normalization of the form</p><formula xml:id="formula_12">S ? = D -1/2 ? AD -1/2 ? i.e. (S ? ) ij = a ij / d ? i d ? j .</formula><p>To motivate this choice, note that the normalization modulates the aggregation based on the out-degree of i and the in-degree of j as one would expect given that we are computing a message going from i to j. We can then take S ? = S ? and write the update at layer k of Dir-GCN as</p><formula xml:id="formula_13">X (k) = ? S ? X (k-1) W (k) ? + S ? X (k-1) W (k) ? ,<label>(7)</label></formula><p>for learnable channel-mixing matrices</p><formula xml:id="formula_14">W (k) ? , W (k)</formula><p>? and with ? a pointwise activation map. Finally, we note that in our implementation of Dir-GNN we use an additional learnable or tunable parameter ? allowing the framework to weight one direction more than the other (a convex combination), depending on the dataset. Dir-GNN extensions of GAT <ref type="bibr" target="#b46">[47]</ref> and GraphSAGE <ref type="bibr" target="#b16">[17]</ref> can be found in Appendix B.</p><p>Dir-GNN leads to more homophilic aggregations. Since our main application amounts to relying on the graph-directionality to mitigate heterophily, here we comment on how information is iteratively propagated in a Dir-GNN, generally leading to an aggregation scheme beneficial on most real-world directed heterophilic graphs. We focus on the Dir-GCN formulation, however the following applies to any other Dir-GNN up to changing the message-passing matrices. Consider a 2-layer Dir-GCN as in Eq. ( <ref type="formula" target="#formula_13">7</ref>), and let us remove the pointwise activation ?<ref type="foot" target="#foot_2">3</ref> . Then, the node representation can be written as</p><formula xml:id="formula_15">X (2) = A 2 ? X (0) W (1) ? W (2) ? + (A ? ) 2 X (0) W (1) ? W (2) ? + A ? A ? X (0) W (1) ? W (2) ? + A ? A ? X (0) W (1) ? W (2) ? .</formula><p>We observe that when we aggregate information over multiple layers, the final node representation is derived by also computing convolutions over 2-hop matrices A ? A ? and A ? A ? . From the discussion in Sec. 3, we deduce that this framework may be more suited to handle heterophilic graphs since generally such 2-hop matrices are more likely to encode similarity than A 2 ? , (A ? ) 2 or A 2 uthis is validated empirically on real-world datasets in Sec. 7.</p><p>Advantages of two-directional updates. We discuss the benefits of incorporating both directions in the layer update, as opposed to using a single direction. Although spatial MPNNs can be adapted to directed graphs by simply utilizing A instead of A u -resulting in message propagation only along out-edges-relying on a single direction presents three primary drawbacks. First, if the layer update only considers one direction, the exploration of the multi-hop neighbourhoods through powers of diffusion operators would not include the mixed terms AA and A A, which have been shown to be particularly beneficial for heterophilic graphs in Sec. 3. Second, by using only one direction we disregard the graph entirely for nodes where the out-degree is zero <ref type="foot" target="#foot_3">4</ref> . This phenomenon frequently occurs in real-world graphs, as reported in Tab. 9. Incorporating both directions in the layer update helps mitigate this problem, as it is far less common for a node to have both in-and out-degree to be zero, as also illustrated in Tab. 9. Third, limiting the update to a single direction reduces expressivity, as we discuss in Sec. 4.1.</p><p>Complexity. The complexity of Dir-GNN depends on the specific instantiation of the framework. Dir-GCN, Dir-Sage, and Dir-GAT maintain the same per-layer computational complexity as their undirected counterparts (O(md + nd 2 ) for GCN and GraphSage, and O(md 2 ) for GAT). However, they have twice as many parameters, owing to their separate weight matrices for in-and out-neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Expressive power of Dir-GNN</head><p>It is a well known result that MPNNs are bound in expressivity by the 1-WL test, and that it is possible to construct MPNN models which are as expressive as the 1-WL test <ref type="bibr" target="#b49">[50]</ref>. In this section, we show that Dir-GNN is the optimal way to extend MPNNs to directed graphs. We do so by proving that Dir-GNN models can be constructed to be as expressive as an extension of the 1-WL test to directed graphs <ref type="bibr" target="#b15">[16]</ref>, referred to as D-WL (for a formal definition, see Appendix C.1). Additionally, we illustrate its greater expressivity over more straightforward approaches, such as converting the graph to its undirected form and utilizing a standard MPNN (MPNN-U) or applying an MPNN that propagates solely along edge direction (MPNN-D) <ref type="foot" target="#foot_4">5</ref> . Formal statements for the theorems in this section along with their proofs can be found in Appendix C.</p><formula xml:id="formula_16">Theorem 4.1 (Informal). Dir-GNN is as expressive as D-WL if AGG (k) ? , AGG (k)</formula><p>? , and COM (k) are injective for all k.</p><p>A discussion of how a Dir-GNN can be parametrized to meet these conditions (similarly to what is done in Xu et al. <ref type="bibr" target="#b49">[50]</ref>) can be found in Appendix C.4. Theorem 4.2 (Informal). Dir-GNN is strictly more expressive than both MPNN-U and MPNN-D.</p><p>Intuitively, the theorem states that while all directed graphs distinguished by MPNNs are also separated by Dir-GNNs, there also exist directed graphs separated by the latter but not by the former. This holds true for MPNNs applied both on the directed and undirected graph. We observe these theoretical findings to be in line with the empirical results detailed in Appendix E and Tab. 10, where Dir-GNN performs comparably or better (typically in the case of heterophily) than MPNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>GNNs for directed graphs. While several papers have alluded to the extension of their spatial models to directed graphs, empirical validation has not been conducted <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. On the other hand, various approaches have been developed to generalize spectral convolutions for directed graphs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. Of particular interest are DiGCN <ref type="bibr" target="#b42">[43]</ref>, which uses Personalized Page Rank matrix as a generalized Laplacian and incorporates k-hop diffusion matrices, and MagNet <ref type="bibr" target="#b50">[51]</ref>, which adopts a complex matrix for graph diffusion where the real and imaginary parts represent the undirected adjacency and edge direction respectively. The above spectral methods share the following limitations: 1) in-neighbors and out-neighbors share the same weight matrix, which restricts expressivity; 2) they are specialized models, often inspired by GCN, as opposed to broader frameworks; 3) their scalability is severely limited due to their spectral nature.</p><p>GNNs for relational graphs. Our Dir-GCN model can be considered as a Relational Graph Convolutional Network (R-GCN) <ref type="bibr" target="#b39">[40]</ref> applied to an augmented relational graph that incorporates two </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on Synthetic Data</head><p>Setup. In order to show the limits of current MPNNs, we design a synthetic task where the label of a node depends on both its in-and out-neighbors: it is one if the mean of the scalar features of their in-neighbors is greater than the mean of the features of their out-neighbors, or zero otherwise (more details in Appendix D.2). We report the results using GraphSage as base MPNN, but similar results were obtained with GCN and GAT and reported in Fig. <ref type="figure">8</ref> of the Appendix. We compare GraphSage on the undirected version of the graph (Sage), with three Dir-GNN extensions of GraphSage using different convex combination coefficients ?: Dir-Sage(? = 0) (only considering in-edges), Dir-Sage(? = 1) (only considering out-edges) and Dir-Sage(? = 0.5) (considering both in-and out-edges equally).</p><p>Results. The results show that only Dir-Sage(?=0.5), which accounts for both directions, is able to almost perfectly solve the task. Using only in-or out-edges results in around 75% accuracy, whereas GraphSage on the undirected graph is no better than a random classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments on Real-World Datasets</head><p>Datasets. We evaluate on the task of node classification on several directed benchmark datasets with varying levels of homophily: Citesee-Full, Cora-ML <ref type="bibr" target="#b6">[7]</ref>, OGBN-Arxiv <ref type="bibr" target="#b19">[20]</ref>, Chameleon, Squirrel <ref type="bibr" target="#b34">[35]</ref>, Arxiv-Year, Snap-Patents <ref type="bibr" target="#b25">[26]</ref> and Roman-Empire <ref type="bibr" target="#b35">[36]</ref> (refer to Tab. 7 for dataset statistics). While the first three are mainly homophilic (edge homophily greater than 0.65), the last five are highly heterophilic (edge homophily smaller than 0.24). Refer to Appendix D.3 for more details on the experimental setup and on dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Extending Popular GNNs with Dir-GNN</head><p>Setup. We evaluate the gain of extending popular undirected GNN architectures (GCN <ref type="bibr" target="#b21">[22]</ref>, Graph-Sage <ref type="bibr" target="#b16">[17]</ref> and GAT <ref type="bibr" target="#b46">[47]</ref>) with our framework. For this ablation, we use the same hyperparameters (provided in Appendix D.4) for all models and datasets. The aggregated results are plotted in Fig. <ref type="figure">1</ref>, while the raw numbers are reported in Sec. 7. For Dir-GNN, we take the best results out of ? ? {0, 0.  Results. On heterophilic datasets, using directionality brings exceptionally large gains (10% to 20% absolute) in accuracy across all three base GNN models. On the other hand, on homophilic datasets using directionality leaves the performance unchanged or slightly hurts. This is in line with the findings of Tab. 1, which shows that using directionality as in our framework generally increases the effective homophily of heterophilic datasets, while leaving it almost unchanged for homophilic datasets. The inductive bias of undirected GNNs to propagate information in the same way in both directions is beneficial on homophilic datasets where edges encode a notion of class similarity. Moreover, averaging information across all your neighbors, independent of direction, leads to a low-pass filtering effect that is indeed beneficial on homophilic graphs <ref type="bibr" target="#b33">[34]</ref>. In contrast, Dir-GNN has to learn to align in-and out-convolutions since they have independent weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Comparison with State-of-the-Art Models</head><p>Setup. Given the importance of directionality on heterophilic tasks, we compare Dir-GNN with simple baselines: MLP and GCN <ref type="bibr" target="#b21">[22]</ref>, heterophilic state-of-the-art models: H 2 GCN <ref type="bibr" target="#b52">[53]</ref>, GPR-GNN <ref type="bibr" target="#b9">[10]</ref>, LINKX <ref type="bibr" target="#b25">[26]</ref>, FSGNN <ref type="bibr" target="#b30">[31]</ref>, ACM-GCN <ref type="bibr" target="#b26">[27]</ref>, GloGNN <ref type="bibr" target="#b23">[24]</ref>, Gradient Gating <ref type="bibr" target="#b36">[37]</ref>, and state-of-the-art models for directed graphs: DiGCN <ref type="bibr" target="#b42">[43]</ref> and MagNet <ref type="bibr" target="#b50">[51]</ref>. Appendix D.6 contains more details on how baseline results were obtained. Differently from the results in Sec. 7, we now tune the hyperparameters of our model using a grid search (see Appendix D.5 for the exact ranges).</p><p>Results. We observe that Dir-GNN obtains new state-of-the-art results on all four heterophilic datasets, outperforming complex methods which were specifically designed to tackle heterophily. This results suggest that, when present, using the edge direction can significantly improve learning on heterophilic graphs, justifying the title of the paper. In contrast, discarding it is so harmful that not even complex architectures can make up for this loss of information. We further note that DiGCN and MagNet, despite being specifically designed for directed graphs, struggle on Squirrel and Chameleon. This is due to their inability to selectively aggregate from one direction while disregarding the other, a strategy that proves particularly advantageous for these two datasets (see Tab. 10). Our proposed Dir-GNN framework overcomes this limitation thanks to its distinct weight matrices and the flexibility provided by the ? parameter, enabling selective directional aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced Dir-GNN, a generic framework to extend any spatial graph neural network to directed graphs, which we prove to be strictly more expressive than MPNNs. We showed that treating the graph as directed improves the effective homophily of heterophilic datasets, and validated empirically that augmenting popular GNN architectures with our framework results in large improvements on heterophilic benchmarks, while leaving performance almost unchanged on homophilic benchmarks. Surprisingly, we found simple instantiations of our framework to obtain state-of-the-art results on the five directed heterophilic benchmarks we experimented on, outperforming recent architectures developed specifically for heterophilic settings as well as previously proposed methods for directed graphs.</p><p>Limitations. Our research has several areas that could be further refined and explored. First, the theoretical exploration of the conditions that lead to a higher effective homophily in directed graphs compared to their undirected counterparts is still largely unexplored. Furthermore, we have yet to investigate the expressivity advantage of Dir-GNN in the specific context of heterophilic graphs, where empirical gains were most pronounced. Finally, we haven't empirically investigated different functional forms for aggregating in-and out-edges. These aspects mark potential areas for future enhancements and investigations.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Compatibility Matrices</head><p>Fig. <ref type="figure" target="#fig_2">4</ref> shows the compatibility matrices for both Citeseer-Full (homophilic) and Chameleon (heterophilic). Additionally, Fig. <ref type="figure" target="#fig_4">5</ref> presents the weighted compatibility matrices of the undirected diffusion operator A u and the two directed diffusion operators A and A for Arxiv-Year. The last two have rows (classes) which are much more distinguishable then the first, despite still being heterophilic. This phenomen, called harmless heterophily, is discussed in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extension of Popular GNNs</head><p>We first consider an extension of GraphSAGE <ref type="bibr" target="#b16">[17]</ref> using our Dir-GNN framework. The main choice reduces to that of normalization. In the spirit of GraphSAGE, we require the message-passing matrices A ? and A ? to both be row-stochastic. This is done by taking A ? = D -1 ? A and A ? = D -1 ? A , respectively. In this case, the directed version of GraphSAGE becomes</p><formula xml:id="formula_17">X (k) = ?(X (k-1) ? (k) + D -1 ? AX (k-1) W (k) ? + D -1 ? A X (k-1) W (k) ? )</formula><p>. Finally, we consider the generalization of GAT <ref type="bibr" target="#b46">[47]</ref> to the directed case. Here we simply compute attention coefficients over the in-and out-neighbours separately. If we denote the attention coefficient over the edge (i, j) by ? ij , then the update of node i at layer k can be computed as</p><formula xml:id="formula_18">h (k) i = ?( (i,j)?E ? ? ij W (k) ? h (k) j + (j,i)?E ? ? ji W (k) ? h (k) j ),</formula><p>where ? ? , ? ? are both row-stochastic matrices with support given by A and A , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis of Expressivity</head><p>In this appendix we prove the expressivity results reported in Sec. 4.1 after restating them more formally. We start by introducing useful concepts which will be instrumental to our discussion. In our analysis, we will assume all nodes to have constant scalar node features c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 (Directed) Weisfeiler-Lehman Test</head><p>The 1-dimensional Weisfeiler-Lehman algorithm (1-WL), or color refinement, is a heuristic approach to the graph isomorphism problem, initially proposed by Weisfeiler and Leman <ref type="bibr" target="#b47">[48]</ref>. This algorithm is essentially an iterative process of vertex labeling or coloring, aimed at identifying whether two graphs are non-isomorphic.</p><p>Starting with an identical coloring of the vertices in both graphs, the algorithm proceeds through multiple iterations. In each round, vertices with identical colors are assigned different colors if their respective sets of equally-colored neighbors are unequal in number. The algorithm continues this process until it either reaches a point where the distribution of vertices colors is different in the two graphs or converges to the same distribution. In the former case, the algorithm concludes that the graphs are not isomorphic and halts. Alternatively, the algorithm terminates with an inconclusive result: the two graphs are 'possibly isomorphic'. It has been shown that this algorithm cannot distinguish all non-isomorphic graphs <ref type="bibr" target="#b8">[9]</ref>.</p><p>Formally, given an undirected graph G = (V, E), the 1-WL algorithm calculates a node coloring C (t) : V (G) ? N for each iteration t &gt; 0, as follows:</p><formula xml:id="formula_19">C (t) (i) = RELABEL C (t-1) (i), { {C (t-1) (j) : j ? N (i)} } (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>where RELABEL is a function that injectively assigns a unique color, not used in previous iterations, to the pair of arguments. The function N (i) represents the set of neighbours of i.</p><p>Since we deal with directed graphs, it is necessary to extend the 1-WL test to accommodate directed graphs. We note that a few variants have been proposed in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Here, we focus on a variant whereby in-and out-neighbours are treated separately, as discussed in <ref type="bibr" target="#b15">[16]</ref>. This variant, which we refer to as D-WL, refines colours as follows:</p><formula xml:id="formula_21">D (t) (i) = RELABEL D (t-1) (i), { {D (t-1) (j) : j ? N ? (i)} }, { {D (t-1) (j) : j ? N ? (i)} } (9)</formula><p>where N ? (i) and N ? (i) are the set of out-and in-neighbors of i, respectively. Our first objective is to demonstrate that Dir-GNN is as expressive as D-WL. Establishing this will enable us to further show that Dir-GNN is strictly more expressive than an MPNN operating on either the directed or undirected version of a graph. Let us start by introducing some further auxiliary tools that will be used in our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Expressiveness and color refinements</head><p>A way to compare graph models (or algorithms) in their expressiveness is by contrasting their discriminative power, that is the ability they have to disambiguate between non-isomorphic graphs.</p><p>Two graphs are called isomorphic whenever there exists a graph isomorphism between the two:</p><formula xml:id="formula_22">Definition C.1 (Graph isomorphism). Let G 1 = (V 1 , E 1 ), G 2 = (V 2 , E 2 ) be two (directed) graphs. An isomorphism between G 1 , G 2 is a bijective map ? : V 1 ? V 2 which preserves adjacencies, that is: ?u, v ? V 1 : (u, v) ? E 1 ?? (?(u), ?(v)) ? E 2 .</formula><p>On the contrary, they are deemed non-isomorphic when such a bijection does not exist. A model that is able to discriminate between two non-isomorphic graphs assigns them distinct representations. This concept is extended to families of models as follows: Definition C.2 (Graph discrimination). Let G = (V, E) be any (directed) graph and M a model belonging to some family M. Let G 1 and G 2 be two graphs. We say</p><formula xml:id="formula_23">M discriminates G 1 , G 2 iff M (G 1 ) = M (G 2 ). We write G 1 = M G 2 .</formula><p>If there exists such a model M ? M, then family M distinguishes between the two graphs and we write</p><formula xml:id="formula_24">G 1 = M G 2 .</formula><p>Families of models can be compared in their expressive power in terms of graph disambiguation: Definition C.3 (At least as expressive). Let M 1 , M 2 be two model families. We say M 1 is at least as expressive as</p><formula xml:id="formula_25">M 2 iff ?G 1 = (V 1 , E 1 ), G 2 = (V 2 , E 2 ), G 1 = M2 G 2 =? G 1 = M1 G 2 . We write M 1 M 2 .</formula><p>Intuitively, M 1 is at least as expressive as M 2 if when M 2 discriminates a pair of graphs, also M 1 does. Additionally, a family can be strictly more expressive than another: Definition C.4 (Strictly more expressive). Let M 1 , M 2 be two model families. We say M 1 is strictly more expressive than</p><formula xml:id="formula_26">M 2 iff M 1 M 2 ? M 2 M 1 . Equivalently, M 1 M 2 ? ?G 1 = (V 1 , E 1 ), G 2 = (V 2 , E 2 ), s.t. G 1 = M1 G 2 ? G 1 = M2 G 2 .</formula><p>Intuitively, M 1 is strictly more expressive than M 2 if M 1 is at least as expressive as M 2 and there exist pairs of graphs that M 1 distinguishes but M 1 does not.</p><p>Many graph algorithms and models operate by generating node colorings or representations. These can be gathered into multisets of colors that are compared to assess whether two graphs are nonisomorphic. Other than convenient, in these cases it is interesting to characterise the discriminative power at the level of nodes by means of the concept of color refinement <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref>. As an example, for any t ? 0 it can be shown that, on any graph, the coloring generated by the 1-WL algorithm at round t + 1 refines that at round t, that is C (t+1) C (t) ; this being essentially due to the injectivity property of the RELABEL function.</p><p>Importantly, as we were anticipating above, this concept can be directly translated into graph discrimination as long as graphs are represented by the multiset of their vertices' colors, or an injection thereof. This link, which explains the use of the same symbol to refer to the concepts of color refinement and discriminative power, is explicitly shown, for example, in Bevilacqua et al. <ref type="bibr" target="#b2">[3]</ref>, Bodnar et al. <ref type="bibr" target="#b4">[5]</ref>. More concretely, it can be shown that, if coloring D refines coloring C, then the algorithm which generates D is at least as expressive as the one generating C, as long as multisets of node colours are directly compared to discriminate between graphs, or they are first encoded by a multiset injection before the comparison is carried out. In the following we will resort to the concept of color refinement to prove some of our theoretical results. This approach is not only practically convenient for the required derivations, but it also informs us on the discriminative power models have at the level of nodes, something which is of relevance to us given our focus on node-classification tasks.</p><p>Furthermore, even though Dir-GNN outputs node-wise embeddings, it can be augmented with a global readout function to generate a single graph-wise embeddings x G = READOUT { {x</p><formula xml:id="formula_27">(K) i : i ? V } } .</formula><p>We will assume that all models discussed in this section are augmented with a global readout function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 MPNNs on Directed Graphs</head><p>Before moving forward to prove our expressiveness results, let us introduce the families of architectures we compare with. These embody straightforward approaches to adapt MPNNs to directed graphs.</p><p>Let MPNN-D be a model that performs message-passing by only propagating messages in accordance with the directionality of edges. Its layers can be defined as follows:</p><formula xml:id="formula_28">m (k) i = AGG (k) { {(x (k-1) j , x (k-1) i ) : j ? N ? (i)} } x (k) i = COM (k) x (k-1) i , m (k) i (10)</formula><p>Instead, let MPNN-U be a model which propagates messages equally along any incident edge, independent of their directionality. Its layers can be defined as follows:</p><formula xml:id="formula_29">m (k) i = AGG (k) { {(x (k-1) j , x (k-1) i ) : j ? N ? (i)} } ? { {(x (k-1) j , x (k-1) i ) : j ? N ? (i)} } x (k) i = COM (k) x (k-1) i , m (k) i (11)</formula><p>Note that if there are no bi-directional edges, MPNN-U is equivalent to first converting the graph to its undirected form (where the edge set is redefined as E (u) = {(i, j) : (i, j) ? E ? (j, i) ? E}) and then running an undirected MPNN( Eq. ( <ref type="formula">2</ref>)). In practice, we observe that the number of bi-directional edges is generally small on average, while extremely small on specific datasets (see Tab. 7). In these cases, we expect the empirical performance of the two approaches to be close to each other. We remark that, in our experiments, we opt for the latter strategy as it is easier and more efficient to implement.</p><p>We can now formally define families for the models we will be comparing. Definition C.6 (Model families). Let M MPNN-D be the family of Message Passing Neural Networks on the directed graph (Eq. ( <ref type="formula">10</ref>)), M MPNN-U that of Message Passing Neural Networks on the undirected form of the graph (Eq. ( <ref type="formula">11</ref>)), and M Dir-GNN that of Dir-GNN models (Eq. ( <ref type="formula" target="#formula_10">6</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Comparison with D-WL</head><p>We start by restating Theorem 4.1 more formally:</p><formula xml:id="formula_30">Theorem C.7. M Dir-GN N is as expressive as D-WL if AGG (k)</formula><p>? , AGG (k) ? , and COM (k) are injective for all k and node representations are aggregated via an injective READOUT function.</p><p>We now prove the theorem by showing that D-WL and Dir-GNN (under the hypotheses of the theorem) are equivalent in their expressive power. We will show this in terms of color refinement and, in particular, by showing that, not only the D-WL coloring at any round t refines that induced by any Dir-GNN at the same time step, but also that, when Dir-GNN's components are injective, the opposite holds.</p><p>Proof of Theorem C.7. Let us begin by showing that Dir-GNN is upper-bounded in expressive power by the D-WL test. We do this by showing that, at any t ? 0, the D-WL coloring D (t) refines the coloring induced by the representations of any Dir-GNN, that is, on any graph</p><formula xml:id="formula_31">G = (V, E), ?v, w ? V, D (t) (v) = D (t) (w) =? h (t) v = h (t)</formula><p>w , where h We proceed by induction. The base step trivially holds for t = 0 given how nodes are initialised. As for the recursion step, let us assume the thesis hold for t &gt; 0; we seek to prove it also hold for t + 1, showing that ?v, w ? V, D (t+1</p><formula xml:id="formula_32">) (v) = D (t+1) (w) =? h (t+1) v = h (t+1) w . D (t+1) (v) = D (t+1) (w)</formula><p>implies the equality of the inputs of the RELABEL function given it is injective. That is:</p><formula xml:id="formula_33">D (t) (v) = D (t) (w), { {D (t) (u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} }, and { {D (t) (u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} }.</formula><p>By the induction hypothesis, we immediately get h</p><formula xml:id="formula_34">(t) v = h (t)</formula><p>w . Also, the induction hypothesis, along with [3, Lemma 2], gives us: { {h</p><formula xml:id="formula_35">(t) u : u ? N ? (v)} } = { {h (t) u : u ? N ? (w)} }, and { {h (t) u : u ? N ? (v)} } = { {h (t) u : u ? N ? (w)} }. Given that h (t) v = h (t) w = h, we also have { {(h (t) u , h (t) v ) : u ? N ? (v)} } = { {(h (t) u , h (t) w ) : u ? N ? (w)} }, and { {(h (t) u , h (t) v ) : u ? N ? (v)} } = { {(h (t) u , h (t)</formula><p>w ) : u ? N ? (w)} }: it would be sufficient, for example, to construct the well-defined function ? : h ? (h, h) and invoke [3, <ref type="bibr">Lemma 3]</ref>. These all represents the only inputs to a Dir-GNN layer -the two AGG (t) and the COM (t) function in particular. Being well defined functions, they must return equal outputs for equal inputs, so that h</p><formula xml:id="formula_36">(t+1) v = h (t+1) w .</formula><p>In a similar way, we show that, when AGG and COM functions are injective, the opposite hold, that is, ?v, w ? V, h</p><formula xml:id="formula_37">(t) v = h (t) w =? D (t) (v) = D (t) (w)</formula><p>. The base step holds for t = 0 for the same motivations above. Let us assume the thesis holds for t &gt; 0 and seek to show that for t + 1, ?v, w ? V, h</p><formula xml:id="formula_38">(t+1) v = h (t+1) w =? D (t+1) (v) = D (t+1) (w). If h (t+1) v = h (t+1) w , then COM (t) h (t) v , m (t) v,? , m (t) v,? = COM (t) h (t) w , m (t) w,? , m (t) w,? . As COM (t) is injective, it must also hold h (t) v = h (t)</formula><p>w , which, by the induction hypothesis, gives D (t) (v) = D (t) (w). Furthermore, by the same argument, we must also have m</p><formula xml:id="formula_39">(t) v,? = m (t)</formula><p>w,? , and m</p><formula xml:id="formula_40">(t) v,? = m (t) w,? .</formula><p>At this point we recall that, for any node v, m</p><formula xml:id="formula_41">(t) v,? = AGG (t) ? ({ {(h (t) u , h (t) v ) : u ? N ? (v)} }) and m (t) v,? = AGG (t) ? ({ {(h (t) u , h (t) v ) : u ? N ? (v)} })</formula><p>, where, by our assumption, AGG (t) ? , AGG (t) ? are injective. This implies the equality between the multisets in input, i.e. { {(h</p><formula xml:id="formula_42">(t) u , h (t) v ) : u ? N ? (v)} } = { {(h (t) u , h (t) w ) : u ? N ? (w)} }, and { {(h (t) u , h (t) v ) : u ? N ? (v)} } = { {(h (t) u , h (t) w ) : u ? N ? (w)} }.</formula><p>From these equalities it clearly follows { {h <ref type="bibr">Lemma 3]</ref> with the well defined function ? : (h 1 , h 2 ) ? h 1 . Again, by the induction hypothesis, and [3, Lemma 2], we have { {D (t) </p><formula xml:id="formula_43">(t) u : u ? N ? (v)} } = { {h (t) u : u ? N ? (w)} }, and { {h (t) u : u ? N ? (v)} } = { {h (t) u : u ? N ? (w)} } -one can invoke [3,</formula><formula xml:id="formula_44">(u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} } and { {D (t) (u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} }.</formula><p>Finally, as all and only inputs to the RELABEL function are equal, D (t+1) (v) = D (t+1) (w). The proof then terminates: as the READOUT function is assumed to be injective, having proved the refinement holds at the level of nodes, this is enough to also state that, if two graphs are distinguished by D-WL they are also distinguished by a Dir-GNN satisfying the injectivity assumptions above.</p><p>As for the existence and implementation of these injective components, constructions can be found in Xu et al. <ref type="bibr" target="#b49">[50]</ref> and Corso et al. <ref type="bibr" target="#b10">[11]</ref>. In particular, in [50, <ref type="bibr">Lemma 5]</ref>, the authors show that, for a countable X , there exist maps f : X ? R n such that function h : X ? x?X f (x) is injective for subsets X ? X of bounded cardinality, and any multiset function g can be decomposed as g(X) = ? x?X f (x) for some function ?. As X is countable, there always exists an injection Z : x ? N, and function f can be constructed, for example, as f (x) = N -Z(x) , with N being the maximum (bounded) cardinality of subsets X ? X . These constructions are used to build multiset aggregators in the GIN architecture <ref type="bibr" target="#b49">[50]</ref> when operating on features from a countable set and neighbourhoods of bounded size. Under the same assumptions, the same constructions can be readily adapted to express the aggregators AGG (k)  ? , AGG (k) ? as well as READOUT in our Dir-GNN. Similarly to the above, under the same assumptions, injective maps for elements (c, X), c ? X , X ? X can be constructed as h(c, X) = (1 + )f (c) + x?X f (x) for infinitely many choice of , including all irrational numbers, and any function g on couples (c, X) can be decomposed as <ref type="bibr">Corollary 6]</ref>. The same approach can be extended to our use-case. In fact, for irrationals X , Y , an injection on triple (c, X, Y ) (with c ? X , X, Y ? X of bounded size and X countable) can be built as</p><formula xml:id="formula_45">g(c, X) = ? (1 + )f (c) + x?X f (x) [50,</formula><formula xml:id="formula_46">h(c, X, Y ) = (1 + X )f X (c) + x?X f X (x), (1 + Y )f Y (c) + y?Y f Y (y)</formula><p>, where is an injection on a countable set and</p><formula xml:id="formula_47">(1 + X )f X (c) + x?X f X (x), (1 + Y )f Y (c) + y?Y f Y (y)</formula><p>realise injections over couples (c, X), (c, Y ) as described above. This construction can be used to express the COM (k) components of Dir-GNN. In practice, in view of the Universal Approximation Theorem (UAT) <ref type="bibr" target="#b18">[19]</ref>, Xu et al. <ref type="bibr" target="#b49">[50]</ref> propose to use Multi-Layer Perceptrons (MLPs) to learn the required components described above, functions f and ? in particular. We note that, in order to resort to the original statement of the UAT, this approach additionally requires boundedness of set X itself. Similar practical parameterizations can be used to build our desired Dir-GNN layers. Last, we refer readers to <ref type="bibr" target="#b10">[11]</ref> for constructions which can be adopted in the case where initial node features have a continuous, uncountable support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Comparison with MPNNs</head><p>In this subsection we prove Theorem 4.2, which we restate more formally and split into two separate parts, one regarding MPNN-D and the other regarding MPNN-U. We start by proving that Dir-GNN is stricly more expressive than MPNN-D, i.e an MPNN which operates on directed graphs by only propagating messages according to the directionality of edges:</p><formula xml:id="formula_48">Theorem C.8. M Dir-GNN is strictly more powerful than M MPNN-D .</formula><p>We begin by first proving the following lemmas: Lemma C.9. M Dir-GNN is at least as expressive as M MPNN-D (M Dir-GNN M MPNN-D ).</p><p>Proof of Lemma C.9. We prove this Lemma by noting that the Dir-GNN architecture generalizes that of an MPNN-D, so that a Dir-GNN model can (learn to) simulate a standard MPNN-D by adopting particular weights. Specifically, Dir-GNN defaults to MPNN-D (which only sends messages along the out edges) if COM (k) x</p><formula xml:id="formula_49">(k-1) i , m (k) i,? , m (k) i,? = COM (k) x (k-1) i , m<label>(k)</label></formula><p>i,? , i.e. COM ignores in-messages, and the two readout modules coincide. Importantly, the direct implication of the above Proof of Lemma C.10. Let G 1 and G 2 be the non-isomorphic graphs illustrated in Fig. <ref type="figure" target="#fig_7">6</ref>. To confirm that they are not isomorphic, simply note that node 1 in G 1 has an in-degree of two, while no node in G 2 has an in-degree of two.</p><p>To prove that no MPNN-D model can distinguish between the two graphs, we will show that any MPNN-D induce the same coloring for the two graphs. In particular, we will show that, if C (t) (v) refers to the representation a MPNN-D computes for node v at time step t, then</p><formula xml:id="formula_50">C (t) (1) = C (t) (2) = C (t) (5) = C (t) (6) and C (t) (3) = C (t) (4) = C (t) (7) = C (t) (8) for any t ? 0.</formula><p>We proceed by induction. The base step trivially holds for t = 0 given that nodes are all initialised with the same color. As for the inductive step, let us assume that the statement holds for t and prove that it also holds for t + 1. Assume C (t) (1) = C (t) (2) = C (t) (5) = C (t) (6) and C (t) (3) = C (t) (4) = C (t) (7) = C (t) (8) (induction hypothesis). Then we have:</p><formula xml:id="formula_51">C (t+1) (1) = COM (t) C (t) (1), AGG (t) ({ {} }) C (t+1) (2) = COM (t) C (t) (2), AGG (t) ({ {} }) C (t+1) (5) = COM (t) C (t) (5), AGG (t) ({ {} }) C (t+1) (6) = COM (t) C (t) (6), AGG (t) ({ {} })</formula><p>The induction hypothesis then gives us that <ref type="bibr" target="#b5">(6)</ref>. As for the other nodes, we have:</p><formula xml:id="formula_52">C (t+1) (1) = C (t+1) (2) = C (t+1) (5) = C (t+1)</formula><formula xml:id="formula_53">C (t+1) (3) = COM (t) C (t) (3), AGG (t) ({ {(C (t) (3), C (t) (1))} }) C (t+1) (4) = COM (t) C (t) (4), AGG (t) ({ {(C (t) (4), C (t) (1))} }) C (t+1) (7) = COM (t) C (t) (7), AGG (t) ({ {(C (t) (7), C (t) (5))} }) C (t+1) (8) = COM (t) C (t) (8), AGG (t) ({ {(C (t) (8), C (t) (6))} })</formula><p>The induction hypothesis then gives us that C (t+1) (3) = C (t+1) (4) = C (t+1) (7) = C (t+1) <ref type="bibr" target="#b7">(8)</ref>. Importantly, the above holds for any parameters of the COM (t) and AGG (t) functions. As any</p><formula xml:id="formula_54">ITERATION NODE 1 NODE 2 NODE 3 NODE 4 NODE 5 NODE 6 NODE 7 NODE 8 1 A A A A A A A A 2 B C C D E E C C M (v) RELABEL(M(v)) INITIALIZE A (A, { {} }, { {(A, A), (A, A)} }) B (A, { {(A, A)} }, { {} }) C (A, { {} }, { {} }) D (A, { {} }, { {(A, A)} }) E</formula><p>Table <ref type="table">4</ref>: Node colorings at different iterations, as well as the RELABEL hash function, when applying D-WL to the two graphs in Fig. <ref type="figure" target="#fig_7">6</ref>. MPNN-D will always compute the same set of node representations for the two graphs, it follows that no MPNN-D can disambiguate between the two graphs, no matter the way they are aggregated.</p><p>To conclude our proof, we show that there exists Dir-GNN models that can discriminate the two graphs. In view of Theorem 4.1, it is enough to show that the two graphs are disambiguated by D-WL. Applying D-WL to the two graphs leads to different colorings after two iterations (see Tab. 4), so the D-WL algorithm terminates deeming the two graphs non-isomorphic. Then, by Theorem 4.1, there exist Dir-GNNs which distinguish them. In fact, it is easy to even construct simple 1-layer architecture that can assign the two graphs distinct representations, an exercise which we leave to the reader. Importantly, note how Dir-GNN can distinguish between the two graphs hinging on the discrimination of non-isomorphic nodes such as 1, 2, something no MPNN-D is capable of doing.</p><p>With the two results above prove Theorem C.8.</p><p>Proof of Theorem C.8. The theorem follows directly from Lemmas C.9 and C.10.</p><p>Next, we focus on the comparison with MPNN-U, i.e. an MPNN on the undirected form of the graph:</p><p>Theorem C.11. M Dir-GNN is strictly more expressive than M MPNN-U .</p><p>Instrumental to us is to consider a variant of the 1-WL test MPNN-U can be regarded as the neural counterpart of. In the following we will show that such a variant, which we call U-WL, generates colorings which refine the ones induced by any MPNN-U and that, in turn, are refined by the D-WL test. In view of Theorem 4.1, this will be enough to show that there exists Dir-GNNs refining any MPNN-U instantiation, so that, ultimately, M Dir-GNN M MPNN-U .</p><p>Lemma C.12. M Dir-GNN is at least as expressive as M MPNN-U (M Dir-GNN M MPNN-U ).</p><formula xml:id="formula_55">ITERATION NODE 1 NODE 2 NODE 3 NODE 4 NODE 5 NODE 6 1 A A A A A A 2 B B B C B D M (v) RELABEL(M(v)) INITIALIZE A (A, { {(A, A)} }, { {(A, A)} }) B (A, { {(A, A), (A, A)} }, { {} }) C (A, { {} }, { {(A, A), (A, A)} }) D</formula><p>Table <ref type="table">5</ref>: Node colorings at different iterations, , as well as the RELABEL hash function, when applying D-WL to the two graphs in Fig. <ref type="figure" target="#fig_8">7</ref>.</p><p>Proof of Lemma C.12. Let us start by introducing the U-WL test, which, on an undirected graph, refines node colors as:</p><formula xml:id="formula_56">A (t+1) (v) = RELABEL A (t) (v), { {A (t) (u) : u ? N ? (v)} } ? { {A (t) (u) : u ? N ? (v)} } ,</formula><p>that is, by the gathering neighbouring colors from each incident edge, independent of its direction.</p><p>It is easy to show that U-WL generates a coloring that, at any round t ? 0 is refined by the coloring generated by D-WL, i.e., for any graph</p><formula xml:id="formula_57">G = (V, E) it holds that ?v, w ? V, D (t) (v) = D (t) (w) =? A (t) (v) = A (t) (w)</formula><p>, where D refers to the coloring of D-WL. Again, proceeding by induction, we have the following. First, the base step hold trivially for t = 0. We assume the thesis holds true for t and seek to show it also holds for t + 1. If D (t+1) (v) = D (t+1) (w) then, by the injectivity of RELABEL we must have</p><formula xml:id="formula_58">D (t) (v) = D (t) (w), which implies A (t) (v) = A (t) (w)</formula><p>via the induction hypothesis. Additionally, we have </p><formula xml:id="formula_59">{ {D (t) (u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} }, and { {D (t) (u) : u ? N ? (v)} } = { {D (t) (u) : u ? N ? (w)} } which,</formula><formula xml:id="formula_60">?,v = { {A (t) (u) : u ? N ? (v)} } = { {A (t) (u) : u ? N ? (w)} } = A (t)</formula><p>?,w , and</p><formula xml:id="formula_61">A (t) ?,v = { {A (t) (u) : u ? N ? (v)} } = { {A (t) (u) : u ? N ? (w)} } = A (t) ?,w .</formula><p>From these equalities we then derive</p><formula xml:id="formula_62">A (t) ?,v ? A (t) ?,v = A (t) ?,w ? A (t)</formula><p>?,w . Indeed, let us suppose that, instead,</p><formula xml:id="formula_63">A (t) ?,v ? A (t) ?,v = A (t) ?,w ? A (t)</formula><p>?,w and that, w.l.o.g., this is due by the existence of a color ? such that its number of appearances in A</p><formula xml:id="formula_64">(t) ?,w ? A (t) ?,w is larger than that in A (t) ?,v ? A (t) ?,v . We write # A (t) ?,w ?A (t) ?,w (?) &gt; # A (t) ?,v ?A (t)</formula><p>?,v (?). Then, as these are all multisets, we can rewrite</p><formula xml:id="formula_65"># A (t) ?,w (?) + # A (t) ?,w (?) &gt; # A (t) ?,v (?) + # A (t) ?,v (?). Since A (t) ?,w = A (t) ?,v , we must have that # A (t) ?,w (?) = # A (t) ?,v<label>(</label></formula><p>?), which leads to necessarily having # A (t) ?,w (?) = # A (t) ?,v (?). However, this entails a contradiction, because by hypothesis we had that A</p><formula xml:id="formula_66">(t) ?,w = A (t) ?,v . Last, given that A (t) (v) = A (t) (w), and A (t) ?,v ? A (t) ?,v = A (t) ?,w ? A (t)</formula><p>?,w , being the only inputs to the RELABEL function in U-WL, then we also have that A (t+1) (v) = A (t+1) (w), concluding the proof of the refinement. Now, in view of Theorem 4.1, it is sufficient to show that U-WL refines the coloring induced by any MPNN-U; the lemma will then follow by transitivity. We want to show that t ? 0, the U-WL coloring A (t) refines the coloring induced by the representations of any MPNN-U, that is, on any</p><formula xml:id="formula_67">graph G = (V, E), ?v, w ? V, A (t) (v) = A (t) (w) =? h (t) v = h (t) w , with h (t)</formula><p>v referring to the representation an MPNN-U assigns to node v at time step t. The thesis is easily proved. It clearly holds for t = 0 if initial node representations are produced by a well-defined function enc : c ? enc(c). Then, if we assume the thesis holds for t &gt; 0, we can show it also holds for t + 1. Indeed, if A (t+1) (v) = A (t+1) (w), from the injectivity of RELABEL, it follows that A (t) (v) = A (t) (w) and A</p><formula xml:id="formula_68">(t) ?,v ? A (t) ?,v = A (t) ?,w ? A (t)</formula><p>?,w . By the induction hypothesis, we have h</p><formula xml:id="formula_69">(t) v = h (t)</formula><p>w and, jointly due to [3, Lemma 2], { {h</p><formula xml:id="formula_70">(t) u : u ? N ? (v)} } ? { {h (t) u : u ? N ? (v)} } = { {h (t) u : u ? N ? (w)} } ? { {h (t) u : u ? N ? (w)} }. Given that h (t) v = h (t) w = h, it also clearly holds that { {(h (t) u , h (t) v ) : u ? N ? (v)} } ? { {(h (t) u , h (t) v ) : u ? N ? (v)} } = { {(h (t) u , h (t) w ) : u ? N ? (w)} } ? { {(h (t) u , h (t) w ) : u ? N ? (w)} } -it is sufficient to construct the well-defined function ? : h ? (h, h) and invoke [3, Lemma 3]. These are 1 A A A A A A A A 2 E E E E E E E E M (v) RELABEL(M(v)) INITIALIZE A (A, { {(A, A)} }, { {(A, A), (A, A)} }) E</formula><p>Table <ref type="table">6</ref>: Node colorings at different iterations, as well as the RELABEL hash function, when applying U-WL to the two graphs in Fig. <ref type="figure" target="#fig_8">7</ref>. the inputs to the well-defined functions constituting the update equations of an MPNN-U architecture, eventually entailing h</p><formula xml:id="formula_71">(t+1) v = h<label>(t+1) w</label></formula><p>. Now, with the following lemma we show that, not only M Dir-GNN is at least as expressive as M MPNN-U , there actually exist pairs of graphs distinguished by former family but not by the latter. Lemma C.13. There exist graph pairs distinguished by a Dir-GNN model which are not distinguished by any MPNN-U model.</p><p>Proof of Lemma C.13. Let G 1 and G 2 be the non-isomorphic graphs illustrated in Fig. <ref type="figure" target="#fig_8">7</ref>. From Tab. 6 we observe that U-WL is not able to distinguish between the two graphs, as after the first iteration all nodes still have the same color: the U-WL is at convergence and terminates concluding that the two graphs are possibly isomorphic. From Lemma C.12, we conclude that no MPNN-U can distinguish between the two graphs. On the other hand, applying D-WL to the two graphs leads to different colorings after two iterations (see Tab. 5, so the D-WL algorithm terminates deeming the two graphs non-isomorphic. Then, by Theorem 4.1, there exists Dir-GNNs which distinguish them. In fact, simple Dir-GNN architectures which distinguish the two graphs are easy to construct. Importantly, we note, again, how these architectures distinguish between the two graphs by disambiguating non-isomorphic nodes such as 4, 6, something no MPNN-U is capable of doing.</p><p>Last, the two results above are sufficient to prove Theorem C.11.</p><p>Proof of Theorem C.11. The theorem follows directly from Lemmas C.12 and C.13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details D.1 Effective Homophily of Synthetic Graphs</head><p>For the results in Fig. <ref type="figure">2a</ref>, we generate directed synthetic graphs with various homophily levels using a modified preferential attachment process <ref type="bibr" target="#b1">[2]</ref>, inspired by Zhu et al. <ref type="bibr" target="#b52">[53]</ref>. New nodes are incrementally added to the graphs until the desired number of nodes is achieved. Each node is assigned a class label, chosen uniformly at random among C classes, and forms out-edges with exactly m pre-existing nodes, where m is a parameter of the process. The m out-neighbors are sampled without replacement from a distribution that is proportional to both their in-degree and the class compatibility of the two nodes. Consequently, nodes with higher in-degree are more likely to receive new edges, leading to a "rich get richer" effect where a small number of highly connected "hub" nodes emerge. This results in the in-degree distribution of the generated graphs following a power-law, with heterophily controlled by the class compatibility matrix H. In our experiments, we generate graphs comprising 1000 nodes and set C = 5, m = 2. Note that by construction, the generated graphs will not have any bidirectional edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Synthetic Experiment</head><p>For the results in Fig. <ref type="figure">2b</ref>, we construct an Erdos-Renyi graph with 5000 nodes and edge probability of 0.001, where each node has a scalar feature sampled uniformly at random from [-1, 1]. The label of a node is set to 1 if the mean of the features of their in-neighbors is greater than the mean of the features of their out-neighbors, or zero otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Experimental Setup</head><p>Real-World datasets statistics are reported in table <ref type="table" target="#tab_6">7</ref>. All experiments are conducted on a GCP machine with 1 NVIDIA V100 GPU with 16GB of memory, apart from experiments on snap-patents which have been performed on a machine with 1 NVIDIA A100 GPU with 40GB of memory. The total GPU time required to conduct all the experiments presented in this paper is approximately two weeks. In all experiments, we use the Adam optimizer and train the model for 10000 epochs, using early stopping on the validation accuracy with a patience of 200 for all datasets apart from Chameleon and Squirrel, for which we use a patience of 400. We do not use regularization as it did not help on heterophilic datasets. For Citeseer-Full and Cora-ML we use random 50/25/25 splits, for OGBN-Arxiv we use the fixed split provided by OGB <ref type="bibr" target="#b19">[20]</ref>, for Chameleon and Squirrel we use the fixed GEOM-GCN splits <ref type="bibr" target="#b34">[35]</ref>, for Arxiv-Year and Snap-Patents we use the splits provided in Lim et al., while for Roman-Empire we use the splits from Platonov et al. <ref type="bibr" target="#b35">[36]</ref>. We report the mean and standard deviation of the test accuracy, computed over 10 runs in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Directionality Ablation Hyperparameters</head><p>For the ablation study in Sec. 7.1, we use the same hyperparameters for all models and datasets: learning_rate = 0.001, hidden_dimension = 64, num_layers = 3, norm = T rue, jk = max. norm refers to applying L2 normalization after each convolutional layer, which we found to be generally useful, while jk refers to the type of jumping knowledge <ref type="bibr" target="#b48">[49]</ref> used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Comparison with State-of-the-Art Results</head><p>To obtain the results for Dir-GNN in Tab.  Table <ref type="table" target="#tab_0">10</ref>: Ablation study comparing base MPNNs on the undirected graph versus their Dir-GNN extensions on the directed graph. We conducted experiments with ? = 0 (only in-edges), ? = 1 (only out-edges), and ? = 0.5 (both in-and out-edges, but with different weight matrices). For homophilic datasets (to the left of the dashed line), incorporating directionality does not significantly enhance or may slightly impair performance. However, for heterophilic datasets (to the right of the dashed line), the inclusion of directionality substantially improves accuracy.</p><p>are taken from their paper <ref type="bibr" target="#b30">[31]</ref> for Actor, Squirrel and Chameleon, whereas we re-implement it to generate results on Arxiv-year and Snap-Patents, performing the same gridsearch outlined in Appendix D.5. Results for GloGNN are taken from their paper <ref type="bibr" target="#b23">[24]</ref>. Results on Roman-Empire are taken from Platonov et al. <ref type="bibr" target="#b35">[36]</ref> for GCN, H 2 GCN, GPR-GNN, FSGNN and GloGNN whereas we re-implement and generate results for MLP, LINKX, ACM-GCN and Gradient Gating performing the same gridsearch outlined in Appendix D.5.</p><p>Directed GNNs For DiGCN and MagNet, we used the classes provided by PyTorch Geometric Signed Directed library <ref type="bibr" target="#b17">[18]</ref>. For MagNet, we tuned the learning_rate ? {0.01, 0.005, 0.001, 0.0005}, the hidden_dim ? {32, 64, 128, 256, 512}, the num_layers ? {2, 3, 4, 5, 6}, the K parameter for its chebyshev convolution to ? {1, 2}, and its q hyperparameter ? {0, 0.05, 0.10, 0.15, 0.20}. For DiGCN, we tune the learning_rate ? {0.01, 0.005, 0.001, 0.0005}, the hidden_dim ? {32, 64, 128, 256, 512}, the num_layers ? {2, 3, 4, 5, 6}, and the ? ? {0.05, 0.10, 0.15, 0.20}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Synthetic Experiment</head><p>We also evaluate GCN and GAT (and their Dir-GNN extensions) on the synthetic task outlined in Appendix D.2. Similarly to what observed for GraphSage (Fig. <ref type="figure">2b</ref>), the Dir-GNN variant using both directions (? = 0.5) significantly outperforms the other configurations, despite not reaching 100% accuracy. The undirected models are akin to a random classifier, whereas the models using only one directions obtain between 70% and 75% of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Ablation Study on Using Directionality</head><p>Table <ref type="table" target="#tab_0">10</ref> compares using the undirected graph vs using the directed graph with our framework with different ?. We observe that only on Chameleon and Squirrel, using only one direction of the edges, in particular the out direction, performs better than using both direction. Moreover, for these two </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2305.10498v1 [cs.LG] 17 May 2023</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: In our synthetic experiments, we observe the following: (a) the effective homophily of directed graphs is consistently higher compared to their undirected counterparts. Interestingly, this gap widens for graphs that are less homophilic. (b) When examining the performance of GraphSage and its Dir-GNN extensions on a synthetic task requiring directionality information, only Dir-Sage(?=0.5), which utilizes information from both directions, is capable of solving the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Compatibility matrices for the undirected version of Citeseer-Full (homophilic, left) and Chameleon (heterophilic, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Weighted compatibility matrices of the undirected diffusion operator A u and the two directed diffusion operators A and A for Arxiv-Year. The last two have rows (classes) which are much more distinguishable then the first, despite still being heterophilic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition C. 5 (</head><label>5</label><figDesc>Color refinement). Let G = (V, E) be a graph and C, D two coloring functions. Coloring D refines colouring C when ?v, w ? V, D(v) = D(w) =? C(v) = C(w). Essentially, when D refines C, if any two nodes are assigned the same color by D, the same holds for C. Equivalently, if two nodes are distinguished by C (because they are assigned different colors), then they are also distinguished by D. When, for any graph, D refines C, then we write D C and, when also the opposite holds, (C D), we then write D ? C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>v</head><label></label><figDesc>refers to the representation of node v in output from any Dir-GNN at layer t &gt; 0. For t = 0 nodes are populated with a constant color: ?v ? V : D (0) v = c, or an appropriate encoding thereof in the case of the Dir-GNN h (0) v = enc(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two non-isomorphic directed graphs that cannot be distinguished by any MPNN-D model but can be distinguished by Dir-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two non-isomorphic directed graphs that cannot be distinguished by any MPNN-U model but can be distinguished by Dir-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Dir-GNN test accuracy on Arxiv-Year for different values of the hyperparameter ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Weighted node homophily for different diffusion matrices, and effective homophily for both undirected (h</figDesc><table><row><cell>)</cell></row><row><cell>gain</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study comparing base MPNNs on the undirected graphs versus their Dir-GNN extension on the directed graphs. Homophilic datasets, located to the left of the dashed line, show little to no improvement when incorporating directionality, sometimes even experiencing a minor decrease in performance. Conversely, heterophilic datasets, found to the right of the dashed line, demonstrate large accuracy improvements when directionality is incorporated into the model. relation types: one for the original edges and another for the inverse edges added to the graph. It is important to note that although the R-GCN paper addresses (multi-relational) directed graphs, it does not include inverse edges, and thus, only performs aggregation from a single direction. Later papers then proposed the idea of incorporating inverse edges<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. Our Dir-GNN framework, however, extends these methods by introducing more versatile AGG and COMB functions. Moreover, we are also the first to study the relation between directionality and homophily.</figDesc><table><row><cell></cell><cell></cell><cell>HOMOPHILIC</cell><cell></cell><cell></cell><cell></cell><cell>HETEROPHILIC</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CITESEER_FULL</cell><cell>CORA_ML</cell><cell>OGBN-ARXIV</cell><cell>CHAMELEON</cell><cell>SQUIRREL</cell><cell>ARXIV-YEAR</cell><cell>SNAP-PATENTS</cell><cell>ROMAN-EMPIRE</cell></row><row><cell>HOM.</cell><cell>0.949</cell><cell>0.792</cell><cell>0.655</cell><cell>0.235</cell><cell>0.223</cell><cell>0.221</cell><cell>0.218</cell><cell>0.05</cell></row><row><cell>HOM. GAIN</cell><cell>1.36%</cell><cell>2.84%</cell><cell>6.30%</cell><cell>15.71%</cell><cell>2.38%</cell><cell>22.67%</cell><cell>40.32%</cell><cell>66.85%</cell></row><row><cell>GCN</cell><cell>93.37 ? 0.22</cell><cell cols="5">84.37 ? 1.52 68.39 ? 0.01 71.12 ? 2.28 62.71 ? 2.27 46.28 ? 0.39</cell><cell>51.02 ? 0.07</cell><cell>56.23 ? 0.37</cell></row><row><cell>DIR-GCN</cell><cell>93.44 ? 0.59</cell><cell cols="5">84.45 ? 1.69 66.66 ? 0.02 78.77 ? 1.72 74.43 ? 0.74 59.56 ? 0.16</cell><cell>71.32 ? 0.06</cell><cell>74.54 ? 0.71</cell></row><row><cell>SAGE</cell><cell>94.15 ? 0.61</cell><cell cols="5">86.01 ? 1.56 67.78 ? 0.07 61.14 ? 2.00 42.64 ? 1.72 44.05 ? 0.02</cell><cell>52.55 ? 0.10</cell><cell>72.05 ? 0.41</cell></row><row><cell>DIR-SAGE</cell><cell>94.14 ? 0.65</cell><cell cols="5">85.84 ? 2.09 65.14 ? 0.03 64.47 ? 2.27 46.05 ? 1.16 55.76 ? 0.10</cell><cell>70.26 ? 0.14</cell><cell>79.10 ? 0.19</cell></row><row><cell>GAT</cell><cell>94.53 ? 0.48</cell><cell cols="5">86.44 ? 1.45 69.60 ? 0.01 66.82 ? 2.56 56.49 ? 1.73 45.30 ? 0.23</cell><cell>OOM</cell><cell>49.18 ? 1.35</cell></row><row><cell>DIR-GAT</cell><cell>94.48 ? 0.52</cell><cell cols="5">86.21 ? 1.40 66.50 ? 0.16 71.40 ? 1.63 67.53 ? 1.04 54.47 ? 0.14</cell><cell>OOM</cell><cell>72.25 ? 0.04</cell></row></table><note><p><p><p><p><p>Heterophilic GNNs. Several GNN architectures have been proposed to handle heterophily. One way amounts to effectively allow the model to enhance the high-frequency components by learning 'generalized' negative weights on the graph</p><ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref></p>. A different approach tries to enlarge the neighbourhood aggregation to take advantage of the fact that on heterophilic graphs, the likelihood of finding similar nodes increases beyond the 1-hop</p><ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">53]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on real-world directed heterophilic datasets. OOM indicates out of memory.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Statistics of the datasets used in this paper.</figDesc><table><row><cell>DATASET</cell><cell># NODES</cell><cell># EDGES</cell><cell cols="4"># FEAT. # C UNID. EDGES EDGE HOM.</cell></row><row><cell>CITESEER-FULL</cell><cell>4,230</cell><cell>5,358</cell><cell>602</cell><cell>6</cell><cell>99.61%</cell><cell>0.949</cell></row><row><cell>CORA-ML</cell><cell>2,995</cell><cell>8,416</cell><cell>2,879</cell><cell>7</cell><cell>96.84%</cell><cell>0.792</cell></row><row><cell>OGBN-ARXIV</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell><cell>99.27%</cell><cell>0.655</cell></row><row><cell>CHAMELEON</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell><cell>5</cell><cell>85.01%</cell><cell>0.235</cell></row><row><cell>SQUIRREL</cell><cell>5,201</cell><cell>217,073</cell><cell>2,089</cell><cell>5</cell><cell>90.60%</cell><cell>0.223</cell></row><row><cell>ARXIV-YEAR</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell><cell>99.27%</cell><cell>0.221</cell></row><row><cell>SNAP-PATENTS</cell><cell cols="2">2,923,922 13,975,791</cell><cell>269</cell><cell>5</cell><cell>99.98%</cell><cell>0.218</cell></row><row><cell>ROMAN-EMPIRE</cell><cell>22,662</cell><cell>44,363</cell><cell>300</cell><cell>18</cell><cell>65.24%</cell><cell>0.050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Best hyperparameters for each dataset, determined through grid search, for our model.</figDesc><table><row><cell>DATASET</cell><cell>MODEL_TYPE</cell><cell>LR</cell><cell cols="2"># HIDDEN_DIM # NUM_LAYERS</cell><cell>JK</cell><cell>NORM</cell><cell>DROPOUT</cell><cell>?</cell></row><row><cell>CHAMELEON</cell><cell>DIR-GCN</cell><cell>0.005</cell><cell>128</cell><cell>5</cell><cell>MAX</cell><cell>TRUE</cell><cell>0</cell><cell>1.</cell></row><row><cell>SQUIRREL</cell><cell>DIR-GCN</cell><cell>0.01</cell><cell>128</cell><cell>4</cell><cell>MAX</cell><cell>TRUE</cell><cell>0</cell><cell>1.</cell></row><row><cell>ARXIV-YEAR</cell><cell>DIR-GCN</cell><cell>0.005</cell><cell>256</cell><cell>6</cell><cell>CAT</cell><cell>FALSE</cell><cell>0</cell><cell>0.5</cell></row><row><cell>SNAP-PATENTS</cell><cell>DIR-GCN</cell><cell>0.01</cell><cell>32</cell><cell>5</cell><cell>MAX</cell><cell>TRUE</cell><cell>0</cell><cell>0.5</cell></row><row><cell>ROMAN-EMPIRE</cell><cell>DIR-SAGE</cell><cell>0.01</cell><cell>256</cell><cell>5</cell><cell>CAT</cell><cell>FALSE</cell><cell>0.2</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>3, we perform a grid search over the following hyperparameters: model_type ? {Dir-GCN, Dir-SAGE}, learning_rate ? {0.01, 0.005, 0.001, 0.0005}, hidden_dimension ? {32, 64, 128, 256, 512}, num_layers ? {2, 3, 4, 5, 6}, jk ? {max, cat, none}, norm ? {T rue, F alse}, dropout ? {0, 0.2, 0.4, 0.6, 0.8, 1} and ? ? {0, 0.5, 1}. The best hyperparameters for each dataset are reported in table 8.D.6 Baseline ResultsGNNs for Heterophily Results for MLP, GCN, H 2 GCN, GPR-GNN and LINKX were taken from Lim et al.. Results for Gradient Gating are taken from their paper<ref type="bibr" target="#b36">[37]</ref>. Results for FSGNN</figDesc><table><row><cell></cell><cell>IN_DEGREE</cell><cell>OUT_DEGREE</cell><cell>TOTAL_DEGREE</cell></row><row><cell>CORA_ML</cell><cell>41.70%</cell><cell>11.65%</cell><cell>0.00%</cell></row><row><cell>CITESEER_FULL</cell><cell>63.45%</cell><cell>21.35%</cell><cell>0.00%</cell></row><row><cell>OGBN-ARXIV</cell><cell>36.62%</cell><cell>10.30%</cell><cell>0.00%</cell></row><row><cell>CHAMELEON</cell><cell>62.06%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>SQUIRREL</cell><cell>57.60%</cell><cell>0.00%</cell><cell>0.00%</cell></row><row><cell>ARXIV-YEAR</cell><cell>36.62%</cell><cell>10.30%</cell><cell>0.00%</cell></row><row><cell>SNAP-PATENTS</cell><cell>23.38%</cell><cell>30.16%</cell><cell>6.09%</cell></row><row><cell>DIRECTED-ROMAN-EMPIRE</cell><cell>0.00%</cell><cell>0.00%</cell><cell>0.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Percentage of nodes with either in-, out-or total-degree equal to zero.</figDesc><table><row><cell></cell><cell>CITESEER_FULL</cell><cell>CORA_ML</cell><cell>OGBN-ARXIV</cell><cell>CHAMELEON</cell><cell>SQUIRREL</cell><cell>ARXIV-YEAR</cell><cell>SNAP-PATENTS</cell><cell>ROMAN-EMPIRE</cell></row><row><cell>HOM.</cell><cell>0.949</cell><cell>0.792</cell><cell>0.655</cell><cell>0.235</cell><cell>0.223</cell><cell>0.221</cell><cell>0.218</cell><cell>0.050</cell></row><row><cell>GCN</cell><cell>93.37?0.22</cell><cell>84.37?1.52</cell><cell>68.39?0.01</cell><cell cols="3">71.12?2.28 62.71?2.27 46.28?0.39</cell><cell>51.02?0.07</cell><cell>56.23?0.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This Pytorch-Geometric routine is used to load datasets stored in an npz format. It makes some directed datasets, such as Cora-ML and Citeseer-Full, automatically undirected without any option to get the directed version instead.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is attributed to the fact that while A is typically quite sparse, A k grows increasingly dense as k increases, quickly approaching n 2 non-zero entries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that this does not affect our discussion, in fact any observation can be extended to the non-linear case by computing the Jacobian of node features as in Topping et al.<ref type="bibr" target="#b44">[45]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Or in-degree, depending on which direction is selected.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The same results apply to a model which sends messages only along in-edges.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Accuracy</head><p>Dir-GCN GCN Figure <ref type="figure">9</ref>: Performance of GCN (on the undirected version of the graph) and Dir-GCN on Arxiv-Year when using only one layer. Remarkably, directionality yields significant benefits, even in the absence of access to the homophilic directed 2-hop. This is largely attributable to the harmless heterophily exhibited by the directed graph.</p><p>datasets, the gap between the two directions (? = 0 vs ? = 1) is extremely large (more than 40% absolute accuracy). We find that this is likely due to the high number of nodes with zero in neighbors, as reported in Table <ref type="table">9</ref>. Chameleon and Squirrel have respectively about 62% and 57% of nodes with no in-edges: when propagating only over in edges, these nodes would get zero features. We observe a similar trend for other datasets, where ? = 1 performs generally better than ? = 0, in line with the fact that all these datasets have more nodes with zero in edges than out edges (Table <ref type="table">9</ref>). In general, using both in-and out-edges is the preferred solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Ablation Study on Using a Single Layer</head><p>In Sec. 3 we discuss how Arxiv-Year and Snap-Patents exhibit harmless heterophily when treating as directed. This suggest that even a 1-layer Dir-GNN model should be able to perform much better of its undirected counterpart, despite not being able to access the much more homophilic 2-hop. We verify this empirically by comparing a 1-layer GCN (on the undirected version of the graph) with a 1-layer Dir-GCN on Arxiv-Year. Fig. <ref type="figure">9</ref> presents the results, showing that Dir-GCN does indeed significantly outperform GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Ablation Study on Different Values of ?</head><p>We train Dir-GNN models on Arxiv-Year with varying values of ?, using the hyperparameters outlined in Appendix D.4. Fig. <ref type="figure">10</ref> presents the results: while a large drop is observed for ? = 0</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">Albert-Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Equivariant subgraph aggregation networks</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopinath</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Guido F Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identifications</title>
		<author>
			<persName><forename type="first">Jin-Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Benjamin P Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph neural networks as gradient flows</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Color refinement and its applications</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An Introduction to Lifted Probabilistic Inference</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Cucuringu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gesine</forename><surname>Reinert</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">edgnn: a simple and powerful GNN for directed labeled graphs</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mar?a</forename><surname>Rodr?guez Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gabrani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Directed graph auto-encoders</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Kalantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Id'e, Aur?lie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding global homophily in graph neural networks when meeting heterophily</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caihua</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weining</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Matthew Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Prasad Bhalerao</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revisiting heterophily for graph neural networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is homophily a necessity for graph neural networks?</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Spectral-based graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving graph neural networks with simple architecture design</title>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Kumar Maurya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Murata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motifnet: A motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Science Workshop (DSW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A critical look at the evaluation of GNNs under heterophily: Are we really making progress?</title>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Platonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kuznedelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Diskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liudmila</forename><surname>Prokhorenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient gating for deep multi-rate learning on graphs</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Digraph inception convolutional networks</title>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Directed graph convolutional network. arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NTI Series</title>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Magnet: A neural network for directed graphs</title>
		<author>
			<persName><forename type="first">Xitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Brugnone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Perlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hirn</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dir-Gcn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>?=0</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dir-Sage</surname></persName>
		</author>
		<idno>?=0.5) 94.14?0.65 85.81?1.18 65.06?0.28 60.22?1.16 43.29?1.04 55.76?0.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dir-Gat</surname></persName>
		</author>
		<idno>?=0.0) 94.48?0.52 86.13?1.58 52.57?0.05 40.44?3.11 28.28?1.02 46.01?0.06 OOM 53.58?2.51</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dir-Gat</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>?=1</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dir-Gat</surname></persName>
		</author>
		<idno>?=0.5) 94.12?0.49 86.05?1.71 66.44?0.41 55.57?1.02 37.75?1.24 54.47?0.14 OOM 72.25?0.04</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
