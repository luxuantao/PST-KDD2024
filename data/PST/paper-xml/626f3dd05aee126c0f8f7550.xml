<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PIE: A PARAMETER AND INFERENCE EFFICIENT SOLUTION FOR LARGE SCALE KNOWLEDGE GRAPH EMBEDDING REASONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-29">29 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linlin</forename><surname>Chao</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
							<email>taifeng.wang@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
						</author>
						<title level="a" type="main">PIE: A PARAMETER AND INFERENCE EFFICIENT SOLUTION FOR LARGE SCALE KNOWLEDGE GRAPH EMBEDDING REASONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-29">29 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.13957v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) embedding methods which map entities and relations to unique embeddings in the KG have shown promising results on many reasoning tasks. However, the same embedding dimension for both dense entities and sparse entities will cause either over parameterization (sparse entities) or under fitting (dense entities). Normally, a large dimension is set to get better performance. Meanwhile, the inference time grows log-linearly with the number of entities for all entities are traversed and compared. Both the parameter and inference become challenges when working with huge amounts of entities. Thus, we propose PIE, a parameter and inference efficient solution. Inspired from tensor decomposition methods, we find that decompose entity embedding matrix into low rank matrices can reduce more than half of the parameters while maintaining comparable performance. To accelerate model inference, we propose a self-supervised auxiliary task, which can be seen as fine-grained entity typing. By randomly masking and recovering entities' connected relations, the task learns the co-occurrence of entity and relations. Utilizing the fine grained typing, we can filter unrelated entities during inference and get targets with possibly sub-linear time requirement. Experiments on link prediction benchmarks demonstrate the proposed key capabilities. Moreover, we prove effectiveness of the proposed solution on the Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the state of the art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs store structured data as the form of triples, where each triple (h, r, t) represents a relation r between a head entity h and a tail entity t. Typical projects such as WordNet <ref type="bibr" target="#b0">Miller [1995]</ref>, Freebase <ref type="bibr" target="#b1">Bollacker et al. [2008]</ref>, YAGO Suchanek et al. <ref type="bibr">[2007]</ref> and DBpedia <ref type="bibr" target="#b3">Lehmann et al. [2015]</ref>, have gained widespread traction from their successful use in tasks such as question answering <ref type="bibr" target="#b4">Bordes et al. [2014]</ref>, <ref type="bibr" target="#b5">Hamilton et al. [2018]</ref>, information extractionHoffmann et al. <ref type="bibr">[2011]</ref>, recommender systems <ref type="bibr" target="#b7">Zhang et al. [2016]</ref> and so on. Particularly, knowledge graph embedding (KGE) methods, which embed all entities and relations into a low dimension space, play a key role for these downstream tasks. It has been an active research area for the past couple of years. Methods such as TransE <ref type="bibr" target="#b8">Bordes et al. [2013]</ref>, <ref type="bibr">ComplExTrouillon et al. [2016]</ref>, <ref type="bibr">RotatESun et al. [2019]</ref> provide a way to perform reasoning in knowledge graphs with simple numerical computation in continuous spaces. Although widely used in many reasoning tasks, these methods have to respond to several challenges when working with large scale KGs.</p><p>Parameter redundancy. KGE methods assign the same embedding dimension for all the entitiesWang et al. <ref type="bibr">[2017]</ref>. However, entities in KGs naturally have long-tailed distributions, where different entities have different degrees (see Figure <ref type="figure" target="#fig_0">1</ref>). The same embedding dimension for both dense entities and sparse entities will cause either over parameterization (sparse entities) or under fitting (dense entities). On small conventional benchmark datasets, such as FB15kBordes et al. <ref type="bibr">[2013]</ref>, <ref type="bibr">YAGO3-10Dettmers et al. [2018]</ref>, embedding dimensions for the state-of-the-art models can be as many as thousands <ref type="bibr" target="#b10">Sun et al. [2019]</ref>, <ref type="bibr" target="#b13">Ebisu and Ichise [2018]</ref>. Furthermore, <ref type="bibr" target="#b14">Hu et al. [2020]</ref>, <ref type="bibr" target="#b15">Chao et al. [2021]</ref> show on some dataset larger dimensions can achieve better performance. Given that there are many sparse entities, especially for large scale KGs, the redundancy in the parameterization of entity embeddings will be a severe problem. Meanwhile, the parameter issue has been a challenge for huge KGs. Since the parameters of knowledge graph embedding methods grow linearly with the number of entities and embedding dimension, increasing the dimension for huge KGs can lead to the explosion of the parameters rapidly. With more parameters, the models consume more memory, storage and computing resources. This motivates us to reduce parameter redundancy and improve parameter efficiency to alleviate this challenge.</p><p>Inefficiency inference. The inference efficiency, which has long been neglected, is also an inevitable challenge. Take the one-hop question answering reasoning task as an example. Given a test query (?, P residentOf, U nitedStates), all the entities in KG should be traversed and sorted to find the target entities. The time complexity is O(n + n log(n)), where n is the size of entity set. Such a high complexity makes it difficult to scale up to large scale KGs, such as WikiKG90Mv2Hu et al. <ref type="bibr">[2021]</ref>. One way to accelerate inference is only selecting and comparing relevant entities based on the query relation. For example, according to P residentOf , we can infer the type of answers is person, or more precisely, politician. However, there are two obstacles to getting the wanted typing. The first one is KGs are incomplete, so as the entity type <ref type="bibr" target="#b17">Zhao et al. [2020]</ref>. The other one is that the types are often several discrete classes <ref type="bibr" target="#b18">Yogatama et al. [2015]</ref>, Zhou et al. such as "person" and "Location". We argue these coarse typing might not fine grained enough to improve inference efficiency.</p><p>To solve these two challenges, we propose PIE, a Parameter and Inference Efficient solution. In the field of KGE, one group of works utilize latent factorization methods <ref type="bibr" target="#b20">Nickel et al. [2011]</ref>, <ref type="bibr" target="#b9">Trouillon et al. [2016]</ref>, <ref type="bibr" target="#b21">Amin et al. [2020]</ref>. Taking triples in KG as the form of a partially observed 3D tensor, they decompose it into a product of embedding matrices with much smaller rank, resulting in fixed-dimensional vector representations for each entity and relation. Following this line of works, we propose low rank entity embedding (LRE). Rather than decomposing the observed 3D tensor directly, LRE decomposes the entity embedding matrix to low rank matrices. Extensive experiments show adding LRE to existing KGE methods can reduce more than half of the parameters while maintain comparable performance. When the computing resource is limited, the LRE based methods show significant advantages. To accelerate model inference, we propose an auxiliary self-supervised task, which can be seen as fine grained entity typing. Different with traditional entity typing, which defines entity types to discrete classes <ref type="bibr" target="#b22">Moon et al. [2017]</ref>, <ref type="bibr" target="#b17">Zhao et al. [2020]</ref>, we take the one-hop relation neighbor distribution (observed and unobserved) of entities as their types. This task learns the co-occurrence of entities and relations by randomly masking and recovering entities relation neighbors. Under the proposed task, we explore an inductive, entity independent and storage efficient model. Specifically, a relational message passing based graph neural network <ref type="bibr">Wang et al. [2021a]</ref> is built to model entities' relation subgraphs to infer unobserved relations. With the learned fine grained entity typing, we can filter unrelated entities during inference process and get targets with possibly sub-linear time requirement.</p><p>Our key contributions are outlined as follows:</p><p>? We propose utilizing tensor factorization to improve parameter efficiency of existing KGE methods, which is general and efficient;</p><p>? We propose a new perspective for entity typing, which is self-supervised and fine grained. It can be used to improve inference efficiency seamlessly. Under the proposed task, we explore an effective relational message passing based model;</p><p>? On real-world extremely large benchmark, WikiKG90Mv2, the proposed solution improves both parameter and inference efficiency for existing KGE models and achieves state-of-the-art performance under limited computing resources.</p><p>2 Related Work Based on the scoring function, previous works can be roughly classified into tensor factorization models and distance based models. Tensor factorization models use different factorization methods to decompose tensor T . RESCAL <ref type="bibr" target="#b20">Nickel et al. [2011]</ref> and DistMult <ref type="bibr" target="#b24">Yang et al. [2014]</ref> factorize its slices in the relation dimension. Others such as ComplEx <ref type="bibr">Trouillon et al. [2016], SimplE Kazemi and</ref><ref type="bibr" target="#b25">Poole [2018]</ref> use Canonical Polyadic decomposition Hitchcock <ref type="bibr">[1927]</ref> to factorize this 3D binary tensor directly. Distance based methods <ref type="bibr" target="#b8">Bordes et al. [2013]</ref>, <ref type="bibr" target="#b27">Wang et al. [2014]</ref>, <ref type="bibr" target="#b10">Sun et al. [2019]</ref>, <ref type="bibr" target="#b28">Tang et al. [2019]</ref> use additive dissimilarity scoring functions, whereby they all map entities to vector representations. All these methods do not consider the redundancy in embedding matrices, especially for entity embedding, which may be problematic for large scale KGs.</p><p>Compression and parameter efficient models. Inspired by the distillation technique <ref type="bibr" target="#b29">Hinton et al. [2015]</ref>, <ref type="bibr" target="#b30">Sanh et al. [2019]</ref>, <ref type="bibr" target="#b31">Sachan [2020]</ref> compress the trained entity matrices into discrete codes. <ref type="bibr">Wang et al. [2021b]</ref> also utilizes multiple KGE models as teachers to distill a student model, which has lower dimension. One limitation of these works are that one or more well trained full embedding matrix is needed before distillation. NodePiece <ref type="bibr" target="#b33">Galkin et al. [2021]</ref> selects some of the entities as anchor entities and represents all the entities based on these anchor entities' embedding. However, it needs compute the shortest paths from all the entities to anchor entities, which is time and resource consuming. For KG with 1M+ entities, the preprocessing step might be a computational bottleneck<ref type="foot" target="#foot_0">1</ref> . Contrary to those works, the proposed method learns embeddings more efficiently.</p><p>Entity typing. This task aims at inferring possible missing entity type instances in KG. One way to get the type information is to infer from external text information <ref type="bibr" target="#b34">Xu and Barbosa [2018]</ref>, which might not be feasible for some KGs. Others utilize embedding based models. The embedding models <ref type="bibr" target="#b22">Moon et al. [2017]</ref>, <ref type="bibr" target="#b17">Zhao et al. [2020]</ref> combine triple knowledge and entity type instances for type prediction, which require entity type instance as supervised signal. Meanwhile, the types defined in these methods is often a small set of coarse labels. Instead, our proposed self-supervised fine grained entity typing task can overcome these limitations, providing a new perspective to this task. Besides, the proposed inductive relational message passing model, which aggregates the neighborhood relations, can also be a useful supplement to existing entity typing models.</p><p>Relation prediction. A recent line of works, such as GraILTeru et al. <ref type="bibr">[2020]</ref> and <ref type="bibr">PathConWang et al. [2021a]</ref>, utilize entity-pair based subgraph and paths to predict the relation between two entities. We want to emphasize these works are different to our proposed fine grained entity typing task, where they belong to link prediction and ours are node property prediction. Besides, the fine grained entity typing is much more challenge. Because the target space for entity pair based relation prediction is certain. Due to the incomplete character of KGs, the label set can be very ambiguous for the proposed fine grained entity typing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter Efficient: Low Rank Entity Embedding</head><p>Conventional KGE methods assign an unique vector for each entity. Suppose the dimension of entity embedding is d and the size of entity set is |E|. The total size of the entity embedding matrix is d * |N |, which is growing linearly with entity dimension and size of entity set. Normally, assigning the same large dimension vectors for both dense entities and sparse entities will lead to parameter redundancy. To this end, we propose LRE, which decompose entity embedding matrix into two low rank matrices. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the entity embedding matrix Z is decomposed into matrices</p><formula xml:id="formula_0">Z d and W , Z = Z d * W,<label>(1)</label></formula><p>where Although the total parameter is reduced, LRE based model has the potential to recover the performance of the larger models. In LRE, the matrix W is shared for all entities. During training, the gradients of W are biased to denser entities as the imbalanced distribution of entity degrees. The biased learning ensures the performance of denser entities to some extent. While sparse entity can also learn from denser entity through the shared matrix W . As experiment results show with the same experiment settings, LRE can get comparable or even better performance than the 2x larger models.</p><formula xml:id="formula_1">Z d ? R |E|?r , W ? R r?d and r &lt; d.</formula><p>LRE can be used to both tensor decomposition based models and distance based models. When combined with tensor decomposition based models, it can be seen as the further decomposition of the target 3D binary tensor T (see Section 2). When getting the entity embedding, all the subsequent steps keep the same with existing KGE methods. NodePiece randomly selects some anchor entities as vocabulary while LRE uses the shared matrix W . One of the major differences is how to combine vocabulary to represent entities. NodePiece utilizes shortest path based encoding way, which is inductive but not friendly for large scale KGsGalkin et al. <ref type="bibr">[2021]</ref>. LRE utilizes a low rank learnable look-up table, which is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference Efficient: Fine Grained Typing Aware Inference</head><p>We propose to utilize neighborhood and entity type aware inference. Given test triple (h, r, ?), we first extract neighborhoods of entity h. Based on query relation r we can also infer the type of answer entities. Then, we can select some candidates based on the inferred entity type. At last, the KGE models are utilized to rank all the candidates to get the answers. One of the key steps is to get the candidates based on entity type. However, KG is incomplete, including entity type. Besides, the existing entity typing may be too coarse to support efficient inference. Thus, a fine-grained entity typing is necessary. Definition 1 (Fine Grained Entity Typing). Given a knowledge graph with entity set E and relation set R, ?e ? E, we call the distribution p(r|e), r ? R, which measures the likelihood of relation types given a particular entity, as fine grained entity typing of entity e.</p><p>Once we get the fine grained entity typing, we can select candidates based on the posterior distribution, which is calculated by</p><formula xml:id="formula_2">p(e|r) = p(e)p(r|e) p(r) ? p(e)p(r|e), e ? N (e q ),<label>(2)</label></formula><p>where N (e q ) is the neighborhood entities of query entity e q . p(e) and p(r) are prior distributions over entities and relations respectively, which can be calculated based on their degrees.</p><p>Self-supervised entity typing model. To learn entity typing, we propose a self supervised task. Given an entity in KG, we randomly mask one of the one-hop relations and recover it by reasoning from the masked KG. The key idea is that the missing relations can be inferred from neighborhood relations and entities. Note that the observed triples in KG can only provide partial information of the fine grained entity typing. The challenge is to infer the unobserved relation types for a particular entity. Apart from structure information, node attributes also provide important clues. In order to generalize KG without node attributes, we only focus on structure information in this paper.</p><p>To  Reasoning on the relational subgraph. We adopt the alternate relational message passing framework described in <ref type="bibr">Wang et al. [2021a]</ref>, where node and edge representations are iteratively updated by combining their selves with aggregation of their neighbor edges' or neighbor nodes' representations. In particular, define s i e as the hidden state of edge e in iteration i, and m i v as the message stored at node v in iteration i. The representation of each edge is learned by:</p><formula xml:id="formula_3">m i v = e?N (v) s i e ,<label>(3)</label></formula><formula xml:id="formula_4">s i+1 e = ?([m i v , m i u , s i e ] ? W i + b i ), v, u ? N (e),<label>(4)</label></formula><p>where [?] is the concatenation function, W i , b i , and ?(?) are the learnable transformation matrix, bias and nonlinear activation function respectively. In our implementation, ? is the ReLU function. s 0 e = x e is initial feature of edge e, which is the one hot identity vector of the relation type that e belongs to. The message passing in Equation. 3 and 4 are repeated for K times. Inspired by the JK-connection mechanism <ref type="bibr">Xu et al. [2018]</ref>, we use the representations from all the intermittent layers and the last layer, while the final representation for the target node t is represented by</p><formula xml:id="formula_5">m t = [m 0 t , m 1 t , ..., m K-1 t ].<label>(5)</label></formula><p>Loss function. During learning process, the model randomly masks one relation and recover it by reasoning on the subgraph. Taking this task as a multi-classification problem, where the softmax function and cross entropy loss are utilized to maximize probability of the masked relation as other self-supervised models <ref type="bibr" target="#b39">Devlin et al. [2018]</ref>, is one choice. However, KG is incomplete and the unobserved relation types except the masked relation can also be the correct labels. With larger relation set or more missing triples, the incompleteness brings new challenge for this task. Besides, we hope the model outputs the distribution regard to all relation types rather than one specific type. Thus, we task this task as a multi-label learning task and utilize the following loss <ref type="bibr" target="#b40">Sun et al. [2020]</ref>, which contains the pair wised ranking between the observed relation types and the unobserved relations. The loss is represented as</p><formula xml:id="formula_6">L = log[1 + i?? obs j??uno exp(?(s j -s i + m))] = log[1 + j??uno exp(?(s j + m)) i?? obs exp(?(-s i ))],<label>(6)</label></formula><p>where We first comprehensively evaluate the effectiveness of the proposed two capabilities on relative small benchmarks (Section 4.1 and Section 4.2). Then we report out results on one of the largest benchmark WikiKG90Mv2 (Section 4.3).</p><formula xml:id="formula_7">?</formula><p>Dataset. We choose link prediction as evaluation task. We believe our solution can also be generalize to other reasoning tasks, such as complex question answering on incomplete <ref type="bibr">KG Hamilton et al. [2018]</ref>, <ref type="bibr" target="#b41">Saxena et al. [2021]</ref>. The chosen benchmarks are listed in Table <ref type="table" target="#tab_2">1</ref>. Except wikiKG90Mv2, we choose ogbl-wikikg2 <ref type="bibr" target="#b14">Hu et al. [2020]</ref>, <ref type="bibr">YAGO3-10 Dettmers et al. [2018]</ref>, ogbl-biokg Hu et al.</p><p>[2020] and <ref type="bibr">CoDEx-LargeSafavi and Koutra [2020]</ref> for their entity sets are relative large. The FB15kBordes et al. <ref type="bibr">[2013]</ref> is chosen for its large relation set, which is more difficult to infer fine grained entity typing.</p><p>Implementation. For Open Graph Benchmark (OGB) related datasets, ogbl-wikikg2, ogbl-biokg and WikiKG90Mv2<ref type="foot" target="#foot_1">2</ref> , we utilize the official implementations. For the other two datasets, YAGO3-10 and FB15k, code from RotatE is utilized. Since part of the OGB official implementations are based on the repository of RotatE, all the codes share the same self-adversarial negative sampling loss function <ref type="bibr" target="#b10">Sun et al. [2019]</ref>. Our code is public available<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Efficiency</head><p>Setup. We choose PairRE as baseline model. NodePiece, the entity hashing based efficient model, is also compared.</p><p>The designed experiments are mainly to prove the parameter efficiency of LRE based models rather to outperform the best existing models.</p><p>Following the state-of-the-art methods, we measure the quality of the ranking of each test triple among all possible head entity and tail entity substitutions. Two evaluation metrics, including Mean Reciprocal Rank (MRR) and Hit ratio with cut-off value 10, are utilized. For experiments on ogbl-wikikg2 and ogbl-biokg, we follow the evaluation protocol of these two benchmarks <ref type="bibr" target="#b14">Hu et al. [2020]</ref>. We also report the total parameters for all models and the required hardware for larger dataset.</p><p>- Table <ref type="table">2</ref>: Link prediction results on ogbl-wikikg2 and ogbl-biokg. Best results for each group are in bold. Each experiment is run ten times. We do not add the standard deviations for limited space. Table <ref type="table">3</ref>: Link prediction results on YAGO3-10 and CoDEx-Large. Results. Comparisons for ogbl-wikikg2 and ogbl-biokg are shown in Table <ref type="table">2</ref>. All the models are in the same experiment settings and implementation. The results shows larger dimension can improve the Test MRR significantly on ogbl-wikikg2. While the LRE based models can boost the performance further with the same budget of parameters and computing resource. The same phenomenon can also be observed on ogbl-biokg. The LRE based models can improve test MRR with 6.9% and 1.96% respectively. Results for YAGO3-10 and CoDEx-Large are shown in Table <ref type="table">3</ref>. When compared to NodePiece based model, on ogbl-wikikg2, LRE based model consumes more parameters to outperform their performance. However, as stated before, LRE based models can avoid the long preprocess time of NodePiece based methods. On YAGO3-10 and CoDEx-Large, the LRE based model can outperform NodePiece based model with smaller budget of parameters (see Table <ref type="table">3</ref>). All these experiments show LRE based models is advantageous when meet with limited resource and large scale datasets.</p><p>We further study the influence of the parameter size to the performance (Figure <ref type="figure" target="#fig_5">4</ref>). On YAGO3-10, the performances of PairRE and PairRE+LRE increase with the increase of parameter size. The dimension of PairRE+LRE is set to 1000 and the LRE rank is increasing from 50 to 300. While the dimension of PairRE is increased from 50 to 500. The performance of PairRE+LRE saturates at 20M, while PairRE at 60M. When the size of parameter is less than 40M, PairRE+LRE shows much better performances. Particularly, LRE powered models can achieve comparable performances compared to 3x larger models (LRE+PairRE@12M VS PairRE@38M). This indicates the proposed LRE can improve the parameter efficiency for KGE methods.  Then two baselines are compared to the fine grained typing model powered inference. We name the proposed inference method as Finegrained entity Typing Aware Inference (FTAI). Baseline one is the conventional inference, which is scoring and ranking all the entities. Baseline two is the neighborhood based inference. The difference between FTAI and baseline two is that only entity degree is used to select candidates from neighborhood entities for baseline two. The second baseline can be seen as the ablation analysis for the proposed entity typing model. Two evaluation metrics are utilized, inference time and Hit@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference Efficiency</head><p>Results. Table <ref type="table">4</ref> shows the evaluation results of the entity typing models. Clearly, the FB15k dataset is more challenge as there is much larger relation sets compared to YAGO3-10 (1,315 vs 37). The table also shows pronounced gap between softmax loss and the proposed ranking loss on FB15k. One of the reasons might be the incompleteness of FB15k is more severe. Under this situation, the ranking loss based model can largely outperform softmax based model.</p><p>Incorporate the entity typing model, the proposed FTAI shows clear advantages compared to these two baselines (See Figure <ref type="figure" target="#fig_7">5</ref>). Compared to the conventional inference, FTAI can achieve 1.8x speedup and the same test Hit@10 performance when at most 20k candidates are selected for each test triple. Ablation analysis shows for both FTAI and baseline two, with larger candidate set, the better performance and the slower inference. The results also show the entity typing model enables a much large recall compared with baseline two, which proves the crucial function of the proposed entity typing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on WikiKG90Mv2</head><p>Setup. We follow the official evaluation of WikiKG90Mv2. That is, for each (h, r, ?), the model is asked to predict the top 10 tail entities that are most likely to be positive. Since traversal all the 90 million entities is impractical during inference, the organizers of this dataset provide at most 20,000 tail candidates for each test triple. They pick tail candidates t by choosing the entities with at least one triple (_, r, t) on the KG, and sort all the tails by its degree. We take these candidates as baseline candidates. In accordance with baseline models, we also pick at most 20k candidates for FTAI.</p><p>Results. The reported results (Table <ref type="table" target="#tab_4">5</ref>) show competitive performances of PIE powered ComplEx and TransE models. We tune the baseline models carefully and get much better performances compared to the official results. With the same hyper-parameter settings and parameter budget, LRE can improve ComplEx and TransE 0.56% and 0.75% respectively. Utilizing the fine grained entity typing model, we can improve ComplEx and TransE 0.97% and 2.47% respectively. This is because the candidates predicted by our entity typing model have much higher recall. As Table <ref type="table" target="#tab_5">6</ref> shows the candidates output by FTAI have a much higher recall than the baseline candidates. When combine TransE and the proposed PIE, we achieve state-of-the-art performance, with 2.82% improvement compared to the entity attribute powered model. Ablation analysis result of the proposed FTAI is shown in Table <ref type="table" target="#tab_5">6</ref>. With the same candidate number, the recall of FTAI is higher than the baseline candidates with close to 10%, which proves the necessity and the effectiveness of the fine grained typing model. The evaluation metrics for entity typing model on this dataset is 65% for MRR and 72% for Hit@5. Compared to results on FB15k, there is still large room for further improvement. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>With the explosion of data, the emerging large scale KGs bring new challenges for KGE based reasoning. We explored the parameter redundancy and inefficiency inference. For parameter redundancy, we propose tensor factorization based method LRE, which decomposes the entity embedding matrix. Experiment results show LRE based model can approximate the performance of full rank entity matrix based models, which saves more than 50% parameters.</p><p>For inference optimization, we proposed a fine grained entity typing model. Without extra annotation, we learn the concurrence distribution of entity and relation with a lightweight and inductive model. Utilizing the learned entity typing, the full entity set traversing is avoid during inference. Experiment results on FB15k shows we achieve a 1.8x speedup over the full entity set traversing inference. We also verify the proposed solution on large scale dataset WikiKG90Mv2 and achieve state of the art performance.</p><p>The deep models have shown tremendous successes <ref type="bibr" target="#b45">Goodfellow et al. [2016]</ref>. Compared to models from computer vision <ref type="bibr" target="#b46">He et al. [2016] and</ref><ref type="bibr">NLPDevlin et al. [2018]</ref>, where the layer-wise hierarchy representations are widely used, KGE models are shallow. The proposed LRE can be seen as an exploration from shallow to deep, where the large look-up table is replaced with a smaller table plus a linear layer. For future work, we will try more "decomposition" for the look-up table with techniques such as residual connectionHe et al. <ref type="bibr">[2016]</ref> and auxiliary losses in intermediate layersSzegedy et al. <ref type="bibr">[2015]</ref>. The incompleteness of KGs also leads to challenge for the fine grained entity typing. A framework that unifies KG reasoning and fine grained entity typing should boot performances of these two tasks. We will leave these for future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Entity degree distribution</figDesc><graphic url="image-1.png" coords="2,398.27,75.69,141.73,106.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Low rank entity embedding for KGE methods.</figDesc><graphic url="image-2.png" coords="4,142.20,72.00,327.60,154.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>scale up to large scale KGs efficiently, we build a lightweight inductive model. The model is built around the Graph Neural Network (GNN)<ref type="bibr" target="#b36">Scarselli et al. [2008</ref><ref type="bibr" target="#b37">], Bronstein et al. [2017]</ref>. Two modules are required: (i) relational subgraph extraction, (ii) reasoning on the relational subgraph. Example of fine grained entity typing based on relation subgraph is shown in Figure3.Relational subgraph extraction. We assume that local graph neighborhood of a particular entity will contain the evidence needed to deduce the fine grained entity typing. As neighborhood triples with different relation types are more important than triples with the same relation, we try to keep all the relation types when we have to sample the neighbors. We also add the reverse triples with new relation types to the subgraph because edge directions for entity relation pair can have totally different semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of fine grained entity typing based on relational subgraph. We randomly mask one of the relation types during model learning.</figDesc><graphic url="image-3.png" coords="5,95.40,72.00,421.19,127.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>LRE_Rank MRR Hit@10 #Param(M) #Dim #LRE_Rank MRR Hit@10 #Param(M) RotatESun et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parameter efficiency comparison on YAGO3-10 dataset.</figDesc><graphic url="image-4.png" coords="6,355.75,521.80,184.26,138.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.9972 0.7005 0.8495 Ranking loss 0.9898 0.9971 0.9017 0.9391Table 4: Evaluations for the entity typing model on YAGO3-10 and FB15k.Setup. We report the proposed entity typing model performances on YAGO3-10 and FB15k. The comparison between softmax based loss and the proposed ranking loss is added as we find loss function is a vital step in our experiments. When evaluating the entity typing model, we follow the data splits of the conventional link prediction task. For a given triple (h, r, t) in the test or valid set, we rank the ground truth relation type r against all other relation types based on the relation subgraph of entity h. We also use MRR and Hit@5 as evaluation metrics. The rankings are computed after removing all the other observed relation types that appear in training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of the candidate number per test triple of FTAI on test MRR and inference time on FB15k for the PairRE model. Baseline 1 is scoring all entities. Baseline 2 is the proposed FTAI without use entity typing result.</figDesc><graphic url="image-5.png" coords="7,355.75,313.91,184.26,138.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(e s , e o ), where e s , e o ? E are the entities, and r ? R is the relation between them. The scoring function f estimates the general binary tensor T ? |E| ? |R| ? |E|, by assigning a score of 1 to T ijk if relation r j exists between entities e s and e o , 0 otherwise.</figDesc><table><row><cell>Knowledge graph embedding methods. Given a set of entities E and relations R in a KG, KGE is learned by</cell></row><row><cell>assigning a score s to a triple (e s , r, e o ):</cell></row><row><cell>s = f r</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>r is the rank of the proposed LRE. The matrix Z d is entity dependent and matrix W is shared for all entities. The total number of entity parameters for LRE based KGE models can be |E| * r + r * d. Compared to the corresponding full entity embedding matrix based model, LRE can reduce parameters close to |E| * (d -r), where the shared parameter W can be neglected for large entity set. The larger the entity set, the more efficiency of LRE based model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>obs and ? uno represent the observed type set and unobserved type set respectively and |? obs | + |? uno | = |R|. s represent the logits corresponding to each relation type. ? is a scale factor and m is a margin for better separation. Number of entities, relations and observed triples in each split for the five utilized benchmarks.</figDesc><table><row><cell>4 Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>|R|</cell><cell>|E|</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell cols="5">WikiKG90Mv2 1,387 91,230,610 601,062,811 15,000</cell><cell>15,000</cell></row><row><cell>ogbl-wikikg2</cell><cell>535</cell><cell>2,500,604</cell><cell cols="3">16,109,182 429,456 598,543</cell></row><row><cell>YAGO3-10</cell><cell>37</cell><cell>123,143</cell><cell>1,079,040</cell><cell>4,978</cell><cell>4,982</cell></row><row><cell>ogbl-biokg</cell><cell>51</cell><cell>93,773</cell><cell>4,762,678</cell><cell cols="2">162,886 162,870</cell></row><row><cell>FB15k</cell><cell>1,345</cell><cell>14,951</cell><cell>483,142</cell><cell>50,000</cell><cell>59,071</cell></row><row><cell>CoDEx-Large</cell><cell>69</cell><cell>77,951</cell><cell>551,193</cell><cell>30,622</cell><cell>30,622</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Link prediction results on WikiKG90Mv2.? means the entity attribute is utilized.</figDesc><table><row><cell>Model</cell><cell cols="5">#Dim #LRE_Rank Test MRR Valid MRR #Param(B)</cell></row><row><cell>TransE-ConcatHu et al. [2021]?</cell><cell>200</cell><cell>-</cell><cell>0.1761</cell><cell>0.2060</cell><cell>18.2</cell></row><row><cell>ComplExHu et al. [2021]</cell><cell>100</cell><cell>-</cell><cell>0.0985</cell><cell>0.1150</cell><cell>18.2</cell></row><row><cell>ComplEx(ours)</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>0.1795</cell><cell>18.2</cell></row><row><cell>+LRE</cell><cell>400</cell><cell>200</cell><cell>-</cell><cell>0.1851</cell><cell>18.2</cell></row><row><cell>+FTAI</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>0.1892</cell><cell>18.2</cell></row><row><cell>+LRE+FTAI</cell><cell>400</cell><cell>200</cell><cell>-</cell><cell>0.1919</cell><cell>18.2</cell></row><row><cell>TransEHu et al. [2021]</cell><cell>200</cell><cell>-</cell><cell>0.0824</cell><cell>0.1103</cell><cell>18.2</cell></row><row><cell>TransE(ours)</cell><cell>200</cell><cell>-</cell><cell>-</cell><cell>0.2049</cell><cell>18.2</cell></row><row><cell>+LRE</cell><cell>600</cell><cell>200</cell><cell>-</cell><cell>0.2124</cell><cell>18.2</cell></row><row><cell>+FTAI</cell><cell>200</cell><cell>-</cell><cell>-</cell><cell>0.2296</cell><cell>18.2</cell></row><row><cell>+LRE+FTAI</cell><cell>600</cell><cell>100</cell><cell>-</cell><cell>0.2247</cell><cell>9.1</cell></row><row><cell>+LRE+FTAI</cell><cell>600</cell><cell>200</cell><cell>-</cell><cell>0.2342</cell><cell>18.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell>Recall@20k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BaselineHu et al. [2021]</cell><cell>0.5770</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FTAI</cell><cell>0.6695</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Evaluations for entity typing model on WikiKG90Mv2 valid set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As the author states, it takes 55 hours to finish the preprocess step for ogb-wiki2 (2.5M nodes and 16M edges). https://github.com/migalkin/NodePiece/issues/1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The official code for wikiKG90M is utilized, which is based on DGL-KEZheng et al.[2020]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/alipay/Parameter_Inference_Efficient_PIE</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242667</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242667" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dbpedia a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/file/1cecc" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
	<note>a77928ca8133fa24680a88d2f9-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toruse: Knowledge graph embedding on a lie group</title>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PairRE: Knowledge graph embeddings via paired relation vectors</title>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.336</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.336" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10873</idno>
		<title level="m">Connecting embeddings for knowledge graph entity typing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Zero-shot open entity typing as type-compatible grounding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lowfer: Low-rank bilinear pooling for link prediction</title>
		<author>
			<persName><forename type="first">Saadullah</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stalin</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?nter</forename><surname>Neumann</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning entity type embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagiza</forename><forename type="middle">F</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on conference on information and knowledge management</title>
		<meeting>the 2017 ACM on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2215" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relational message passing for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1697" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Orthogonal relation transforms with graph context modeling for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04910</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding compression</title>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mulde: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1716" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Nodepiece: Compositional and parameterefficient representations of large knowledge graphs</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12144</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural fine-grained entity type classification with hierarchy-aware loss</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03378</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">Komal</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Question answering over temporal knowledge graphs</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01515</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07810</idno>
		<title level="m">Codex: A comprehensive knowledge graph completion benchmark</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dgl-ke: Training knowledge graph embeddings at scale</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Da Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
