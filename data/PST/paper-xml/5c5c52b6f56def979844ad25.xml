<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Potential of Conditional Adversarial Networks for Optical and SAR Image Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nina</forename><surname>Merkle</surname></persName>
							<email>nina.merkle@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Remote Sensing Technology Institute</orgName>
								<orgName type="department" key="dep2">German Aerospace Center (DLR)</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Remote Sensing Technology Institute</orgName>
								<orgName type="department" key="dep2">German Aerospace Center (DLR)</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Auer</surname></persName>
							<email>stefan.auer@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Remote Sensing Technology Institute</orgName>
								<orgName type="department" key="dep2">German Aerospace Center (DLR)</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rupert</forename><surname>Müller</surname></persName>
							<email>rupert.mueller@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Remote Sensing Technology Institute</orgName>
								<orgName type="department" key="dep2">German Aerospace Center (DLR)</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Peter</forename><surname>Reinartz</surname></persName>
							<email>peter.reinartz@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Remote Sensing Technology Institute</orgName>
								<orgName type="department" key="dep2">German Aerospace Center (DLR)</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>Wessling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Potential of Conditional Adversarial Networks for Optical and SAR Image Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B7A4C64226CDE3F876E86DCBF6CBA77</idno>
					<idno type="DOI">10.1109/JSTARS.2018.2803212</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial image generation</term>
					<term>conditional generative adversarial networks (cGANs)</term>
					<term>multisensor image matching</term>
					<term>optical satellite images</term>
					<term>synthetic aperture radar (SAR)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tasks such as the monitoring of natural disasters or the detection of change highly benefit from complementary information about an area or a specific object of interest. The required information is provided by fusing high accurate coregistered and georeferenced datasets. Aligned high-resolution optical and synthetic aperture radar (SAR) data additionally enable an absolute geolocation accuracy improvement of the optical images by extracting accurate and reliable ground control points (GCPs) from the SAR images. In this paper, we investigate the applicability of a deep learning based matching concept for the generation of precise and accurate GCPs from SAR satellite images by matching optical and SAR images. To this end, conditional generative adversarial networks (cGANs) are trained to generate SAR-like image patches from optical images. For training and testing, optical and SAR image patches are extracted from TerraSAR-X and PRISM image pairs covering greater urban areas spread over Europe. The artificially generated patches are then used to improve the conditions for three known matching approaches based on normalized crosscorrelation (NCC), scale-invariant feature transform (SIFT), and binary robust invariant scalable key (BRISK), which are normally not usable for the matching of optical and SAR images. The results validate that a NCC-, SIFT-, and BRISK-based matching greatly benefit, in terms of matching accuracy and precision, from the use of the artificial templates. The comparison with two state-of-theart optical and SAR matching approaches shows the potential of the proposed method but also revealed some challenges and the necessity for further developments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ULTISENSOR image fusion is a prerequisite for the pro- vision of complementary information through the combination of different data. Aligned multisensor data enable a more robust interpretation of image scenes or specific objects and is, therefore, crucial for tasks such as monitoring natural disasters and change detection. In the case of optical and synthetic aperture radar (SAR) satellites, the images acquired by both sensors exhibit quite different characteristics: SAR satellites have an active sensor on board which emits electromagnetic signals and measures the strength and time delay of the returned signal backscattered from ground objects. The visual interpretation of SAR images is a challenging task, due to the specific imaging principle and the presence of speckle in the images. In contrast, optical sensors measure the sun radiation reflected from objects on ground. The interpretation of optical images is easier, which makes the development of feature detectors and, therefore, the detection of features more efficient and robust. An advantage of an SAR sensor (especially TerraSAR-X and TanDEM-X) is that the images exhibit absolute geolocation accuracies within few decimeters, whereas high-resolution optical sensors still require ground control points (GCPs) to reach similar accuracies. This can be traced back to the different image acquisition concepts. SAR sensors determine the distance to ground object via the signal traveling time, which can be measured precisely if atmospheric effects also are taken into account and lead to images with high geolocation accuracy. Due to recent developments in SAR geodesy, high-resolutions SAR satellites such as TerraSAR-X exhibit an absolute geolocalization accuracy in the range of a few decimeters <ref type="bibr" target="#b0">[1]</ref>. Optical sensors in contrast, require the measurement of the attitude angles in space to determine the satellite-viewing direction to ground objects, which often suffers from insufficient accuracy of the measurements and results in images with lower absolute geolocation accuracy.</p><p>The main objective of our research is, therefore, the improvement of the absolute geolocalization accuracy of optical satellite images via automatic extracted GCPs from images acquired by the high-resolution radar satellite TerraSAR-X. If GCPs are available, the geolocalization accuracy of the optical images can be enhanced by using these points to correct the underlying sensor model parameters for the georeferencing process. However, GCPs are commonly measured by tedious in situ GPS measurements or from very exact maps and are, therefore, available only in the minority of cases. To overcome this shortage, this paper focuses on an automatic procedure to generate GCPs through the matching of optical and SAR images. Note that we will leave out the subsequent step of geolocalization accuracy improvement of the optical images as, e.g., performed in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>.</p><p>The process of image matching and registration is of interests for a variety of applications in fields, such as medicine, computer vision and remote sensing, and hundreds of different approaches have been developed <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Common methods for the matching of optical and SAR images are mostly based on intensity-or feature-based matching concepts. Intensity-based methods often exploit similarity measures, such as normalized cross-correlation (NCC) <ref type="bibr" target="#b6">[7]</ref>, mutual information (MI) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, or cross-cumulative residual entropy <ref type="bibr" target="#b8">[9]</ref>. On the other hand features like lines <ref type="bibr" target="#b9">[10]</ref>, contours <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, or regions <ref type="bibr" target="#b12">[13]</ref> are widely used for feature-based matching approaches. A modification of the common feature detector scale-invariant feature transform (SIFT), which usually fails to detect corresponding features in SAR and optical images, has been investigated in <ref type="bibr" target="#b13">[14]</ref>. All these approaches suffer from speckle in the SAR images and different geometric and radiometric properties induced by the disparate image acquisition concepts of the two sensors. This leads to the problem of either finding a reliable similarity measure between the images, or extracting reliable features from the image scenes. To circumvent this problem Han and Byun <ref type="bibr" target="#b14">[15]</ref> proposed an approach, which combines aspects of feature-and intensity-based methods. In our previous work, we investigated the applicability of a deep learning based method for the task of optical and SAR images matching <ref type="bibr" target="#b2">[3]</ref>. To handle the matching problems arising from optical and SAR data, we propose to select specific areas, where only the radiometry is different in both images. Using these areas we successfully trained a Siamese neural network to learn the matching between SAR and optical image patches and achieved better results than state-of-the-art approaches.</p><p>Inspired by the high potential and the possibilities provided by new developments in the field of deep learning, we continue our investigation and propose a new deep learning based technique for automatic GCPs generation through matching of optical and SAR image patches. Toward this goal, we trained a conditional generative adversarial network (cGAN) to generate artificial SAR-like image patches from optical satellite images. In contrast to our previous work in <ref type="bibr" target="#b2">[3]</ref>, where the matching between optical and SAR patches was directly learned by a Siamese network, the idea here is to use the artificially generated patches to improve the accuracy and precision of common matching approaches, which are usually inapplicable for the matching between optical and SAR images. The evaluation focuses on one intensity-based, NCC <ref type="bibr" target="#b15">[16]</ref>, and on two feature-based matching approaches, SIFT <ref type="bibr" target="#b16">[17]</ref> and binary robust invariant scalable key (BRISK) <ref type="bibr" target="#b17">[18]</ref>. Optical and SAR image pairs acquired over Europe (from 46 TerraSAR-X and PRISM scenes) and manually aligned are used for training and evaluating the network.</p><p>The results are compared with two state-of-the art optical and SAR matching approaches and demonstrate the effectiveness of the proposed method. A visualization of the method is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>This paper represents an extension of our earlier work presented in <ref type="bibr" target="#b18">[19]</ref>. Compared to <ref type="bibr" target="#b18">[19]</ref>, we extended the method by two additional cGAN loss functions and extensively investigated and discussed the influence of the different losses, the different batch sizes, the different training datasets, and the influence of a speckle filter on the matching results. Furthermore, we compared the obtained results with two available state-ofthe-art optical and SAR matching approaches. The main contributions of our paper are: 1) Providing a new concept to handle the problem of multisensor image matching based on cGAN, which 2) improves the results of common techniques (NCC, SIFT, and BRISK) for the matching between optical and SAR images while 3) achieving comparable results in regard to two state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERATIVE ADVERSARIAL NETWORKS</head><p>Neural networks, especially convolutional neural networks, proved their high potential in various fields like computer vision, biology, medical imaging, and remote sensing. Recently, Goodfellow et al. <ref type="bibr" target="#b19">[20]</ref> introduced a new machine-learning architecture, GANs, which earned a lot of attention in the field of machine learning and offers new possibilities for several research problems by generating high quality images. In computer vision, GANs find application for problems such as semantic segmentation <ref type="bibr" target="#b20">[21]</ref> or single image super-resolution <ref type="bibr" target="#b21">[22]</ref>. In the field of medicine, GANs are successfully applied for the generation of computed tomography images from magnetic resonance imaging to reduce the radiation exposure to patients during acquisition <ref type="bibr" target="#b22">[23]</ref>. In the context of remote sensing, Guo et al. <ref type="bibr" target="#b23">[24]</ref> investigated the application of GANs for the synthesis of SAR images.</p><p>GANs are generative models with the goal of training a generator network G to map random noise z to output images y. The training is realized through an adversarial process, which is based on the simultaneous training of two networks, the generator G and the discriminator D. The task of D is to distinguish as good as possible between real images and images G(z) generated by G, whereas G tries to produce more and more realistic images to "fool" D as often as possible. The problem can be expressed through a two-player minimax game</p><formula xml:id="formula_0">min G max D L GAN (G, D) = E y ∼p r e a l (y ) [log D(y)] + E y ∼p r e a l (y ),z ∼p z (z ) [log(1 -D(G(z)))] (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where E denotes the expected value, p real denotes the real data distribution, and p z denotes the noise distribution. D is realized by a binary classification network and outputs the possibility that an input image belongs either to the class 0 ("fake") or to the class 1 ("real"). The aim of D during training is to get D(G(z)) close to 0, which means to detect all images generated by G and label them correctly as "fake." In contrast, G aims to get D(G(z)) close to 1, which means that D does not identify the artificial images generated by G and wrongly label them as "real." To ensure that the output values of D lie in the range of [0, 1] a sigmoid layer can be used as the last layer of D.</p><p>In this paper, we investigate the applicability of conditional GANs (cGANs) and we, therefore, utilize the open source implementations from Isola et al. <ref type="bibr" target="#b24">[25]</ref>. In the following section, we will describe the concept of cGANs and how to use them for the matching of optical and SAR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTISENSOR IMAGE MATCHING</head><p>The proposed method deals with the problem of matching SAR and optical image patches in three steps. In the first stage, suitable matching areas are selected from optical and SAR images. The second stage is the generation of artificial SAR-like patches from optical image patches through cGANs. The third stage is the matching of artificially generated SAR patches with the real SAR image patches using an intensity-based (NCC) and two feature-based matching approaches (SIFT and BRISK).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Matching Area Selection</head><p>The preselection of suitable matching areas increases the probability to obtain accurate and reliable matching points between SAR and optical images. Candidates for such areas contain almost only planar objects, which exhibit the same (at least to a certain degree) geometric appearance in the optical and in the corresponding SAR image. Furthermore, these areas should contain salient features to increase the probability of a successful matching. In most cases, these features are related to man-made infrastructure objects such as streets, street crossings, roundabouts, and borders between agricultural fields. The reason for excluding three-dimensional objects are the different geometric distortions induced by the different sensors of optical and SAR satellites. Elevated objects like buildings appear differently in SAR and optical images and get projected to different positions within the image. These features are, therefore, not suitable for the identification of GCPs. The collection of suitable patches is realized via a semimanual selection procedure. For obtaining a first indication of areas containing fitting patterns, the CORINE land cover layer <ref type="bibr" target="#b25">[26]</ref> was applied. By applying this layer to the images all cities, industrial, and forest areas can be excluded from the image search space. This first selection was refined manually to ensure that features within these areas are actually visible in both the optical and the SAR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Artificial Image Generation</head><p>In contrast to the common GAN setup, where new images are generated only from noise, we want to generate artificial images based on a specific input image (an optical image patch). The aim is to generate an artificial SAR-like image with geometric properties of an optical image and with radiometric properties of an SAR image (the impact of geometric distortion is reduced by the preselection of patches as described in the previous Section III-A). Therefore, we utilize cGANs, which rely, next to noise z, on observed images x. The cGAN loss can be stated as</p><formula xml:id="formula_2">L cGAN (G, D) = E x,y ∼p r e a l (x,y ) [log D(x, y)] + E x,y ∼p r e a l (x,y ),z ∼p z (z ) [log(1 -D(x, G(x, z)))] (2)</formula><p>where x denotes an optical patch, y denotes the corresponding SAR patch (the ground truth image patch) and G(x, z) denotes the artificially generated SAR-like patch. As in <ref type="bibr" target="#b24">[25]</ref> we extend (2) by an additional term</p><formula xml:id="formula_3">L L 1 (G) = E x,y ∼p d a t a (x,y ),z ∼p z (z ) [ y -G(x, z) 1 ].<label>(3)</label></formula><p>This term forces G to produce output images, which are close to the ground truth SAR patches y (in sense of the L 1 distance). Adding this term lead to the final objective</p><formula xml:id="formula_4">G * = arg min G max D L cGAN (G, D) + λL L 1 (G).<label>(4)</label></formula><p>A common problem of (conditional) GANs with an objective based on the negative log-likelihood [see <ref type="bibr" target="#b1">(2)</ref>], is the unstable training. Recent research studies like <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> try to overcome this problem by describing more stable training procedures. We, therefore, investigate the influence of two alternative training procedures on our matching results. The first was proposed in <ref type="bibr" target="#b26">[27]</ref> and only requires a change in the loss function L cGAN . The idea is to replace the cGAN loss from Equation (2) by a least square loss, which leads to the new cGAN setup denoted with cLSGAN and defined as:</p><formula xml:id="formula_5">L cLSGAN (G, D) = E x,y ∼p r e a l (x,y ) [(D(x, y) -1) 2 ] + E x,y ∼p r e a l (x,y ),z ∼p z (z ) [D(x, G(x, z))) 2 ].<label>(5)</label></formula><p>The second approach was proposed in <ref type="bibr" target="#b27">[28]</ref>. Here, the idea is to restate the problem with the aim of minimizing the Wasserstein distance instead of the Jensen-Shannon divergence, which is the case for the common GAN problem. This can be achieved by employing the conditional Wasserstein GAN (cWGAN) loss</p><formula xml:id="formula_6">L cWGAN (G, D) = E x,y ∼p r e a l (x,y ) [D(x, y)] + E x,y ∼p r e a l (x,y ),z ∼p z (z ) [D(x, G(x, z))]. (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Applying cWGANs also requires to clip the weights of the discriminator network D to be in the interval from -0.01 to 0.01. In the following, this type of cGAN will be called the cWGAN setup and a detailed theoretical overview of it can be found in <ref type="bibr" target="#b27">[28]</ref>. Network architecture: The generator G is realized via a U-net, which is an encoder-decoder type of network with skip connections between layer i and layer ni (n is the total number of layers). A skip connection between the layers i and ni means to concatenate all channels of layer i with those of layer ni. An example of this network type is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The discriminator is realized via several convolutional layers and the sigmoid function as the last layer. For a detailed overview of the network architecture we refer to <ref type="bibr" target="#b24">[25]</ref>.</p><p>Network training: The training dataset consists of optical and SAR image patches, where we determined to train on patches with a size of 201 × 201 pixels [large enough to ensure the existence of salient features within the patches but not too large to run into problems caused by memory limits of our available general public utilities (GPUs)]. Before extracting the patches all images must be geometrically aligned. The discriminator network D is alternately trained on two different kinds of training pairs. Half of the training pairs are "fake" examples and are composed of optical and artificial generated SAR-like patch pairs. The other half are "real" examples and are composed optical and SAR patch pairs. An illustration of the two different training setups are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The networks are trained with stochastic gradient descent with the ADAM optimizer <ref type="bibr" target="#b28">[29]</ref> and an initial learning rate of 0.01 for the cGAN and cLSGAN setups and with the RMSProp optimizer <ref type="bibr" target="#b29">[30]</ref> and an initial learning rate of 0.0002 for the cWGAN setup. For all setups the two networks are trained at the same time by alternating the training of D and G (one gradient descent step of D is followed by one gradient descent step of G in the cGAN and cLSGAN setups and five gradient descent steps of D are followed by one gradient descent step of G in the cWGAN setup). To improve the quality of the learning while utilizing the L cGAN loss, we follow the common practice for the training of G, which is to maximize log(D(x, G(x, z))) instead of minimizing log(1 -D(x, G(x, z))).</p><p>Network testing: The networks are tested by comparing the quality improvement of the results between the matching of artificial SAR-like (generated by the trained cGAN, cLSGAN, and cWGAN setups) with SAR patches and the matching of optical with SAR patches. Different techniques are utilized for the patch matching and are introduced in the following Section III-C. Note that in this phase of the process only the trained generator network G is required (as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>) and the weights of G are not modified during the test phase (one input patches will always lead to the same artificial output patch). To guarantee a fair evaluation, we only utilize artificial patches for the matching, which are generated from a test set. The test set contains optical patches, which were never shown to the networks during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Artificial Image Matching</head><p>Several approaches exist to realize the matching between a template T and a corresponding reference image R. In our investigations, we focus on one intensity-based and two featurebased approaches, which usually lead to inaccurate results for the matching of optical and SAR images. For the later evaluation, the template T will either be a patch cropped from the optical image or the generated artificial SAR-like patch and R a patch cropped from the SAR image.</p><p>Intensity-based approaches measure the similarity between T and a larger reference image R at all locations within the search space. We use a sliding window technique to compute the NCC <ref type="bibr" target="#b15">[16]</ref> value for every location of T within R. The correct matching position is given by the highest NCC value within the search space. Since we are only interested in reliable and accurate matching points, we use the NCC value as a quality measure to detected outliers in the set of matching points. More precisely, we remove all matching points with an NCC value of less than 0.4.</p><p>In contrast, feature-based approaches are based on the detection of features in both images, called key points, and the measurement of their similarity in the feature space. The two feature detectors utilized in this paper are the SIFT <ref type="bibr" target="#b16">[17]</ref> and the BRISK <ref type="bibr" target="#b17">[18]</ref>. The idea of both algorithms is to find key points in T and R and to return a descriptor for every key point. The descriptors of two images are then matched by utilizing the Euclidean distance for SIFT and the Hamming distance for BRISK in combination with a nearest neighbor search. To increase the quality and reliability of the detected matching points, we remove outliers through RANSAC <ref type="bibr" target="#b30">[31]</ref> with an underlying affine model and with a distance threshold of five pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>For the network training and for the evaluation of the results, training and test datasets are generated out of 46 orthorectified and aligned optical (PRISM) and SAR (TerraSAR-X acquired in spotlight mode) satellite image pairs. The manual alignment was realized within the Urban Atlas project <ref type="bibr" target="#b31">[32]</ref> with an overall alignment error in the range of 3 m. The images cover greater </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Matching Area Selection</head><p>For the selection of suitable regions within the images (as described in Section III-A), we utilized the CORINE land cover layer <ref type="bibr" target="#b25">[26]</ref> from the year 2012 and with a pixel spacing of 100 m. The following classes are chosen as suitable regions: airports, nonirrigated arable land, permanently irrigated land, annual crops associated with permanent crops and complex cultivation patterns, land principally occupied by agriculture, with significant areas of natural vegetation. After a manual refinement, we generated two different training datasets and one test dataset. The first training dataset contains 69 900 optical and SAR patch pairs with a resolution of 2.5 m. The second training dataset contains all patch pairs from the first training dataset, but with a resolution of 2.5 and 3.75 m. The patches with 3.75-m resolution are centered around the same location as the 2.5-m resolution patches but contain bigger areas and only exists in the dataset if the patches do not exceed the image boundaries. This led to a total number of 1 37 450 patch pairs. The second training dataset is deployed to enlarge the number of training samples and to investigate the influence of different image resolutions on the quality of the patch generation and, hence, of the later matching. Since the matching should be as precise as possible, the test dataset contains only patches with a resolution </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Artificial Image Generation</head><p>We investigated several setups for the generation of artificial SAR-like patches. This includes the generation of (despeckled) SAR-like patches at varying scales (pixel size: 2.  <ref type="table" target="#tab_0">I</ref>. Note that all artificially generated patches shown in this paper are generated from test set patches. Fig. <ref type="figure" target="#fig_2">3</ref> shows examples of artificial (despeckled) SAR patch with a pixel size of 2.5 m generated by utilizing three different  setups: The first setup utilizes the cWGAN, a batch size of 40, and the 2.5 m dataset. The second setup utilizes the cGAN, a batch size of 1 and the 2.5-m dataset. In contrast to the other two setups, here the filtered SAR images were used for the training procedure. The third setup utilizes the cLSGAN loss, a batch size of 4, and the 2.5-m dataset. These examples illustrate that the geometric structures of streets from optical images are preserved in the generated templates, while the radiometric properties are adapted to SAR or despeckled SAR images. The generator learned that, in contrast to optical images, streets normally appear with a lower intensity in SAR images. Furthermore, G tries to represent the characteristics of speckle or the resulting pattern from the speckle filter in case of the first and second setup. A characteristic of the third setup is the blurry appearance of objects such as fields and hence the absence of speckle in the generated patches, which is caused by the utilization of the L 2 loss. The development of the learning process of the generator G of the first (cWGAN) setup over the training time is exemplified by Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>Future prospects: We further considered to reverse the whole process and to generate artificial optical images out of SAR images. An example of such artificial optical images is shown in Fig. <ref type="figure" target="#fig_5">5</ref>. Despite the reasonable visual appearance, the artificial optical images could not improve the later image matching and partly led to a deterioration of the matching results. We attribute this to the fact that optical images reveal a higher level of detail as SAR images and that the extraction and generation of features from SAR images is more difficult as from optical images. Therefore, it is more difficult to preserve image features, which The matching accuracy is measured as the percentage of matching points having a L 2 distance to the ground truth location smaller than 3 pixels, and as the average over the L 2 distances between the predicted matching points and the ground truth locations (measured in pixel units). The matching precision is represented by the standard deviation σ (measured in pixel units).</p><p>is important for a reliable and accurate matching. Nevertheless, this direction provides a possibility for a better interpretation or visual understanding of SAR images for nonexperts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Artificial Image Matching</head><p>To investigate the influence of the artificialy generated patches on the NCC-, SIFT-, and BRISK-based matching, we evaluated the matching accuracy and precision between SAR and optical image patches and we compared the results with two state-ofthe-art methods CAMRI <ref type="bibr" target="#b1">[2]</ref> and DeepMatch <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table" target="#tab_1">II</ref> gives an overview of the different methods and the corresponding matching accuracies and precisions, which are all evaluated over the same test set. The matching accuracy is measured as the percentage of matching points having a L 2 distance with less than three pixels to the ground truth location, and as the average over the L 2 distances between the computed matching points and the ground truth locations (measured in pixel units). The matching precision is represented by the standard deviation σ (measured in pixel units).</p><p>The test patches are extracted from six different optical and SAR image pairs. Note that we applied the SIFT-and BRISKbased matching in combination with RANSAC (with an affine model) on the patches of each image scene separately. All artificial patches used to obtain the results from Table II are generated by utilizing the same cLSGAN setup, which is trained on the larger test set (2.5 m + 3.75 m) and with a batch size of 4. This setup let to the best overall results (see later discussion The matching accuracy is measured as the percentage of matching points having a L 2 distance to the ground truth location smaller than three pixels, and as the average over the L 2 distances between the predicted matching points and the ground truth locations (measured in pixel units). The matching precision is represented by the standard deviation σ (measured in pixel units).</p><p>about the influence of the loss function and Table <ref type="table" target="#tab_2">III</ref>). For the six image scenes and the application of SIFT and RANSAC, we obtained 84, 7, 10, 9, 55, 110 matching points between the optical and the SAR patches and 235, 120, 70, 25, 363, 286 matching points between the artificial generated and the SAR patches. For the combination of BRISK and RANSAC we obtained 460, 52, 592, 101, 1409, 687 points for the matching between the optical and the SAR patches and 697, 393, 520, 164, 3834, 1052 matching points between the artificialy generated and the SAR patches. For the NCC-based matching we only considered points with an NCC value of 0.4 or higher as valid matching points and obtained in total 346 points (for all 6 image pairs) for the optical and SAR patch matching and 155 points for artificial and SAR patch matching.</p><p>In the case of the SIFT and BRISK batch matching, the use of artificial templates increased the number of obtained matching points and in all cases it significantly improved the matching accuracy and precision of the NCC-, SIFT-, and BRISK-based matching (see Table <ref type="table" target="#tab_1">II</ref>). This is an important requirement for the intended application of the proposed method for the geolocation accuracy improvement of optical images. For this application only few matching points are required for every image scene, but these points have to exhibit a high accuracy and precision.</p><p>Influence of the loss function: To identify the best setup for our application, we investigated the influence of the three different loss functions introduced in Section III-B and their dependency on the batch size and the dataset size. An overview of the results of the tested setups is shown in Table <ref type="table" target="#tab_0">I</ref>. We achieved the best results for the cGAN and cWGAN setup by training on the smaller dataset and with a batch size of 40 and 1, respectively, and for the cLSGAN setup by training on the larger dataset with a batch size of 4. Table <ref type="table" target="#tab_2">III</ref> shows a comparison of the obtained results by applying the three loss function of the cGAN, cLSGAN, or cWGAN setup. The setup that generated the best matching results (with respect to the matching accuracy and precision) is the cLSGAN, which utilizes the least square loss. As stated in Section IV-B, the utilization of the least square loss causes the absence of artificial speckle in the generated patches. Therefore, the better matching performance of the cLSGAN (compared to the cGAN and cWGAN setups) can be traced back to the fact that the applied matching methods (NCC, SIFT, and BRISK) normally suffer from speckle in the image patches. Moreover, since the "real" speckle structure of the SAR patches cannot be derived from the optical patches, it cannot be learned by the generator. As a consequence, the generator network will produce patches, which contain random speckle that looks real enough to "fool" the discriminator network. Overall, the occurrence of artificial speckle in the generated patches makes the matching more difficult.</p><p>Influence of the speckle filter: The application of a speckle filter is an important preprocessing step for many matching methods and is used to improve the results of CAMRI <ref type="bibr" target="#b1">[2]</ref> and DeepMatch <ref type="bibr" target="#b2">[3]</ref>. Therefore, we exploited two application cases of the speckle filter. First, we investigated the influence of the despeckled SAR patches on the NCC-, SIFT-, and BRISK-based matching (without the use of cGANs). Only in the case of the BRISK-based matching, the usage of the speckle filter led to an improvement of the matching results (# matching points &lt; 3 pixels = 52.21%, avg L 2 = 3.00, σ = 1.37). Second, we investigated the generation of SAR-like despeckled patches via cGANs and their influence on the NCC-, SIFT-, and BRISKbased matching. Utilizing these patches led in none of the matching setups to better matching results compared to the matching using SAR-like artificial patches. We trace this back to the fact that even if the texture of the speckle filter is well imitated (as illustrated in the second row of Fig. <ref type="figure" target="#fig_2">3</ref>) it is randomly generated and independent from the real-image objects or their properties and, therefore, led to unreliable matching results.</p><p>Comparison with baselines: For a better assessment of the quality of the results, a comparison with two state-of-the-art approaches is carried out. By applying the SIFT-and BRISK-based matching, we can achieve better results than the first baseline called CAMRI <ref type="bibr" target="#b1">[2]</ref>. CAMRI is a MI-based method and is tailored to the problem of optical and SAR images matching. The second baseline, called DeepMatch <ref type="bibr" target="#b2">[3]</ref>, is a deep learning based matching approach, where the matching between optical and SAR images is realized through a trained Siamese network. Regarding the accuracy of the matching points DeepMatch achieves better results than applying a SIFT-and BRISK-based matching between the artificial patches and the SAR patches and in regard to the matching precision our proposed approach achieves slightly better results compared to DeepMatch.</p><p>Qualitative results of NCC: Fig. <ref type="figure" target="#fig_6">6</ref> shows a qualitative comparison of the NCC-based matching between optical and SAR patches, and generated templates and SAR patches. The search space is Δ x = Δ y = 20 pixels in each direction around the center position. The used templates are generated by using the best setup (cLSGAN setup with the least square loss). The correct matching positions are for all examples in the center of the SAR patches. The brighter the color of the score map, the higher is the NCC value at the corresponding location. The examples emphasize that the generated SAR-like templates can improve the matching between SAR and optical images through NCC.</p><p>Limitations: A problem of (conditional) GANs is the difficult validation of the training success. In contrast to other machine- learning architectures, where the loss function or different metrics can be used to evaluate the quality of the training progress over a validation set, GANs require an evaluation mainly via the visual quality of the generated images or (in our case) the evaluation of the matching results. This is time consuming, since every setup has to be trained till the end to find the best one. A further time consuming task is the training of the cGANs, which takes from some days to several weeks. Besides high computational cost of the network training and data quality evaluation, the experiments revealed that it is important to generate patches that retain the geometric structures of the optical patches instead of generating patches that visually look like real SAR images (see Table <ref type="table" target="#tab_2">III</ref> and corresponding discussion). Therefore, not every loss and cGAN setup is applicable for the problem of optical and SAR image matching.</p><p>Strengths: An advantage of the proposed method is that it enables the application of well-know matching techniques (NCC, BRISK, and SIFT) on the problem of matching optical and SAR images. These three matching methods proved their high quality for the matching of images acquired from the same sensor (e.g. NCC for SAR to SAR matching <ref type="bibr" target="#b33">[34]</ref> and SIFT and BRISK for matching optical images <ref type="bibr" target="#b34">[35]</ref>), but normally fail in the case of optical and SAR images. The evaluation of the results and the comparison with two state-of-the-art matching approaches revealed the potential of the proposed method and the possibility to apply it for the problem of absolute geolocation accuracy improvement of optical images. A further benefit is the fast applicability of the proposed method to new image scenes once the generator is trained. For the matching of new images scenes, artificial SAR-like patches can be generated within minutes from given optical patches. Furthermore, through the variety in our training dataset, which contains images acquired at different times of the year and over different locations in Europe, our proposed approach is applicable to a wide range of images acquired over different countries.</p><p>Future prospects: The future the proposed method could be further improved by utilizing the sensor model of the input image for RANSAC instead of an affine model. The affine model works well for relatively flat areas but is not suitable for every image scene. Moreover, the investigation of different generator architectures represents a further interesting investigation. Another possible enhancement for the future is the combination of the proposed technique with DeepMatch <ref type="bibr" target="#b2">[3]</ref>. So far, the training of the cGANs is geared to the problem of generating images, which look realistic enough to "fool" the discriminator. The results reveal that patches, which look more like real SAR images not necessarily lead to better matching results. Therefore, it is more important to preserve features such as edges or corners, which are beneficial for a matching technique, in the artificial patches. By replacing the discriminator with the Siamese matching network proposed in <ref type="bibr" target="#b2">[3]</ref> the training of the generator G could be tailored toward the problem of generating artificial patches, which lead to better matching results than using the original optical patches. This combination represents a promising development for the future to further improve the results obtained by the proposed method and by DeepMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed a new concept for the problem of multisensor image matching based on cGANs. Different cGANs setups are trained for the task of generating SAR-like image patches from optical images. We showed the feasibility to improve the matching accuracy and precision of an NCC-, SIFT-, and BRISKbased matching between optical and SAR image patches by artificial generated patches. By applying BRISK for the matching of SAR and artificial SAR-like patches we achieve matching points with an average L 2 distance to the ground truth locations of 2.22 pixels and a precision (standard deviation) of 1.10 pixels. The results further validate the potential of the proposed approach in comparison to two state-of-the-art methods but also revealed the need for further enhancements of the proposed method. Especially, the necessity for a generator network, which reliably and precisely retain the geometric structures of the optical images, should be the main focus of further investigations. Overall, the proposed method opens up new possibilities for future developments toward the goal of matching optical and SAR images. The combination of a generator network with a deep learning based matching approach represents thereby a promising future extension to generate even more suitable artificial images patches and, hence, to further improve the quality of the image matching. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical overview of the proposed method for the absolute geolocalization improvement of optical images by matching SAR and artificial patches generated by cGANs.</figDesc><graphic coords="2,58.55,88.85,480.26,121.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of cGAN training procedure. On the left side the training setup for "fake" examples (optical-and artificial-generated patch) as input and on the right side for "real" examples (optical and SAR patch pair) as input.</figDesc><graphic coords="4,45.11,76.13,245.06,168.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Side by side comparison between optical, artificial SAR, and original (despeckled) SAR image patches with pixel size of 2.5 m for three different setups in two columns. Setup 1: SAR-like patch generation utilizing the cWGAN loss with a batch size of 1; Setup 2: despeckled SAR-like patch generation utilizing the cGAN loss with a batch size of 40; Setup 3: SAR-like patch generation utilizing the cLSGAN loss with a batch size of 4. urban zones including suburban, industrial and rural areas of 13 cities in Europe. The pixel spacing of the PRISM images is 2.5 m and of the TerraSAR-X images 1.25 m. To obtain larger training datasets the TerraSAR-X images are resampled to 2.5 and 3.75 m and the PRISM images to 3.75 m through bilinear interpolation. To investigate the influence of speckle on the matching results, we despeckled all SAR images applying the probabilistic patch-based filter introduced in [33].</figDesc><graphic coords="5,68.39,81.89,461.90,229.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5 and 3.75 m), the training of G through different losses (cGAN, cLSGAN, and cWGAN), the training with different batch sizes (1, 4, and 40) and the training with despeckled and original SAR images as reference. Here, the batch size refers to the number of training instances used in one iteration of the training. For every setup, the cGANs are trained over 200 epochs (one epoch refers to one whole cycle through the entire training set) on a single NVIDIA GeForce GTX Titan X GPU. The training time varied from several days to several weeks depending on the batch size, the size of the training dataset, and the chosen cGAN setup. An overview of the different training setups can be seen in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Development of the generator over training: the optical input patch, the artificial patches at epoch 1, 10, 50, 200, and the SAR target patch (from left to right).</figDesc><graphic coords="6,42.15,186.91,250.80,167.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Side by side comparison between SAR, artificial optical and original optical example patches with a pixel size of 3.75 m in two rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of the score maps between the NCC-based matching of the optical image with the SAR image and the generated template (from the optical image) with the despeckled SAR image (from top down and in two columns).</figDesc><graphic coords="8,59.16,68.01,480.00,153.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Nina</head><label></label><figDesc>Merkle received the B.Sc. and M.Sc. degrees in mathematics from the University of Applied Science Regensburg, Regensburg, Germany, in 2011 and 2013, respectively. She is working toward the Ph.D. degree in remote sensing at the Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany. Since 2014, she has been a Research Fellow with the Remote Sensing Technology Institute, German Aerospace Center. In spring and autumn 2016, she was a Guest Scientist with the Computer Science Department, University of Toronto, Toronto, ON, Canada. Her current research interests include image matching and registration of multisensor satellite data and machine learning techniques, especially deep learning and its applications in the field of remote sensing. Stefan Auer received the Dipl.Ing.(Univ.) degree in geodesy, and the Dr. Ing. degree in remote sensing, from Technical University of Munich, Munich, Germany, in 2005 and 2011. Since December 2014, he has been a Senior Researcher and Project Manager with the Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Germany. Besides managing projects in the fields of multispectral and hyperspectral imaging, he works on the alignment of multimodal remote sensing data. He developed the opensource 3D SAR simulator RaySAR which can be used to better understand the nature of prominent SAR image signatures based on object models. In the context of his doctoral thesis, he spent three months as a Guest Researcher with the Department of Electronic and Telecommunication Engineering, University of Naples "Federico II." His research interests include 3-D simulation/data fusion techniques and the interpretation of high-resolution SAR and optical images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I OVERVIEW</head><label>I</label><figDesc>OF THE DIFFERENT TRAINING SETUPS</figDesc><table><row><cell>Setup</cell><cell>dataset</cell><cell>batch size</cell><cell>filter</cell></row><row><cell>cGAN</cell><cell>2.5 m/2.5 m + 3.75 m</cell><cell>1/4/40</cell><cell>yes/no</cell></row><row><cell>cLSGAN</cell><cell>2.5 m/2.5 m + 3.75 m</cell><cell>1/4/40</cell><cell>yes/no</cell></row><row><cell>cWGAN</cell><cell>2.5 m/2.5 m + 3.75 m</cell><cell>1/4/40</cell><cell>yes/no</cell></row><row><cell cols="4">of 2.5 m, which are in total 14 400 patch pairs. Note that patches</cell></row><row><cell cols="4">extracted from one image are either used for the training or the</cell></row><row><cell>test dataset.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II INFLUENCE</head><label>II</label><figDesc>OF THE ARTIFICIAL GENERATED TEMPLATES ON THE MATCHING ACCURACY AND PRECISION OF NCC<ref type="bibr" target="#b15">[16]</ref>, SIFT<ref type="bibr" target="#b16">[17]</ref>, BRISK<ref type="bibr" target="#b17">[18]</ref>, AND A COMPARISON WITH TWO BASELINE METHODS</figDesc><table><row><cell></cell><cell cols="2">Matching accuracy</cell><cell>Matching precision</cell></row><row><cell>Methods</cell><cell>&lt;3 pixels</cell><cell>avg L 2</cell><cell>σ</cell></row><row><cell>NCC [16]</cell><cell>35.55%</cell><cell>5.50</cell><cell>4.76</cell></row><row><cell>SIFT [17]</cell><cell>31.10%</cell><cell>5.61</cell><cell>1.64</cell></row><row><cell>BRISK [18]</cell><cell>39.58%</cell><cell>3.61</cell><cell>1.70</cell></row><row><cell>NCC cL S G A N</cell><cell>75.48%</cell><cell>2.94</cell><cell>5.79</cell></row><row><cell>SIFT cL S G A N</cell><cell>68.85%</cell><cell>2.40</cell><cell>1.05</cell></row><row><cell>BRISK cL S G A N</cell><cell>75.21%</cell><cell>2.22</cell><cell>1.10</cell></row><row><cell>CAMRI [2]</cell><cell>57.06%</cell><cell>2.80</cell><cell>2.86</cell></row><row><cell>DeepMatch [3]</cell><cell>82.80%</cell><cell>1.91</cell><cell>1.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III INFLUENCE</head><label>III</label><figDesc>OF LOSS FUNCTION ON THE MATCHING ACCURACY AND PRECISIONOF NCC<ref type="bibr" target="#b15">[16]</ref>, SIFT<ref type="bibr" target="#b16">[17]</ref>, BRISK<ref type="bibr" target="#b17">[18]</ref> </figDesc><table><row><cell></cell><cell cols="2">Matching accuracy</cell><cell>Matching precision</cell></row><row><cell>Methods</cell><cell>&lt;3 pixels</cell><cell>avg L 2</cell><cell>σ</cell></row><row><cell>NCC cG A N</cell><cell>30.64%</cell><cell>4.76</cell><cell>4.40</cell></row><row><cell>SIFT cG A N</cell><cell>54.55%</cell><cell>2.84</cell><cell>1.19</cell></row><row><cell>BRISK cG A N</cell><cell>36.48%</cell><cell>4.50</cell><cell>1.63</cell></row><row><cell>NCC cL S G A N</cell><cell>75.48%</cell><cell>2.94</cell><cell>5.79</cell></row><row><cell>SIFT cL S G A N</cell><cell>68.85%</cell><cell>2.40</cell><cell>1.05</cell></row><row><cell>BRISK cL S G A N</cell><cell>75.21%</cell><cell>2.22</cell><cell>1.10</cell></row><row><cell>NCC cW G A N</cell><cell>24.00%</cell><cell>6.51</cell><cell>4.08</cell></row><row><cell>SIFT cW G A N</cell><cell>56.51%</cell><cell>2.89</cell><cell>1.39</cell></row><row><cell>BRISK cW G A N</cell><cell>58.06%</cell><cell>3.08</cell><cell>1.30</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rupert M üller received the Dipl.-Phys. degree in physics from the Ludwig Maximilians University of Munich, Munich, Germany.</p><p>He is a Team Leader with the Processors and Traffic Monitoring group, Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany, and is currently a Project Manager with the Ground Segment of MUSES (Multiuser System for Earth Sensing) with the hyperspectral instrument DESIS (DLR Earth Sensing Imaging Spectrometer) to be installed on the International Space Station. His main research interests include photogrammetric evaluation of optical data from air-and space-borne sensors, digital image processing, machine learning, and hyperspectral imaging.</p><p>Peter Reinartz (M'09) received the Dipl.-Phys. degree in theoretical physics from the University of Munich, Munich, Germany, in 1983, and the Ph.D. (Dr. Ing) degree in civil engineering from the University of Hannover, Hannover, Germanny, in 1989.</p><p>His dissertation is on optimization of classification methods for multispectral image data. He is currently the Department Head with the Department of Photogrammetry and Image Analysis, German Aerospace Centre (DLR), Remote Sensing Technology Institute (IMF), Cologne, Germany, andis a Professor of computer science with the University of Osnabrueck, Osnabrueck, Germany. He has more than 30 years of experience in image processing and remote sensing and more than 400 publications in these fields. His research interests include machine learning, stereo-photogrammetry and data fusion using space borne and airborne image data, generation of digital elevation models, and interpretation of very high resolution data from sensors like WorldView, GeoEye, and Pleiades. He is also involved in using remote sensing data for disaster management and using high frequency time series of airborne image data for real time image processing and for operational use in case of disasters as well as for traffic monitoring.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imaging geodesy-toward centimeter-level ranging accuracy with TerraSAR-X</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eineder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Minet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steigenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="661" to="671" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="939" to="949" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting deep matching and SAR data for the geo-localization accuracy improvement of optical satellite images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="586" to="603" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of image registration techniques</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="376" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image registration methods: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="977" to="1000" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A critical review of image registration methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Data Fusion</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="158" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A visual circle based image registration algorithm for optical and SAR imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
			<biblScope unit="page" from="2109" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic registration of SAR and optical images based on mutual information assisted Monte Carlo</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bornemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hellwich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="1813" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust automatic registration of multimodal satellite images using CCRE with partial volume interpolation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4050" to="4061" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A robust technique for precise registration of radar and optical satellite images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S T</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="593" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A contour-based approach to multisensor image registration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="320" to="334" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multisource data registration based on NURBS description of contours</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="569" to="591" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An improved model for automatic feature-based registration of SAR and SPOT images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dowmanb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="28" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Registration of optical and SAR satellite images by exploring the spatial relationship of the improved SIFT</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="661" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic and accurate registration of VHR optical and SAR images using a quadtree structure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2277" to="2295" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Burge</surname></persName>
		</author>
		<title level="m">Principles of Digital Image Processing: Core Algorithms</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BRISK: Binary robust invariant scalable keypoints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2548" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the possibility of conditional adversarial networks for multi-sensor image matching</title>
		<author>
			<persName><forename type="first">N</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp<address><addrLine>Fort Worth, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2633" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop Adversarial Training</title>
		<meeting>NIPS Workshop Adversarial Training<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recog<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">Jul. 21-26, 2017</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="2017-09">Sep. 2017</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Quebec City, QC, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synthetic aperture radar image synthesis by using generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1111" to="1115" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CORINE land cover technical guide-Addendum 2000</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feranec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Otahel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Environmental Agency, Copenhagen</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2000">2000, 2000</date>
			<pubPlace>Denmark</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Mach. Learn</title>
		<meeting>34th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations<address><addrLine>San Diego, California, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Machine Learning</title>
		<meeting><address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>COURSERA</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Urban atlas-DLR processing chain for orthorectification of prism and AVNIR-2 images and TerraSAR-X as possible GCP source</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hörsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internet Proc., 3rd ALOS PI Symp</title>
		<meeting>Internet ., 3rd ALOS PI Symp<address><addrLine>Kona, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iterative weighted maximum likelihood denoising with probabilistic patch-based weights</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deledalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2661" to="2672" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An improved normalized cross correlation algorithm for SAR image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="2086" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating the applicability of BRISK for the geometric registration of remote sensing images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dangelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="677" to="686" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
