<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omnivergent Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision Technology Group Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">The Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Omnivergent Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F45796892CED397C563257342A35449B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The notion of a virtual sensor for optimal 3D reconstruction is introduced. Instead of planar perspective images that collect many rays at a fixed viewpoint, omnivergent cameras collect a small number of rays at many different viewpoints. The resulting 2D manifold of rays are arranged into two multiple-perspective images for stereo reconstruction. We call such images omnivergent images, and the process of reconstructing the scene from such images omnivergent stereo. This procedure is shown to produce 3D scene models with minimal reconstruction error, due to the fact that for any point in the 3D scene, two rays with maximum vergence angle can be found in the omnivergent images. Furthermore, omnivergent images are shown to have horizontal epipolar lines, enabling the application of traditional stereo matching algorithms, without modification. Three types of omnivergent virtual sensors are presented: spherical omnivergent cameras, center-strip cameras and dual-strip cameras.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Planar perspective images have long been the primary substrate of computer vision algorithms, due to the ease of acquiring such images from conventional cameras. The last few years, however, have seen a growing interest in fundamentally new types of image representations for a variety of applications in computer vision and computer graphics. Such new representations include mosaics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> that encode radiance information from all angles converging at a single optical center, as well as non-perspective representations like light fields <ref type="bibr" target="#b4">[5]</ref>, lumigraphs <ref type="bibr" target="#b5">[6]</ref>, multiperspective panoramas <ref type="bibr" target="#b6">[7]</ref>, and manifold mosaics <ref type="bibr" target="#b7">[8]</ref>. These new image representations can be derived from one or more conventional images or acquired directly from new imaging devices, e.g., catadioptic cameras <ref type="bibr" target="#b8">[9]</ref>. In the former case, we can think of derived images as representing the output of a virtual sensor with certain characteristics.</p><p>£ The support of a grant from the Microsoft Corporation is gratefully acknowleged. Part of this work was conducted while Steven Seitz was employed by the Vision Technology Group at Microsoft Research.</p><p>The emergence of these new types of images suggests a powerful new paradigm for solving computer vision problems-rather than adapting the algorithm to suit the image, we can modify the image to best suit the algorithm. This approach has a powerful advantage: it becomes possible to boost the performance of a specific vision algorithm or even an entire class of algorithms by providing input data that is carefully crafted to improve the performance of the target algorithm(s). To demonstrate this approach, in this paper we formulate a new type of virtual sensor that is specifically designed to optimize the performance of stereo reconstruction algorithms.</p><p>Our approach is motivated by the fact that perspective image sequences are poorly suited as input for 3D reconstruction tasks for a variety of reasons. First, image sequences are highly redundant and represent scene appearance in a form that is difficult to process. Second, storing, transmitting, and processing long image sequences at high-resolution is cumbersome and computationally expensive on today's computer workstations. Third, traditional and multi-baseline stereo algorithms do not easily scale to handle hundreds or thousands of input images. In contrast, an ideal sensor would sample scene radiance in a manner that is designed to optimize the task of scene reconstruction. Furthermore, images from such a sensor would be amenable to efficient processing with scanline stereo algorithms.</p><p>To accomplish these objectives, we introduce a virtual sensor called an omnivergent camera that has the following properties:</p><p>¯Omnivergence: every point in the scene is imaged from two cameras that are vergent on that point (see Fig. <ref type="figure" target="#fig_0">1</ref>) with maximum vergence angle. This strategy is shown to simultaneously optimize the reconstruction accuracy of every point in the scene.</p><p>¯Uniform Accuracy: scene radiance is sampled uniformly in all directions, yielding isotropic depth resolution. In contrast, stereo reconstructions obtained from planar or panoramic <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> images are shown to generate strongly biased (non-isotropic) scene reconstructions.</p><p>¯Compactness: radiance information from hundreds or thousands of input images is distilled into two or more composite images, thereby reducing an N-view reconstruction problem to a binocular stereo matching task, with minimal loss of accuracy.</p><p>¯Boosting Property: due to their linear epipolar geometry, omnivergent images may be used directly as input to existing stereo algorithms, providing an algorithmindependent mechanism for optimizing reconstruction accuracy.</p><p>The idea behind the omnivergent camera is as follows, suppose you are given the task of providing two images as input to a binocular stereo matching algorithm, with the objective of obtaining the most accurate possible scene reconstruction. Suppose further that you can acquire images from as many viewpoints as you like, but may store at most a twodimensional set of pixels (viewing rays) from which to construct the two images (note that the space of all viewing rays is five-dimensional <ref type="bibr" target="#b9">[10]</ref>). Finally, you should construct images that satisfy the constraints of most stereo algorithms, i.e., epipolar lines should correspond to horizontal lines in the image.</p><p>Given this objective, which viewpoints and viewing rays would you select to form your images? In this paper, we demonstrate that the answer can be obtained on a point-bypoint basis: for each 3D scene point P, we wish to choose the best pair of views from which to reconstruct P. Given certain assumptions, these views correspond to the two cameras that are vergent on P and for which the vergence angle is maximized. This strategy can be shown to yield a 2D manifold of rays that, when arranged into two images, produce the best possible stereo reconstruction. These two images can be thought of as an omnivergent stereo pair, output from a virtual sensor that is simultaneously vergent on every point in the scene. We call such images omnivergent panoramas, and the process of reconstructing the scene from such images omnivergent stereo.</p><p>This work was inspired by recent work on stereo from panoramas <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. The beauty of the panoramic stereo approach is that the same class of algorithms that have proven effective for binocular stereo reconstruction can also produce ¿ ¼ AE reconstructions, simply by changing the input to the algorithm. However, stereo on panoramas has some critical shortcomings-first the epipolar geometry for cylindrical panoramas is nonlinear, significantly complicating the stereo search procedure <ref type="bibr" target="#b9">[10]</ref>. Second, and more important, we shall see that stereo on panoramic images does not produce a fair reconstruction (accuracy is good in some regions but extremely poor in others). These two limitations are remedied by the omnivergent approach set forth in this paper.</p><p>Two of the mosaic representations presented in this pa- per, namely the center-strip and dual-strip varieties, were independently discovered by Peleg and Ben-Ezra <ref type="bibr" target="#b11">[12]</ref> for the purpose of creating stereo panoramas for scene visualization. A similar stereo panorama representation based on rotating a stereo head was previously proposed by Huang and Hung <ref type="bibr" target="#b12">[13]</ref>. In both of these approaches, an explicit 3D model is not computed, rather the images are visually fused with stereo glasses to achieve a stereo effect. In contrast, our objective is to obtain an explicit 3D scene reconstruction using stereo matching algorithms.</p><p>In the remainder of this paper, we describe the omnivergent stereo approach in detail. Section 2 introduces the omnivergent approach for the case of a 2D camera confined to move within a circular region of the plane. Sections 3 and 4 generalize the analysis to 3D where a camera moves along a sphere and a circle, respectively. Section 5 presents a practical implementation by taking two symmetrical offcentered slit images from a regular camera rotating around a circle. Several examples of omnivergent images are given, with results of processing these images with existing stereo algorithms. The paper concludes with a summary and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Omnivergent Imaging</head><p>Suppose you can move a camera anywhere within some fixed region of space, with the objective of reconstructing the scene outside of that region as accurately as possible. Suppose further that you can acquire images from as many viewpoints as you like, but may store only a fixed number of pixels, due to limitations in resources. Which pixels would you save and from which cameras? In this section, we study the case of a camera moving within a circular region of the plane, depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. In order to accurately localize a point È , we argue that it is best to choose a pair of cameras that are vergent on È with maximum vergence angle. By choosing different cameras to reconstruct different scene points, it is possible to model virtual sensor that is simultaneously vergent on every point on the scene.</p><p>In this section, we introduce the omnivergent virtual sensor, which maximizes the vergence angle for all points in the scene, and compare its performance to other plausible sensors for the specific case of a camera viewpoints within a circle. A theoretical justification for why maximizing vergence angle is desirable is given in the appendix. In our terminology, a pixel captures a beam of rays, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, and the direction of the beam is defined to be its angle bisector. The intersection of two beams is a voxel and a virtual sensor with resolution AE denotes a collection of AE beams.</p><p>The central idea behind binocular stereo algorithms is that the location of a point in the world is identified by triangulation from corresponding pixels in two input images. Each pixel corresponds to a beam of viewing rays that project to a finite region of the image plane. Given two pixels in correspondence, one can deduce that the true location of the point is in a particular voxel, corresponding to the intersections of the two pixel beams (Fig. <ref type="figure" target="#fig_0">1</ref>). This voxel represents the uncertainty of the reconstruction, since the point may lie anywhere within the voxel region. For purposes of visualization, it is convenient to assume that the point is at the center of the voxel, corresponding to the intersection of the two beam directions.</p><p>By plotting the voxel centers for a specified camera configuration, we can visualize the spatial sampling of a particular virtual sensor and the quality of the resulting reconstruction. Note that if we limit the number of beams (pixels) to AE, there is a finite number of possible beam intersections and thus a finite number of voxels. To begin with, consider the traditional binocular stereo configuration, of two cameras oriented in a parallel or vergent configuration. A critical shortcoming of this configuration is that it enables reconstructing only the subset of the scene that is in front of the cameras. A recently proposed solution is to use two or more panoramic cameras <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> instead. We can model such a pair of panoramic images as representing the output of a single sensor with two focal points.</p><p>Fig. <ref type="figure">2</ref>(b) shows the voxel centers for the binocular panoramic sensor used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, with 180 beams total. Clearly, this approach yields very biased reconstructions, as the voxels are distributed highly non-uniformly. In particular, the portion of the scene that lies near the line between the two camera centers is reconstructed with relatively poor accuracy, where the distance between voxel centers is much higher than in other parts of the scene.</p><p>To improve uniformity, a natural idea would be to use three cameras, as shown in Fig. <ref type="figure">2(c-d</ref>). While the resulting distribution of voxels is somewhat more uniform than the binocular case, some portions of the plane are now covered by three beams and others by two, yielding an uneven distribution of voxels. Fig. <ref type="figure">2(e-f</ref>) captures only the rays of the For any scene point È , we wish to choose a pair of cameras in the sphere vergent on È with maximal vergence angle.</p><p>To achieve this goal, it is sufficient choose a revolute axis Ä of the sphere and capture rays along the longitude lines (a). (b) For any scene point È , there exists a pair of longitude rays vergent on È for which the vergence angle is maximized over the entire sphere.</p><p>exterior angles of the triangle spanned by the three optical centers. This configuration covers every point in the scene by exactly two beams, and its voxel centers are more evenly distributed. If we extend this idea to a regular n-gon instead of a triangle, then for Ò AE we get the omnivergent sensor.</p><p>Note that for a fixed number of beams, the number of voxels can vary for different sensors, due to the fact that not every pair of beams will intersect. For the four sensors depicted in the figure, the total number of voxels are approximately 2000, 3000, 3200, and 4000, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Omnivergent Images</head><p>The omnivergent sensor of resolution AE captures two sets of AE ¾ beams, evenly spaced around the circle, pointing in the direction of the tangents. Half of these we call forward and the other half backward tangents, according to whether the ray points in the clockwise or counter clockwise direction, respectively (Fig. <ref type="figure">2</ref>(g)). Recording the forward tangents in one image and the backward tangents in another yields two one-dimensional omnivergent images. A correspondence between pixels in these two images can be obtained using standard stereo matching algorithms (restricted to a 1D search). For any two pixels that correspond, we know precisely which two beams are stored in these pixels, and straightforward trigonometry locates the voxel of the intersection between these two beams, yielding the reconstruction and its associated uncertainty.</p><p>The omnivergent sensor has three key advantages: first, it gives uniform angular accuracy over the entire scene. Second, it reduces the AE-view stereo problem to a binocular stereo problem, amenable to conventional stereo algorithms. Third, it provides a mechanism for maximizing the vergence angle for every point in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spherical Omnivergent Stereo</head><p>In this section we generalize the analysis of 2D camera motion to 3D camera motion within a spherical volume. The statement of the problem is similar to the planar case: suppose you could move a camera anywhere within a closed sphere and acquire as many images as you like. For any given point È in the scene, you would like to choose a pair of rays vergent on È such that the vergence angle is maximized. For any two points and in the sphere, we define the vergence angle to be È .</p><p>It is easily seen that the desired set of rays are tangent to the sphere. For any given point È , the set of all rays from È that graze the sphere defines a circle Ã È where these rays intersect the sphere. The vergence angle is maximized by choosing any pair of rays on opposite poles of Ã È : Spherical Vergence Property: Let Ë be a sphere with center and let È be a point outside Ë. È is maximized for all points and in Ë when and are on Ã È and È and are co-planar.</p><p>If we allow È to vary over the entire scene, it is evident that capturing the incoming light along all tangent rays to the sphere is sufficient to image every point in the scene with two rays having maximum vergence angle. Note that the set of all tangent rays is three-dimensional-in contrast, the set of all rays from points within the sphere is fourdimensional. Therefore, we can obtain a 3D omnivergent image by storing tangent rays in a volumetric data structure. While some stereo algorithms have been developed to process 3D image volumes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, storing and processing 3D images poses a daunting task. Fortunately, it is possible to further reduce the dimensionality to a 2D set of rays, which we demonstrate next using a constructive argument.</p><p>Observe that a sphere can be generated by rotating a circle ½ ¼ AE about an axis Ä passing through the center of the circle. The resulting set of circles correspond to the longitude lines of a globe with polar axis Ä, as shown in Fig. <ref type="figure">3(a)</ref>.</p><p>Each circle lies in a plane ¥ , and the set of all such planes defines a one-parameter family of circles about Ä called a pencil <ref type="bibr" target="#b13">[14]</ref>. Now consider a point È outside the sphere. È must lie in at least one plane in the pencil; let ¥ be one such plane 1 . Consider the two rays from È that are tangent to . Let and be the two tangency points on . Note that these two rays are also tangent to the sphere and therefore and lie on Ã È . Since, and È all lie on ¥ , a plane that passes through the center of the sphere, it follows from the vergence property that these two rays yield the maximum possible vergence angle for all rays in the sphere. 1 Points on Ä lie on all planes in the pencil. Consequently, it is sufficient to capture the 2D space of rays corresponding to the tangents to all circles . These rays represent the subset of tangents to the sphere that lie in the pencil defined by Ä. This construction ensures that every point in the scene is captured by a pair of rays on the sphere with maximum vergence angle. These rays may be acquired and stored in images by moving a camera in increments of an angle along and putting pixels corresponding to tangent rays in a row vector. Stacking row vectors for subsequent values of yields a spherical omnivergent image (SOI). Note that opposite sides of this image corresponding to</p><formula xml:id="formula_0">¼ ¿ ¼ and ¼ ¿ ¼ are identified,</formula><p>so the image has the topology of a torus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stereo on Spherical Omnivergent Images</head><p>We can capture all longitudinal tangent rays, shown in Fig. <ref type="figure">3</ref>(a) by capturing only the forward tangents to each of the circles and sweeping ¿ ¼ AE degrees in , yielding a single image in which each point is imaged twice 2 . Fig. <ref type="figure">6</ref>(b) shows an example of an SOI image constructed in this way. Alternatively, we can construct an SOI stereo pair by producing two hemispherical omnivergent images; one for the forward tangents and one for the backward tangents. The forward (backward) image is formed by choosing the forward (backward) tangents to for a ½ ¼ AE rotation in . For stereo matching, the latter construction is useful because it produces two images directly (Fig. <ref type="figure">7</ref>).</p><p>A key property of SOI's is that their epipolar geometry is identical to that of a pair of rectified plane-perspective images. In particular, corresponding points appear in the same horizontal scanline of both images of the spherical stereo pair. The epipolar planes for the SOI's correspond to the pencil ¥ through Ä. To see why this is true, observe that all points in ¥ are imaged in the row of the SOI pair corresponding to . The fact that SOI's have linear epipolar geometry is rather surprising, given the nonlinear geometry underlying the construction of these images. However, this property is extremely useful in that it enables spherical omnivergent images to be processed directly by existing stereo 2 Observe that the ¿ ¼ AE rotation of about Ä covers the sphere twice, and that the tangents are "flipped" in the second pass. matching algorithms. To experimentally verify the horizontal epipolar lines, we show the results of applying a standard area-based correlation stereo algorithm to the stereo pair in Fig. <ref type="figure">7(c</ref>).</p><formula xml:id="formula_1">O P 0 P V 1 V 2 V 2 ' V 1 ' R ' R O V 1 V 1 ' R ' R a a (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Center-Strip Stereo</head><p>Because it is difficult to move a camera over a spherical surface, we have designed another variant of an omnivergent camera, called a center-strip omnivergent sensor, that acquires only a single camera motion along a circular path.</p><p>At each point on the circle, the camera captures incoming light along the 1D space of rays that are orthogonal to the circle radius, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. When the optical axis of the camera is tangent to the circle and camera Y-axis points normal to the motion plane, these rays project to the center strip (center column) of the image. Center strips from consecutive positions along the circle are placed in consecutive columns of a new mosaic image called a center-strip omnivergent image (COI), an example of which is shown in Fig. <ref type="figure">6(c</ref>). COI's are instances of manifold mosaics <ref type="bibr" target="#b7">[8]</ref> and multiperspective panoramas <ref type="bibr" target="#b6">[7]</ref> which were introduced by other authors, but to our knowledge have not been previously used for 3D reconstruction tasks.</p><p>COI's have many similarities with SOI's. As in the spherical case, the COI has toroidal topology, as shown in Fig. <ref type="figure">7</ref>(f). It can also be decomposed into a stereo pair by placing forward rays in one image and backward rays in another. The COI's also have horizontal epipolar lines; this can be shown by a simple geometric argument that is illustrated by Fig. <ref type="figure" target="#fig_2">4(b)</ref>. Consider projecting a scene point È orthographically to È ¼ on the plane of the circle. È is imaged by two points Î ½ and Î ¾ on the circle which lie on rays from È ¼ that are tangent to the circle. Since</p><formula xml:id="formula_2">È ¼ Î ½ È ¼ Î ¾ , it is easily seen that È Î ½ È ¼ È Î ¾ È ¼ .</formula><p>Since rays in the same row of a COI correspond to the same value of (offplane angle), it follows that scene points appear in the same row of both the forward and backward COI images. Unlike the spherical case, however, the COI does not maximize the vergence angle for every 3D scene point.</p><p>Rather, it maximizes È ¼ Î ½ Î ¾ (horizontal vergence angle), which is a good approximation when the vertical field of view is limited. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dual-Strip Stereo</head><p>The center-strip omnivergent camera shown above requires capturing a full circle of rays at a point. Special optical devices (e.g., mirrors <ref type="bibr" target="#b15">[16]</ref>) are needed to capture both forward and backward rays at a single point. In this section, we propose a practical alternative called a dual-strip sensor. Like the center-strip omnivergent sensor, a dual-strip sensor captures rays from viewpoints along a circle but is formed from two symmetrical off-center slit images. A topdown view of a dual-strip camera illustrates this property in Fig. <ref type="figure" target="#fig_3">5</ref>(a). Both rays from the dual-strip camera on the plane are tangent to an inner circle Ê ¼ Ê × Ò « where Ê is the radius of the outer circle that the camera moves along and «</p><p>is the angle of dual-strip camera from the normal direction.</p><p>It can be easily shown (Fig. <ref type="figure" target="#fig_3">5(b</ref>)) that the epipolar constraints from dual-strip images are again horizon-</p><formula xml:id="formula_3">tal lines. Because È ¼ Î ¼ ½ È ¼ Î ¼ ¾ (tangent to Ê ¼ ) and Î ½ Î ¼ ¾ Î ¾ Î ¼ ¾ (dual cameras are symmetric), È Î ½ È ¼ È Î ¾ È ¼ .</formula><p>For dual-strip cameras, horizontal vergence angle relative to the inner circle Ê ¼ , not to the outer circle Ê, is maximized.</p><p>The significance of the dual-strip images is that they can be readily acquired from a regular perspective camera moved along a circle. Two columns of pixels, symmetric about the center, are selected from each image. One mosaic image is formed from the left column for consecutive positions on the circle, and another from the right. A pair of dual-strip images formed in this manner is shown in Fig. <ref type="figure" target="#fig_5">8</ref>. These images were created by stacking the ¼Ø and ¾ ¼Ø columns, respectively, from a sequence of ½¿ ¼ frames (with resolution of ¿¾¼ ¢ ¾ ¼) taken by a regular digital video camera. These two images have significant horizontal parallax but no vertical parallax due to the linear epipolar geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Based on the belief that traditional perspective images are not best suited for solving many computer vision tasks, this paper introduced the concept of a virtual sensor. A key idea behind this approach is that it enables optimizing the input to computer vision algorithms in order to produce superior results. Toward this end, we introduced a new family of sensor called omnivergent sensors that are simultaneously vergent on every point in the scene, with maximum vergence angle. These sensors are ideally suited for the task of 3D reconstruction. Rather than physically construct these sensors, we described how to create omnivergent images by processing a sequence of perspective input images. Omnivergent images represent scene appearance simultaneously from numerous perspective viewpoints, as opposed to traditional single perspective images, and are constructed so that the vergence angle for any 3D point is maximized. Consequently, omnivergent stereo produces accurate 3D scene models with minimal reconstruction error. Furthermore, omnivergent images have horizontal epipolar geometry, enabling the direct application of conventional binocular stereo algorithms for determining pixel correspondence.</p><p>We presented three different flavors of omnivergent sensors. The first is a spherical omnivergent sensor that satisfies the maximum vergence angle property. The second is a center-strip sensor that maximizes only the horizontal vergence angle, but is more straightforward to implement in practice. The center-strip omnivergent sensor is a good choice when the scene is relatively far away and the vertical field of view is limited. We also presented a dual-strip sensor that can be easily implemented by moving a regular camera on a circle.</p><p>There remain several important topics to be further studied. First, in this paper we considered only the problem of binocular stereo-better results may be obtained in practice by formulating an AE-ocular stereo problem <ref type="bibr" target="#b16">[17]</ref>. Because the omnivergent cameras have maximum vergence angle, stereo matching with omnivergent images is more sensitive to lighting changes than narrow baseline stereo images. Additional images may be generated by choosing more offtangent rays and/or choosing different sets of input viewpoints <ref type="bibr" target="#b17">[18]</ref>. Second, the generalization of the circle and sphere case to arbitrary camera paths and surfaces is an important topic of future work. Some of the analysis generalizes to any smooth surface-i.e., saving tangent rays is sufficient to maximize the vergence angle. The choice of camera path should ideally depend on a priori information about scene occupancy. Lastly, we are investigating how to build real devices that implement the omnivergent sensors presented in this paper in hardware, using mirrors to collect front and backwards rays with a single CCD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: To reconstruct a point È with minimal uncertainty, the omnivergent sensor captures two beams of maximal vergence angle that contain È .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure2: Each virtual sensor at left represents a configuration of pixel ray directions from cameras on a circle. The intersections of these rays define a lattice of voxels whose centers are shown at right. In each case, the sensor has 180 pixels total. (a) corresponds to a panoramic binocular camera rig and (g) to an omnivergent sensor. (g) Incoming light sensed along the rays tangent to a circle can be recorded in two separate 1D images, one for the clockwise and one for the counter-clockwise rays. Performing stereo matching on these two omnivergent images yields a reconstruction with optimal characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Construction of center-strip omnivergent images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Construction of dual-strip images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Omnivergent images. (a) Perspective, (b) spherical omnivergent, and (c) cylindrical omnivergent images of the same synthetic room scene. Observe that each point appears twice in the omnivergent images.</figDesc><graphic coords="6,58.80,258.70,179.96,89.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Dual-strip images formed by moving a single perspective camera ¿ ¼ AE on a circle and building mosaics from the ¼Ø column (top) and ¾ ¼Ø column (bottom) of the input image sequence.</figDesc><graphic coords="7,66.00,126.81,460.81,63.21" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Vergent stereo rigs have been shown to obtain better stereo reconstructions than parallel rigs for objects near the vergence point (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>). But why is maximizing vergence angle desirable? In this section we provide justification for maximizing the vergence angle that holds for an arbitrarily shaped connected region of camera positions. Suppose two beams intersect at points È and É, defining two corners of a voxel, and that both È and É lie out- side the convex hull of , as shown in Fig. <ref type="figure">9</ref>. We claim that maximizing vergence angle approximately minimizes the area and perimeter among voxels with corners È and É, and this approximation becomes perfect as the number of beams goes to infinity. While this does not prove that the omnivergent sensor is optimal for a finite set of beams, it demonstrates that it performs extremely well as the resolution increases. We can choose many different pairs of beams from within that result in a voxel with È and É as corners, and each of these voxels actually has two different vergence angles: one at the top corner (R) and one at the bottom corner (S), as shown in the figure. Now if we decrease the distance between È and É, i.e., by decreasing the width of the beams, the top and bottom vergence angles become closer, and, in the limit, are equal. Thus, if we chose to maximize the vergence angle between the two angle bisectors of the beams, we are approximately maximizing the top and bottom vergence angles, and this approximation gets better as the number of pixels increases.</p><p>Let the two tangents to that pass through È intersect at and . Similarly, let the two tangents to that pass through É intersect at and . By construction, È Ê É Ë is the intersection of all beams from points in that contain both È and É. Consequently, È Ê É Ë must have smaller area and perimeter than any voxel containing È and É. Note, however, that È Ê É Ë is not the intersection of two pixel beams originating from within the circle. Also notice that È Ê É is the maximum top vergence angle for any voxel, while È Ë É is the maximum bottom vergence angle. So, we see that placing the beams' bases at and maximizes the top vergence angle, while placing them at and maximize the bottom vergence angle. These correspond to minimizing the areas of triangles È Ê É and È Ë É , respectively. As the number of pixels in increased, È and É converge to the same point, as do and , so that in the limit, maximizing the vergence angle produces the smallest possible voxel.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation of scenes from collections of images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Representation of Visual Scenes</title>
		<meeting>IEEE Workshop on Representation of Visual Scenes</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Virtual bellows: Constructing highquality images from video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int. Conf. on Image Processing</title>
		<meeting>First Int. Conf. on Image essing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video mosaics for virtual environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quicktime VR -An image-based approach to virtual environment navigation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 96</title>
		<meeting>SIGGRAPH 96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 96</title>
		<meeting>SIGGRAPH 96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiperspective panoramas for cel animation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 97</title>
		<meeting>SIGGRAPH 97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Panoramic mosaics by manifold projection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conf</title>
		<meeting>Computer Vision and Pattern Recognition Conf</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="338" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theory of catadioptric image formation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth Int. Conf. on Computer Vision</title>
		<meeting>Sixth Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plenoptic modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3-D scene data recovery using omnidirectional multibaseline stereo</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conf</title>
		<meeting>Computer Vision and Pattern Recognition Conf</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="364" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stereo panorama with a single camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conf</title>
		<meeting>Computer Vision and Pattern Recognition Conf</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="395" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panoramic stereo imaging system with automatic disparity warping and seaming</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphical Models and Image Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="196" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epipolarplane image analysis: An approach to determining structure from motion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Marimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="55" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A viewpoint dependent stereoscopic display using interpolation of multi-viewpoint images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oshino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2409</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A true omnidirectional viewer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nalwa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-02">Feb 1996</date>
			<pubPlace>Holmodel, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>tech. rep., Bell Labs</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stereo reconstruction from multiperspective panoramas</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int. Conf. on Computer Vision</title>
		<meeting>Seventh Int. Conf. on Computer Vision<address><addrLine>Kerkyra, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereo ranging with verging cameras</title>
		<author>
			<persName><forename type="first">E</forename><surname>Krotkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kories</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1200" to="1205" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multibaseline stereo system with active illumination and real-time image acquisition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int. Conf. on Computer Vision</title>
		<meeting>Fifth Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
