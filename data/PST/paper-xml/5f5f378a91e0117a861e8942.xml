<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSFER LEARNING OF GRAPH NEURAL NETWORKS WITH EGO-GRAPH INFORMATION MAXIMIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-11">11 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
							<email>haonan3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chaozhang@gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
							<email>j.carlyang@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSFER LEARNING OF GRAPH NEURAL NETWORKS WITH EGO-GRAPH INFORMATION MAXIMIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-11">11 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.05204v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been shown with superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards the transferability of GNNs. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI, a novel GNN framework based on ego-graph information maximization to analytically achieve this goal. Secondly, we specify the requirement of structure-respecting node features as the GNN input, and derive a rigorous bound of GNN transferability based on the difference between the local graph Laplacians of the source and target graphs. Finally, we conduct controlled synthetic experiments to directly justify our theoretical conclusions. Extensive experiments on real-world networks towards role identification show consistent results in the rigorously analyzed setting of direct-transfering, while those towards large-scale relation prediction show promising results in the more generalized and practical setting of transfering with fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have been intensively studied recently <ref type="bibr" target="#b25">Kipf &amp; Welling (2017)</ref>; <ref type="bibr" target="#b23">Keriven &amp; Peyr? (2019)</ref>; <ref type="bibr" target="#b7">Chen et al. (2019)</ref>; <ref type="bibr" target="#b31">Oono &amp; Suzuki (2020)</ref>; <ref type="bibr" target="#b21">Huang et al. (2018)</ref>, due to their established performance towards various real-world tasks <ref type="bibr" target="#b14">Hamilton et al. (2017)</ref>; <ref type="bibr">Ying et al. (2018b)</ref>; <ref type="bibr" target="#b44">Velickovic et al. (2018)</ref>, as well as close connections to spectral graph theory <ref type="bibr" target="#b9">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b5">Bruna et al. (2014)</ref>; <ref type="bibr" target="#b15">Hammond et al. (2011)</ref>. While most GNN architectures are not very complicated, the training of GNNs can still be costly regarding both memory and computation resources on real-world large-scale graphs <ref type="bibr" target="#b6">Chen et al. (2018)</ref>; <ref type="bibr">Ying et al. (2018a)</ref>. Moreover, it is intriguing to transfer learned structural information across different graphs and even domains in settings like few-shot learning <ref type="bibr">Vinyals et al. (2016)</ref>; <ref type="bibr" target="#b11">Finn et al. (2017)</ref>; <ref type="bibr" target="#b36">Ravi &amp; Larochelle (2017)</ref>. Therefore, several very recent studies have been conducted on the transferability of GNNs from the perspective of pre-training <ref type="bibr">Hu et al. (2019a,b)</ref>. However, they do not provide theoretical analysis and guarantee towards the transferability of their models, and thus it is unclear in what situations the models will excel or fail.</p><p>In this work, we aim to establish a theoretically grounded framework for the transfer learning of GNNs, and leverage it to design a practically transferable GNN model. Figure <ref type="figure" target="#fig_0">1</ref> gives an overview of our framework. It is based on a novel view of a graph as samples from the joint distribution of its k-hop ego-graph structures and node features, which allows us to rigorously define graph information and similarity, so as to analyze GNN transferability ( ?2). This view motivates us to design EGI, a novel GNN model based on ego-graph information maximization, which is effective in capturing the graph information as we define ( ?2.1). Then we further specify the requirement on transferable node features and derive a rigorous bound for the transferability of EGI that is dependent on the local graph Laplacians of source and target graphs ( ?2.2). (1) we represent graphs as a combination of its 1-hop ego-graph and node feature distributions; (2) we design a transferable GNN regarding the capturing of such essential graph information; (3) we establish a rigorous guarantee of GNN transferability based on the requirement on nodes features and difference between graph structures.</p><p>All of our theoretical conclusions have been directly validated through controlled synthetic experiments (Table <ref type="table">1</ref>), where we use structural-equivalent role identification in a direct-transfering setting to analyze the impacts of different model designs, node features and source-target structure similarities on GNN transferability. In ?3, we conduct real-world experiments on multiple publicly available network datasets. On the Airport and Gene graphs ( ?3.1), we closely follow the settings of our synthetic experiments and observe consistent but more detailed results supporting the design of EGI and the utility of our theoretical analysis. On the YAGO graphs ( ?3.2), we further evaluate EGI on the more generalized and practical setting of transfer learning with task-specific fine-tuning. We find our theoretical insights still indicative in such scenarios, where EGI consistently outperforms state-of-the-art GNN models and transfer learning frameworks with significant margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRANSFERABLE GRAPH NEURAL NETWORKS</head><p>We aim to establish a theoretically grounded and practically useful framework to argue about and quantify the transferability of GNN. In particular, based on the connection between GNN and spectral graph theory <ref type="bibr" target="#b25">Kipf &amp; Welling (2017)</ref>, we describe the output of a GNN as a combination of its input node features, fixed graph Laplacian and learnable graph filters. The goal of training a GNN is then to improve its utility by learning the graph filters that are compatible with the other two components towards specific tasks.</p><p>In the graph transfer learning setting where downstream tasks are often unknown during pre-training, we argue that the general utility of a GNN should be optimized and quantified w.r.t. its ability of capturing the essential graph information in terms of the joint distribution of its link structures and node features, which motivates us to design a novel ego-graph information maximization model (EGI) ( ?2.1). The general transferability of a GNN is then quantified by the gap between its abilities to capture the source and target graphs. Under reasonable requirements such as using structurerespecting node features as the GNN input, we derive a rigorous bound on this gap for EGI based on the structural difference between two graphs w.r.t. their local graph Laplacians ( ?2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TRANSFERABLE GNN VIA EGO-GRAPH INFORMATION MAXIMIZATION</head><p>In this work, we focus on the direct-transfering setting where a GNN is pre-trained on a source graph G a in an unsupervised fashion and applied on a target graph G b without fine-tuning. <ref type="foot" target="#foot_0">1</ref> Consider a graph G = {V, E}, where the set of nodes V are associated with certain features and the set of links E form certain structures. Intuitively, the transfer learning will be successful only if both the features and structures of G a and G b are similar in some ways, so that the graph filters of a GNN learned on G a are compatible with the features and structures of G b .</p><p>We introduce a novel view of a graph as samples from the joint distribution of its k-hop ego-graph structures and node features. This view allows us to give concrete definitions towards structural information of graphs in the transfer learning setting, which facilitates the measuring of similarity (difference) among graphs. Definition 2.1 (K-hop ego-graph). We call a graph g i = {V (g i ), E(g i )} a k-hop ego-graph centered at v i if it has a k-layer centroid expansion <ref type="bibr" target="#b2">Bai &amp; Hancock (2016)</ref> such that the greatest shortest path rooted from v i has length k, i.e., k = max vj ?V |S(v i , v j )|, where S(v i , v j ) is the shortest path between v i and v j .</p><p>For an ordered k-hop ego-graph, we denote v p,q as the q-th node in the p-th layer of the ego-graph (i.e., |S i (v i , v p,q )| = p), where p = 0, . . . , k, and e vv as the edge between v p,q and v p+1,q . Definition 2.2 (Structural information). Let G be a topological space of sub-graphs <ref type="bibr" target="#b46">Verma &amp; Zhang (2019)</ref>. We view a graph G as samples of k-hop ego-graphs</p><formula xml:id="formula_0">G = {g i } n i=1 drawn i.i.d. from G with probability ?, i.e., g i i.i.d. ? ? ?i = 1, ? ? ? , n.</formula><p>The structural information of G is then defined to be the combination of the distribution ? and the set of spectrum of {g i } n i=1 .</p><p>The structural information of a graph G can be characterized by {g i } vi?V and its empirical distribution, where each</p><formula xml:id="formula_1">g i is a k-hop ego-graph of G centered at node v i with V (g i ) = {u ? V (G) : S(u, v i ) ? k}, and E(g i ) = {e uv ? E(G) : u, v ? V (g i )}.</formula><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, three graphs G 0 , G 1 and G 2 are characterized by a set of 1-hop ego-graphs and their empirical distributions, which allows us to rigorously quantify the structural similarity among graphs as shown in ?2.2 (i.e., G 0 is more similar to G 1 than G 2 under such characterization).</p><p>Note that, our view of structural information is closely related to graph kernels <ref type="bibr" target="#b2">Bai &amp; Hancock (2016)</ref> and graph perturbation <ref type="bibr" target="#b46">Verma &amp; Zhang (2019)</ref>. Specifically, our Def 2.1 is motivated by the concept of k-layer expansion sub-graph in <ref type="bibr" target="#b2">Bai &amp; Hancock (2016)</ref>. However, <ref type="bibr" target="#b2">Bai &amp; Hancock (2016)</ref> used the Jensen-Shannon divergence between pairwise representations of sub-graphs to define a depth-based sub-graph kernel, while we depict G as samples of its ego-graphs. In this sense, our view is related to the setup in <ref type="bibr" target="#b46">Verma &amp; Zhang (2019)</ref>, which derived a uniform algorithmic stability bound of a 1-layer GNN under 1-hop structure perturbation of G. Akin to their work, we study the transferability bound of k-layer GNNs.</p><p>In practice, the nodes in a graph G are characterized not only by their k-hop ego-graph structures but also their associated node features. Therefore, G should be regarded as samples {(g i , x i )} n ? G ? X , drawn with the joint distribution p on the product space of G and a node feature space X . To capture such joint distributions of structural information and node features, we design ego-graph information maximization (EGI), which recursively reconstructs the k-hop ego-graph of each node in graphs based on their features, and optimize it through efficient ego-graph information maximization in an unsupervised fashion.</p><p>Assume we are given a set of ego-graphs {(g i , x i )} with empirical joint distribution P. Similarly with the "local" version of <ref type="bibr">DIM Hjelm et al. (2019)</ref>, we define U ?(gi,xi) as the empirical distribution of the embedding produced by the GNN encoder ? for the the center node v i of ego-graph g i .</p><p>Unlike <ref type="bibr">DGI Velickovic et al. (2019)</ref> that models the local-global mutual information (MI), EGI optimizes ? to maximize the MI of I(g i , ?(g i , x i )), which is directly between the structural input and output of GNN, with a particular focus on the structural information g i . Specifically, we use the Jensen-Shannon MI estimator in <ref type="bibr" target="#b18">Hjelm et al. (2019)</ref>,</p><formula xml:id="formula_2">I (JSD) (G, ?) = E P [-sp (-T D,? (g i , ?(g i , x i )))] -E P? ? [sp (T D,? (g i , ?(g i , x i )))] ,</formula><p>(1) where (g i , x i ) is a negative ego-graph sampled from P;</p><formula xml:id="formula_3">T D,? = D ? (g i , ?(g i , x i )), where D is a discriminator D : g i ? ?(g i , x i ) ? R + .</formula><p>Next, we present the design of our GNN discriminator D to cope with the samples of (g i , x i )'s. The input space of D is at least as large as the number of graph permutations |V (g i )|!. In Eq. 1, instead of enumerating all possible graphs, we sample from the marginal distribution ? on GNN's output ?(g i , x i ) by uniformly sampling (g i , x i ) ? P, P = P. The correspondence between sampling from P and G = {g i } n i=1 is discussed in Remark 2 when node features are strcuture-respecting (Def. 2.3).</p><p>Formally, we characterize the decision process of D with a fixed graph ordering, i.e., the BFS-ordering ? over edges E(g i ). D is a GNN scoring function over an edge sequence E ? : {e 1 , e 2 , ..., e n }, which makes predictions on BFS-ordered edges. Let z i = ?(g i , x i ) , then we have,</p><formula xml:id="formula_4">D(g i , z i ) = k p=0 |Vp(gi)| q=1 log D(e ?v |h q p,q , x i p,q , z i ),<label>(2)</label></formula><p>where h is the hidden representation output by D, e ?v ? E(g i ) is an edge between node ? in layer p and v in layer p + 1, following the notation defined below Def 2.1. More specifically, we have D(e ?v |h q p,q , x i p,q , z i ) = ? U T ? ? W T [h q p,q ||x i p,q ||z i ] ,</p><p>(3) where ? and ? are Sigmoid and ReLU activation functions, respectively. Thus, the discriminator is asked to distinguish positive (e ?v , ?(g i , x i )) and negative pair (e ?v , ?(g i , x i )) that consists of an observed edge and positive/negative center node embeddings ?(?).</p><p>Due to the fact that the output of a k-layer GNN only depends on a k-hop ego-graphs, EGI can be trained in parallel by sampling batches of g i 's. Besides, the training objective of EGI is transferable as long as (g i , x i ) across source graph G a and G b satisfies the conditions given in ?2.2. More details about the model are in Appendix ?2 and source code in the Supplementary Materials.</p><p>Connection with existing work. To provide more insights into the EGI objective, we also present it as a natural dual problem of ego-graph reconstruction. Recall our definition of ego-graph mutual information I(g i , ?(g i , x i )). It can be related to an ego-graph reconstruction loss as max</p><formula xml:id="formula_5">I(g i , ?(g i , x i )) = H(g i ) -H(g i |?(g i , x i )) ? H(g i ) -R(g i |?(g i , x i )).</formula><p>(4) When EGI is maximizing the mutual information, it simultaneously minimizes the upper error bound of reconstructing an ego-graph g i . In this view, the key difference between EGI and GVAE Kipf &amp; Welling (2016) is they assume each edge in a graph to be observed independently during the reconstruction, while we assume the edges in an ego-graph to be observed jointly. Since we reconstruct the ego-graphs in a BFS ordering, our discriminator D is also analogous to sequential graph generation models like GraphRNN <ref type="bibr">You et al. (2018)</ref> and <ref type="bibr">GatedGNN Li et al. (2016)</ref>. Moreover, EGI also has close relation with the other mutual information based GNNs, such as <ref type="bibr">DGI Velickovic et al. (2019)</ref> and <ref type="bibr">GMI Peng et al. (2020)</ref>. However, these methods explicitly measure the mutual information between node features x and GNN output ?. In this way, they may rely too much on node features instead of graph structures, which we deem more essential in graph transfer learning as discussed in ?2.2.</p><p>As we can observe in the first three columns of Table <ref type="table">1</ref>, in both cases of transfering GNNs between similar graphs (F-F) and dissimilar graphs (B-F), EGI significantly outperforms all competitors when using node degree one-hot encoding as transferable node features. In particular, the performance gains over the untrained GIN and GCN show the effectiveness of training and transfering, and our gains are always larger than the two state-of-the-art unsupervised GNNs. Such results clearly indicate advantageous structure preserving capability and transferability of EGI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TRANSFERABILITY BOUND BASED ON LOCAL GRAPH LAPLACIANS</head><p>We now study the transferability of a GNN (in particular, EGI) between the source graph G a and target graph G b based on the graph similarity between G a and G b . We firstly establish the requirement towards node features, under which we then focus on bounding transferability of EGI w.r.t. the structural information of G a and G b .</p><p>Recall our view of the GNN output as a combination of its input node features, fixed graph Laplacian and learnable graph filters. The utility of a GNN is determined by the compatibility among the three. In order to fulfill such compatibility, we require the node features to be structure-respecting: Definition 2.3 (Structure-respecting node features). Let g i be an ordered ego-graph centered on node v i with a set of node features {x i p,q } k,|Vp(gi)| p=0,q=1 , where V p (g i ) is the set of nodes in p-th hop of g i . Then we say the node features on g i are structure-respecting if x i p,q = [f (g i )] p,q ? R d for any node v q ? V p (g i ), where f : G ? R d?|V (gi)| is a function. In the strict case, f should be injective.</p><p>In its essence, Def 2.3 requires the node features to be a function of the graph structures, which is sensitive to changes in the graph structures, and in an ideal case, injective to the graph structures. In this way, when the learned graph filters of a transfered GNN is compatible to the graph structure of G, they are also compatible to the node features of G. As we will explain in Remark 2 of Theorem 2.1, this requirement is also essential for the derivation of our GNN transferability bound which eventually only depends on the structural difference between two graphs.</p><p>In practice, commonly used node features like node degrees, PageRank scores <ref type="bibr" target="#b32">Page et al. (1999</ref><ref type="bibr">), spectral embeddings Chung &amp; Graham (1997)</ref>, and many pre-computed unsupervised network embeddings <ref type="bibr" target="#b34">Perozzi et al. (2014)</ref>; <ref type="bibr" target="#b42">Tang et al. (2015)</ref>; <ref type="bibr" target="#b13">Grover &amp; Leskovec (2016)</ref> are all structurerespecting in nature (although not always strict). However, other commonly used node features like random vectors <ref type="bibr">Yang et al. (2019)</ref> or uniform vectors Xu et al. ( <ref type="formula">2019</ref>) are not structure-respecting and thus non-transferable. When organic node attributes are available, they are transferable as long as the concept of homophily <ref type="bibr" target="#b29">McPherson et al. (2001)</ref> applies, which also implies Def 2.3, but we do not have a rigorous analysis on it yet.</p><p>As we can observe in the fifth and sixth columns in Table <ref type="table">1</ref>, where we use uniform embedding as non-transferable node features to contrast with the first three columns, there is almost no or even negative transferability for all compared methods when non-transferable features are used, as the performance of trained GNNs are similar to or worse than their untrained baselines.</p><p>With our view of graphs and requirement on node features both established, now we derive the following theorem by characterizing the performance difference of EGI on two graphs based on Eq. 1.</p><formula xml:id="formula_6">Theorem 2.1 (GNN transferability bound). Let G a = {(g i , x i )} n i=1 and G b = {(g i , x i )} m i =1</formula><p>be two graphs. Then denote L gi as the (normalised) graph Laplacian of g i ?i = 1, ? ? ? , n, and let the node features of g i be structure-respecting and normalized (similarly for g i ). Consider GNN ? ? with k layers and a 1-hop polynomial filter ? ? . With reasonable assumptions on the local spectrum of G a and G b , the empirical performance difference of ? ? with ? ? evaluated on L EGI satisfies</p><formula xml:id="formula_7">|L EGI (G a ) -L EGI (G b )| ? O M + 1 nm n i=1 m i =1 ?(L gi ) -?(L g i ) 2 , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where M is a constant dependant on k, ? ? , {L gi }, {L g i }, {x i }, {x i }, and ?(L gi ) denotes the ordered eigenvalues of the graph Laplacian of g i ? G a (similarly for g i ).</p><p>Proof. The full proof is detailed in Appendix ?1.</p><p>Remark 1. Our view of a graph G as samples of k-hop ego-graphs is important, as it allows us to make node-wise characterization of GNN similarly as in <ref type="bibr" target="#b46">Verma &amp; Zhang (2019)</ref>. It also allows us to set the depth of ego-graphs in the analysis to be the same as the number of GNN layers (k), since the GNN embedding of each node mostly depends on its k-hop ego-graph instead of the whole graph of G.</p><p>Remark 2. For Eq. 1, Def 2.3 ensures the sampling of GNN embedding at a node always corresponds to sampling an ego-graph from G, which reduces to uniformly sampling from G = {g i } n i=1 under the setting of Theorem 2.1. Therefore, the requirement of Def 2.3 in the context of Theorem 2.1 guarantees the bound to be only depending on the structural information of the graph. In practice, the computation of eigenvalues on the small ego-graphs can be rather efficient <ref type="bibr" target="#b1">Arora et al. (2005)</ref>, and we do not need to enumerate all pairs of ego-graphs. Suppose we need to sample M pairs of k-hop ego-graphs to compare two large graphs, and the average size of ego-graphs are L, then the overall complexity of computing the bound is O(M L 2 ), where M is often less than 1K and L less than 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The bound in</head><p>As we can observe in Table <ref type="table">1</ref>, in the d columns, we compute the average structural difference between two Forest-fire graphs ( d(F, F )) and between Barabasi and Forest-fire graphs ( d(B, F )), based on the RHS of Eq. 5. The results validate our usage of the two graph models to generate structurally different graphs, while also verify our novel view of graphs and the way we propose based on it to characterize structural information and similarity of graphs. We further highlight in the ? columns the performance difference between the GNNs transfered from Forest-fire graphs and Barabasi graphs to Forest-fire graphs. Since Forest-fire graphs are more similar to Forest-fire graphs than Barabasi graphs (as verified in the d columns), we expect ? to be positive and large, indicating more positive transfer between the more similar graphs. Indeed, the behaviors of EGI align well with the expectation, which indicates its well-understood transferability and the utility of our theoretical analysis.</p><p>Table <ref type="table">1</ref>: Synthetic experiments of identifying structural equivalent nodes. We randomly generate 40 graphs with the Forest-fire model (F) <ref type="bibr" target="#b26">Leskovec et al. (2005)</ref> and 40 graphs with the Barabasi model (B) <ref type="bibr" target="#b0">Albert &amp; Barab?si (2002)</ref>, The GNN models we use include the untrained encoders of <ref type="bibr">GCN Kipf &amp; Welling (2017)</ref> and <ref type="bibr">GIN Xu et al. (2019)</ref> with random parameters (baselines with only the neighborhood aggregation function), GVAE with GCN encoder Kipf &amp; Welling (2016), DGI with GIN encoder <ref type="bibr" target="#b45">Velickovic et al. (2019)</ref>, and EGI with GIN encoder. We train GVAE, DGI and EGI on one graph from either set (F and B), and test them on the rest of Forest-fire graphs (F). More details about the results and dataset can be found in Appendix ?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>transferable features non-transferable feature structural difference Protocols. By default, we use node degree one-hot encoding as the transferable feature across all different graphs. As stated before, other transferable features like spectral and other pre-computed node embeddings are also applicable. We focus on the setting where the downstream tasks on target graphs are unspecified but assumed to be structure-relevant, and thus pre-train the GNNs on source graphs in an unsupervised fashion. <ref type="foot" target="#foot_2">3</ref> In terms of evaluation, we notice that existing pre-training GNNs all assume smaller transfer gap on the pre-training task and further generalization to different tasks. To push forward a fair comparison, we skip the direct evaluation towards the ego-graph reconstruction task which clearly favors our model, but design two more realistic experimental settings: (1) Direct-transfering on the more structure-relevant task of role identification without given node features to directly evaluate the utility and transferability of EGI.</p><formula xml:id="formula_9">F-F B-F ? F-F B-F ? d(F,F) d(B,</formula><p>(2) Few-shot learning on the less structure-relevant task of relation prediction with task-specific node features to evaluate the generalization ability of EGI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DIRECT-TRANSFERING ON ROLE IDENTIFICATION</head><p>First, we use the role identification without node features in a direct-transfering setting as a reliable proxy to evaluate transfer learning performance regarding different pre-training objectives. Role in a network is defined as nodes with similar structural behaviors, such as clique members, hub and bridge <ref type="bibr" target="#b17">Henderson et al. (2012)</ref>. Across graphs in the same domain, we assume the definition of role to be consistent, and the task of role identification is highly structure-relevant, which can directly reflect the transferability of different methods and allows us to analyze the bound in Theorem 2.1. Upon convergence of pre-training each model on the source graphs, we directly apply them on the target graphs and further train a multi-layer perceptron (MLP) upon their outputs. The GNN parameters are fixed during the MLP training. We refer to this strategy as direct-transfering since there is no fine-tuning of the models after transfering to the target graphs. The experimental setup on the Airport dataset closely resembles that of our synthetic experiments in Table <ref type="table">1</ref>, but with real data and more detailed comparisons. We train all models (except for the untrained ones) on the Europe network, and test them on all three networks. The results are presented in Table <ref type="table" target="#tab_6">7</ref>. We notice that the node degree features themselves (with MLP) show reasonable performance in all three networks, which is not surprising since the popularity-based airport role labels are highly relevant to node degrees. The untrained GIN encoder yields a significant margin over both node degrees and the untrained vanilla GCN encoder, indicating the importance of proper aggregation mechanisms. While training of the GCN (through GVAE) and GIN (through DGI) can further improve the performance on the source graph, EGI shows the best performance there with the structure-respecting node degree features (59.15), corroborating the claimed effectiveness of EGI in capturing the essential graph information as we stress in ?2.</p><p>When transfering the models to USA and Brazil networks, EGI further achieves the best performance compared with all baselines when node degree features are used (64.55 and 73.15), which reflects the most significant positive transfer. Interestingly, direct application of GVAE and GIN without the consideration of essential graph information as we stress leads to rather limited and even negative transferrability (through comparison against the untrained GCN and GIN encoders). The recently proposed transfer learning frameworks for GNN like Mask-GIN and Structural Pre-train are able to mitigate negative transfer to some extent, but their performances are still inferior to EGI. We believe this is because their models do not aim to capture the underlying ego-graph distributions as we deem important, so they are prune to learn the graph-specific information that is less transferable across different graphs. Similarly as in Table <ref type="table">1</ref>, we also compute the structural difference among three networks w.r.t. to RHS of Eq. 5. The structural difference is 12.03 between the Europe and USA networks, and 12.14 between the Europe and Brazil datasets, which are pretty close. Consequently, the transferability of EGI regarding its performance gain over the untrained GIN baseline is 4.8% on the USA network and 4.4% on the Brazil network, which are also pretty close. Such observations once again align well with our conclusion in Theorem 2.1 that the transferability of EGI is closely bounded by the structural different between source and target graphs. On the Gene dataset, with more graphs available, we focus on EGI to further analyze the utility of our derived bound in Eq. 5, regarding the connection between the structural difference of two graphs and the performance gap of EGI on them. As shown in Figure <ref type="figure" target="#fig_3">2</ref>, we train EGI on one graph and test it on six different graphs. The x-axis shows the structural difference measured w.r.t. the RHS of Eq. 5, and y-axis shows the performance loss compared with an untrained GIN. The positive correlation between two quantities is obvious. Specifically, when the structural difference is small, positive transfer is observed as the performance of transfered EGI is better than untrained GIN, and when the structural difference becomes large, negative transfer is observed. Note that, at its current stage, our bound in Theorem 5 mainly gives a relative indication on the transferability of EGI, because the absolute values of structural difference may vary a lot across different datasets.  ? p. Then with graph G, we have access to the empirical distributions of the three. So the sampling reduces to bootstrapping in the procedure of evaluating the objective.</p><p>Note that, in Eq. 2 of the main paper, we used a d dimensional hidden state h q p,q , specified in Eq. 13 to denote an edge encoding derived from the structure of the ego-graph and the associated source node feature from (p -1)-th layer. For simplicity, we consider the concatenated vector f (x i ) z i , where f (x i ) = h q p,q x i p,q and h q p,q , x i p,q are as defined in the EGI model and in 13. Additionally, since both of h q p,q and x i p,q are normalised, f is bounded. Finally, since we are considering GNN with k layers, its computation only depends on the k-hop ego-graphs of G, which is an important consideration when unfolding the embedding of GNN at a centre node.</p><p>6.1 PROOF FOR THEOREM 3.1 Lemma 6.1. For any A ? R m?n , where m ? n, and A is a submatrix of B ? R m ?n , where m &lt; m , we have</p><formula xml:id="formula_10">A 2 ? B 2 .</formula><p>Proof. Note that, AA T is a principle matrix of BB T , i.e., AA T is obtained by removing the same set of rows and columns from BB T . Then, by Eigenvalue Interlacing Theorem <ref type="bibr" target="#b22">Hwang (2004)</ref> and the fact that A T A and AA T have the same set of non-zero singular values, the matrix operator norm satisfies</p><formula xml:id="formula_11">A 2 = ? max (A T A) = ? max (AA T ) ? ? max (BB T ) = B 2 .</formula><p>We restate Theorem 3.1 from the main paper as below.</p><p>Theorem 6.2 (GNN transferability bound). Let G a = {(g i , x i )} n i=1 and G b = {(g i , x i )} m i =1 be two graphs. Then denote L gi as the (normalised) graph Laplacian of g i ?i = 1, ? ? ? , n, and let the node features of g i be structure-respecting and normalized (similarly for g i ). Consider GNN ? ? with k layers and a 1-hop polynomial filter ? ? , the empirical performance difference of ? ? with ? ? evaluated on L EGI satisfies</p><formula xml:id="formula_12">|L EGI (G a ) -L EGI (G b )| ? O M + 1 nm n i=1 m i =1 ? max (L gi -L g i ) 1/2 , (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where M is a constant dependant on k, ? ? , {L gi }, {L</p><formula xml:id="formula_14">g i }, {x i }, {x i }. In addition, if ?U ? O(n ? m) 4 s.t., U L gi U T = Diag(?(L gi )), U L g i U T = Diag(?(L g i ))</formula><p>we have</p><formula xml:id="formula_15">O M + 1 nm n i=1 m i =1 ?(L gi ) -?(L g i ) 2</formula><p>, ?(L gi ) denotes the ordered eigenvalues of the graph Laplacian of g i ? G a (similarly for g i ).</p><p>Proof. We denote ? s (t) = log(1 + e t ), the softplus activation function, which is 1-Lipschitz continuous. Now,</p><formula xml:id="formula_16">|L EGI (G) -L EGI (G )| = 1 n 2 n i,j=1 (D(g i , z j )) - 1 n n i=1 (-(-D(g i , z i )) -( 1 m 2 m i ,j =1 (D(g i , z j )) - 1 m m i =1 (-(-D(g i , z i )))) ? 1 n 2 m 2 n i,j=1 m i ,j =1 |D(g i , z j ) -D(g i , z j )| + 1 nm n i=1 m i =1 |D(g i , z i ) -D(g i , z i )| = 1 n 2 m 2 n i,j=1 m i ,j =1 A + 1 nm n i=1 m i =1 B.</formula><p>First we consider B. Recall that, V p (g i ) is the set of nodes in layer p of g i ,</p><formula xml:id="formula_17">D(g i , z i ) = k p=1 |Vp(gi)| q=1 log(? sig U T ? W T [f (x i ) z i ] ),</formula><p>where ? sig (t) = 1 1+e -t is the sigmoid function, ? is some ? ? -Lipschitz activation function and [? ?] denotes the concatenation of two vectors. Then we have</p><formula xml:id="formula_18">U T ? W T [f (x i ) z i ] = U T ? W T 1 f (x i ) + W T 2 z i . WLOG, assume d p = |V p (g i )| = |V p (g i )| ?u = 1, ? ? ? , k. In addition, since log(? sig (t)) = -log(1 + e -t ) = -? s (-t), which is 1-Lipschitz, it gives B ? k p=1 dp q=1 |U T ? W T 1 f (x i ) + W T 2 z i -U T ? W T 1 f (x i ) + W T 2 z i | ? ? ? s U k p=1 dp q=1 ( W T 1 f (x i ) -W T 1 f (x i ) 2 + W T 2 z i -W T 2 z i 2 ) ? ? ? s U s W k p=1 dp q=1 ( f (x i ) -f (x i ) 2 + z i -z i 2 ),<label>(7)</label></formula><p>where s U is the largest singular value of U , and similarly s W = s W1 ? s W2 . Since we assumed the node features are normalised, then f</p><formula xml:id="formula_19">(x i ) -f (x i ) 2 ? c D .</formula><p>From Eq. 7, we only care about x i 's embedding obtained from a k-layer GNN with 1-hop polynomial (linear in L) filter. Inspired by the characterization of GNN from a node-wise view in <ref type="bibr" target="#b46">Verma &amp; Zhang (2019)</ref>, we similarly denote the embedding of node x i ?i = 1, ? ? ? , n in the final layer of the GNN as</p><formula xml:id="formula_20">z k i = z i = ? ? (x i ) = ?( j?N (xi) e ?j z k-1 j ) ? R d ,</formula><p>where e ?j = [? ? (L)] ?j ? R. We may denote z i ? R d similarly for = 1, ? ? ? , k -1, and z 0 i = x i ? S d-1 the node feature of node x i . With the assumption of GNN stated in the statement, it is clear that only the k-hop ego-graph g i centered at x i is needed to compute z k i for any i = 1, ? ? ? , n instead of the whole of G. With such observation in mind, let us denote the matrix of node embeddings of g i at the th layer as (z i( ) p,q ) ? R |V (gi)|?d , for = 1, ? ? ? , k; and let (z i(0) p,q ) ? (x i p,q ) ? (S d-1 ) |V (gi)| denote the matrix of node features in the k-hop ego-graph g i . In addition, we denote (z i( ) p,q ) p?t to be the submatrix that is obtained by selecting rows that corresponds to v ? V p (g i ) for p = 0, ? ? ? , t ? k. Similarly for g i .</p><p>Moreover, let us denote ? ? (L gi ) ? [? ? (L)] gi , i.e., the filtered full graph Laplacian of G subsetted by the k-hop ego-graph g i . Then, let ? ? (L gi ) p?t denotes the submatrix that is obtained by selecting rows and columns that corresponds to v ? V p (g i ) for p = 0, ? ? ? , t ? k. Similarly for g i .</p><p>Therefore, by Lemma 6.1, for any = 1, ? ? ? , k, the following holds</p><formula xml:id="formula_21">(z i ( ) p,q ) p?t -(z i ( ) p,q ) p?t 2 ? (z i ( ) p,q ) p?t+1 -(z i ( ) p,q ) p?t+1 2 . Assume (z i ( -1) p,q</formula><p>) 2 ? c z &lt; ? ? . Now, at the final layer,</p><formula xml:id="formula_22">z i -z i 2 = (z i (k) p,q ) p=0 -(z i (k) p,q ) p=0 2 ? [?(? ? (L gi ) p?1 (z i(k-1) p,q ) p?1 ) -?(? ? (L g i ) p?1 (z i (k-1) p,q ) p?1 )] p=0 2 ?? ? ? ? (L gi ) p?1 (z i(k-1) p,q ) p?1 -? ? (L g i ) p?1 (z i (k-1) p,q ) p?1 2 ?? ? ? ? (L gi ) p?1 2 (z i(k-1) p,q ) p?1 -(z i (k-1) p,q ) p?1 2 +? ? (z i (k-1) p,q ) p?1 2 ? ? (L gi ) p?1 -? ? (L g i ) p?1 2 ?? ? ? ? (L gi ) 2 (z i(k-1) p,q ) p?1 -(z i (k-1) p,q ) p?1 2 + ? ? c z ? ? (L gi ) -? ? (L g i ) 2 . (8) In general, for = 1, ? ? ? , k -1, the following holds with t = k -, (z i ( ) p,q ) p?t -(z i ( ) p,q ) p?t 2 ?? ? ? ? (L gi ) p?t+1 (z i( -1) p,q ) p?t+1 -? ? (L g i ) p?t+1 (z i ( -1) p,q ) p?t+1 2 ?? ? ? ? (L gi ) 2 (z i( -1) p,q ) p?t+1 -(z i ( -1) p,q ) p?t+1 2 + ? ? c z ? ? (L gi ) -? ? (L g i ) 2 .<label>(9)</label></formula><p>Then we equivalently write Eq. 9 as E ? bE -1 + a, which gives</p><formula xml:id="formula_23">E ? b E 1 + b + 1 b -1 a.</formula><p>Then, with (x i p,q ) = (z i(0) p,q ), we see the following is only dependant on the structure of g i and g i ,</p><formula xml:id="formula_24">(z i ( ) p,q ) -(z i ( ) p,q ) 2 ? ? ? ? ? (L gi ) 2 (x i p,q ) -(x i p,q ) 2 + ? ? ? ? (L gi ) 2 + 1 ? ? ? ? (L gi ) 2 -1 ? ? c z ? ? (L gi ) -? ? (L g i ) 2 .</formula><p>Since the features are normalised, and so are the graph Laplacians, we have ? ? (L gi ) 2 ? c L and (x i p,q ) -(x i p,q )) 2 ? c x . Then with Eq. 8, we have</p><formula xml:id="formula_25">z i -z i 2 ? ? k ? c k L c x + ? k ? c k L + 1 ? ? c L -1 ? ? ? ? c z L gi -L g i 2 ? c ?,? (M + L gi -L g i 2 ) = c ?,? (M + ? max (L gi -L g i ) 1/2 ).<label>(10)</label></formula><p>Now, by Eq. 7, we have</p><formula xml:id="formula_26">B ? kd max ? ? s(c D + c ?,? (M + ? max (L gi -L g i ) 1/2 ))</formula><p>, where d max = max p d p . Similarly, the above holds for A, since from Eq. 8, the node features and embedded features are bounded by separate terms. We therefore arrive at</p><formula xml:id="formula_27">|L EGI (G) -L EGI (G )| ? 2kd max ? ? c ?,? s(M + 1 nm n i=1 m i =1 ? max (L gi -L g i ) 1/2 )) ? 2kd max ? ? c ?,? s(M + 1 nm n i=1 m i =1 L gi -L g i F )).<label>(11)</label></formula><p>Moreover, by Von Neumann's Trace Inequality Grigorieff (1991</p><formula xml:id="formula_28">), if ?U ? O(?) 5 , where ? = k p=0 d p , s.t. U L gi U T = Diag(?(L gi )), U L g i U T = Diag(?(L g i )), we have L gi -L g i F = ?(L gi ) -?(L g i ) 2 , then Eq. 11 ? c ?,? (M + ?(L gi ) -?(L g i ) 2</formula><p>). Therefore Eq. 11 becomes</p><formula xml:id="formula_29">|L EGI (G) -L EGI (G )| ? 2kd max ? ? c ?,? s(M + 1 nm n i=1 m i =1 ?(L gi ) -?(L g i ) 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">MODEL DETAILS</head><p>Following the same notations used in the paper, EGI consists of a GNN encoder ? and a GNN discriminator D. In general, the GNN encoder ? and decoder D can be any existing GNN models.</p><p>For each ego-graph and its node features {g i , x i }, the GNN encoder returns node embedding z i for the center node v i . As mentioned in Eq. 2 in the main paper, the GNN discriminator D makes edge-level predictions as follows, D(e ?v |h q p,q , x i p,q , z i</p><formula xml:id="formula_30">) = ? U T ? ? W T [h q p,q ||x i p,q ||z i ] ,<label>(12</label></formula><p>) where e ?v ? E(g i ) and h q p,q ? R d is the representation for edge e ?v between node v p-1,q in hop p -1 and v p,q in hop p. Specifically, we denote the source node at p -1 hop as q ? Qp,q , Qp,q = {q : v p-1,q ? V p-1 (g i ), e (p-1,q)(p,q) ? E(g i )}. Hence, the edge prediction relies on the combination of center node embedding z i , destination node feature x i p,q and edge message h q p,q .</p><p>Ego-graph (?? ?? , ?? ?? ) In Figure <ref type="figure" target="#fig_4">3</ref>, {g i , x i } and {g i , x i } are the positive and negative training samples w.r.t ego-graph topology g i . The discriminator D operates on a reversed ego-graph gi comparing encoder's forward propagation on g i . It starts from the center node v i and compute the hidden representation m p-1,q for node v p-1,q at each hop. The edge message h q p,q is calculated between source node's hidden representation m p-1,q and destination node features x p,q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ego</head><formula xml:id="formula_31">h q p,q = ReLU W T p m p-1,q + x i p,q , m p-1,q = 1 | Qp-1,q | q ? Qp-1q h q p-1,q<label>(13)</label></formula><p>When p = 1, every edge origins from the center node v i and m 0,q is the center node feature x vi .</p><p>In every batch, we sample a set of ego-graphs and their node features {g i , x i }. During the forward pass of encoder ?, it aggregates from neighbor nodes to the center node v i . Then, the discriminator calculates the edge embedding in Eq. 12 from center node v i to its neighbors and make edge-level predictions-fake or true. The training framework of EGI is depicted in Figure <ref type="figure" target="#fig_4">3</ref> and Algorithm 1.</p><p>Sample M ego-graphs {(g 1 , x 1 ), ..., (g M , x M )} from empirical distribution P without replacement, and obtained their positive and negative node embeddings z i , z i through ? z i = ?(g i , x i ), z i = ?(g i , x i ), /* Initialize positive and negative expectation in Eq. 1 in the main paper*/ 5 E pos = 0, E neg = 0 6 for p = 1 to k do 7 /* Compute JSD on edges at each hop*/ 8 for e (p-1,q)(p,q) ? E(g i ) do 9 generate edge embedding h q p,q in Eq. ( <ref type="formula" target="#formula_31">13</ref>) ;</p><p>10 We implement our method and all of the baselines using the same encoders ?: 2-layer <ref type="bibr">GIN Xu et al. (2019)</ref> for synthetic and role identification experiments, 2-layer GraphSAGE <ref type="bibr" target="#b14">Hamilton et al. (2017)</ref> for the relation prediction experiments. We set hidden dimension as 32 for both synthetic and role identification experiments, For relation prediction fine-tuning task, we set hidden dimension as 256. We train EGI in a mini-batch fashion since all the information for encoder and discriminators are within the k-hop ego-graph g i and its features x i . Further, we conduct neighborhood sampling and set maximum neighbors as 10 to speed up the parrallel training. The space and time complexity of EGI is O(BN K ), where B is the batch size, N is the number of the neighbors and k is the number of hops of ego-graphs. Notice that both the encoder ? and discriminator D propagate message on the k-hop ego-graphs, so the extra computation cost of D compared with a common GNN module is a constant multiplier over the original one. The scalability of EGI on million scale YAGO network is reported in section 8.3.</p><formula xml:id="formula_32">E pos = E pos + ? U T ? ? W T [h q p,q ||x p,q ||z i ] 11 E neg = E neg + ? U T ? ? W T [h q p,q ||x p,q ||z i ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">TRANSFER LEARNING SETTINGS</head><p>The goal of transfer learning is to train a model on a dataset or task, and use it on another. In our graph learning setting, we focus on training the model on one graph and using it on another. In particular, we focus our study on the setting of direct-transfering, where the model learned on the source graph is directly applied on the target graph without fine-tuning. We study this setting because it allows us to directly measure the transferability of GNNs, which is not affected by the fine-tuning process on the target graph. In other words, the fine-tuning process introduces significant uncertainty to the analysis, because there is no guarantee on how much the fine-tuned GNN is different from the pre-trained one. Depending on specific tasks and labels distributions on the two graphs, the fine-tuned GNN might be quite similar to the pre-trained one, or it can be significantly different. It is then very hard to analyze how much the pre-trained GNN itself is able to help. Another reason is about efficiency. The fine-tuning of GNNs requires the same environment set-up and computation resource as training GNNs from scratch, although it may take less training time eventually if pre-training is effective. It is intriguing if this whole process can be eliminated when we guarantee the performance with direct-transfering.</p><p>In our experiments, we also study the setting of transfer learning with fine-tuning, particularly on the real-world large-scale YAGO graphs. Since we aim to study the general transferability of GNNs not bounded to specific tasks, we always pre-train GNNs with the unsupervised graph reconstruction objective on source graphs. Then we enable two types of fine-tuning. The first one is post-fine-tuning, where the pre-trained GNNs are fine-tuned with the supervised task specific objective on the target graphs. The second on is joint-fine-tuning, where pre-training is the same, but fine-tuning is done w.r.t. both the graph reconstruction objective and task specific objective on target graphs in a semisupervised learning fashion. The unsupervised pre-training objective L u of EGI is Eq. 1, while those of the compared algorithms are as defined in their papers. The supervised fine-tuning objective L s is the same as in the DistMult paper <ref type="bibr">Yang et al. (2014)</ref> for all algorithms.</p><p>8 EXPERIMENT DETAILS 8.1 SYNTHETIC EXPERIMENTS Data. As mentioned in the main paper, we use two traditional graph generation models for synthetic data generation: (1) barabasi-albert graph <ref type="bibr" target="#b3">Barab?si &amp; Albert (1999)</ref> and ( <ref type="formula" target="#formula_4">2</ref>) forest-fire graph <ref type="bibr" target="#b26">Leskovec et al. (2005)</ref>. We generate 40 graphs each with 100 nodes with each model. We control the parameters of two models to generate two graphs with different ego-graph distributions. Specifically, we set the number of attached edges as 2 for barabasi-albert model and set p forward = 0.4, p backward = 0.3 for forest-fire model. In Figure <ref type="figure" target="#fig_6">4a</ref> and 4b, we show example graphs from two families in our datasets. They have the same size but different appearance which leads to our study on the transferability gap in Table <ref type="table">1</ref> in the main paper. The accuracy of this task defined as the percentage of nearest neighbors for target node in the embedding space that are structure-equivalent, i.e. #correct k-nn neighbors / #ground truth equivalent nodes. Results. The structural equivalence label is obtained by a 2-hop WL-test Weisfeiler &amp; Lehman (1968) on the ego-graphs. If two nodes have the same 2-hop ego-graphs, they will be assigned the same label.</p><p>In the example of Figure <ref type="figure" target="#fig_6">4c</ref>, the nodes labeled with same number (e.g. 2, 4) have the isomorphic 2-hop ego-graphs. Note that this task is exactly solvable when node features and GNN architectures are powerful enough like <ref type="bibr">GIN Xu et al. (2019)</ref>. In order to show the performance difference among different methods, we set the length of one-hot node degree encoding to 3 (all nodes with degrees higher than 3 have the same encoding). Here, we present the performance comparison with different length of degree encodings (d) in Table <ref type="table" target="#tab_3">4</ref>. When the capacity of initial node features is high (d=10), the transfer learning gap diminishes between different methods and different graphs because the structural equivalence problem can be exactly solved by neighborhood aggregations. However, when the information in initial node features is limited, the advantage of EGI in learning and transfering the graph structural information is obvious. In Table <ref type="table" target="#tab_4">5</ref>, we also show the performance of different transferable and non-transferable features, i.e. node embedding <ref type="bibr" target="#b34">Perozzi et al. (2014)</ref> and random feature vectors. The observation is similar with Table <ref type="table">1</ref> in the main paper: the transferable feature can reflect the performance gap between similar and dissimilar graphs while non-transferable features can not.</p><p>In both Table <ref type="table">1</ref> and 2 here as well as Table <ref type="table">1</ref> in the main paper, we repeat the structural difference among graphs in the two sets ( d) calculated w.r.t. the term 1 nm n i=1 m i =1 ?(L gi ) -?(L g i ) 2 on the RHS of Eq. 5 in the main paper. This indicates that the Forest fire graphs are structurally similar to the Forest fire graphs, while less similar to the Barabasi graphs, as can be verified from Figure <ref type="figure" target="#fig_6">4a</ref> and 4b. Our bound in Theorem 3.1 then tells us that the GNNs (in particular, EGI) should be more transferable in the F-F case than B-F. This is verified when using the transferable node features of degree encoding with limited dimension (d=3) as well as DeepWalk embedding, as EGI trained on Forest fire graphs performs significantly better on Forest fire graphs than on Barabasi graphs (with +0.094 and +0.057 differences, respectively). Data. We report the number of nodes, edges and classes for both airport and gene dataset. The numbers for the Gene dataset are the aggregations of the total 52 gene networks iin the dataset. For the three airport networks, Figure <ref type="figure">5</ref> shows the power-law degree distribution on log-log scale. The class labels are between 0 to 3 reflecting the level of the airport activities <ref type="bibr" target="#b37">Ribeiro et al. (2017)</ref>. For the Gene dataset, we matched the gene names in the TCGA dataset <ref type="bibr">Yang et al. (2019)</ref> to the list of transcription factors on wikipedia 6 . 75% of the genes are marked as 1 (transcription factors) and some gene graphs have extremely imbalanced class distributions. So we conduct experiments on the relatively balanced gene graphs of brain cancers (Figure <ref type="figure" target="#fig_3">2</ref> in the main paper). Both datasets do not have organic node attributes. The role-based node labels are highly relevant to their local graph structures, but are not trivially computable such as from node degrees.</p><p>Results. As we can observe from Figure <ref type="figure">5</ref>, the three airport graphs have quite different sizes and structures (e.g., regarding edge density and connectivity pattern). Thus, the absolute classification accuracy in both Table <ref type="table" target="#tab_1">2</ref> in the main paper and Table <ref type="table" target="#tab_6">7</ref> here varies across different graphs. However, as we mention in the main paper, the structural difference we compute based on Eq. 5 in Theorem 3.1 is close among the Europe-USA and Europe-Brazil graph pairs (12.03 and 12.14), which leads to close transferability of EGI from Europe to USA and Brazil. This indicates the effectiveness of our view over essential structural information.</p><p>Note that, the results present in Table <ref type="table" target="#tab_6">7</ref> are the accuracy of GNNs directly trained and evaluated on each network without transfering. Therefore, only the Europe column has the same results as in Table <ref type="table" target="#tab_1">2</ref> in the main paper, while the USA and Brazil columns can be regarded as providing an upper-bound performance of GNN transfered from other graphs. As we can see, EGI gives the closest results from 6 https://en.wikipedia.org/wiki/Transcription_factor  Table <ref type="table" target="#tab_1">2</ref> in the main paper to Table <ref type="table" target="#tab_6">7</ref> here, demonstrating the its plausible transferability. The scores are so close, showing a possibility to skip fine-tuning when the source and target graphs are similar enough. Also note that, although the variances are pretty large (which is also observed in other works like Ribeiro et al. ( <ref type="formula">2017</ref>) since the networks are small), our t-tests have shown the improvements of EGI to be significant. 55.56% ? 6.83% DGI (GIN) <ref type="bibr" target="#b45">Velickovic et al. (2019)</ref> 57.75% ? 4.47% 62.44% ? 4.46% 68.15% ? 6.24% Mask- <ref type="bibr">GIN Hu et al. (2019a)</ref> 56.37% ? 5.07% 63.78% ? 2.79% 61.85% ? 10.74% ContextPred- <ref type="bibr">GIN Hu et al. (2019a)</ref> 52.69% ? 6.12% 56.22% ? 4.05% 58.52% ? 10.18% Structural Pre-train <ref type="bibr">Hu et al. (2019b)</ref> 56.00% ? 4.58% 62.29% ? 3.51% 71.48% ? 9.38 % EGI (GIN) 59.15% ? 4.44% 65.88% ? 3.65% 74.07% ? 5.49%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">REAL-WORLD LARGE-SCALE RELATION PREDICTION EXPERIMENTS</head><p>Data. As shown in Table <ref type="table" target="#tab_7">8</ref>, the source graph we use to pre-train GNNs is the full graph cleaned from the YAGO dump <ref type="bibr" target="#b40">Suchanek et al. (2007)</ref>, where we assume the relations among entities are unknown.</p><p>The target graph we use is a subgraph uniformed sampled from the same YAGO dump (we sample the nodes and then include all edges among the sampled nodes). The similar ratio between number of nodes and edges can be observed in Table <ref type="table" target="#tab_7">8</ref>. On the target graph, we also have the access to 24 different relations <ref type="bibr" target="#b39">Shi et al. (2018)</ref> such as isAdvisedBy, isMarriedTo and so on. Such relation labels are still relevant to the graph structures, but the relevance is lower compared with the structural role labels. We use the 256-dim degree encoding as node features for pre-training on the source graph, then we use the 128-dim positional embedding generated by <ref type="bibr">LINE Tang et al. (2015)</ref> for fine-tuning on the target graph, to explicitly make the features differ across source and target graphs.</p><p>Results. In Section 7.1, we introduced two different types of fine-tuning, i.e., post-fine-tuning and joint-fine-tuning. For both types of fine-tuning, we add one feature encoder E before feeding it into the GNNs for two purposes. First, the target graph fine-tuning feature usually has different dimensions with the pre-training features, such as the node degree encoding we use. Second, the semantics and distributions of fine-tuning features can be different from pre-training features. The feature encoder aims to bridge the gap between feature difference in practice. The supervised loss used in this experiment is the same as in DistMult <ref type="bibr">Yang et al. (2014)</ref>. In particular, the bilinear score function is calculated as s(h, r, t) = z T h M r z t , where M r is a diagonal matrix for each relation r, z h and z t the the embedding of GNN encoder ? for head and tail entities. The experiments were run on GTX1080 with 12G memories. We report the average training time per epoch of our algorithm in pre-training and fine-tuning stage in Table <ref type="table" target="#tab_7">8</ref> as well. The pre-training and fine-tuning takes about 40 epochs and 10 epochs to converge, respectively. In Table <ref type="table" target="#tab_7">8</ref>, we also present the per-epoch training time of EGI. EGI takes about 338 seconds per epoch for optimizing the ego-graph information maximization objective on YAGO-source. As we can see, fine-tuning also takes significant time compared to pre-training, which strengthens our arguments about avoiding or reducing fine-tuning through structural analysis. We implement all baselines within the same pipeline, and the runtimes are all at the same scale.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our GNN transfer learning framework: (1) we represent graphs as a combination of its 1-hop ego-graph and node feature distributions; (2) we design a transferable GNN regarding the capturing of such essential graph information; (3) we establish a rigorous guarantee of GNN transferability based on the requirement on nodes features and difference between graph structures.</figDesc><graphic url="image-1.png" coords="2,117.90,71.90,376.20,196.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Theorem 2.1 naturally instantiates our insight about the correspondence between structural similarity and GNN transferability. It tells us how well a GNN trained on G a can work on G b by only checking the local graph Laplacians of G a and G b without actually training the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We use two real-world network datasets with role-based node labels: (1) Airport<ref type="bibr" target="#b37">Ribeiro et al. (2017)</ref> contains three networks different regions-Brazil, USA and Europe. Each node is an airport and each link the flight between airports. The airports are assigned with external labels based their level of popularity. (2) Gene Yang et al. (2019) contains the gene interactions regarding 50 different cancers. Each gene has a binary label indicating whether it is a transcription factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Role identification on the Gene dataset. Due to severe label imbalance that vanishes the performance gaps, we only use the 7 brain cancer networks that have a more consistent balance of labels. We also visualize the source graph G0 and two example target graphs that are relatively more similar (G5) and different (G6) with G0. More details about the results and dataset can be found in Appendix ?3.3.2 FEW-SHOT LEARNING ON RELATION PREDICTIONHere we evaluate EGI in the more generalized and practical setting of few-shot learning on the less structure-relevant task of relation prediction, with task-specific node features and fine-tuning. The source graph contains a cleaned full dump of 579K entities from YAGO Suchanek et al. (2007), and we investigate 20-shot relation prediction on a target graph with 24 relation types, which is a sub-graph of 115K entities sampled from the same dump. In post-fine-tuning, the models are pre-trained with an unsupervised loss on the source graph and fine-tuned with the task-specific loss on the target graph. In joint-fine-tuning, the same pre-trained models are jointly optimized w.r.t. the unsupervised pre-training loss and task-specific fine-tuning loss on the target graph. In Table3, we observe most of the existing models fail to transfer across pre-training and fine-tuning tasks, especially in the joint-fine-tuning setting. In particular, both Mask-GIN and ContextPred-GIN rely a lot on task-specific fine-tuning, while EGI focuses on the capturing of similar ego-graph structures that are transferable across graphs. As a consequence, EGI significantly outperforms all compared methods in both settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall EGI training framework.</figDesc><graphic url="image-7.png" coords="15,213.93,454.72,66.16,69.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>L</head><label></label><figDesc>EGI = E neg -E pos 16 /* Update ?, D */ 17 ? ? + ? --? ? L EGI , ? D + ? --? D L EGI 18 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualizations of the graphs and labels we use in the synthetic experiments.</figDesc><graphic url="image-10.png" coords="17,385.20,332.72,142.56,106.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )Figure 5 :</head><label>a5</label><figDesc>Figure 5: Visualizations of power-law degree distribution on three airport dataset.</figDesc><graphic url="image-11.png" coords="19,110.49,97.61,126.72,95.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of role identification with direct-transfering on the Airport dataset. The performance reported (%) are the average over 100 runs. The scores marked with * * passed t-test with p &lt; 0.01 over the second best results. More details about the results and dataset can be found in Appendix ?3.</figDesc><table><row><cell>Method</cell><cell cols="6">Europe (source) node degree uniform node degree uniform node degree uniform USA (target) Brazil (target)</cell></row><row><cell>MLP</cell><cell>52.81</cell><cell>20.59</cell><cell>55.67</cell><cell>20.22</cell><cell>67.11</cell><cell>19.63</cell></row><row><cell>GCN (untrained)</cell><cell>52.96</cell><cell>20.11</cell><cell>55.30</cell><cell>22.07</cell><cell>68.30</cell><cell>17.63</cell></row><row><cell>GIN (untrained)</cell><cell>55.75</cell><cell>53.88</cell><cell>61.56</cell><cell>58.32</cell><cell>70.04</cell><cell>70.37</cell></row><row><cell>GVAE (GCN) Kipf &amp; Welling (2016)</cell><cell>53.90</cell><cell>21.12</cell><cell>55.51</cell><cell>22.39</cell><cell>66.33</cell><cell>17.70</cell></row><row><cell>DGI (GIN) Velickovic et al. (2019)</cell><cell>57.75</cell><cell>22.13</cell><cell>54.90</cell><cell>21.76</cell><cell>67.93</cell><cell>18.78</cell></row><row><cell>Mask-GIN Hu et al. (2019a)</cell><cell>56.37</cell><cell>55.53</cell><cell>60.82</cell><cell>54.64</cell><cell>66.71</cell><cell>74.54</cell></row><row><cell>ContextPred-GIN Hu et al. (2019a)</cell><cell>52.69</cell><cell>49.95</cell><cell>50.38</cell><cell>54.75</cell><cell>62.11</cell><cell>70.66</cell></row><row><cell>Structural Pre-train Hu et al. (2019b)</cell><cell>56.00</cell><cell>53.83</cell><cell>62.17</cell><cell>57.49</cell><cell>68.78</cell><cell>72.41</cell></row><row><cell>EGI (GIN)</cell><cell>59.15  *  *</cell><cell>54.98</cell><cell>64.55  *  *</cell><cell>57.40</cell><cell>73.15  *  *</cell><cell>70.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of few-shot relation prediction on YAGO. Structural Pre-trainHu et al. (2019b)  can not scale to the YAGO graphs with 100K+ nodes. More details about the results and dataset can be found in Appendix ?3.GNNs are presented, such as the auto-encoder-based ones like GVAE Kipf &amp; Welling (2016) and GNFs<ref type="bibr" target="#b28">Liu et al. (2019)</ref>, as well as the deep-infomax-based ones likeDGI Velickovic et al. (2019)  and InfoGraph<ref type="bibr" target="#b41">Sun et al. (2019)</ref>. Their potential in the transfer learning of GNN remains unclear when the node features and link structures can also change across graphs.</figDesc><table><row><cell>Method</cell><cell cols="2">post-fine-tuning AUROC MRR</cell><cell cols="2">joint-fine-tuning AUROC MRR</cell></row><row><cell>No pre-train</cell><cell>0.6866</cell><cell>0.5962</cell><cell>N.A.</cell><cell>N.A</cell></row><row><cell>GVAE Kipf &amp; Welling (2016)</cell><cell>0.7009</cell><cell>0.6009</cell><cell>0.6786</cell><cell>0.5676</cell></row><row><cell>DGI Velickovic et al. (2019)</cell><cell>0.6885</cell><cell>0.5861</cell><cell>0.6880</cell><cell>0.5366</cell></row><row><cell>Mask-GIN Hu et al. (2019a)</cell><cell>0.7041</cell><cell>0.6242</cell><cell>0.6720</cell><cell>0.5603</cell></row><row><cell>ContextPred-GIN Hu et al. (2019a)</cell><cell>0.6882</cell><cell>0.6589</cell><cell>0.5293</cell><cell>0.3367</cell></row><row><cell>EGI</cell><cell cols="4">0.7389  *  *  0.6695 0.7870  *  *  0.7289  *  *</cell></row><row><cell>4 RELATED WORK</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Representation learning on graphs has been studied for decades, with earlier spectral-based methods</cell></row><row><cell cols="5">Belkin &amp; Niyogi (2002); Roweis &amp; Saul (2000); Tenenbaum et al. (2000) theoretically grounded but</cell></row><row><cell cols="5">hardly scaling up to graphs with over a thousand of nodes. With the emergence of neural networks,</cell></row><row><cell cols="5">unsupervised network embedding methods based on the Skip-gram objective Mikolov et al. (2013)</cell></row><row><cell cols="5">have replenished the field Tang et al. (2015); Grover &amp; Leskovec (2016); Perozzi et al. (2014);</cell></row><row><cell cols="5">Ribeiro et al. (2017). Equipped with efficient structural sampling (random walk, neighborhood, etc.)</cell></row><row><cell cols="5">and negative sampling schemes, these methods are easily parallelizable and scalable to graphs with</cell></row><row><cell cols="5">thousands to millions of nodes. However, these models are essentially transductive as they compute</cell></row><row><cell cols="5">fully parameterized embeddings only for nodes seen during training, which are impossible to be</cell></row><row><cell>transfered to unseen graphs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p>More recently, researchers introduce the family of graph neural networks (GNNs) that are capable of inductive learning and generalizing to unseen nodes given meaningful node features Kipf &amp; Welling (2017);</p><ref type="bibr" target="#b9">Defferrard et al. (2016)</ref></p>;</p><ref type="bibr" target="#b14">Hamilton et al. (2017)</ref></p>. However, most existing GNNs require task-specific labels for training in a semi-supervised fashion to achieve satisfactory performance Kipf &amp; Welling (2017);</p><ref type="bibr" target="#b14">Hamilton et al. (2017)</ref></p>;</p><ref type="bibr" target="#b44">Velickovic et al. (2018)</ref></p>;</p><ref type="bibr" target="#b6">Chen et al. (2018)</ref></p>, and their usage is limited to single graphs where the downstream task is fixed. To this end, several unsupervised Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.</p>Hierarchical graph representation learning with differentiable pooling. In NIPS, 2018b.</p>Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In Proceedings of the 35th International Conference on Machine Learning, pp. 5708-5717. PMLR, 2018. 6 THEORY DETAILS From the L EGI objective, we have assumed g i i.i.d. ? ?, x i i.i.d. ? ?, and (g i , x i ) i.i.d.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Synthetic experiments of identifying structural-equivalent nodes with different degree encoding dimensions.</figDesc><table><row><cell>Method</cell><cell cols="6">#dim degree encoding d = 3 # dim degree encoding d = 10 structural difference F-F B-F ? F-F B-F ? d(F,F) d(B,F)</cell></row><row><cell cols="2">GCN (untrained) 0.478 0.478</cell><cell>/</cell><cell>0.940 0.940</cell><cell>/</cell><cell></cell><cell></cell></row><row><cell>GIN (untrained)</cell><cell>0.572 0.572</cell><cell>/</cell><cell>0.940 0.940</cell><cell>/</cell><cell></cell><cell></cell></row><row><cell>GVAE (GCN)</cell><cell>0.498 0.432</cell><cell>+0.066</cell><cell>0.939 0.937</cell><cell>0.002</cell><cell>1.78</cell><cell>2.17</cell></row><row><cell>DGI (GIN)</cell><cell>0.578 0.591</cell><cell>-0.013</cell><cell>0.939 0.941</cell><cell>-0.002</cell><cell></cell><cell></cell></row><row><cell>EGI (GIN)</cell><cell>0.710 0.616</cell><cell>+0.094</cell><cell>0.942 0.942</cell><cell>0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Synthetic experiments of identifying structural-equivalent nodes with different transferable and nontransferable features.</figDesc><table><row><cell>Method</cell><cell cols="2">DeepWalk embedding F-F B-F ?</cell><cell cols="2">random vectors F-F B-F ?</cell><cell>structural difference d(F,F) d(B,F)</cell></row><row><cell cols="2">GCN (untrained) 0.658 0.658</cell><cell>/</cell><cell>0.246 0.246</cell><cell>/</cell></row><row><cell>GIN (untrained)</cell><cell>0.663 0.663</cell><cell>/</cell><cell>0.520 0.520</cell><cell>/</cell></row><row><cell>GVAE (GCN)</cell><cell cols="3">0.713 0.659 +0.054 0.266 0.264</cell><cell>0.002</cell><cell>1.78</cell><cell>2.17</cell></row><row><cell>DGI (GIN)</cell><cell cols="4">0.640 0.613 +0.027 0.512 0.576 -0.064</cell></row><row><cell>EGI (GIN)</cell><cell cols="4">0.772 0.715 +0.057 0.507 0.485 +0.022</cell></row><row><cell cols="4">8.2 REAL-WORLD ROLE IDENTIFICATION EXPERIMENTS</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Overall Dataset Statistics</figDesc><table><row><cell cols="4">Dataset # Nodes # Edges # Classes</cell></row><row><cell>Europe</cell><cell>399</cell><cell>5,995</cell><cell>4</cell></row><row><cell>USA</cell><cell>1,190</cell><cell>13,599</cell><cell>4</cell></row><row><cell>Brazil</cell><cell>131</cell><cell>1,074</cell><cell>4</cell></row><row><cell>Gene</cell><cell>9,228</cell><cell>57,029</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Role identification that identifies structurally similar nodes on real-world networks. The performance reported are the average and standard deviation for 10 runs. Our classification accuracy on three datasets all passed the t-test (p&lt;0.01) with the second best result in the table.</figDesc><table><row><cell>Method</cell><cell>Europe</cell><cell cols="2">Airport Ribeiro et al. (2017) USA</cell><cell>Brazil</cell></row><row><cell>node degree</cell><cell cols="2">52.81% ? 5.81% 55.67% ? 3.63%</cell><cell cols="2">67.11% ? 7.58%</cell></row><row><cell>GCN (random-init)</cell><cell cols="2">52.96% ? 4.51% 56.18% ? 3.82%</cell><cell cols="2">55.93% ? 1.38%</cell></row><row><cell>GIN (random-init)</cell><cell cols="2">55.75% ? 5.84% 62.77% ? 2.35%</cell><cell cols="2">69.26% ? 9.08%</cell></row><row><cell cols="3">GVAE (GIN) Kipf &amp; Welling (2016) 53.90% ? 4.65% 58.99% ? 2.44%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>dataset statistics and running time of EGI</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="3"># Relations # Train/Test Training time per epoch</cell></row><row><cell cols="3">YAGO-Source 579,721 2,191,464</cell><cell>/</cell><cell>/</cell><cell>338 seconds</cell></row><row><cell cols="2">YAGO-Target 115,186</cell><cell>409,952</cell><cell>24</cell><cell>480/409,472</cell><cell>134 seconds</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the experiments, we show our model to be generalizable to the more practical settings with task-specific pre-training and fine-tuning, while the study of rigorous bound in such scenarios is left as future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We are not exploring graph-level tasks and but focusing on transfer knowledge between two graphs. Thus, we drop the graph-level pre-training tasks in the paper since it is not applicable to our setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The downstream tasks are unspecified because we aim to study the general transferability of GNNs that is not bounded to specific tasks. Nevertheless, we assume the tasks to be relevant to graph structures, because otherwise the study is irrelevant to GNNs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>CONCLUSIONTo the best of our knowledge, this is the first research effort towards establishing a theoretically grounded framework to analyze GNN transferability, which we also demonstrate to be practically useful for guiding the design and conduct of transfer learning with GNNs. For future work, it is intriguing to further strengthen the bound with relaxed assumptions, rigorously extend it to the more complicated and less restricted settings regarding node features and downstream tasks, as well as analyze and improve the proposed framework over more transfer learning scenarios and datasets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>O(n ? m) is the orthogonal group of order n ? m. So we have Lg i and Lg i admits simultaneous ordered spectral decomposition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>O(?)  is the orthogonal group of square matrix ?. So we have Lg i and Lg i admits simultaneous ordered spectral decomposition.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for approximate semidefinite programming using the multiplicative weights update method</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast depth-based subgraph kernels for unattributed graphs</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A note on von neumann&apos;s trace inequalitv</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Dieter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorieff</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Nachrichten</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="328" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>David K Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACHA</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pre-training graph neural networks for generic structural feature extraction</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13728</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cauchy&apos;s interlace theorem for eigenvalues of hermitian matrices</title>
		<author>
			<persName><forename type="first">Suk-Geun</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="159" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7090" to="7099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph normalizing flows</title>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13556" to="13566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Easing embedding learning by comprehensive transcription of heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2190" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
