<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaohui</forename><forename type="middle">Mei</forename><surname>Mei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Learning Sensor-Specific Spatial-Spectral Features of Hyperspectral Images via Convolutional Neural Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics and Informa-tion</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710129</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<postCode>39762</postCode>
									<settlement>Starkville</settlement>
									<region>MS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">06AA76C39715FAB602B2B656DFC365AF</idno>
					<idno type="DOI">10.1109/TGRS.2017.2693346</idno>
					<note type="submission">Manuscript received January 25, 2017; revised March 13, 2017; accepted April 4, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Classification</term>
					<term>convolutional neural network (CNN)</term>
					<term>feature learning</term>
					<term>hyperspectral</term>
					<term>spatial-spectral</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural network (CNN) is well known for its capability of feature learning and has made revolutionary achievements in many applications, such as scene recognition and target detection. In this paper, its capability of feature learning in hyperspectral images is explored by constructing a five-layer CNN for classification (C-CNN). The proposed C-CNN is constructed by including recent advances in deep learning area, such as batch normalization, dropout, and parametric rectified linear unit (PReLU) activation function. In addition, both spatial context and spectral information are elegantly integrated into the C-CNN such that spatial-spectral features are learned for hyperspectral images. A companion feature-learning CNN (FL-CNN) is constructed by extracting fully connected feature layers in this C-CNN. Both supervised and unsupervised modes are designed for the proposed FL-CNN to learn sensorspecific spatial-spectral features. Extensive experimental results on four benchmark data sets from two well-known hyperspectral sensors, namely airborne visible/infrared imaging spectrometer (AVIRIS) and reflective optics system imaging spectrometer (ROSIS) sensors, demonstrate that our proposed C-CNN outperforms the state-of-the-art CNN-based classification methods, and its corresponding FL-CNN is very effective to extract sensor-specific spatial-spectral features for hyperspectral applications under both supervised and unsupervised modes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Such within-class variation may blur the discrimination among different types of ground objects. Therefore, in many applications (e.g., classification and target detection), feature extraction is usually performed as a preprocessing step to enhance the separability among different objects.</p><p>During the past decades, many feature extraction algorithms have been proposed and applied to hyperspectral image processing, such as principal component analysis (PCA), manifold learning, linear discriminant analysis (LDA), nonnegative matrix factorization (NMF), and spatial-spectral feature extraction. The PCA seeks the projection that minimizes the mean-square error of data approximation. It has been applied to hyperspectral classification <ref type="bibr" target="#b1">[2]</ref>, target detection <ref type="bibr" target="#b2">[3]</ref>, compression <ref type="bibr" target="#b3">[4]</ref>, and so on. Variants of the PCA have also been applied to hyperspectral image processing. For example, maximum noise fraction, also known as noise adjusted PCA, seeks the projection that optimizes signal-to-noise ratio of original data and has been used as a popular dimensionality reduction tool <ref type="bibr" target="#b4">[5]</ref>. Segmented PCA, in which a hyperspectral image cube is divided into several nonoverlapping blocks in accordance with spectral correlation and then the PCA is performed in each block, has been applied to hyperspectral display and classification <ref type="bibr" target="#b5">[6]</ref>, band selection <ref type="bibr" target="#b6">[7]</ref>, and compression <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The folded PCA, where spectral vectors are folded into a matrix to determine the covariance matrix, improves the performance of hyperspectral classification by extracting the correlation between bands and band groups <ref type="bibr" target="#b9">[10]</ref>. Moreover, PCA has also been extended to nonlinear versions in hyperspectral applications by kernel methods <ref type="bibr" target="#b10">[11]</ref>, autoassociative neural network <ref type="bibr" target="#b11">[12]</ref>, and so on. Different from the PCA that is optimal for Gaussian data under the uncorrelated noise assumption, the independent component analysis imposes a stronger independent assumption to reveal interesting features in usual non-Gaussian hyperspectral data <ref type="bibr" target="#b12">[13]</ref>.</p><p>In manifold learning, the hyperspectral data are assumed to lie on sparse and nonlinear manifolds <ref type="bibr" target="#b13">[14]</ref>. Such geometric and topological structures can be exploited via manifold-learning techniques, such as isometric feature mapping <ref type="bibr" target="#b14">[15]</ref>, kernel PCA <ref type="bibr" target="#b15">[16]</ref>, and local linear embedding <ref type="bibr" target="#b15">[16]</ref>. Many manifold learning-based methods have been applied for hyperspectral feature extraction <ref type="bibr" target="#b16">[17]</ref>, classification <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and anomaly detection <ref type="bibr" target="#b19">[20]</ref>. The LDA seeks a linear transform such that class separability, measured by the Rayleigh quotient, can be maximized. Its variants have been developed for feature extraction of hyperspectral data, e.g., linear constrained distance-based discriminant analysis <ref type="bibr" target="#b20">[21]</ref> and modified LDA for small training sample problems <ref type="bibr" target="#b21">[22]</ref>. The LDA is optimal under the assumption of homoscedastic Gaussian class-conditional distributions. In order to accommodate class distributions that are not unimodal homoscedastic Gaussian, the local fisher discriminant algorithm was developed by combining the discriminative properties of the LDA with the properties of unsupervised locality-preserving projections <ref type="bibr" target="#b17">[18]</ref>. The NMF has been demonstrated to learn parts representation of the data. Such nonnegative features are very effective for hyperspectral applications <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>Spatial context, which models the relationship between pixels and their neighbors, has been demonstrated to be critical to hyperspectral image processing <ref type="bibr" target="#b24">[25]</ref>. By integrating both spectral and spatial information, classification accuracy can be significantly improved. For instance, the extended morphological transformations extracted spatial-spectral feature based on the generalization of mathematical morphology theory to multichannel imagery <ref type="bibr" target="#b25">[26]</ref>. Experimental results demonstrated that, by designing morphological filtering methods that take into account the complementary nature of spatial and spectral information in a simultaneous manner, it is possible to alleviate the problems related to each of them when taken separately.</p><p>The features extracted in traditional feature extraction algorithms, including PCA, manifold learning, LDA, and NMF, can be viewed as hand-designed features, since they are developed according to prior experience of the designers. It may be difficult for them to take full advantage of the characteristics of big data in hyperspectral applications. In fact, they explore a certain characteristics of the data by tuning the involved parameters. Generally, in a pattern recognition task, feature extraction and classification can be jointly optimized to achieve better performance, since they are highly correlated. However, traditional feature extraction algorithms are usually designed or trained separately from the subsequent classifier.</p><p>Different from traditional feature extraction algorithms, deep learning can learn features automatically from the data itself. In general, thousands of parameters are involved in a deep neural network (DNN) to explore the characteristic of big data. For example, in the convolutional neural network (CNN) proposed by Krizhevsky et al. <ref type="bibr" target="#b26">[27]</ref> for ImageNet competition, more than 60 000 000 parameters are involved. All of these parameters can be optimized jointly with subsequent classifiers to take full advantage of both feature extraction and classification. Traditional learning algorithms, e.g., the support vector machine (SVM) and the boosting algorithm, can be viewed as shallow learning with limited layers. Though the theoretical analysis has demonstrated that any classification function can be approached by shallow learning algorithms, e.g., for a threelayer neural network, including an input layer, an output layer, and a hidden layer, its learning ability may be limited if the hidden layer is not sufficiently large. The local representation of such shallow learning may fail in complex classification tasks. However, the deep structure in deep learning, which is generally realized by more layers in a DNN, possesses powerful ability to extract global features that represent contextual information due to its laywise and distributed featurerepresentation ability. Such powerful learning ability, together with efficient feature expression ability, enables deep learning to achieve excellent performance in many image processing tasks, especially for classification and target detection.</p><p>Recently, many researchers have attempted to apply deep learning techniques for hyperspectral classifications. For example, Chen et al. <ref type="bibr" target="#b27">[28]</ref> first used stacked autoencoders (SAEs) to extract discriminative features for classification. Hu et al. <ref type="bibr" target="#b28">[29]</ref> first deployed a deep CNN to classify hyperspectral images directly in spectral domain. Makantasis et al. <ref type="bibr" target="#b29">[30]</ref> integrated a spatial-spectral input into the for classification using Randomized PCA (R-PCA) for dimensionality reduction. Spatial-related features extracted from the CNN, together with spectral-related features extracted by balanced local discriminant embedding, have also been used for hyperspectral classification <ref type="bibr" target="#b30">[31]</ref>. Li et al. <ref type="bibr" target="#b31">[32]</ref> also proposed to use the CNN to classify pixel-pairs and fuse the classification results of pixels in different pairs from its neighborhood. Though the idea of the CNN for feature extraction has been proposed <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the CNN must be trained using samples from the target hyperspectral scenes.</p><p>In this paper, spatial-spectral feature learning by deep learning theory is proposed for hyperspectral sensors. First, a novel five-layer CNN is constructed for hyperspectral classification by integrating the latest developments in deep learning areas, including batch normalization <ref type="bibr" target="#b32">[33]</ref>, dropout <ref type="bibr" target="#b33">[34]</ref>, and Parametric Rectified Linear Unit (PReLU) activation function <ref type="bibr" target="#b34">[35]</ref>. Both spatial context and spectral information are elegantly integrated into the framework by adopting the mean and/or the deviation of pixels in a spatial neighborhood as the input. Second, the proposed CNN for classification (C-CNN) is extended for feature learning of hyperspectral sensors, i.e., the airborne visible/infrared imaging spectrometer (AVIRIS) sensor and the reflective optics system imaging spectrometer (ROSIS) sensor. As a result, a feature-learning CNN (FL-CNN) is proposed to learn features under either unsupervised or supervised mode. For the unsupervised mode, the FL-CNN inherits parameters from the C-CNN trained on other data sets acquired from the same sensor with the target data set, and then is applied to the target data set for feature extraction without tuning any parameters. For the supervised mode, the trained FL-CNN with the unsupervised mode is further fine-tuned by using only a few training samples from the target scene. It is worth pointing out that the supervised mode is more effective than the unsupervised mode. We fully exploit the ability of feature learning by deep learning for hyperspectral sensors, including feature extraction ability, transfer ability to other images by the same sensor, and fine-tune ability to other images taken by the same sensor, which also provides useful insights for other deep learning-based works for hyperspectral images. Experiments on two benchmark hyperspectral image acquired by the AVIRIS sensor and two by the ROSIS sensor demonstrate their superior performance over the state-of-theart methods.</p><p>In summary, the main contributions of this paper are threefold:</p><p>1) constructing a novel five-layer C-CNN of hyperspectral images by integrating both spatial context and spectral features; 2) proposing the concept of sensor-specific spatial-spectral feature learning, including ability of feature extraction, transfer ability to other images by the same sensor and fine-tune ability to other images by the same sensor; 3) conducting systematic experiments for feature learning and classification of hyperspectral images/sensors. This will deeply inspire other deep learning ideas for hyperspectral applications. The remainder of this paper is organized as follows. Section II presents the proposed C-CNN and FL-CNN. Section III reports and discusses experimental results on images acquired by different sensors. Finally, conclusions are drawn in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHODS</head><p>Although the CNN proposed by Krizhevsky et al. <ref type="bibr" target="#b26">[27]</ref> is trained on the ImageNet data sets for natural image classification, the output of the second-last fully connected layer, known as deep learning features, can discriminate different classes that have not been fed to the network for training. For example, the features extracted by such a network improve the accuracy of object detection in PASCAL VOC data set by more than 20% <ref type="bibr" target="#b35">[36]</ref>. Therefore, in order to construct and train the CNN for feature learning of hyperspectral data, it can be first trained with classification tasks. In this paper, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we construct spatial-spectral FL-CNN by the following steps: 1) constructing and training CNN for spatial-spectral hyperspectral classification (denoted as C-CNN); 2) constructing FL-CNN according to its companion C-CNN for different hyperspectral sensors, e.g., AVIRIS and ROSIS sensors; and 3) extracting features by the proposed sensor-specific FL-CNN under unsupervised or supervised mode. More specifically, under the unsupervised mode, the proposed sensor-specific FL-CNN is directly applied on target data sets without any training, while under the supervised mode, the parameters involved in the proposed sensor-specific FL-CNN are further fine-tuned by only a few training samples from the target data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proposed C-CNN for Hyperspectral Classification</head><p>The CNN is one of feedforward neural networks where individual neurons are tiled in such a way that they respond to overlapping regions in the visual field. In general, a typical CNN consists of one or more pairs of convolution and Max pooling layers and finally ends with a fully connected neural networks. Inspired by <ref type="bibr" target="#b28">[29]</ref>, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we propose a novel five-layer CNN-based framework for hyperspectral classification. It includes more components, namely batch normalization layer, dropout, and PReLU activation function, so that the learning ability can be significantly enhanced. In the following description, we describe each component in detail.</p><p>1) Input Layer: b and o represent the numbers of spectral bands and pixels of the image, respectively. The aim of the classification is to assign a class label to each pixel r i . Since the CNN is effective to conduct end-to-end tasks, the original b-dimensional pixels are regarded as inputs directly fed to the proposed CNN, such that the input to the proposed CNN (denoted as I) becomes:</p><formula xml:id="formula_0">Let r i ∈ R b×1 (i = 1, 2, . . . , o) denote the i th b-dimensional pixel of a hyperspectral image where</formula><formula xml:id="formula_1">I = r i (i = 1, 2, . . . , o).</formula><p>2) Convolutional Layer: As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(a), in traditional neural networks, e.g., MLP, a neuron is connected to all neurons in the successive layer. However, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>(b), neurons in the convolutional layer of a CNN are only sparsely connected to those of the next layer, based on their relative locations. Actually, the convolutional layer consists of a set of learnable filters (or kernels) that have a small receptive field. These filters are trained to learn specific types of features at some spatial position in the input. In the proposed CNN, the convolutional layer is constructed as the same as that in <ref type="bibr" target="#b28">[29]</ref>. It filters the n 1 × 1 input data with n K kernels of size 1 × k, indicating that each filter is trained to learn features from k continuous spectral bands. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(b), a neuron in the convolutional layer receives input from k continuous neurons in the input layer, and its output is the weighted summation of all the inputs defined as</p><formula xml:id="formula_2">C j q, p = k l=1 W j p,q,l I p+l-1 , p = 1, 2, . . . , n 1 q = 1, 2, . . . , n 2 ; j = 1, 2, . . . , n K (1)</formula><p>where I p ( p = 1, 2, . . . , n 1 ) represents the input to the convolutional layer (i.e., the input to the proposed CNN), C j q, p represents the output of the (q, p)th neuron in the j th kernel, and W j p,q,l represents the weight between the lth input neuron and (q, p)th neuron in the j th convolutional kernel. As a result, n 2 = n 1k + 1 and the layer contains K × (n 1k + 1) × 1 nodes. By using the weight sharing strategy <ref type="bibr" target="#b36">[37]</ref>, the total number of trainable parameters is reduced to n K × k, leading to more efficient training.</p><p>3) Batch Normalization Layer: Regarding hyperspectral images, each band represents the spectral response of a specific wavelength. In <ref type="bibr" target="#b28">[29]</ref>, Max pooling layer following the convolutional layer was proposed to achieve transformationinvariant and more compact representations <ref type="bibr" target="#b37">[38]</ref>. Though such a Max pooling layer can also reduce the number of training parameters of the CNN, it may result in information loss in the spectral dimension, because it blurs the discrimination among different spectral bands, which is crucial to classify different materials in hyperspectral applications. We also experimentally observe that the accuracy improvement by adopting "Convolutional layer + Max pooling" is limited due to its low fitting ability in hyperspectral classification. Therefore, in the proposed CNN, batch normalization is used to normalize the features generated by the convolutional layer such that the feature weight of each band will be close to each other. Thus, a large learning rate can be used to speed up the training process <ref type="bibr" target="#b32">[33]</ref>.</p><p>Assume that there are n B neurons in a batch of neurons, and their inputs (i.e., the output of convolutional layer) are denoted as C i (i = 1, 2, . . . , n B ). The batch normalization is represented as</p><formula xml:id="formula_3">B i = γ Ĉi + β (2) Ĉi = C i -μ B σ 2 B + ε (3) μ B = 1 n B n B i=1 C i (4) σ 2 B = 1 n B n B i=1 (C i -μ B ) 2<label>(5)</label></formula><p>where B i represents the output of the batch, μ B and σ 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head><p>represent the average and variance in this batch, respectively, γ and β are trainable parameters, and ε is an arbitrarily small positive number to prevent division by zero. Since layer B3 normalizes the results of layer C2, the size of layer B3 is the same as that of layer C2, i.e., n 3 = n K × (n 2 × 1). 4) Fully Connected Layer: Like the neuron connection shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>, neurons in the fully connected layer have full connections to all activations in the previous layer so as to integrate information across all locations in all the feature maps of the previous layer <ref type="bibr" target="#b26">[27]</ref>. Therefore, following the convolutional layer and the batch normalization layer, a fully connected layer labeled as "F4" is used for high-level reasoning. Let F i (i = 1, 2, . . . , n 4 ) represent the output of neurons in the fully connected layer, which is determined as</p><formula xml:id="formula_4">F i = PReLU n 3 l=1 W F i,l B l + b F i , i = 1, 2, . . . , n 4 (6)</formula><p>where B l (l = 1, 2, . . . , n 3 ) represents the output of neurons in batch normalization layer, n 4 denotes the number of neurons in the fully connected layer, W F i,l represents the weights between the lth neuron in the batch normalization layer and the i th neuron in the fully connected layer, b F i represents the bias of the i th neuron in the fully connected layer, and PReLU(•) represents the PReLU activation function defined as <ref type="bibr" target="#b34">[35]</ref> PReLU(x) =</p><p>x, if x &gt; 0 αx, if x 0 <ref type="bibr" target="#b6">(7)</ref> with α being a trainable parameter involved in this activation function. Such an activation function is adopted because it adaptively learns the parameters of the rectifiers and improves accuracy at negligible extra computational cost.</p><p>In order to avoid overfitting, dropout is also used in this layer to prevent units from co-adapting too much <ref type="bibr" target="#b33">[34]</ref>. During training, neurons along with their connections are randomly dropped to sample from an exponential number of different "thinned" networks. Thus, in testing, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network with smaller weights. As a result, overfitting can be avoided.</p><p>5) Output Layer: In hyperspectral classification tasks, the number of units in the output layer (denoted as n 5 ) is set as the number of classes. The "Softmax" classifier is used in this layer. The input of each neuron is the weighted summation of all the neurons in the previous fully connected layer</p><formula xml:id="formula_5">O I i = n 4 l=1 W O i,l F l + b O i , i = 1, 2, . . . , n 5 (<label>8</label></formula><formula xml:id="formula_6">)</formula><p>where </p><formula xml:id="formula_7">O i (i = 1,</formula><formula xml:id="formula_8">O i = e O I i n 5 l=1 e O I l , i = 1, 2, . . . , n 5 . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>Such an output measures the probability of a pixel belonging to the i th class. As a result, when a pixel r is fed to the network, it is classified to the neuron with the maximum output</p><formula xml:id="formula_10">L(r) = arg max i O i (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where L(r) represents the class label of the pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Use of Spatial Context in the Proposed CNN</head><p>Spatial context has been widely utilized to improve the performance of hyperspectral classification, since it can reduce the disturbance of within-class variation. For example, the SVM with composite kernels (SVM-CKs) classifier incorporates spatial information directly into the SVM kernels <ref type="bibr" target="#b38">[39]</ref>, and average signatures from spatial neighbors were used to improve classification accuracy <ref type="bibr" target="#b39">[40]</ref>. The simultaneous orthogonal matching pursuit (OMP)-based classifiers improve the performance of OMP-based method by classifying pixels in a neighborhood simultaneously <ref type="bibr" target="#b40">[41]</ref>. Therefore, in order to achieve high performance, we must also explore the spatial context in the proposed CNN-based method. To achieve this objective, one straightforward method is that all the neighboring pixels can be fed to the network when classifying a specific pixel, such as using 3-D input by directly feeding all the pixels in a spatial neighborhood into the network simultaneously as the "S0" layer in Fig. <ref type="figure" target="#fig_1">2</ref>. However, more layers (or more neurons in each layer) are required to learn their correlations, and much more parameters are induced. In the proposed CNN, inspired by the SVM-CK algorithm, spatial context is elegantly explored by applying a simple feature extraction to the surrounding areas of pixels, which yields spatial (contextual) features, e.g., the mean or standard deviation per spectral band <ref type="bibr" target="#b38">[39]</ref>. It should be noted that such preprocessing actually sets the spatial weights according to prior knowledge, indicating that the spatial weights between "S0" and "I1" in Fig. <ref type="figure" target="#fig_1">2</ref> are fixed during the training process. As a result, the requirement of a large number of samples to train the spatial weights and many neurons or layers to learn their inner relation is unnecessary. Here, three types of inputs are considered for the proposed CNN.</p><p>1) (n 1 , 1) units representing the original b-dimensional pixels r i such that n 1 = b. Therefore, the input of the proposed CNN is</p><formula xml:id="formula_12">I = r i , i = 1, 2, . . . , o.<label>(11)</label></formula><p>2) (n 1 , 1) units representing the mean of neighboring pixels around r i (denoted by ri ) such that n 1 = b. Therefore, the input of the proposed CNN is</p><formula xml:id="formula_13">I = ri = 1 p r j ∈N(r i ) r j , i = 1, 2, . . . , o<label>(12)</label></formula><p>where N(r i ) is the neighboring pixels around r i and p is the number of neighboring pixels. 3) (n 1 , 1) units representing both the mean and standard deviation per spectral band of neighboring pixels around r i such that n 1 = 2b. Therefore, the input of the proposed CNN is</p><formula xml:id="formula_14">I = ( ri ) T Std(r i ) T T (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where Std(r i ) denotes the standard deviation per spectral band of neighboring pixels around r i that can be obtained as</p><formula xml:id="formula_16">Std(r i ) k = 1 p r j ∈N(r i ) (r j ) k -ri k 2 , k = 1, 2, . . . , b, i = 1, 2, . . . , o. (<label>14</label></formula><formula xml:id="formula_17">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial-Spectral Feature Learning by the Proposed CNN</head><p>The output layer of the proposed C-CNN shown in Fig. <ref type="figure" target="#fig_1">2</ref> can be viewed as a classifier, and thus, its input, namely "F4," can be viewed as the features extracted from the network input. Such an idea for feature learning has been widely validated in computer vision fields. For example, the features learned by AlexNet that is trained on ImageNet data sets are very effective for other visual tasks. Therefore, we propose to use the CNN composed by "(S0-)I1-C2-B3-F4," namely FL-CNN shown in Fig. <ref type="figure" target="#fig_4">4</ref>, for spatial-spectral feature learning of hyperspectral images. In order to achieve sensor-specific spatialspectral feature learning, the proposed FL-CNN [actually "(S0-)I1-C2-B3-F4"] is trained to learn the separability of a sensor using a specific data set under the classification task. Such a learning process can be fulfilled with its companion C-CNN shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Once the C-CNN is well trained, its parameters can be transferred to the FL-CNN. As a result, the proposed FL-CNN can be directly applied for other data sets acquired by the same sensor.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, when the FL-CNN is adopted for feature extraction, it can be performed under the following two modes.</p><p>1) Unsupervised Mode: The parameters of the FL-CNN are inherited from the C-CNN trained on other data sets acquired by the same sensor as the target data set. As a result, the training samples from the target data set are not required, and thus, such an FL-CNN can be viewed as a sensor-specific learning system, which can be directly applied for feature extraction without tuning parameters. Under this mode, the training of the proposed FL-CNN is separated from validation to avoid the requirement of a large number of training samples on the target applications.</p><p>2) Supervised Mode: The sensor-specific FL-CNN is further fine-tuned using training samples from the target data set. Such fine-tuning can be conducted under C-CNN mode by designing an output classification layer for the target data set. With this processing, the FL-CNN will adapt to the target data set better to extract discriminative features for better classification. It should be noted that such a fine-tuning process requires not as many labeled samples as those in the training process. Even with only a few training samples, the supervised FL-CNN is effective to extract spatial-spectral features for classification (see the quantitative evaluation in Section III-C).</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, when the proposed FL-CNN is used for classification, traditional classifiers, e.g., SVM, can be directly used for classification. In addition, the proposed FL-CNN can also be adopted to further construct a C-CNN for classification. An output layer should be designed and trained for the FL-CNN to form the C-CNN and only the weights between FL-CNN and output classification layer should be trained. Moreover, all the parameters in the C-CNN can also be finetuned together to achieve better performance of classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, extensive experiments are conducted to verify the performance of the proposed spatial-spectral C-CNN and FL-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Data sets from two well-known hyperspectral sensors are used for evaluation: AVIRIS and ROSIS. For the AVIRIS sensor, two benchmark data sets are adopted, i.e., the Indian Pine data set and the Salinas Valley data set. As shown in Fig. <ref type="figure" target="#fig_6">5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of the Proposed C-CNN</head><p>The performance of the proposed C-CNN for classification is evaluated over the four selected scenes. For each class in a certain scene, 200 samples are randomly selected for   training while the remaining ones are used for evaluation. For the Indian Pines data set, the classes containing less than 200 pixels are discarded, and only nine classes are used in this experiment. The proposed C-CNN is designed, trained, and tested using the Caffe framework <ref type="bibr" target="#b41">[42]</ref>. The backpropagation strategy is used to train the network. The parameter settings of the proposed C-CNN are listed in Table <ref type="table" target="#tab_0">I</ref>. For the input layer, 3 × 3 and 5 × 5 neighboring pixels are considered as spatial context, respectively. The two data sets of the AVIRIS sensor are complemented to 224 bands by setting the missing bands as 0. Similarly, all the data sets of the ROSIS sensor are complemented to 103 bands. According to (11)-( <ref type="formula" target="#formula_14">13</ref>), the size of the input layer of C-CNN n 1 is set to 224 or 448 for the AVIRIS sensor, and 103 or 206 for the ROSIS, respectively. The number of convolution kernels n K is set as 20, and the size of each convolution kernel is set to k × 1 with k = 1/9n 1 according to the experimental setting in <ref type="bibr" target="#b28">[29]</ref>. Specifically, the parameter k in the convolutional layer is set to 24 or 48 for the AVIRIS sensor, and 11 or 22 for the ROSIS sensor. The number of neurons in the fully connected layer (i.e., n 4 ) is set to 100, indicating that 100-D feature is extracted. For the output layer, it equals to the number of classes to be classified, which is 16 for Salinas Valley data set, and nine for the other three data sets. In addition, the learning rate and the dropout ratio are 0.01 and 0.1, respectively. The overall classification accuracy (OA) and average classification accuracy (AA) are adopted as quantitative metrics to evaluate the performance of classification. All the experiments are repeated ten times by randomly selected training samples and the average results are reported. The traditional SVM classifier with RBF kernel is implemented using the LIBSVM package <ref type="bibr" target="#b42">[43]</ref>. The stateof-the-art CNN-based methods in <ref type="bibr" target="#b28">[29]</ref>, R-PCA CNN <ref type="bibr" target="#b29">[30]</ref>, and CNN based on pixel-pair features (CNN-PPF) <ref type="bibr" target="#b31">[32]</ref> are adopted for comparison.</p><p>The quantitative comparison in terms of accuracy per class, AA and OA, is listed in Tables II-V. It is observed that:</p><p>1) The proposed C-CNN outperforms SVM-RBF to a large extent over all of the four data sets.</p><p>2) The proposed C-CNN with only spectral as input slightly outperforms the CNN in <ref type="bibr" target="#b28">[29]</ref>, demonstrating the superiority of our proposed C-CNN structure; it produces comparable performance to that of R-PCA CNN, which takes both spectral information and spatial context as input.</p><p>3) The proposed C-CNN with both mean and standard variation as inputs achieves higher OA and AA over all of the four data sets, except that its AA is slightly inferior to that of CNN-PPF over the Indian Pines data set, indicating that the implementation of spatial information in the proposed C-CNN is effective to explore spatial context for classification. These conclusions are also confirmed by the classification maps shown in Figs. 9-12. The proposed C-CNN with both mean and standard variation as inputs over 5 × 5 spatial neighborhood outperforms in all the cases. Therefore, in the following experiments, both mean and standard variation inputs over 5 × 5 spatial neighborhood is used as the input of our proposed FL-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of the Proposed FL-CNN</head><p>In this section, a series of experiments over the four selected scenes are conducted to evaluate the performance of the proposed FL-CNN for classification. First of all, the visual results of part of features extracted by the FL-CNN trained with its companion C-CNN are shown in Fig. <ref type="figure" target="#fig_15">13</ref>. It is observed that the features learned by the FL-CNN obviously enlarge the interclass differences. In the following experiments, classification by adopting the features extracted by the FL-CNN is conducted to further demonstrate its superiority.</p><p>1) Feature Extraction With FL-CNN: For previous experiment on AVIRIS data set, the C-CNN is trained using labeled samples from nine classes with abundant pixels. We further evaluate the performance of the FL-CNN trained by these nine classes for feature extraction of all the 16 classes in this data set. The extracted features are then classified by the SVM trained with only 10% of samples in each classes, and tenfolded cross validation is used for evaluation. The per class classification results, together with that on the original data set by SVM, are listed in Table VI. It is observed that the performance of the SVM has been significantly improved by using the features extracted by the proposed FL-CNN, about 18% in AA and 15% in OA, demonstrating that the proposed FL-CNN is effective to extract distinctive features though it is trained only with part of classes with abundant training samples. The classification maps shown in Fig. <ref type="figure" target="#fig_16">14(a</ref>) and (b) also confirmed such improvement by the proposed FL-CNN.</p><p>2) Unsupervised Feature Extraction: In this experiment, the FL-CNN is used in an unsupervised manner to extract sensor-specific features from the four scenes. Specifically, the FL-CNN trained on Indian Pines data set is used for feature extraction of Salinas data set, and the one trained on the Salinas data set is used for feature extraction of the Indian Pines data set. Similar experiments are conducted on the two data sets acquired by the ROSIS sensor. After feature extraction by the proposed FL-CNN, the SVM trained by 10% of samples is used for classification. According to the results shown in Table <ref type="table" target="#tab_0">VI</ref>, even the FL-CNN is not trained by the targeted Indian Pines data set, the results of the SVM are also significantly improved by the proposed FL-CNN, more than 18% for AA and 10% for OA. The classification results using features extracted with sensor-specific FL-CNN under unsupervised mode are listed in Table <ref type="table" target="#tab_3">VII</ref>. It is also observed that the performance of the SVM has been improved by using features extracted with the proposed FL-CNN, though it is constructed using samples from other data sets acquired by the same sensors. These conclusions also hold by comparing the classification maps in Fig. <ref type="figure" target="#fig_16">14</ref>   training the DNN using samples from a specific data set of a sensor and then applying it to other data sets of the sensor. Such an unsupervised approach avoids the requirement of a large number of training samples and long training time.</p><p>3) Supervised Feature Extraction: According to Table VI, it is observed that the classification performance by adopting features extracted by FL-CNN trained with only part of classes in Indian Pines data set obviously outperforms that trained on Salinas Valley data set, indicating that the features are more effective if the FL-CNN can be trained on the target scene even if it is trained by part of the considered classes. Therefore, in this experiment, we further evaluate the performance of the proposed FL-CNN in a supervised manner by fine-tuning it with labeled samples of the target scene. First of all, 10% of samples for each class are used to fine-tune the proposed unsupervised sensor-specific FL-CNN for feature extraction of the four data sets. As shown in Table <ref type="table" target="#tab_3">VII</ref>, when the SVM is adopted as classifiers, the performance of feature extraction is further improved by using samples on the target scene to fine-tune the proposed unsupervised sensor-specific FL-CNN. It is also observed from Table <ref type="table" target="#tab_3">VII</ref> that the same conclusion is also confirmed by using the proposed C-CNN as classifier. The performance of fine-tuning the proposed sensor-specific FL-CNN is further tested using a small number of training samples on the target data set. In this experiment, only two, four, six, eight, and ten samples are adopted for supervised fine-tuning. Both the SVM and the proposed C-CNN are used for classification. Fig. <ref type="figure" target="#fig_17">15</ref> lists the supervised classification results of our proposed FL-CNN (labeled as sFL-CNN). The results of unsupervised FL-CNN (uFL-CNN) are also considered for comparison. Note that sFL-CNN + C-CNN means fine-tuning all the parameters in the C-CNN, while uFL-CNN + C-CNN means only training the fully classification layer for classification in the C-CNN. It is observed from Fig. <ref type="figure" target="#fig_17">15</ref> that the proposed FL-CNN is very effective for feature extraction, significantly improving the performance of classification with the SVM. When the SVM is adopted, the proposed FL-CNN can slightly improve the performance of unsupervised sensor-specific feature extraction even when a very small number of training samples are used, and more improvement will be achieved when more samples are used for tuning. When the C-CNN is used for classification, it is better to only fine-tune the fully classification layer for classification in the C-CNN. This is because small training samples are not enough to well train all the parameters in the C-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Complexity</head><p>Generally, the CNN involves many parameters to be optimized using a large number of training samples, making the training phase time-consuming. However, the testing phase is very fast given a trained CNN, since only the forward calculation in the CNN is conducted. In this paper, all the experiments are carried out using a PC equipped with Intel i7-6850k CPU and a single GeForce GTX 1080 GPU. The computational time of the proposed C-CNN, including both training and testing, is summarized in Table VIII. For the proposed C-CNN, it costs 27 min for the Indian Pines data set, 28 min for the Salinas Valley data set, and 10.5 min for both the Pavia Centre and Pavia University data sets. The two data sets by the AVIRIS sensor consume more time than those by the because contain more bands. This observation indicates that the computational complexity is dominated by the number of bands. It is also observed that the time consumed in the testing of the proposed C-CNN is nearly negligible compared with that spent in the training process. Table VIII also lists the running time of the proposed FL-CNN for feature extraction.</p><p>In the supervised mode, when 10% training sample is used for fine-tuning, it still spends less computation time than the training process, indicating that fine-tuning is much faster than training. However, when the proposed sensor-specific FL-CNN is used under the unsupervised mode, it runs very fast, solving the time-consuming problem of the CNN for hyperspectral applications.   IV. CONCLUSION In this paper, a novel CNN structure that integrates both spatial context and spectral signature is developed for hyperspectral classification. Based on this neural network, the FL-CNN is further constructed to learn spatial-spectral features for different image scenes acquired by a specific hyperspectral sensor. Such sensor-specific feature extraction can be conducted in either unsupervised or supervised mode; that is, the uFL-CNN can be trained by training samples acquired by the same sensor as the considered hyperspectral data, or by part of classes from the target scene, while the supervised FL-CNN is realized by fine-tuning the uFL-CNN with training samples from the target data set. The supervised FL-CNNs are more effective than the unsupervised one even when a small amount of training samples is used for fine-tuning. Experiments on four benchmark data sets by two well-known hyperspectral sensors have demonstrated the proposed C-CNN achieves higher classification accuracy than the state-of-the-art CNN-based methods, and the proposed FL-CNN is also very effective to learn sensor-specific features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of the proposed FL-CNN.</figDesc><graphic coords="3,48.95,58.73,514.10,259.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the proposed CNN for hyperspectral classification.</figDesc><graphic coords="3,323.99,362.69,226.94,86.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Illustration of the fully connected layer in traditional neural networks, e.g., MLP. (b) Illustration of the convolutional layer in the proposed CNN. One output unit and its input units are highlighted.</figDesc><graphic coords="4,166.43,58.85,131.54,84.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2, . . . , n 5 ) represents the input of neurons in this layer, W O i,l represents the weight between the lth neuron in the fully connected layer and the i th neuron in the output layer, and b O i represents the bias of the i th neuron in the output layer. The output of each neuron O i (i = 1, 2, . . . , n 5 ) is calculated as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the proposed CNN for feature learning.</figDesc><graphic coords="5,312.47,58.85,250.10,112.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a), the Indian Pine data set contains 145 × 145 pixels with a ground resolution of 17 m. The 200 bands of the original 224 atmospherically corrected bands are adopted by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Pseudocolor image of the Indian Pine data set. (b) Ground-truth classification map of Indian Pine data set.</figDesc><graphic coords="6,332.51,216.05,60.14,141.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Pseudocolor image of the Salinas Valley data set. (b) Ground-truth classification map of Salinas Valley data set.</figDesc><graphic coords="6,406.07,216.29,136.10,141.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Pseudocolor image of the Pavia Centre data set. (b) Ground-truth classification map of Pavia Centre data set.</figDesc><graphic coords="7,314.99,161.33,79.45,79.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Pseudocolor image of the Pavia University data set. (b) Groundtruth classification map of Pavia University data set.</figDesc><graphic coords="7,71.03,204.41,63.24,113.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Classification maps generated by different algorithms for Indian Pines data set with nine classes. (a) SVM-RBF. (b) CNN [29]. (c) R-PCA CNN [30]. (d) CNN-PPF [32]. (e) Proposed C-CNN with spectral input. (f) Proposed C-CNN with 3 × 3 mean input. (g) Proposed C-CNN with 3 × 3 Mean and Std input. (h) Proposed C-CNN with 5 × 5 Mean input. (i) Proposed C-CNN with 5 × 5 Mean and Std input.</figDesc><graphic coords="7,314.99,264.05,79.34,79.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Classification maps generated by different algorithms for Salinas Valley data set. (a) SVM-RBF. (b) CNN [29]. (c) R-PCA CNN [30]. (d) CNN-PPF [32]. (e) Proposed C-CNN with spectral input. (f) Proposed C-CNN with 3 × 3 Mean input. (g) Proposed C-CNN with 3 × 3 Mean and Std input. (h) Proposed C-CNN with 5 × 5 Mean input. (i) Proposed C-CNN with 5 × 5 Mean and Std input. (j) uFL-CNN +SVM.</figDesc><graphic coords="8,314.99,264.05,79.34,79.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Classification maps generated by different algorithms for Pavia Centre data set. (a) SVM-RBF. (b) CNN [29]. (c) R-PCA CNN [30]. (d) CNN-PPF [32]. (e) Proposed C-CNN with spectral input. (f) Proposed C-CNN with 3 × 3 Mean input. (g) Proposed C-CNN with 3 × 3 Mean and Std input. (h) Proposed C-CNN with 5 × 5 Mean input. (i) Proposed C-CNN with 5 × 5 Mean and Std input. (j) uFL-CNN +SVM.</figDesc><graphic coords="8,397.79,367.13,79.46,79.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Classification maps generated by different algorithms for Pavia University data set. (a) SVM-RBF. (b) CNN [29]. (c) R-PCA CNN [30]. (d) Proposed C-CNN with spectral input. (e) Proposed C-CNN with 3 × 3 Mean input. (f) Proposed C-CNN with 3 × 3 Mean and Std input. (g) Proposed C-CNN with 5 × 5 Mean input. (h) Proposed C-CNN with 5 × 5 Mean and Std input. (i) uFL-CNN +SVM.</figDesc><graphic coords="9,112.79,320.09,59.90,107.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) and (c) for Indian Pines data set, Fig. 10(a) and (i), Fig. 11(a) and (i), and Fig. 12(a) and (i) for the other three data sets. The proposed sensor-specific feature extraction CNN indicates an effective way of using the DNN for hyperspectral applications that is developing and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Visual results of part of features extracted by the proposed FL-CNN over these four data sets. (a) Indian Pines data. (b) Salinas Valley data set. (c) Pavia Centre data set. (d) Pavia University data set.</figDesc><graphic coords="9,239.51,189.29,59.90,107.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Classification maps generated by different algorithms for Indian Pines data set with 16 classes. (a) SVM on the original data. (b) SVM on the features extracted by FL-CNN trained with only nine classes. (c) SVM on the features extracted by FL-CNN trained on the Salinas Valley data set.</figDesc><graphic coords="9,314.99,306.89,79.34,79.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Results of supervised FL-CNN for classification. (a) OA on Indian Pines data set. (b) AA on Indian Pines data set. (c) OA on Salinas Valley data set. (d) AA on Salinas Valley data set. (e) OA on Pavia Centre data set. (f) AA on Pavia Centre data set. (g) OA on Pavia University data set. (h) AA on Pavia University data set. (i) Legend for these figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PARAMETERS</head><label>I</label><figDesc></figDesc><table /><note><p>OF THE PROPOSED C-CNN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ACCURACY OF THE PROPOSED C-CNN ON NINE-CLASS INDIAN PINES DATA SET TABLE III CLASSIFICATION ACCURACY OF THE PROPOSED C-CNN ON SALINAS VALLEY DATA SET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY OF THE PROPOSED C-CNN ON PAVIA CENTRE DATA SET TABLE V CLASSIFICATION ACCURACY OF THE PROPOSED C-CNN ON PAVIA UNIVERSITY DATA SET TABLE VI PERFORMANCE OF CLASSIFICATION BY FEATURES LEARNED BY THE PROPOSED FL-CNN ON 16-CLASS INDIAN PINES DATA SET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>OF UNSUPERVISED FEATURE EXTRACTION BY SENSOR-SPECIFIC FL-CNN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VIII COMPUTATIONAL</head><label>VIII</label><figDesc>TIME OF THE PROPOSED CNN IN DIFFERENT EXPERIMENTS (m: MINUTE AND s: SECOND)</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61671383 and Grant 61301195 and in part by the Fundamental Research Funds for the Central Universities under Grant 3102016ZB012 and Grant 3102016ZB029.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endmember variability in hyperspectral analysis: Addressing spectral variability during spectral unmixing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Principal component analysis for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rodarmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surv. Land Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the impact of PCA dimension reduction for hyperspectral detection of difficult targets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="195" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperspectral image compression using JPEG2000 and principal component analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="205" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A transformation for ordering multispectral data in terms of image quality with implications for noise removal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Switzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Craig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1988-01">Jan. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmented principal components transformation for efficient hyperspectral remote-sensing image display and classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="538" to="542" />
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Research on segmented PCA based on band selection algorithm of hyperspectral image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Surv. Mapping</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmented PCA-based compression for hyperspectral image analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
			<biblScope unit="volume">5268</biblScope>
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmented principal component analysis for parallel compression of hyperspectral imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="717" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Novel Folded-PCA for improved feature extraction and data reduction with hyperspectral imaging and SAR in remote sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zabalza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis for the classification of hyperspectral remote sensing data over urban areas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009-01">2009. Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear versus nonlinear PCA for the classification of hyperspectral data based on the extended morphological profiles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Licciardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="451" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Independent component analysis as a tool for the dimensionality reduction and the representation of hyperspectral images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mouchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert-Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2001-07">Jul. 2001</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2893" to="2895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manifold-learningbased feature extraction for classification of hyperspectral data: A review of advances in manifold learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ersoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting manifold geometry in hyperspectral imagery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fusina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locality-preserving discriminant analysis in kernel-induced feature spaces for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="894" to="898" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localitypreserving dimensionality reduction and classification for hyperspectral image analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse transfer manifold embedding for hyperspectral target detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1030" to="1043" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A linear constrained distance-based discriminant analysis for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="373" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modified Fisher&apos;s linear discriminant analysis for hyperspectral imagery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="507" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comparative study of spatial approaches for urban mapping using hyperspectral ROSIS images over Pavia City, northern Italy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3205" to="3221" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neighborhood preserving orthogonal PNMF feature extraction for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="759" to="768" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advances in spectral-spatial classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Tilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="652" to="675" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality reduction and classification of hyperspectral image data using sequences of extended morphological transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="466" to="479" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sens</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">258619</biblScope>
			<date type="published" when="2015-07">Jul. 2015</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep supervised learning for hyperspectral data classification through convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Geosci. Remote Sens. Symp</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using deep pixel-pair features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="844" to="853" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.01852" />
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Composite kernels for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munoz-Mari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vila-Frances</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Calpe-Maravilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint within-class collaborative representation for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2200" to="2208" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using dictionary-based sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3973" to="3985" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1408.5093" />
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
