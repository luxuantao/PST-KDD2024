<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Driver Drowsiness Detection via a Hierarchical Temporal Deep Belief Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ching-Hua</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ying-Hsiu</forename><surname>Lai</surname></persName>
							<email>lai@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shang-Hong</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Driver Drowsiness Detection via a Hierarchical Temporal Deep Belief Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">316F5AEFA0F3F28B42F0B51898409505</idno>
					<idno type="DOI">10.1007/978-3-319-54526-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Drowsy driver alert systems have been developed to minimize and prevent car accidents. Existing vision-based systems are usually restricted to using visual cues, depend on tedious parameter tuning, or cannot work under general conditions. One additional crucial issue is the lack of public datasets that can be used to evaluate the performance of different methods. In this paper, we introduce a novel hierarchical temporal Deep Belief Network (HTDBN) method for drowsy detection. Our scheme first extracts high-level facial and head feature representations and then use them to recognize drowsiness-related symptoms. Two continuous-hidden Markov models are constructed on top of the DBNs. These are used to model and capture the interactive relations between eyes, mouth and head motions. We also collect a large comprehensive dataset containing various ethnicities, genders, lighting conditions and driving scenarios in pursuit of wide variations of driver videos. Experimental results demonstrate the feasibility of the proposed HTDBN framework in detecting drowsiness based on different visual cues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent reports have suggested that drowsy driving is one of the main factors in fatal motor vehicle crashes each year <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. In 2014, the National Sleep Foundation (NSF) pledged an initiative that seeks to raise public awareness on drowsy driving and asked legislators to have law enforcement, regulations and recommendations on drowsy driving and distraction prevention <ref type="bibr" target="#b3">[4]</ref>. Therefore, developing active monitoring systems that help drivers avoid accidents in a timely manner is of utmost importance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In recent drowsy driver detection systems, most of the work focus on using limited visual cues (often just one) <ref type="bibr" target="#b6">[7]</ref>. However, human drowsiness is a complicated mechanism. If various cues are combined dynamically <ref type="bibr" target="#b7">[8]</ref>, results can be improved. Furthermore, drowsiness has an accumulative property and the decision cannot usually be made in a short period of time, i.e. drowsy status at a previous time point is a factor for the drowsy status at the current time point and the duration depends on the behavior of individuals. For example, frequent yawning is an important behavioral feature, but it does not always occur before the driver goes into a drowsy state. It should be used as a preemptive measure Fig. <ref type="figure">1</ref>. A Hierarchical Temporal Deep Belief Network (HTDBN) for drowsy driver detection: inputs from the face or head after frontalization and pairwise feature extraction are first fed to deep belief networks to extract high level features and output each motion probabilities to form an observable vector Xt at each time stamp t. The deep neural nets are first pre-trained and then fine-tuned by the target drowsiness-related class. Then Xt is regarded as observation vector ot for HMM. Two HMMs are learned on top of the deep neural nets and employed to analyze the likelihoods of the observable vector sequence within a fixed length duration. Finally, the driver's drowsiness level is predicted by inverse logit transform. and memorized until other symptoms are captured. Otherwise, the probability of drowsiness will drop as nothing is detected for some period of time.</p><p>To resolve these issues, we introduce a novel and unified Hierarchical Temporal Deep Belief Network (HTDBN) for detecting drowsiness. The overall architecture is shown in Fig. <ref type="figure">1</ref>. Through the proposed HTDBN framework, a set of high-level facial landmark features can first be extracted and used to learn representations for classification of several drowsiness-related symptoms. We demonstrate that drowsiness-related symptoms can be well classified using facial landmark points and head posture and the results are further composed to form observation vector sequences. For modeling temporal information, we resort to a continuous-Hidden Markov Model (HMM) which can be extended to long-term temporal information. We train two sets of parameters, drowsiness-HMM and non-drowsiness-HMM, covering many possible kinds of driving scenarios using the Baum-Welch algorithm. Finally, by using the forward-backward algorithm, the maximum likelihoods can be calculated and their differences are accumulated over a predetermined period of time.</p><p>Despite the importance of research in a practical drowsy driver detection system, most research have used relatively limited datasets. The generalization of different approaches to drowsy driver detection analysis remains unknown. In the absence of performance evaluation on a common public dataset, the comparative strength and weakness of different approaches is difficult to determine. In the field of facial expression and action recognition, comparative performance evaluations have proven valuable <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and similar benefits should be gained in the field of driver drowsiness detection. Therefore, we also describe in this paper a dataset that we specifically designed and collected for drowsy driver detection. This dataset will be soon made publicly available to researchers in the field. The dataset contains a wide variety of human subjects with various races, ethnicities and genders. The data was also collected at different situations like wearing glasses, sunglasses and various lighting conditions. We expect this dataset to be a representative test-bed for drowsy driver detection approaches.</p><p>In summary, we make the following contributions in this paper: <ref type="bibr" target="#b0">(1)</ref> To the best of our knowledge, we are the first to combine DBN with HMM for drowsy driver detection. <ref type="bibr" target="#b1">(2)</ref> The proposed framework captures the temporal information as well as the interactive relation among eyes, mouth and head. (3) We provide a dataset that contains drowsiness and non-drowsiness videos captured under various kinds of driving circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Drowsiness-related symptom measurement methods can be generally grouped into two categories <ref type="bibr" target="#b4">[5]</ref>: Physiological and Physical. Physiological methods offer an objective and precise way to measure sleepiness. They are based upon the fact that physiological signals start to change in earlier stages of drowsiness <ref type="bibr" target="#b10">[11]</ref>. Despite of their reliability, the intrusive nature of measuring physiological signals remains an issue that makes them unacceptable for real-world applications. Physical methods are based upon non-invasive observation of a driver's external state. A typical focus is on facial expressions that might express some characteristics, such as eyelid movement, head movement, gaze, and facial expression <ref type="bibr" target="#b11">[12]</ref>. The research in this area can be classified into four groups <ref type="bibr" target="#b12">[13]</ref>:</p><p>Threshold-Based Approach. The simplest method to predict a driver's drowsiness level is to set a threshold on extracted drowsiness-related symptoms. In the system presented in <ref type="bibr" target="#b13">[14]</ref>, the percentage of eyelid closure (PERCLOS) in a time window has shown to provide meaningful message of drowsiness. Teyeb et al. <ref type="bibr" target="#b6">[7]</ref> further showed that when the head inclination angle exceeds a certain value and duration, the level of alertness of the driver is lowered. In <ref type="bibr" target="#b14">[15]</ref>, yawning is detected based on the rate of change of the mouth contour and is determined as the only sign of drowsiness. This approach may encounter false-alarms when the required visual cues cannot be distinguished from the similar motions, e.g. talking or laughing.</p><p>Knowledge-Based Approach. In the knowledge-based approaches, decision of driver's drowsiness is made based on knowledge of an expert. In this approach, knowledge usually appears to be evaluated according to if-then rules. Rezaei and Klette <ref type="bibr" target="#b15">[16]</ref> implemented a fuzzy control fusion system to prevent road crush. In <ref type="bibr" target="#b16">[17]</ref>, a Finite State Machine (FSM) was used for hypo-vigilance detection. However, it is difficult to provide an accurate definition of driver drowsiness with some rules, since rules defined in the system do not provide sufficient expressive power to accommodate the large variations and uncertainties in driver videos.</p><p>Probability Theory Based Approach. Ji et al. <ref type="bibr" target="#b7">[8]</ref>, proposed a Dynamic Bayesian Network (DBN) system to determine the level of driver's drowsiness. In <ref type="bibr" target="#b11">[12]</ref>, they included frequent yawning, nodding, gaze distribution and eyelid movement as observation nodes and many other subjective factors such as sleep quality, sleeping time and driving environment as contextual nodes in DBN. Although DBN has the ability to represent the spatio-temporal characteristics for determining drowsiness, it also has a large computational complexity. In addition, the statistical analysis of large-scale subjective training data is difficult to obtain.</p><p>Statistical Approach. Support Vector Machine (SVM) and Neural network (NN) are the main methods in statistical pattern recognition. Jin et al. <ref type="bibr" target="#b6">[7]</ref> utilized SVM and Eskandarian and Sayed <ref type="bibr" target="#b17">[18]</ref> applied NN on distinguishing driver's drowsiness both based on the combination of driver behavioral measures and driving performance measures. Neither SVM nor NN was able to model temporal information. Thus, it is unrealistic to port such methods into real-world applications.</p><p>In summary, most of the approaches have drawbacks due to impractical reasons mentioned above or do not provide sufficient discrimination to capture the uncertainties. Moreover, most of the existing methods do not evaluate the robustness of their system against subjects from different ethnicities, races, genders, various illumination conditions and partial occlusion (e.g. glasses, sun-glasses and facial hair). In contrast, our work aims to systematically integrate spatiotemporal information using pure visual cues representing the driver's behaviors. Our algorithm is evaluated on a driver video dataset we collected while covering a wide variety of scenerios. All the knowledge needed in the detection system is learned from the training data itself without subjective and sophisticated parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed HTDBN Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>The proposed model is inspired by the framework successfully applied to the speech and gesture recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. But instead of limiting the learning ability to Restricted Boltzmann Machine (RBM), the proposed HTDBN based method utilizes the nice property of DBN <ref type="bibr" target="#b19">[20]</ref>, i.e. modeling high-order dependencies.</p><p>As shown in Fig. <ref type="figure">1</ref>, the learned DBNs are used to extract drowsiness-related symptoms after frontalization along with pairwise feature extractions. On top of the DBNs, two continuous-HMMs are adopted for modeling higher level temporal relationship among drowsiness and non-drowsiness using the observation vectors obtained from the probabilities of mouth-, eye-and head-motions. To evaluate driver's drowsiness level, the accumulated differences of both HMM maximum likelihoods collected from previous time stamps to current time stamp is passed to the inverse logit transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Frontalization</head><p>Specifically with faces, the success of the learned network in capturing facial appearance is highly dependent on a rapid 3D normalization step. Faces captured by the camera are considered unconstrained (due to the non-planarity of the face) and non-rigid expressions. Rezaei and Klette <ref type="bibr" target="#b15">[16]</ref> focused on head-pose estimation, but ignored the importance of pose normalization on facial cues. Similar to the recent literature <ref type="bibr" target="#b20">[21]</ref>, our alignment is based on using facial landmark point detectors to direct the normalization process but is simplified by discarding pixel-wise transformation. In our framework, the recent algorithm of <ref type="bibr" target="#b21">[22]</ref>, based on the supervised descent concept, is to detect and track 49 facial landmarks in a video sequence.</p><p>We start our alignment process by extracting 9 2D facial landmark points as temporary anchor points. The points are corners of both eyes and mouth, center of the brows and tip of the nose as illustrated in Fig. <ref type="figure" target="#fig_0">2a</ref>. They are used to approximate the pose matrix P given by</p><formula xml:id="formula_0">P = R 3×3 t 3×1 0 1×3 1<label>(1)</label></formula><p>by applying POSIT algorithm <ref type="bibr" target="#b22">[23]</ref> in which R is a rotation matrix, t is a translation vector. After obtaining these parameters, the reference 3D face model is then rotated and translated to align with the 2D facial image plane. Four affine transformations are respectively applied on each eye, nose and mouth regions to warp 2D anchor points to the xy-image plane of the 3D reference face (Fig. <ref type="figure" target="#fig_0">2b</ref>). This generates a 3D-aligned version of the 2D facial contour and the corresponding depth information of each facial point can be estimated <ref type="bibr" target="#b23">[24]</ref>. Since texture of the face is not considered in our framework, the fitted P can be directly used without concerning about the corruption between pixelwise warping. Finally, the frontalization is achieved by using transpose of the rotation matrix R on the 3D-aligned facial points, as illustrated in Fig. <ref type="figure" target="#fig_0">2d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pairwise Feature Extraction</head><p>Once the system is activated, the system will collect frontalized facial landmark points to form the pairwise features and then the features are passed to mouth-DBN, head-DBN or eye-DBN. The 2D coordinates of facial landmark points of current frame-c are given as:</p><formula xml:id="formula_1">F c = {f c 1 , f c 2 , . . . , f c N },</formula><p>where N is the number of points used. We deploy 2D pairwise differences of points for input in the first layer of DBN. The idea comes from the usage of joints in action recognition <ref type="bibr" target="#b24">[25]</ref>, because we believe that facial landmark points are similar to 3D joints in a way that they can both use position differences to characterize motion information; however, we introduce innovations on some expects. The pairwise differences of points capture posture features, motion features, and max-pool motion features by directed concatenation: A = [f cc , f cp , f cd ] in which:</p><formula xml:id="formula_2">f cc = {f c i -f c j |i, j = 1, 2, . . . , N; i = j} f cp = {f c i -f p i |f c i ∈ F c ; f p i ∈ F p } f cd = {f c i -f d i,w |f c i ∈ F c ; f d i,w ∈ F d ; w = 10, 20, . . . , 60}<label>(2)</label></formula><p>where f p denotes the landmark points extracted from preceding frame-p and f d denotes the landmark points in the frame-d whose sum of differences of points to current frame is maximum in a window size w. Since our system cannot discover where the initial frame of motion is, to characterize more motion information on continuous frames, we replace the offset features f ci in <ref type="bibr" target="#b24">[25]</ref> with max-pool motion features f cd . The illustration is demonstrated in Fig. <ref type="figure" target="#fig_1">3</ref>. Specifically, we consider that a motion can be captured by computing the differences of points between current frame and dozens of proceeding frames (previous 2, 3 seconds). However, the more the number of proceeding frames be used for features, the more dimension of features would be, which makes the following DBNs on drowsiness-related symptoms cost too much time on computation. Therefore, we design an innovative way just like max-pool layer in convolutional neural network <ref type="bibr" target="#b25">[26]</ref> to discard less important information in time order, i.e. to find frames which has maximum sum of differences on certain window size, and then calculate their differences of points as features.</p><p>Dimension of A results in</p><formula xml:id="formula_3">N A = (N × (N -1)/2 + N + N × 6) × 2.</formula><p>Ten facial landmarks (N = 10) at both mouth corners, the middle of uppers and lower lips are considered for yawning and talking/laughing detection. All the points are normalized by the scheme mentioned in Sect. 3.2 before extraction. As for head features, they are extracted in the same fashion as facial landmark points whilst the 2D positional points are replaced by yaw, pitch and roll angles decomposed from the rotation matrix R.</p><p>Nevertheless, as for eye features, we consider the average eye angles θ lef t , θ right at both eyes (Fig. <ref type="figure" target="#fig_0">2e</ref>) in previous 10 seconds as features, because eyes have small variation that only can be identified to 3 states, opened, halfclosed and closed eyes, which may make extracting eye pairwise features be less meaningful as mouth or head motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning the Higher Level Representation</head><p>RBMs were originally developed using binary stochastic units for both the visible and hidden units, but logistic units are a very poor representation for real-world data such as pixel intensities in natural images <ref type="bibr">[27]</ref>. One solution is to replace the binary visible units by linear units with independent Gaussian noise <ref type="bibr" target="#b27">[28]</ref>, so called Gaussian RBM (GRBM). The original energy function can be replaced by:</p><formula xml:id="formula_4">E(v, h) = i∈vis (v i -a i ) 2 2σ 2 i - j∈hid b j h j - i,j v i σ i h j w ij (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where w ij denotes the matrix of connection between visible unit i and hidden unit j with their bias terms a i and b j and σ i is the standard deviation of the Gaussian noise for visible unit i. Because our pairwise features are continuous features, we use the above GRBM in the same fashion of <ref type="bibr" target="#b9">[10]</ref> to model the energy term of the first visible layer. It is possible to learn the variance of the noise for each visible unit but it is much easier to normalize the data (mean subtraction and standard deviation division) to have zero mean and unit variance (e.g. σ 2 i = 1) in the preprocessing phase.</p><p>Specifically, the DBN in our system consists of one visual layer (i.e. the lowest layer in GRBM) with continuous pairwise features and four hidden layers to learn a hierarchical feature representation given training data extracted from mass pool of facial landmark points and head posture data. Given the nodes at the third hidden layer and the classification labels (i.e. stillness, yawning or laughing/talking; stillness, nodding or looking aside; normal eye or sleepy eye), the output of the DBN (i.e. last hidden layer) can classify by comparing the values of these nodes. The DBNs in HTDBN are activated at all times to detect mouth-motion, head-motion and eye-motion. For mouth-DBN, the model aims to discriminate yawning from mouth stillness and laughing/talking, and for head-DBN, it is used to classified head stillness, nodding and head looking aside. In addition, eye-DBN can differentiate sleepy eye from normal eye.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Continuous-HMM for Drowsy Driver Detection</head><p>A hidden Markov model describes the statistical behavior of a process in time. At each time step t, we have one 8-dimensional feature vector X t taking values of each motion class probabilities obtained from DBNs, i.e. X t = {x eye , x mouth , x head }, where x eye = {p normal , p sleepy }, x mouth = {p stillness , p yawning , p laughing/talking }, x head = {p stillness , p nodding , p looking-aside }. The intuition behind this is that a motion in drowsiness-related symptoms consists of a sequence of state transition, e.g. yawning is a long-term motion of mouth that has its fullest open state in between stillness at onset and offset. Therefore, the variation of motion probabilities in a sequence of feature vectors can describe a motion or a transition from different motions.</p><p>Here, X t not only serves as an observation vector O t for HMM but also describes the relation between eyes, mouth and head. Existing work tend to neglect such relations when detecting drowsiness-related symptoms, e.g. eyes might be closed when yawning, but only mouth features are considered.</p><p>Assume a HMM has J (unobserved) states {s 1 , s 2 , . . . , s J } and K observation vectors {o 1 , o 2 , . . . , o K }. At time t, HMM occupies a state s i and may undergo a state transition from the s i = i to a state s i+1 = j at time t + 1 with the state transition probability a ij = P (s t+1 = j|s t = i). Associated with each state is a set of observation vectors o t with their respective observation probability densities, Gaussian M-component mixture densities,</p><formula xml:id="formula_6">b i (o) = M k=1 c ik N [o, μ ik , U ik ],</formula><p>where c jk is the mixture weight, N is the normal density and μ ik and U ik are the mean vector and co-variance matrix associated with state i, mixture k. Starting from an initial state s 1 = i with probability π = P (s 1 = i), the process undergoes a sequence of state transitions over a time duration T and generates an observation vector sequence O = {o 1 , o 2 , . . . , o T } with a certain probability. In a sense, the HMM is specified by a triplet λ = (Π, A, B), where A = {a ij } denotes transition probabilities, B = {b j (o)} denotes observation symbol probabilities, and Π = {π i }.</p><p>For each HMM, the model parameters can be trained from the selected training vector sequences by applying Baum-Welch algorithm <ref type="bibr" target="#b28">[29]</ref>. Here we assume our HHM to be an ergodic model. A model is ergodic means its transition matrix is fully connected, which means all transitions have non-zero probabilities.</p><p>With the above definition for a model λ, a given observation symbol sequence O may be generated from one or more state sequences S = {s 1 , s 2 , . . . , s T }. The probability requires summation over all possible state sequences and an efficient way to do so is a forward-backward algorithm <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_7">P (O|λ) = allS P (O|S, λ)P (S|λ) = allS π s1 b s1 (o 1 ) T t=2 a st-1st b st (o t ). (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>Given a video sequence with length T , we obtain the likelihood difference d = P(O|λ drowsy )-P(O|λ nondrowsy ) from every 300 frames. We then accumulate the likelihood differences within a specific among of time τ to obtain dt . Finally, the drowsiness level (DL) is determined using the inverse logit transform (Eq. 5). While drowsiness level is more than 50%, then the drowsiness detector would consider the current state as drowsiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DL( dt ) = 1</head><p>1 + e -dt * 100% (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Acquisition</head><p>Most of the previous works on drowsy driver detection attempted to recognize a small set of cases for driver drowsiness detection. Although <ref type="bibr" target="#b30">[31]</ref> provided a freely-available dataset for yawning detection, it is still insufficient for a comprehensive drowsy driver study based on pure visual cues. Drowsiness detection from yawning alone is too restricted for use in practice. It should be combined with some additional indicators of drowsiness. Therefore, we collect a large video dataset for performance evaluation of drowsy driver detection methods.</p><p>Camera Setting. To cope with the night time or poor lighting problem, we used the active infrared (IR) illumination and acquire IR videos in the dataset collection. To ensure a realistic setup, all the videos were captured by D-Link DCS-932L, a stand-alone surveillance digital camera with the resolution set at 640 × 480 pixels. The built-in infrared LEDs allow us to view in any light condition from daytime to nighttime. The advantage of activating IR illuminators also in daytime is because it captures occluded eyes with people wearing sunglasses better than using RBG camera (Fig. <ref type="figure" target="#fig_3">4b</ref>). However, for the sake of completeness, 24-bit true color (RGB) Logitech C310 HD at 30 frames per second webcam was also set up simultaneously to record the data at 720p in daytime only (Fig. <ref type="figure" target="#fig_3">4c</ref>).</p><p>Environment Setting. In the collection of our dataset, two rounds of video recordings were performed for each subject. The first round was recorded during the daytime and the second one was performed in the night. In order to  simulate the condition of sunny day, for approximately one third of subjects, ambient room lighting augmented by a high-intensity lamp was used (Fig. <ref type="figure" target="#fig_3">4a</ref>) for the daytime recording. Illuminance was measured by a light meter to ensure environment was well-established.</p><p>Our camera was placed on the top left hand side of the subject to emulate the position in the A-Pillar, a common used location in cars. In contrast, most datasets and the corresponding algorithms were based upon fully frontal face views, which is impractical to set up in a real cars since the camera would block the driver's view and the dashboard.</p><p>Participants. To make sure the algorithm works for various skin race and genders, 36 adults, aged from 18 to 40 years old with various ethnicities and diverse skin colors (32.5% of black or brown, 32.5% of white, and 35% of yellow) and genders (50% of female), participated in the video collection. The subjects with different hairstyles and clothing were recorded with and without glasses/sunglasses to simulate a wide variety of driving scenarios. Figure <ref type="figure" target="#fig_3">4c</ref> and<ref type="figure">d</ref> show some samples of the participants under different conditions and driving scenarios. Driver Videos. were recorded when they sit on a chair and play a plain driving game with simulated driving wheel and pedals; meanwhile, they were instructed by an experimenter to perform a series of 8 actions under 5 kinds of scenarios: BareFace, Glasses, Sunglasses, Night-BareFace and Night-Glasses. The sequences recorded from each subject can be regarded as two branches: drowsiness and non-drowsiness. For drowsiness-related sequences, yawning, slow blink rate (high PERCLOS) and falling asleep (high PERCLOS followed by frequent nodding) were taken about 1 min long, and the combination of drowsinessrelated symptoms sequences (yawning, high PERCLOS, frequent nodding) were recorded about 1.5 min. On the other hand, sequences of normal driving (low PERCLOS), shocked face and burst out laughing in about 1 min and the combination of non-drowsiness actions (talking, laughing, looking at both sides) recorded about 1.5 min are represented as the non-drowsiness data. Some examples can be seen in Fig. <ref type="figure" target="#fig_3">4d</ref>. Overall, 360 videos were taken to complete the dataset.</p><p>Moreover, to simulate more practical driving situations, 18 subjects from the proposed dataset are randomly selected yet kept the balance of various gender, skin races. Their sequences are edited and combined into a 2-10 min mixing video for each subjects under 5 kinds of scenarios which contains various situations with different number of transitions from non-drowsiness state to drowsiness state, or drowsiness state to non-drowsiness states. Overall, there are 90 mixing videos be added to the dataset for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate the proposed HTDBN framework by using the provided dataset mentioned in Sect. 4. We first train the DBNs with a four-hidden-layer structure, where the numbers of the nodes in all the layers are [N A , 1000, 1000, 500, N y ] from the lowest layer to the highest one, respectively. The numbers of nodes in the visible layer are N A = 230, 45, and 200 for mouth-, head-, and eye-DBN, respectively. The number of outputs N y for each DBNs is equivalent to the number of classes, i.e. there are 3 outputs for mouth-and head-DBN, 2 outputs for eye-DBN.</p><p>In our experiments, the dataset is divided into two parts: training, testing dataset. The subjects that have edited mixing videos are for testing, and the sequences from the other subjects are for training. In drowsiness-related symptoms detection, all the sequences from each subject in the training dataset are taken to train DBNs. For each of the sequence, the mouth-motion, head-motion, and eye-motion class probabilities are extracted at each frame and the observable vectors can be obtained. In drowsy driver detection, to maintain accurate classification capability, the selection of training observable vectors sequences is very important. This is since the chosen sequences can be used to adjust the model parameters that can also be used to recognize other sequences of observable symbols, e.g. a drowsy observable vector sequence should get higher probability of re-generation from drowsiness HMM than non-drowsiness HMM. The we captured for each person consists of long videos. To perform our training, two kinds of videos, the combination of drowsiness-related symptoms videos and the combination of non-drowsiness-related actions videos, are used for training data. We randomly subdivide each of the videos into various shorter overlapping and non-overlapping videos with fixed length 300 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Drowsy Driver Detection Performance Evaluation</head><p>We present the performance of the proposed HTDBN for driver's drowsiness detection using the collected dataset. We use not only accuracy but also F 1score to evaluate the performance of the proposed detection algorithm since it is a relatively fair measure for unbalanced data:</p><formula xml:id="formula_9">F 1 -score = 2 * prec(Δ) * rec(Δ) prec(Δ) + rec(Δ) (6)</formula><p>Δ is the length of the sequence of its likelihoods decoded from drowsiness-HMM and non-drowsiness-HMM. To ensure the stability and to fully scrutinize the strength of the proposed framework, we conduct an experiment to show the performance of the fine-tuned system on BareFace, Glasses, Sunglasses, Night-BareFace and Night-Glasses separate scenarios. The average scores of different scenarios for drowsiness and non-drowsiness detection are shown in Table <ref type="table" target="#tab_0">1</ref>. As shown in the table, the first three scenarios were evaluated in daytime. It can be seen that for the cases with no occlusion on face (BareFace) and enough ambient lighting, we can achieve about 92% accuracies on both drowsiness and non-drowsiness detections. As the detector encounters driver-wearingglasses scenario, sometimes the reflections in glasses and the glasses frames may cause disturbance of eye openness detection and thus lose a good information from eye-motion detection.</p><p>In addition, the performance of Sunglasses falls behind very much with other scenarios, although eyes occluded by sunglasses have better visibility using IR camera compared with RGB camera, the stronger reflections and hindrance deteriorate on eye-contour-point detection. Therefore, to deal with the big challenge on driver-wearing-sunglasses scenario, firstly we design a sunglasses detector to find the region of sunglasses by the image intensity from detected eye points area, and then the sunglasses region is employed gamma correction (γ = 0.4) to adjust brightness and simple reflection removal on the reflection that would block eye contours. After these improvements, the result of Sunglasses is increased 9% which is about 77% accuracy.</p><p>The rest two scenarios were experimented in nighttime. The IR videos collected from the chosen camera can still capture clear faces, but facial landmark points are less accurate than in daytime resulting in lower accuracies. The accuracy of the overall scenarios is about 85%, and so do the scores for both drowsiness and non-drowsiness detection. Figure <ref type="figure">5a</ref> depicts an example of processing a 60-second video of an alert driver gradually falling asleep by using the proposed algorithm. Orange, green and yellow curves represent the predictions of mouth-and head-and eye-motions from DBNs, respectively, co-existing at every time stamp. As shown, when mouth-DBN first recognizes a yawning cycle the level of drowsiness goes up drastically to about 70%. As the blink rate goes slower and eye closure duration goes longer, the drowsiness percentage keeps growing up to the top 100% until the head-DBN detects a nodding cycle, a high-risk warning should be raised. In contrast, a drowsy driver waking up case illustrated in Fig. <ref type="figure">5b</ref> shows the sustainability of the proposed system. While no sleepy eye is noticed, the level of drowsiness decrease dramatically. Mouth-and head-DBN detect several laughing/talking motions and head looking aside motions, thus the system determines the drowsiness level of the driver to be below 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Other Solutions</head><p>In this experiment, our proposed HTDBN model is compared with baseline model: support vector machine (SVM) on mixed scenarios in the Drowsy Driver Dataset. Before deep learning appears, support vector machine was the most popular technique for data classification, especially the SVM with kernel trick that can efficiently perform a non-linear classification. For a fair comparison, every solution is trained and tested same as the proposed HTDBN solution, yet the only difference is that the determination of the drowsiness state. As for SVM, the probability of drowsiness determines the final drowsiness state, while for HMMs, it is decided by the difference between drowsiness-HMM and nondrowsiness-HMM. Table <ref type="table" target="#tab_1">2</ref> shows the effectiveness of applying different models (SVM or DBN, HMM) on the proposed framework. The accuracy is apparently lower when SVMs (SVM+SVM solution) are substituted for all detectors in the proposed algorithm. To further discuss the separate effectiveness in different part of the framework, the SVM+HMM solution only replaces DBNs with SVMs in drowsiness-related symptoms, and only HMMs are replaced with a binary-class SVM in drowsy driver detection for DBN+SVM solution. From the results, DBNs play important roles in the overall system which improves 5% accuracy, whilst the usage of HMMs increases 3%.</p><p>In conclusion, our proposed HTDBN algorithm is more suitable for classification in time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Yawning Detection Performance on YawDD Dataset</head><p>Although Yawning Detection Dataset (YawDD) <ref type="bibr" target="#b30">[31]</ref> is not a sufficient dataset for comprehensive drowsy driver detection because it is only determined by yawning detection. However, to compare with existing approaches, the yawning detector mouth-DBN in our proposed system has been evaluated on the YawDD dataset as well. According to the evaluation scheme from <ref type="bibr" target="#b31">[32]</ref>, we got 94% yawning detection accuracy with 2% false alarm rate on CASE I (camera under the mirror), and 92% accuracy with 5% false alarm rate on CASE II (camera on the dash). Our results outperform recent methods presented in <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>, whose accuracies on the better case are 60%, 75%, and 92% but 13% false alarm rate, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computational Complexity</head><p>Our driver drowsiness detection system consists of an off-line training phase and on-line detection phase. Though the learning in the network is uninterestingly long, once the model training is finished, with low inference cost, the entire system is able to perform in real-time with MATLAB implementation using a Core i5, 3.1GHz PC with 16GB RAM, at an average speed of 20 fps. More precisely, a single multi-layer feedforward neural networks incurs in linear running time O(T ) and the forward-backward algorithm applied in HMM has time complexity O(N 2 T ), where T is the length of the sequence and N is the number of states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a novel HTDBN that utilizes DBNs for learning contextual frame-level representations for drowsiness-related symptoms. By encoding dynamic structure of drowsiness and non-drowsiness information into HMMbased models, the results are robust and promising under different circumstances. The proposed continuous-HMM can model the interactive relations among eyes, mouth and head. Moreover, for performance evaluation, we collected a large drowsy driver detection dataset in which various skin colors, scenarios and lighting conditions are considered. Experimental results on various kinds of scenarios and fusion all together demonstrated the power of the proposed framework in estimating the driver's drowsiness level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The detected face with 9 initial facial landmark anchor points. (b) Four affine transformations are respectively applied on each eye, nose and mouth to warp 2D anchors points to the xy-image plane of the 3D reference model. Red dots are the original points on the 2D detected face, yellow dots are the results after warping. (c) Frontalization of the 3D shape model along with the aligned points. (d) Project the 3D frontalized points to 2D image plane. (e) The points used and the equation to calculate the average eye angle θ for feature extraction. (Color figure online)</figDesc><graphic coords="5,60.96,433.82,330.55,74.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Raw feature formation: three feature channels fcc, fcp, f cd for capturing information of posture, motion, max-pool motion.</figDesc><graphic coords="7,58.98,54.62,334.69,154.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Samples of participants. (d) Different and performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Participants were equipped with a fixed but tunable chair and a simulated driving wheel with pedals; they are instructed to perform a series of facial displays shown in Fig. 4d. A standalone IR camera and RBG webcam were placed at the left hand side of the driver while the ambient light was augmented with high-intensity lamp to simulate the condition of sunny day. (b) IR images can capture better occluded eyes for subjects wearing sunglasses. (c) Diversity of skin colors, genders and ethnicities among the participants in the dataset collection. IR and RGB videos were taken simultaneously. (d) Situations like yawning in BareFace, nodding when falling asleep in Night-BareFace, laughing in Sunglasses, looking aside in Night-Glasses, blink slowly in BareFace and etc. are considered and separately recorded.</figDesc><graphic coords="10,111.33,163.07,270.73,56.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.</head><label></label><figDesc>Fig. Plots of the processed data aligned with level of drowsiness versus time: (a) Non-drowsy (alert) driver falling asleep example and (b) drowsy driver awake example. Blue curve represents level of drowsiness; below are orange, green and yellow curves represent predictions of mouth-and head-and eye-motions from DBNs, respectively. (Color figure online)</figDesc><graphic coords="13,55.98,54.59,340.18,98.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The 1 st experiment: performance of HTDBN on separate scenarios in the drowsy driver detection dataset</figDesc><table><row><cell>Scenario</cell><cell cols="3">Drowsiness F1-score Non-drowsiness F1-score Accuracy</cell></row><row><cell>BareFace</cell><cell>92.17%</cell><cell>92.64%</cell><cell>92.42%</cell></row><row><cell>Glasses</cell><cell>88.17%</cell><cell>85.04%</cell><cell>86.79%</cell></row><row><cell>Sunglasses</cell><cell>74.17%</cell><cell>78.59%</cell><cell>76.58%</cell></row><row><cell cols="2">Night-BareFace 92.60%</cell><cell>90.97%</cell><cell>91.87%</cell></row><row><cell>Night-Glasses</cell><cell>77.74%</cell><cell>73.73%</cell><cell>75.90%</cell></row><row><cell>Overall</cell><cell>85.39%</cell><cell>84.19%</cell><cell>84.82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The 2 nd experiment: comparison between HTDBN and baseline solution on mixed scenarios in the drowsy driver detection dataset</figDesc><table><row><cell></cell><cell cols="3">Drowsiness Non-drowsiness Accuracy</cell></row><row><cell cols="2">SVM+SVM 81.16%</cell><cell>74.99%</cell><cell>78.51%</cell></row><row><cell cols="2">SVM+HMM 81.30%</cell><cell>77.14%</cell><cell>79.43%</cell></row><row><cell cols="2">DBN+SVM 84.26%</cell><cell>79.20%</cell><cell>82.08%</cell></row><row><cell>Ours</cell><cell>85.39%</cell><cell>84.19%</cell><cell>84.82%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The authors would like to thank Qualcomm Technologies Inc. for supporting this research work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time system for monitoring driver vigilance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nuevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">World Health Organization: Global status report on road safety 2013: supporting a decade of action: summary</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>World Health Organization</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drowsy driving and risk behaviors 10 states and Puerto Rico</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wheaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shults</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online article</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">National Sleep Foundation: Drowsy driving reduction act of</title>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Driver Drowsiness Detection: Systems and Solutions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention assist: drowsiness-detection system warns drivers to prevent them falling asleep momentarily</title>
		<author>
			<persName><forename type="first">Mercedes-Benz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online article</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A drowsy driver detection system based on a new method of head posture estimation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Teyeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jemai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ben Amar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10840-7_44</idno>
	</analytic>
	<monogr>
		<title level="m">IDEAL 2014</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Corchado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Quintián</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8669</biblScope>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A probabilistic framework for modeling and realtime monitoring human fatigue</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Looney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part A: Syst. Hum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="862" to="875" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The extended Cohn-Kanade dataset (CK+): a complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Cohn</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A driver fatigue recognition model based on information fusion and dynamic Bayesian network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="1942" to="1954" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time nonintrusive monitoring and prediction of driver fatigue</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1052" to="1068" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A vision-based system for monitoring the loss of attention in automotive drivers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1825" to="1838" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Drivers fatigue detection based on yawning extraction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alioua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rziza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Veh. Technol</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Look at the driver, look at the road: no distraction! No accident</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining driver visual attention with one camera</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da Vitoria Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="205" to="218" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis of driver impairment, fatigue, and drowsiness and an unobtrusive vehicle-based detection scheme</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eskandarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of International Conference on Traffic Accidents</title>
		<meeting>eeding of International Conference on Traffic Accidents</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepface: closing the gap to humanlevel performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervised descent method and its application to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model-based object pose in 25 lines of code</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gender and ethnicity specific generic elastic models from a single 2D image for novel 2D pose face synthesis and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2341" to="2350" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eigenjoints-based action recognition using naive-bayes-nearestneighbor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_32</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7700</biblScope>
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
	<note>2nd edn</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised learning of distributions on binary vectors using two layer networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Santa Cruz, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California at Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Application of hidden Markov models for signature verification. Pattern Recogn</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Baum&apos;s forward-backward algorithm revisited</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="369" to="373" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">YawDD: a yawning detection dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omidyeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirmohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Multimedia Systems Conference</title>
		<meeting>the 5th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yawning detection using embedded smart cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omidyeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirmohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khurshid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scharcanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="570" to="582" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Driver yawning detection based on deep convolutional neural learning and robust nose tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Murphey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
