<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-term time series prediction with the NARX network: An empirical evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-07-15">15 July 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jose</forename><surname>´maria</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Teleinformatics Engineering</orgName>
								<orgName type="department" key="dep2">Center of Technology</orgName>
								<orgName type="institution">Federal University of Ceara ´</orgName>
								<address>
									<addrLine>Campus of Pici, Av. Mister Hull</addrLine>
									<postCode>6005, 60455-760</postCode>
									<settlement>Fortaleza</settlement>
									<region>S/N, CP, CEP, CE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Menezes</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Teleinformatics Engineering</orgName>
								<orgName type="department" key="dep2">Center of Technology</orgName>
								<orgName type="institution">Federal University of Ceara ´</orgName>
								<address>
									<addrLine>Campus of Pici, Av. Mister Hull</addrLine>
									<postCode>6005, 60455-760</postCode>
									<settlement>Fortaleza</settlement>
									<region>S/N, CP, CEP, CE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guilherme</forename><forename type="middle">A</forename><surname>Barreto</surname></persName>
							<email>guilherme@deti.ufc.br</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Teleinformatics Engineering</orgName>
								<orgName type="department" key="dep2">Center of Technology</orgName>
								<orgName type="institution">Federal University of Ceara ´</orgName>
								<address>
									<addrLine>Campus of Pici, Av. Mister Hull</addrLine>
									<postCode>6005, 60455-760</postCode>
									<settlement>Fortaleza</settlement>
									<region>S/N, CP, CEP, CE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-term time series prediction with the NARX network: An empirical evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-07-15">15 July 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">9EADCAB776EEB044851A096E39AFF3C8</idno>
					<idno type="DOI">10.1016/j.neucom.2008.01.030</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>NARX neural network Long-term prediction Nonlinear traffic modeling Chaotic time series Recurrence plot</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The NARX network is a dynamical neural architecture commonly used for input-output modeling of nonlinear dynamical systems. When applied to time series prediction, the NARX network is designed as a feedforward time delay neural network (TDNN), i.e., without the feedback loop of delayed outputs, reducing substantially its predictive performance. In this paper, we show that the original architecture of the NARX network can be easily and efficiently applied to long-term (multi-step-ahead) prediction of univariate time series. We evaluate the proposed approach using two real-world data sets, namely the well-known chaotic laser time series and a variable bit rate (VBR) video traffic time series. All the results show that the proposed approach consistently outperforms standard neural network based predictors, such as the TDNN and Elman architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Artificial neural networks (ANNs) have been successfully applied to a number of time series prediction and modeling tasks, including financial time series prediction <ref type="bibr" target="#b11">[12]</ref>, river flow forecasting <ref type="bibr" target="#b2">[3]</ref>, biomedical time series modeling <ref type="bibr" target="#b10">[11]</ref>, communication network traffic prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>, chaotic time series prediction <ref type="bibr" target="#b41">[42]</ref>, among several others (see <ref type="bibr" target="#b33">[34]</ref>, for a recent survey). In particular, when the time series is noisy and the underlying dynamical system is nonlinear, ANN models frequently outperform standard linear techniques, such as the well-known Box-Jenkins models <ref type="bibr" target="#b6">[7]</ref>. In such cases, the inherent nonlinearity of ANN models and a higher robustness to noise seem to explain their better prediction performance.</p><p>In one-step-ahead prediction tasks, ANN models are required to estimate the next sample value of a time series, without feeding back it to the model's input regressor. In other words, the input regressor contains only actual sample points of the time series. If the user is interested in a longer prediction horizon, a procedure known as multi-step-ahead or long-term prediction, the model's output should be fed back to the input regressor for a fixed but finite number of time steps <ref type="bibr" target="#b38">[39]</ref>. In this case, the components of the input regressor, previously composed of actual sample points of the time series, are gradually replaced by previously predicted values.</p><p>If the prediction horizon tends to infinity, from some time in the future the input regressor will start to be composed only of estimated values of the time series. In this case, the multi-step-ahead prediction task becomes a dynamic modeling task, in which the ANN model acts as an autonomous system, trying to recursively emulate the dynamic behavior of the system that generated the nonlinear time series <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Multi-step-ahead prediction and dynamic modeling are much more complex to deal with than one-step-ahead prediction, and it is believed that these are complex tasks in which ANN models play an important role, in particular recurrent neural architectures <ref type="bibr" target="#b35">[36]</ref>.</p><p>Simple recurrent networks (SRNs) comprise a class of recurrent neural models that are essentially feedforward in the signal-flow structure, but also contain a small number of local and/or global feedback loops in their architectures. Even though feedforward multilayer perceptron (MLP)-like networks can be easily adapted to process time series through an input tapped-delay line, giving rise to the well-known time delay neural network (TDNN) <ref type="bibr" target="#b35">[36]</ref>, they can also be easily converted to SRNs by feeding back the neuronal outputs of the hidden or output layers, giving rise to Elman and Jordan networks, respectively <ref type="bibr" target="#b22">[23]</ref>. It is worth pointing out that, when applied to long-term prediction, a feedforward TDNN model will eventually behave as a kind of SRN architecture, since a global loop is needed to feed back the current estimated value into the input regressor.</p><p>The aforementioned recurrent architectures are usually trained by means of temporal gradient-based variants of the backpropagation algorithm <ref type="bibr" target="#b34">[35]</ref>. However, learning to perform tasks in which the temporal dependencies present in the input-output signals span long time intervals can be quite difficult using gradient-based learning algorithms <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, the authors report that learning such long-term temporal dependencies with gradient-descent techniques is more effective in a class of SRN model called Nonlinear Autoregressive with eXogenous input (NARX) <ref type="bibr" target="#b27">[28]</ref> than in simple MLP-based recurrent models. This occurs in part because the NARX model's input vector is cleverly built through two tapped-delay lines: one sliding over the input signal together and the other sliding over the network's output.</p><p>Despite the aforementioned advantages of the NARX network, its feasibility as a nonlinear tool for univariate time series modeling and prediction has not been fully explored yet. For example, in <ref type="bibr" target="#b28">[29]</ref>, the NARX model is indeed reduced to the TDNN model in order to be applied to time series prediction. Bearing this under-utilization of the NARX network in mind, we propose a simple strategy based on Taken's embedding theorem that allows the original architecture of the NARX network to be easily and efficiently applied to long-term prediction of univariate nonlinear time series.</p><p>Potential fields of application of our approach are communication network traffic characterization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45]</ref> and chaotic time series prediction <ref type="bibr" target="#b21">[22]</ref>, since it has been shown that these kinds of data present long-range dependence (LRD) due to their self-similar nature. Thus, for the sake of illustration, we evaluate the proposed approach using two real-world data sets obtained from these domains, namely the well known chaotic laser time series and a variable bit rate (VBR) video traffic time series.</p><p>The remainder of the paper is organized as follows. In Section 2, we describe the NARX network model and its main characteristics. In Section 3 we introduce the basics of the nonlinear time series prediction problem and present our approach. The simulations and discussion of results are presented in Section 4. The paper is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The NARX network</head><p>NARX <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> is an important class of discrete-time nonlinear systems that can be mathematically represented as</p><formula xml:id="formula_0">yðn þ 1Þ ¼ f ½yðnÞ; . . . ; yðn À d y þ 1Þ; uðn À kÞ; uðn À k þ 1Þ; . . . ; uðn À d u À k þ 1Þ,<label>(1)</label></formula><p>where uðnÞ 2 R and yðnÞ 2 R denote, respectively, the input and output of the model at discrete time step n, while d u X1 and d y X1, d u pd y , are the input-memory and output-memory orders, respectively. The parameter k (kX0) is a delay term, known as the process dead-time.</p><p>Without lack of generality, we always assume k ¼ 0 in this paper, thus obtaining the following NARX model:</p><formula xml:id="formula_1">yðn þ 1Þ ¼ f ½yðnÞ; . . . ; yðn À d y þ 1Þ; uðnÞ; uðn À 1Þ; . . . ; uðn À d u þ 1Þ,<label>(2)</label></formula><p>which may be written in vector form as</p><formula xml:id="formula_2">yðn þ 1Þ ¼ f ½yðnÞ; uðnÞ,<label>(3)</label></formula><p>where the vectors yðnÞ and uðnÞ denote the output and input regressors, respectively. The nonlinear mapping f ðÁÞ is generally unknown and can be approximated, for example, by a standard MLP network. The resulting connectionist architecture is then called a NARX network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, a powerful class of dynamical models which has been shown to be computationally equivalent to Turing machines <ref type="bibr" target="#b37">[38]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows the topology of a two-hidden-layer NARX network.</p><p>In what concern training the NARX network, it can be carried out in one out of two modes:</p><p>Series-parallel (SP) mode. In this case, the output's regressor is formed only by actual values of the system's output: ŷðn þ 1Þ ¼ f ½y sp ðnÞ; uðnÞ, ¼ f ½yðnÞ; . . . ; yðn À d y þ 1Þ; uðnÞ; uðn À 1Þ; . . .</p><formula xml:id="formula_3">; uðn À d u þ 1Þ,<label>(4)</label></formula><p>where the hat symbol (^) is used to denote estimated values (or functions).</p><p>Parallel (P) mode. In this case, estimated outputs are fed back and included in the output's regressor 1 :</p><p>ŷðn þ 1Þ ¼ f ½y p ðnÞ; uðnÞ, ¼ f ½ŷðnÞ; . . . ; ŷðn À d y þ 1Þ; uðnÞ; uðn À 1Þ; . . .</p><formula xml:id="formula_4">; uðn À d u þ 1Þ. (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>As a tool for nonlinear system identification, the NARX network has been successfully applied to a number of real-world input-output modeling problems, such as heat exchangers, waste water treatment plants, catalytic reforming systems in a petroleum refinery and nonlinear time series prediction (see <ref type="bibr" target="#b28">[29]</ref> and references therein).</p><p>As mentioned in the Introduction, the particular topic of this paper is the issue of nonlinear univariate time series prediction with the NARX network. In this type of application, the outputmemory order is usually set d y ¼ 0, thus reducing the NARX network to the TDNN architecture <ref type="bibr" target="#b28">[29]</ref>, i.e., yðn þ 1Þ ¼ f ½uðnÞ, ¼ f ½uðnÞ; uðn À 1Þ; . . .</p><formula xml:id="formula_6">; uðn À d u þ 1Þ,<label>(6)</label></formula><p>where uðnÞ 2 R du is the input regressor. This simplified formulation of the NARX network eliminates a considerable portion of its representational capabilities as a dynamic network; that is, all the dynamic information that could be learned from the past memories of the output (feedback) path is discarded. For many practical applications, however, such as self-similar traffic modeling <ref type="bibr" target="#b15">[16]</ref>, the network must be able to robustly store information for a long period of time in the presence of noise. In gradient-based training algorithms, the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large. This effect is called the problem of vanishing gradient and has been pointed out as the main cause for the poor performance of standard dynamical ANN models when dealing with LRDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>The original formulation of the NARX network does not circumvent the problem of vanishing gradient, but it has been demonstrated that it often performs much better than standard dynamical ANNs in such a class of problems, achieving much faster convergence and better generalization performance <ref type="bibr" target="#b27">[28]</ref>. As pointed out in <ref type="bibr" target="#b26">[27]</ref>, an intuitive explanation for this improvement in performance is that the output memories of a NARX neural network are represented as jump-ahead connections in the timeunfolded network that is often encountered in learning algorithms such as the backpropagation through time (BPTT). Such jumpahead connections provide shorter paths for propagating gradient information, reducing the sensitivity of the network to long-term dependencies.</p><p>Hence, if the output memory is discarded, as shown in Eq. ( <ref type="formula" target="#formula_6">6</ref>), performance improvement may no longer be observed. Bearing this in mind as a motivation, we propose a simple strategy to allow the computational resources of the NARX network to be fully explored in nonlinear time series prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Nonlinear time series prediction with NARX network</head><p>In this section we provide a short introduction of the theory of embedding and state-space reconstruction. Interested readers are referred to <ref type="bibr" target="#b0">[1]</ref> for further details.</p><p>The state of a deterministic dynamical system is the information necessary to determine the evolution of the system in time. In discrete time, this evolution can be described by the following system of difference equations:</p><formula xml:id="formula_7">xðn þ 1Þ ¼ F½xðnÞ,<label>(7)</label></formula><p>where xðnÞ 2 R d is the state of the system at time step n, and F½Á is a nonlinear vector valued function. A time series is a time-ordered set of measures fxðnÞg; n ¼ 1; . . . ; N, of a scalar quantity observed at the output of the system. This observable quantity is defined in terms of the state xðnÞ of the underlying system as follows:</p><formula xml:id="formula_8">xðnÞ ¼ h½xðnÞ þ eðtÞ,<label>(8)</label></formula><p>where hðÁÞ is a nonlinear scalar-valued function, e is a random variable which accounts for modeling uncertainties and/or measurement noise. It is commonly assumed that eðtÞ is drawn from a Gaussian white noise process. It can be inferred immediately from Eq. ( <ref type="formula" target="#formula_8">8</ref>) that the observations fxðnÞg can be seen as a projection of the multivariate state space of the system onto the one-dimensional space. Eqs. ( <ref type="formula" target="#formula_7">7</ref>) and ( <ref type="formula" target="#formula_8">8</ref>) describe together the state-space behavior of the dynamical system. In order to perform prediction, one needs to reconstruct (estimate) as well as possible the state space of the system using the information provided by fxðnÞg only. In <ref type="bibr" target="#b39">[40]</ref>, Takens has shown that, under very general conditions, the state of a deterministic dynamic system can be accurately reconstructed by a time window of finite length sliding over the observed time series as follows:</p><p>x 1 ðnÞ9½xðnÞ; xðn À tÞ; . . . ; xðn À ðd E À 1ÞtÞ, <ref type="bibr" target="#b8">(9)</ref> where xðnÞ is the sample value of the time series at time n, d E is the embedding dimension and t is the embedding delay. Eq. ( <ref type="formula">9</ref>) implements the delay embedding theorem <ref type="bibr" target="#b21">[22]</ref>. According to this theorem, a collection of time-lagged values in a d E -dimensional vector space should provide sufficient information to reconstruct the states of an observable dynamical system. By doing this, we are indeed trying to unfold the projection back to a multivariate state space whose topological properties are equivalent to those of the state space that actually generated the observable time series, provided the embedding dimension d E is large enough.</p><p>The embedding theorem also provides a theoretical framework for nonlinear time series prediction, where the predictive relationship between the current state x 1 ðtÞ and the next value of the time series is given by the following equation:</p><formula xml:id="formula_9">xðn þ 1Þ ¼ g½x 1 ðnÞ. (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>Once the embedding dimension d E and delay t are chosen, one remaining task is to approximate the mapping function gðÁÞ. It has been shown that a feedforward neural network with enough neurons is capable of approximating any nonlinear function to an arbitrary degree of accuracy. Thus, it can provide a good approximation to the function gðÁÞ by implementing the following mapping:</p><formula xml:id="formula_11">xðn þ 1Þ ¼ ĝ½x 1 ðnÞ,<label>(11)</label></formula><p>where xðn þ 1Þ is an estimate of xðn þ 1Þ and ĝðÁÞ is the corresponding approximation of gðÁÞ. The estimation error,</p><formula xml:id="formula_12">eðn þ 1Þ ¼ xðn þ 1Þ À xðn þ 1Þ</formula><p>, is commonly used to evaluate the quality of the approximation.</p><p>If we set uðnÞ ¼ x 1 ðnÞ and yðn þ 1Þ ¼ xðn þ 1Þ in Eq. ( <ref type="formula" target="#formula_6">6</ref>), then it leads to an intuitive interpretation of the nonlinear state-space reconstruction procedure as equivalent to the time series prediction problem whose goal is to compute an estimate of xðn þ 1Þ. Thus, the only thing we have to do is to train a TDNN model <ref type="bibr" target="#b35">[36]</ref>.</p><p>Once training is completed, the TDNN can be used for predicting the next samples of the time series.</p><p>Despite the correctness of the TDNN approach, recall that it is derived from a simplified version of the NARX network by eliminating the output memory. In order to use the full computational abilities of the NARX network for nonlinear time series prediction, we propose novel definitions for its input and output regressors. Firstly, the input signal regressor, denoted by uðnÞ, is defined by the delay embedding coordinates of Eq. ( <ref type="formula">9</ref>): uðnÞ ¼ x 1 ðnÞ ¼ ½xðnÞ; xðn À tÞ; . . . ; xðn À ðd E À 1ÞtÞ, <ref type="bibr" target="#b11">(12)</ref> where we set d u ¼ d E . In words, the input signal regressor uðnÞ is composed of d E actual values of the observed time series, separated from each other of t time steps.</p><p>Secondly, since the NARX network can be trained in two different modes, the output signal regressor yðnÞ can be written accordingly as</p><formula xml:id="formula_13">y sp ðnÞ ¼ ½xðnÞ; . . . ; xðn À d y þ 1Þ<label>(13)</label></formula><p>or</p><formula xml:id="formula_14">y p ðnÞ ¼ ½xðnÞ; . . . ; xðn À d y þ 1Þ. (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>Note that the output regressor for the SP mode shown in Eq. ( <ref type="formula" target="#formula_13">13</ref>) contains d y past values of the actual time series, while the output regressor for the P mode shown in Eq. ( <ref type="formula" target="#formula_14">14</ref> </p><p>where the nonlinear function f ðÁÞ is readily implemented through an MLP trained with plain backpropagation algorithm.</p><p>It is worth noting that Figs. 2 and 3 correspond to the different ways the NARX network can be trained; i.e., in SP mode or in P mode, respectively. During the testing phase, however, since long-term predictions are required, the predicted values should be fed back to both, the input regressor uðnÞ and the output regressor y sp ðnÞ (or y p ðnÞ), simultaneously. Thus, the resulting predictive model has two feedback loops, one for the input regressor and the other for the output regressor, as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Thus, unlike the TDNN-based approach for the nonlinear time series prediction problem, the proposed approach makes full use of the output feedback loop. Eqs. ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_13">13</ref>) are valid only for one-step-ahead prediction tasks. Again, if one is interested in multi-step-ahead or recursive prediction tasks, the estimates x should also be inserted into both regressors in a recursive fashion.</p><p>One may argue that, in addition to the parameters d E and t, the proposed approach introduces one more to be determined, namely, d y . However, this parameter can be eliminated if we recall that, as pointed out in <ref type="bibr" target="#b17">[18]</ref>, the delay embedding of Eq. ( <ref type="formula">9</ref>) has an alternative form given by x 2 ðnÞ9½xðnÞ; xðn À 1Þ; . . .</p><formula xml:id="formula_17">; xðn À m þ 1Þ, (<label>17</label></formula><formula xml:id="formula_18">)</formula><p>where m is an integer defined as mXt Á d E . By comparing Eqs. ( <ref type="formula" target="#formula_13">13</ref>) and ( <ref type="formula" target="#formula_17">17</ref>), we find that a suitable choice is given by d y Xt Á d E , which also satisfies the necessary condition d y 4d u . However, we have found by experimentation that a value chosen from the interval d E od y pt Á d E is sufficient for achieving a predictive performance better than those achieved by conventional neural based time series predictors, such as the TDNN and Elman architectures.</p><p>Finally, the proposed approach is summarized as follows. A NARX network is defined so that its input regressor uðnÞ contains samples of the measured variable xðnÞ separated t (t40) time steps from each other, while the output regressor yðnÞ contains actual or estimated values of the same variable, but sampled at consecutive time steps. As training proceeds, these estimates should become more and more similar to the actual values of the time series, indicating convergence of the training process. Thus, it is interesting to note that the input regressor supplies mediumto long-term information about the dynamical behavior of the time series, since the delay t is usually larger than unity, while the output regressor, once the network has converged, supplies shortterm information about the same time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simulations and discussion</head><p>In this paper, our aim is to evaluate, in qualitative and quantitative terms, the predictive ability of the NARX-P and NARX-SP networks using two real-world data sets, namely the chaotic laser and the VBR video traffic time series. For the sake of completeness, a performance comparison with the TDNN and Elman recurrent networks is also carried out.</p><p>It is worth emphasizing that our goal in the experiments is to evaluate if the output regressor y sp (or y p ) in the input layer of the NARX network improves its predictive performance. Thus, to facilitate the performance comparison, all the networks we simulate have two hidden layers and one output neuron.</p><p>All neurons in both hidden layers and the output neuron use hyperbolic tangent activation functions. The standard backpropagation algorithm is used to train the networks with learning rate equal to 0.001 (selected heuristically). No momentum term is used. In what concerns the Elman network, only the neuronal outputs of the first hidden layer are fed back to the input layer.</p><p>The number of neurons, N h;1 and N h;2 , in the first and second hidden layers, respectively, are equal for all simulated networks. These values are chosen according to the following heuristic rules <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_19">N h;1 ¼ 2d E þ 1 and N h;2 ¼ ffiffiffiffiffiffiffiffiffi N h;1 q , (<label>18</label></formula><formula xml:id="formula_20">)</formula><p>where N h;2 is rounded up towards the next integer number. The first rule is motivated by Kolmogorov's theorem on function approximation <ref type="bibr" target="#b18">[19]</ref>. The second rule simply states that the number of neurons in the second hidden layer is the square root   of the product of the dimension of the first hidden layer and the dimension of the output layer. Finally, we set d y ¼ 2td E , where t is selected as the value occurring at the first minimum of the mutual information function of the time series <ref type="bibr" target="#b14">[15]</ref>. The total number M of adjustable parameters (weights and thresholds) for each of the simulated networks are given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_21">M ¼ ðd E þ 1Þ Á N h;1 þ ðN h;1 þ 2Þ Á N h;2 þ 1 ðTDNNÞ, M ¼ ðN h;1 þ d E þ 1Þ Á N h;1 þ ðN h;1 þ 2Þ Á N h;2 þ 1 ðELMANÞ, M ¼ ðd E þ d y þ 1Þ Á N h;1 þ ðN h;1 þ 2Þ Á N h;2 þ 1 ðNARXÞ.<label>(19)</label></formula><p>Once a given network has been trained, it is required to provide estimates of the future sample values of a given time series for a certain prediction horizon N. The predictions are executed in a recursive fashion until desired prediction horizon is reached, i.e., during N time steps the predicted values are fed back in order to take part in the composition of the regressors. The networks are evaluated in terms of the normalized mean squared error (NMSE),</p><formula xml:id="formula_22">NMSEðNÞ ¼ 1 N Á s 2 x X N n¼1 ðxðn þ 1Þ À xðn þ 1ÞÞ 2 , (<label>20</label></formula><formula xml:id="formula_23">)</formula><p>where xðn þ 1Þ is the actual value of the time series, xðn þ 1Þ is the predicted value, N is the horizon prediction (i.e., how many steps into the future a given network has to predict), and ŝ2</p><p>x is the sample variance of the actual time series. The NMSE values are averaged over 10 training/testing runs.</p><p>Chaotic laser time series. The first data sequence to be used to evaluate the NARX-P and NARX-SP models is the chaotic laser time series <ref type="bibr" target="#b41">[42]</ref>. This time series comprises measurements of the intensity pulsations of a single-mode Far-Infrared-Laser NH 3 in a chaotic state <ref type="bibr" target="#b20">[21]</ref>. It was made available worldwide during a time series prediction competition organized by the Santa Fe Institute and, since then, has been used in benchmarking studies.</p><p>The laser time series has 1500 points which have been rescaled to the range ½À1; 1. The rescaled time series was further split into two sets for the purpose of performing one-fold cross-validation, so that the first 1000 samples were used for training and the remaining 500 samples for testing. The embedding dimension was estimated as d E ¼ 7 by applying Cao's method <ref type="bibr" target="#b7">[8]</ref>, which is a variant of the well-known false nearest neighbors method. 2 The embedding delay was estimated as t ¼ 2. For the chosen parameters, the total number of modifiable weights and biases for the three simulated neural architectures are the following: M ¼ 189 (TDNN), M ¼ 414 (Elman) and M ¼ 609 (NARX).</p><p>The results are shown in Figs. <ref type="figure" target="#fig_4">5(a)-(c</ref>), for the NARX-SP, Elman and TDNN networks, respectively. 3 A visual inspection illustrates clearly that the NARX-SP model performed better than the other two architectures. It is important to point out that a critical situation occursaround time step 60, where the laser intensity collapses suddenly from its highest value to its lowest one; then, it starts recovering the intensity gradually. The NARX-SP model is able to emulate the laser dynamics very closely. The Elman's network was doing well until the critical point. From this point onwards, it was unable to emulate the laser dynamics faithfully, i.e., the predicted laser intensities have much lower amplitudes than the actual ones. The TDNN network had a very poor predictive performance. From a dynamical point of view the output of the TDNN seems to be stuck in a limit cycle, since it only oscillates endlessly.</p><p>It is worth mentioning that the previous results did not mean that the TDNN and Elman networks cannot learn the dynamics of the chaotic laser. Indeed, it was shown to be possible in <ref type="bibr" target="#b17">[18]</ref> using sophisticated training algorithms, such as BPTT <ref type="bibr" target="#b42">[43]</ref> or real-time recurrent learning (RTRL) <ref type="bibr" target="#b43">[44]</ref>. In what concern the TDNN network, our results confirms the observations reported by Eric Wan <ref type="bibr">[41, p. 62</ref>] in his Ph.D. Thesis. There, he states that the standard MLP, using the input regressor x 1 ðtÞ only and trained with the instantaneous gradient descent rule, has been unable to accurately predict the laser time series. In his own words, ''the downward intensity collapse went completely undetected'', as in our case.</p><p>In sum, our results show that under the same conditions, i.e., with the same number of hidden neurons, using the standard gradient-based backpropagation algorithm, a short time series for training, and the same number of training epochs, the NARX-SP network performs better than the TDNN and Elman networks. It seems that the presence of the output regressor y sp improves indeed the predictive power of the NARX network.</p><p>For the sake of comparison, under similar training and network evaluation methodologies, the FIR-MLP model proposed by Eric Wan <ref type="bibr" target="#b40">[41]</ref> achieved very good long-term prediction results on the laser time series, which are equivalent to those obtained by the NARX-SP network. However, the FIR-MLP required M ¼ 1105 adjustable parameters to achieve such a good performance, while the NARX-SP model required roughly half the number of parameters (i.e., M ¼ 609).</p><p>The long-term predictive performances of all simulated networks can be assessed in more quantitative terms by means of 2 A recent technique for the estimation of d E can be found in <ref type="bibr" target="#b24">[25]</ref>. 3 The results for the NARX-P network are not shown since they are equivalent to those shown for the NARX-SP network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>NMSE curves as a function of the prediction horizon (N). Figs. <ref type="figure">6(a)-(c</ref>) show the NMSE curve of the NARX-SP model (best performance) in comparison to the curves obtained by the NARX-P, Elman and TDNN models, respectively. It is worth emphasizing two types of behavior in these figures. Below the critical time step (i.e., No60), the reported NMSE values are approximately the same for all models, with a small advantage to the Elman network. This means that, while the critical point is not reached, all networks predict well the time series. For N460, the NARX-P and NARX-SP models reveal their superior performances. For all figures, the confidence intervals of the NSME values, for a significance level of a ¼ 0:05, are also provided. These intervals were generated for a sample size of n ¼ 10, assuming a Gaussian distribution for the NSME values. Thus, the critical value t a=2;nÀ1 ¼ 2:093 used for building the corresponding 95% confidence intervals are obtained from a t-student distribution with n À 1 degrees of freedom.</p><p>A useful way to qualitatively evaluate the performance of the NARX-SP network for the laser series is through recurrence plots <ref type="bibr" target="#b8">[9]</ref>. These diagrams describe how a reconstructed state-space trajectory recurs or repeats itself, being useful for characterization of a system as random, periodic or chaotic. For example, random signals tend to occupy the whole area of the plot, indicating that no value tends to repeat itself. Any structured information embedded in a periodic or chaotic signal is reflected in a certain visual pattern in the recurrence plot.</p><p>Recurrence plots are built by calculating the distance between two points in the state space at times i (horizontal axis) and j (vertical axis):</p><formula xml:id="formula_24">d ij ¼ kDðiÞ À DðjÞk,<label>(21)</label></formula><p>where k Á k is the Euclidean norm. The state vectors DðnÞ ¼ ½xðnÞ; xðn À tÞ; . . . ; xðn À ðd E À 1ÞtÞ are built using the points of the predicted time series. Then, a dot is placed at the coordinate ði; jÞ if d ij or. In this paper, we set r ¼ 0:4 and the prediction horizon to</p><formula xml:id="formula_25">N ¼ 200.</formula><p>The results are shown in Fig. <ref type="figure" target="#fig_6">7</ref>. It can be easily visualized that the recurrence plots shown in Figs. <ref type="figure" target="#fig_6">7(a</ref>) and (b) are more similar with one another, indicating that NARX-SP network reproduced the original state-space trajectory more faithfully.</p><p>VBR video traffic time series. Due to the widespread use of Internet and other packet/cell switching broad-band networks, VBR video traffic will certainly be a major part of the traffic produced by multimedia sources. Hence, many researches have focused on VBR video traffic prediction to devise network management strategies that satisfy QoS requirements. From the point of view of modeling, a particular challenging issue on network traffic prediction comes from the important discovery of self-similarity and LRD in broad-band network traffic <ref type="bibr" target="#b23">[24]</ref>. Researchers have also observed that VBR video traffic typically exhibits burstiness over multiple time scales (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, for example).</p><p>In this section, we evaluate the predictive abilities of the NARX-P and NARX-SP networks using VBR video traffic time series (trace), extracted from Jurassic Park, as described in <ref type="bibr" target="#b36">[37]</ref>. This video traffic trace was encoded at University of Wu ¨rzburg with MPEG-I. The frame rates of video sequence that coded Jurassic Park have been used. The MPEG algorithm uses three different types of frames: intraframe (I), predictive (P) and bidirectionallypredictive (B). These three types of frames are organized as a group (group of picture, GoP) defined by the distance L between I frames and the distance M between P frames. If the cyclic frame pattern is {IBBPBBPBBPBBI}, then L ¼ 12 and M ¼ 3. These values for L and M are used in this paper.</p><p>The resulting time series has 2000 points which have been rescaled to the range ½À1; 1. The rescaled time series was further split into two sets for cross-validation purposes: 1500 samples for training and 500 samples for testing.</p><p>Evaluation of the long-term predictive performances of all networks can also help assessing the sensitivity of the neural models to important training parameters, such as the number of training epochs and the size of the embedding dimension, as shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>Fig. <ref type="figure" target="#fig_7">8</ref>(a) shows the NMSE curves for all neural networks versus the value of the embedding dimension, d E , which varies from 3 to 24. For this simulation we trained all the networks for 300 epochs, t ¼ 1 and d y ¼ 24. One can easily note that the NARX-P and NARX-SP performed better than the TDNN and Elman networks. In particular, the performance of the NARX-SP was rather impressive, in the sense that it remains constant throughout the studied range. From d E X12 onwards, the performances of the NARX-P and NARX-SP are practically the same. It is worth noting that the performances of the TDNN and Elman networks approaches those of the NARX-P and NARX-SP networks when d E is of the same order of magnitude of d y . This suggests that, for NARX-SP (or NARX-P) networks, we can select a small value for d E and still have a very good performance. with t ¼ 1; d E ¼ 12 and d y ¼ 2td E ¼ 24. Again, better performances were achieved by the NARX-P and NARX-SP. The performance of the NARX-SP is practically the same from 100 epochs on. The same behavior is observed for the NARX-P network from 200 epochs on. This can be explained by recalling that the NARX-P uses estimated values to compose the output regressor y p ðnÞ and, because of that, it learns slower than the NARX-SP network.</p><p>Another important behavior can be observed for the TDNN and Elman networks. From 200 epochs onwards, these networks increase their NMSE values instead of decreasing them. We hypothesize that this behavior can be an evidence of overfitting, a phenomenon observed when powerful nonlinear models, with excessive degrees of freedom (too much adjustable parameters), are trained for a long period with a finite size data set. In this sense, the results of Fig. <ref type="figure" target="#fig_7">8(b</ref>) strongly suggest that the NARX-SP and NARX-P networks are much more robust than the TDNN and Elman networks. In other words, the presence of an output regressor in the NARX-SP and NARX-P networks seems to turn them less prone to overfitting than the Elman and TDNN models, even when the number of free parameters in the NARX networks are higher than that in the Elman and TDNN models.  networks, respectively. For this simulation, all the neural networks are required to predict recursively the sample values of the VBR video traffic trace for 300 steps ahead in time. For all networks, we have set d E ¼ 12; t ¼ 1; d y ¼ 24 and trained the neural models for 300 epochs. For these training parameters, the NARX-SP predicted the video traffic trace much better than the TDNN and Elman networks.</p><p>As we did for the laser time series, we again emphasize that the results reported in Fig. <ref type="figure" target="#fig_8">9</ref> did not mean to say that the TDNN and Elman networks cannot ever predict the video traffic trace as well as the NARX-SP. They only mean that, for the same training and configuration parameters, the NARX-SP has greater computational power provided by the output regressor. Recall that the MLP is a universal function approximation; and so, any MLP-based neural model, such as the TDNN and Elman networks, are in principle able to approximate complex function with arbitrary accuracy, once enough training epochs and data are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and further work</head><p>In this paper, we have shown that the NARX neural network can successfully use its output feedback loop to improve its predictive performance in complex time series prediction tasks. We used the well-known chaotic laser and real-world VBR video traffic time series to evaluate empirically the proposed approach in long-term prediction tasks. The results have shown that the proposed approach consistently outperforms standard neural network based predictors, such as the TDNN and Elman architectures.</p><p>Currently, we are evaluating the proposed approach on several other applications that require long-term predictions, such as electric load forecasting and financial time series prediction. Applications to signal processing tasks, such as communication channel equalization, are also being planned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. NARX network with du delayed inputs and dy delayed outputs (z À1 ¼ unit time delay).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the NARX network during training in the SP mode (z Àt ¼ t unit time delays).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of the NARX network during training in the P mode (z Àt ¼ t unit time delays).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Common architecture for the NARX-P and NARX-SP networks during the testing (recursive prediction) phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results for the laser series: (a) NARX-SP, (b) Elman, and (c) TDNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 (Fig. 6 .</head><label>86</label><figDesc>Fig. 6. Multi-step-ahead NMSE values and the corresponding confidence intervals: (a) NARX-P Â NARX-SP models, (b) Elman Â NARX-SP models, and (c) TDNN Â NARX-SP models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Recurrence plot of (a) the original laser time series and the ones produced by (b) NARX-SP, (c) TDNN, and (d) Elman networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Evaluation of the sensitivity of the neural networks with respect to (a) the embedding dimension and (b) the number of training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Recursive predictions obtained by (a) TDNN, (b) Elman, and (c) NARX-SP networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) contains d y past values of the estimated time series. For a suitably trained network, no matter under which training mode, these outputs are estimates of previous values of xðn þ 1Þ. Henceforth, NARX networks trained using the regression pairs fy sp ðnÞ; x 1 ðnÞg and fy p ðnÞ; x 1 ðnÞg are denoted by NARX-SP and NARX-P networks, respectively. These NARX networks implement following predictive mappings, which can be visualized in Figs.2 and 3:</figDesc><table /><note><p><p><p>xðn þ 1Þ ¼ f ½y sp ðnÞ; uðnÞ ¼ f ½y sp ðnÞ; x 1 ðnÞ,</p>(15)</p>xðn þ 1Þ ¼ f ½y p ðnÞ; uðnÞ ¼ f ½y p ðnÞ; x 1 ðnÞ,</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.M.P. Menezes Jr., G.A. Barreto / Neurocomputing 71 (2008) 3335-3343</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank CNPq (Grant #506979/ 2004-0), CAPES/PRODOC and FUNCAP for their financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obtaining order in a world of chaos</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Abarbanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Frison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tsimring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="49" to="65" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse basis selection: new results and application to adaptive prediction of video source traffic</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1136" to="1146" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison between neural-network forecasting techniques-case study: river flow forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>El-Shoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>El-Sherif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="402" to="409" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-range dependence in variable-bit-rate video traffic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Taqqu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">234</biblScope>
			<biblScope unit="page" from="1566" to="1579" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction of MPEG-coded video source traffic using recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2177" to="2190" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinsel</surname></persName>
		</author>
		<title level="m">Time Series Analysis: Forecasting &amp; Control</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical method for determining the minimum embedding dimension of a scalar time series</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="43" to="50" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Casdagli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recurrence plots revisited</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="12" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear system identification using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1214" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A time-series prediction approach for feature extraction in a brain-computer interface</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="467" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Time series forecasting with SOM and local non-linear models-application to the DAX30 index prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dablemont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruttiens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Blayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Fourth Workshop on Self-Organizing Maps, (WSOM)</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An adaptable neural network model for recursive nonlinear traffic prediction and modelling of MPEG video sources</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-similar traffic and network dynamics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Erramilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R D</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="800" to="819" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Independent coordinates for strange attractors from mutual information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Swinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1134" to="1140" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the relevance of long-range dependence in network traffic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grossglauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bolot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="640" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection of signals in chaos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="122" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making sense of a complex world</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66" to="81" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kolmogorov&apos;s mapping neural network existence theorem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What are the implications of long-range dependence for VBR video traffic engineering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lakshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="301" to="317" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH3 laser</title>
		<author>
			<persName><forename type="first">U</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6354" to="6365" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Kantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Nonlinear Time Series Analysis</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A Field Guide to Dynamical Recurrent Networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the self-similar nature of ethernet traffic (extended version)</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Leland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Taqqu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Network</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Forecasting electricity consumption using nonlinear projection and self-organizing maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Input-output parametric models for nonlinear systems-part I: deterministic nonlinear systems</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Leontaritis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="328" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How embedded memory in recurrent neural network architectures helps learning long-term temporal dependencies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies in NARX recurrent neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1424" to="1438" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A delay damage model selection algorithm for NARX neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2719" to="2730" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">System Identification: Theory for the User</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Masters</surname></persName>
		</author>
		<title level="m">Practical Neural Network Recipes in C þ þ</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identification and control of dynamical systems using neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Norgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ravn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Poulsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<title level="m">Neural Networks for Modelling and Control of Dynamic Systems</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Palit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovic</surname></persName>
		</author>
		<title level="m">Computational Intelligence in Time Series Forecasting</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>first ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient calculations for dynamic recurrent neural networks: a survey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1212" to="1228" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Euliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Lefebvre</surname></persName>
		</author>
		<title level="m">Neural Adaptive Systems: Fundamentals Through Simulations</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Statistical properties of MPEG video traffic and their impact on traffic modeling in ATM systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual IEEE Conference on Local Computer Networks (LCN&apos;95)</title>
		<meeting>the 20th Annual IEEE Conference on Local Computer Networks (LCN&apos;95)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Computational capabilities of recurrent NARX neural networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Methodology for long-term prediction of time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorjamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H N</forename><surname>Reyhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="2861" to="2869" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical Systems and Turbulence</title>
		<title level="s">Lecture Notes in Mathematics</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L.-S</forename><surname>Young</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">898</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Finite impulse response neural networks with applications in time series prediction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gershefeld</surname></persName>
		</author>
		<title level="m">Time Series Prediction: Forecasting the Future and Understanding the Past</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic neural-based buffer management for queueing systems with self-similar characteristics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yousefi'zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Jonckheere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1163" to="1173" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">He received the B.S. degree in Electrical Engineering from the Federal University of Ceara in 2003 and his M.Sc. degree in Teleinformatics Engineering from the same university in 2006, working on recurrent neural architectures for time series prediction. Currently, he is pursuing the Ph.D. degree in Teleinformatics. His main research interests are in the areas of neural networks</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Menezes</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<pubPlace>Teresina, Piaui, Brazil</pubPlace>
		</imprint>
	</monogr>
	<note>time series prediction, communication network traffic modelling and nonlinear dynamical systems</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">He is in the organizing committee of the Brazilian Symposium on Neural Networks (SBRN) and has been serving as reviewer for several neural-network related journals (IEEE TNN, IEEE TSMC, IEEE TKDE, IEEE TSP, Neurocomputing and IJCIA), and conferences (IJCNN, ICASSP, SBRN, among others). Currently, he is with the</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guilherme</surname></persName>
		</author>
		<author>
			<persName><surname>Barreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000, he developed part of his Ph.D. studies at the Neuroinformatics Group of the University of</title>
		<editor>
			<persName><forename type="first">Ceara</forename><surname>Fortaleza</surname></persName>
		</editor>
		<editor>
			<persName><surname>Brazil</surname></persName>
		</editor>
		<meeting><address><addrLine>Bielefeld, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
		<respStmt>
			<orgName>Department of Teleinformatics Engineering, Federal University of Ceara</orgName>
		</respStmt>
	</monogr>
	<note>His main research interests are self-organizing neural networks for signal and image processing. time series prediction, pattern recognition, and robotics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
