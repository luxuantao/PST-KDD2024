<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a stateof-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Unsupervised modeling of protein contacts has an important role in computational protein <ref type="bibr">design (Russ et al., 2020;</ref><ref type="bibr" target="#b48">Tian et al., 2018;</ref><ref type="bibr" target="#b7">Blazejewski et al., 2019)</ref> and is a central element of all current state-of-the-art structure prediction methods <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr">Senior et al., 2020;</ref><ref type="bibr" target="#b55">Yang et al., 2019)</ref>. The standard bioinformatics pipeline for unsupervised contact prediction includes multiple components with specialized tools and databases that have been developed and optimized over decades. In this work we propose replacing the current multi-stage pipeline with a single forward pass of a pre-trained end-to-end protein language model. In the last year, protein language modeling with an unsupervised training objective has been investigated by multiple groups <ref type="bibr" target="#b36">(Rives et al., 2019;</ref><ref type="bibr" target="#b2">Alley et al., 2019;</ref><ref type="bibr" target="#b16">Heinzinger et al., 2019;</ref><ref type="bibr">Rao et al., 2019;</ref><ref type="bibr" target="#b28">Madani et al., 2020)</ref>. The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences; by contrast, protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences. High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data <ref type="bibr" target="#b36">(Rives et al., 2019)</ref>.</p><p>A line of work in this emerging field proposes the Transformer for protein language modeling <ref type="bibr" target="#b36">(Rives et al., 2019;</ref><ref type="bibr">Rao et al., 2019)</ref>. Originally developed in the NLP community to represent long range context, the main innovation of the Transformer model is its use of self-attention <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref>. Self-attention has particular relevance for the modeling of protein sequences. Unlike convolutional and recurrent LSTM models, the Transformer constructs a pairwise interaction map between all positions in the sequence. In principle this mechanism has an ideal form to model residue-residue contacts.</p><p>In theory, end-to-end learning with a language model has advantages over the bioinformatics pipeline: (i) it replaces the expensive query, alignment, and training steps with a single forward pass, greatly accelerating feature extraction; and (ii) it shares parameters for all protein families, enabling generalization by capturing commonality across millions of evolutionarily diverse and unrelated sequences.</p><p>We demonstrate that Transformer protein language models learn contacts in the self-attention maps with state-of-the-art performance. We compare ESM-1b <ref type="bibr" target="#b37">(Rives et al., 2020)</ref>, a large-scale (650M parameters) Transformer model trained on UniRef50 <ref type="bibr" target="#b45">(Suzek et al., 2007)</ref> to the Gremlin <ref type="bibr" target="#b21">(Kamisetty et al., 2013)</ref> pipeline which implements a log linear model trained with pseudolikelihood <ref type="bibr" target="#b5">(Balakrishnan et al., 2011;</ref><ref type="bibr" target="#b12">Ekeberg et al., 2013)</ref>. Contacts can be extracted from the attention maps of the Transformer model by a sparse linear combination of attention heads identified by logistic regression. ESM-1b model contacts have higher precision than Gremlin contacts. When ESM and Gremlin are compared with access to the same set of sequences the precision gain from the protein language model is significant; the advantage holds on average even when Gremlin is given access to an optimized set of multiple sequence alignments incorporating metagenomics data.</p><p>We find a linear relationship between language modeling perplexity and contact precision. We also find evidence for the value of parameter sharing: the ESM-1b model significantly outperforms Gremlin on proteins with low-depth MSAs. Finally we explore the Transformer language model's ability to generate sequences and show that generated sequences preserve contact information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Multiple Sequence Alignments (MSAs) A multiple sequence alignment consists of a set of evolutionarily related protein sequences. Since real protein sequences are likely to have insertions, deletions, and substitutions, the sequences are aligned by minimizing a Levenshtein distance-like metric over all the sequences. In practice heuristic alignment schemes are used. Tools like Jackhmmer and HHblits can increase the number and diversity of sequences returned by iteratively performing the search and alignment steps <ref type="bibr" target="#b18">(Johnson et al., 2010;</ref><ref type="bibr" target="#b34">Remmert et al., 2012)</ref>.</p><p>Metrics For a protein of length L, we evaluate the precision of the top L, L/2, and L/5 contacts for short range (|i − j| ∈ [6, 12)), medium range (|i − j| ∈ [12, 24)), and long range (|i = j| ∈ [24, ∞)) contacts. We also separately evaluate local contacts <ref type="bibr">(|i−j| ∈ [3, 6)</ref>) for secondary structure prediction in Section A.9. In general, all contacts provide information about protein structure and important interactions, with shorter-range contacts being useful for secondary and local structure, while longer range contacts are useful for determining global structure <ref type="bibr" target="#b46">(Taylor et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>There is a long history of protein contact prediction <ref type="bibr" target="#b0">(Adhikari &amp; Cheng, 2016)</ref> both from MSAs, and more recently, with protein language models. Supervised contact prediction Recently, supervised methods using deep learning have resulted in breakthrough results in supervised contact prediction <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr" target="#b19">Jones &amp; Kandathil, 2018;</ref><ref type="bibr" target="#b55">Yang et al., 2019;</ref><ref type="bibr">Senior et al., 2020;</ref><ref type="bibr" target="#b1">Adhikari &amp; Elofsson, 2020)</ref>. State-of-the art methods use deep residual networks trained with supervision from many protein structures. Inputs are typically covariance statistics <ref type="bibr" target="#b19">(Jones &amp; Kandathil, 2018;</ref><ref type="bibr" target="#b1">Adhikari &amp; Elofsson, 2020)</ref>, or inferred coevolutionary parameters <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr" target="#b26">Liu et al., 2018;</ref><ref type="bibr">Senior et al., 2020;</ref><ref type="bibr" target="#b55">Yang et al., 2019)</ref>. Other recent work with deep learning uses sequences or evolutionary features as inputs <ref type="bibr" target="#b3">(AlQuraishi, 2018;</ref><ref type="bibr" target="#b17">Ingraham et al., 2019)</ref>. <ref type="bibr" target="#b54">Xu et al. (2020)</ref> demonstrates the incorporation of coevolutionary features is critical to performance of current state-of-the-art methods.</p><p>Unsupervised contact prediction In contrast to supervised methods, unsupervised contact prediction models are trained on sequences without information from protein structures. In principle this allows them to take advantage of large sequence databases that include information from many sequences where no structural knowledge is available. The main approach has been to learn evolutionary constraints among a set of similar sequences by fitting a Markov Random Field (Potts model) to the underlying MSA, a technique known as Direct Coupling Analysis (DCA). This was proposed by <ref type="bibr" target="#b25">Lapedes et al. (1999)</ref> and reintroduced by <ref type="bibr" target="#b47">Thomas et al. (2008)</ref> and <ref type="bibr" target="#b53">Weigt et al. (2009)</ref>.</p><p>Various methods have been developed to fit the underlying Markov Random Field, including meanfield DCA (mfDCA) <ref type="bibr" target="#b30">(Morcos et al., 2011)</ref>, sparse inverse covariance (PSICOV) <ref type="bibr" target="#b20">(Jones et al., 2011)</ref> and pseudolikelihood maximization <ref type="bibr" target="#b5">(Balakrishnan et al., 2011;</ref><ref type="bibr" target="#b12">Ekeberg et al., 2013;</ref><ref type="bibr" target="#b39">Seemayer et al., 2014)</ref>. Pseudolikelihood maximization is generally considered state-of-the-art for unsupervised contact prediction and the Gremlin <ref type="bibr" target="#b5">(Balakrishnan et al., 2011)</ref> implementation is used as the baseline throughout. We also provide mfDCA and PSICOV baselines. Recently deep learning methods have also been applied to fitting MSAs, and <ref type="bibr" target="#b35">Riesselman et al. (2018)</ref> found evidence that factors learned by a VAE model may correlate with protein structure.</p><p>Structure prediction from contacts While we do not perform structure prediction in this work, many methods have been proposed to extend contact prediction to structure prediction. For example, EVFold <ref type="bibr" target="#b30">(Marks et al., 2011)</ref> and DCAFold <ref type="bibr" target="#b44">(Sulkowska et al., 2012)</ref> predict co-evolving couplings using a Potts Model and then generate 3D conformations by directly folding an initial conformation with simulated annealing, using the predicted residue-residue contacts as constraints. Similarly, FragFold <ref type="bibr">(Kosciolek &amp; Jones, 2014)</ref> and Rosetta <ref type="bibr" target="#b31">(Ovchinnikov et al., 2016)</ref> incorporate constraints from a Potts Model into a fragment assembly based pipeline. <ref type="bibr" target="#b41">Senior et al. (2019)</ref>, use features from a Potts model fit with pseudolikelihood maximization to predict pairwise distances with a deep residual network and optimize the final structure using Rosetta. All of these works build directly upon the unsupervised contact prediction pipeline.</p><p>Contact prediction from protein language models Since the introduction of large scale language models for natural language processing <ref type="bibr" target="#b49">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Devlin et al., 2019)</ref>, there has been considerable interest in developing similar models for proteins <ref type="bibr" target="#b2">(Alley et al., 2019;</ref><ref type="bibr" target="#b36">Rives et al., 2019;</ref><ref type="bibr" target="#b16">Heinzinger et al., 2019;</ref><ref type="bibr">Rao et al., 2019;</ref><ref type="bibr" target="#b13">Elnaggar et al., 2020)</ref>. <ref type="bibr" target="#b36">Rives et al. (2019)</ref> were the first to study protein Transformer language models, demonstrating that information about residue-residue contacts could be recovered from the learned representations by linear projections supervised with protein structures. Recently <ref type="bibr">Vig et al. (2020)</ref> performed an extensive analysis of Transformer attention, identifying correspondences to biologically relevant features, and also found that different layers of the model are responsible for learning different features. In particular <ref type="bibr">Vig et al. (2020)</ref> studied the self-attention mechanism and discovered a correlation between self-attention maps and contact patterns, suggesting they could be used for contact prediction.</p><p>Prior work benchmarking contact prediction with protein language models has focused on the supervised problem. <ref type="bibr" target="#b6">Bepler &amp; Berger (2019)</ref> were the first to fine-tune an LSTM pretrained on protein sequences to fit contacts. <ref type="bibr">Rao et al. (2019)</ref> and <ref type="bibr" target="#b37">Rives et al. (2020)</ref> perform benchmarking of multiple protein language models using a deep residual network fit with supervised learning on top of pretrained language modeling features.</p><p>In contrast to previous work on protein language models, we find that a state-of-the-art unsupervised contact predictor can be directly extracted from the Transformer self-attention maps. We perform a thorough analysis of the contact predictor, showing relationships between performance and MSA depth as well as language modeling perplexity. We also provide methods for improving performance using sequences from an MSA and for sampling sequences in a manner that preserves contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODELS</head><p>We compare Transformer models trained on large sequence databases to Potts Models trained on individual MSAs. While Transformers and Potts Models emerged in separate research communities, the two models share core similarities <ref type="bibr" target="#b51">(Wang &amp; Cho, 2019)</ref> which we exploit here. Our main result is that just as Gremlin directly represents contacts via its pairwise component (the weights), the Transformer also directly represents contacts via its pairwise component (the self-attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">OBJECTIVES</head><p>For a set of training sequences, X, Gremlin optimizes the following pseudolikelihood loss, where a single position is masked and predicted from its context. Since inputs to Gremlin are aligned they all have length L:</p><formula xml:id="formula_0">L PLL (X; θ) = E x∼X L i=1 log p(x i |x j =i ; θ)<label>(1)</label></formula><p>The masked language modeling (MLM) loss used by the Transformer models can be seen as a generalization of the Potts Model objective when written as follows:</p><formula xml:id="formula_1">L MLM (X; θ) = E x∼X E mask i∈mask log p(x i |x j ∈mask ; θ)<label>(2)</label></formula><p>In contrast to Gremlin, the MLM objective applied by protein language modeling is trained on unaligned sequences. The key distinction of MLM is to mask and predict multiple positions concurrently, instead of masking and predicting one at a time. This enables the model to scale beyond The regression is trained on a small number (n ≤ 20) of proteins to determine which attention heads are informative. At test time, contact prediction from an input sequence can be done entirely on GPU in a single forward pass.</p><p>individual MSAs to massive sequence datasets. In practice, the expectation under the masking pattern is computed stochastically using a single sample at each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GREMLIN</head><p>The log probability optimized by Gremlin is described in section A.3. Contacts are extracted from the pairwise Gremlin parameters by taking the Frobenius norm along the amino acid dimensions, resulting in an L × L coupling matrix. Average product correction (APC) is applied to this coupling matrix to determine the final predictions (Section A.2).</p><p>Gremlin takes an MSA as input. The quality of the output predictions are highly dependent on the construction of the MSA. We compare to Gremlin under two conditions. In the first condition, we present Gremlin with all MSAs from the trRosetta training set <ref type="bibr" target="#b55">(Yang et al., 2019)</ref>. These MSAs were generated from all of Uniref100 and are also supplemented with metagenomic sequences when the depth from Uniref100 is too low. The trRosetta MSAs are a key ingredient in the state-of-theart protein folding pipeline. See <ref type="bibr" target="#b55">Yang et al. (2019)</ref> for a discussion on the significant impact of metagenomic sequences on the final result. In the second setting, we allow Gremlin access only to the same information as the ESM Transformers by generating MSAs via Jackhmmer on the ESM training set (a subset of Uniref50). See Section A.5 for Jackhmmer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRANSFORMERS</head><p>We evaluate several pre-trained Transformer models, including ESM-1 <ref type="bibr" target="#b36">(Rives et al., 2019)</ref>, ProtBert-BFD <ref type="bibr" target="#b13">(Elnaggar et al., 2020)</ref> and the TAPE Transformer <ref type="bibr">(Rao et al., 2019)</ref>. The key differences between these models are the datasets, model sizes, and hyperparameters (major architecture differences described in Table <ref type="table" target="#tab_2">3</ref>). <ref type="bibr" target="#b27">Liu et al. (2019)</ref> previously showed that these changes can have a significant impact on final model performance. In addition to ESM-1, we also evaluate an updated version, ESM-1b, which is the result of a hyperparameter sweep. The weights of ESM-1b were provided to us by the authors and the differences are described in section A.4. The Transformer processes inputs through a series of blocks alternating multi-head self-attention and feed-forward layers. In each head of a self-attention layer, the Transformer views the encoded representation as a set of query-key-value triples. The output of the head is the result of scaled dot-product attention:</p><formula xml:id="formula_2">Attention(Q, K, V ) = softmax(QK T / √ n) • V</formula><p>Rather than only computing the attention once, the multi-head approach runs scaled dot-product attention multiple times in parallel and concatenates the output. Since self-attention explicitly constructs pairwise interactions (QK T ) between all positions in the sequence, the model can directly represent residue-residue interactions. In this work, we demonstrate that the QK T pairwise "self attention maps" indeed capture accurate contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LOGISTIC REGRESSION</head><p>To extract contacts from a Transformer, we first pass the input sequence through the model to obtain the attention maps (one map for each head in each layer). We then symmetrize and apply APC to each attention map independently. The resulting maps are passed through an L 1 -regularized logistic regression, which is applied independently at each amino acid pair (i, j). At training time, we only train the weights of the logistic regression; we do not backpropagate through the entire model. At test time, the entire prediction pipeline can be run in a single forward pass, providing a single end-toend pipeline for protein contact prediction that does not require any retrieval steps from a sequence database. See Figure <ref type="figure" target="#fig_0">1</ref> for an illustration of this pipeline and Section A.7 for a full description of the logistic regression setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We evaluate models with the 15051 proteins in the trRosetta training dataset <ref type="bibr" target="#b55">(Yang et al., 2019)</ref>, removing 43 proteins with sequence length greater than 1024, since ESM-1b was trained with a context size of 1024. Of these sequences, Jackhmmer fails on 126 when we attempt to construct MSAs using the ESM training set (see Section A.5). This leaves us with 14882 total sequences. We reserve 20 sequences for training, 20 sequences for validation, and 14842 sequences for testing.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows evaluations of Gremlin, ESM-1, ESM-1b as well as the TAPE and ProtBERT-BFD models. Confidence intervals are within 0.5 percentage points for all statistics in Tables <ref type="table" target="#tab_1">1 and 2</ref>. In Table <ref type="table" target="#tab_0">1</ref>, all Transformer model contact predictors are trained with logistic regression on 20 proteins. We find that with only 20 training proteins ESM-1b has higher precision than Gremlin for short, medium, and long range contacts.</p><p>In addition to this set, we also evaluate performance on 15 CASP13 FM Domains in Section A.6. On average ESM-1b has higher short, medium, and long range precision than Gremlin on all metrics,</p><p>1 PSICOV fails to converge on 24 sequences using default parameters. Following the suggestion in github.com/psipred/psicov, we increase ρ to 0.005, 0.01, and thereafter by increments of 0.01, to a maximum of 0.1. PSICOV fails to converge altogether on 6 / 14842 sequences. We assign a score of 0 for these sequences. We also provide a comparison to the bilinear model proposed by <ref type="bibr" target="#b37">Rives et al. (2020)</ref>. The logistic regression model achieves a long-range contact precision at L of 18.6, while the fully supervised bilinear model achieves a long range precision at L of 20.1, an increase of only 1.5 points despite being trained on 700x more structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ABLATIONS: LIMITING SUPERVISION</head><p>While the language modeling objective is fully unsupervised, the logistic regression is trained with a small number of supervised examples. In this section, we study the dependence of the results on this supervision, providing evidence that the contacts are indeed learned in the unsupervised phase, and the logistic regression is only necessary to extract the contacts.</p><p>Top Heads Here we use the logistic regression only to determine the most important heads. Once they are selected, we discard the weights from the logistic regression and simply average the attention heads corresponding to the top-k weight values. By taking the single best head from ESM-1b, we come close to Gremlin performance given the same data, and averaging the top-5 heads allows us to outperform Gremlin. Averaging the top-10 heads outperforms a full logistic regression on all other Transformer models and comes close to Gremlin given optimized MSAs.</p><p>Low-N The second variation we consider is to limit the number of supervised examples provided to the logistic regression. We find that with only a single training example, the model achieves a long range top-L precision of 39.2, which is statistically indistinguishable from Gremlin (p &gt; 0.05). Using only 10 training examples, the model outperforms Gremlin on all the metrics. Since these results depend on the sampled training proteins, we also show a bootstrapped performance distribution using 100 different logistic regression models in Section A.10. We find that with 1 protein, performance can vary significantly, with long range top-L precision mean of 35.6, a median of 38.4, and standard deviation 8.9. This variation greatly decreases when training on 20 proteins, with a long range top-L precision mean of 40.1, median of 41.1, and standard deviation of 0.3. See Figure <ref type="figure" target="#fig_0">11</ref> for the full distribution on all statistics.</p><p>MSA Only Finally, we consider supervising the logistic regression only with MSAs instead of real structures. This is the same training data used by the Gremlin baseline. To do this, we first train Gremlin on each MSA. We take the output couplings from Gremlin and mark the top L couplings with sequence separation ≥ 6 in each protein as true contacts, and everything else as false contacts, creating a binary decision problem. When trained on 20 MSAs, we find that this model achieves a long range P@L of 39.9, and generally achieves similar long range performance to Gremlin, while still having superior short and medium range contact precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ENSEMBLING OVER MSA</head><p>Transformer models are fundamentally single-sequence models, but we can further boost performance by ensembling predictions from multiple sequences in the alignment. To do so, we unalign each sequence in the alignment (removing any gaps), pass the resulting sequence through the Transformer and regression, and realign the resulting contact maps to the original aligned indices. For these experiments, we use the logistic regression weights trained on single-sequence inputs, rather than re-training the logistic regression on multi-sequence inputs. We also simply take the first s sequences in the MSA. Table <ref type="table" target="#tab_1">2</ref> shows performance improvements from averaging over 16, 32, and 64 sequences.</p><p>To better understand this result, we return to the single-sequence setting and study the change in prediction when switching between sequences in the alignment. We find that contact precision can vary significantly depending on the exact sequence input to the model, and that the initial query sequence of the MSA does not necessarily generate the highest contact precision (Figure <ref type="figure" target="#fig_6">9</ref>). Lastly, <ref type="bibr" target="#b2">Alley et al. (2019)</ref> presented a method of fine-tuning where a pretrained language model is further trained on the MSA of the sequence of interest ('evotuning'). Previously this has only been investigated for function prediction and for relatively low-capacity models. We fine-tune the full ESM-1b model (which has 50x more parameters than UniRep) on 380 protein sequence families. We find that after 30 epochs of fine-tuning, long range P@L increases only slightly, with an average of 1.6 percentage points (Figure <ref type="figure" target="#fig_4">15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PERFORMANCE DISTRIBUTION</head><p>Although our model is, on average, better than Gremlin at detecting contacts, the performance distribution over all sequences in the dataset is still mixed. ESM-1b is consistently better at extracting short and medium range contacts (Figure <ref type="figure">7</ref>), but only slightly outperforms Gremlin on long range contacts when Gremlin has access to Uniref100 and metagenomic sequences. Figure <ref type="figure" target="#fig_1">2</ref> shows the distribution of long range P@L for ESM-1b vs. Gremlin. Overall, ESM-1b has higher long range P@L on 55% of sequences in the test set.</p><p>In addition, we examine the relationship between MSA depth and precision for short, medium, and long range contacts (Figure <ref type="figure" target="#fig_2">3</ref>). Although our contact prediction pipeline does not make explicit use of MSAs, there is still some correlation between MSA depth and performance, since MSA depth is a measure of how many related sequences are present in the ESM-1b training set. We again see that ESM-1b consistently outperforms Gremlin at all MSA depths for short and medium range sequences. We also confirm that ESM-1b outperforms Gremlin for long range contact extraction for sequences with small MSAs (depth &lt; 1000). ESM-1b also outperforms Gremlin on sequences with the very largest MSAs (depth &gt; 16000), which is consistent with prior work showing that Gremlin performance plateaus for very large MSAs and suggests that ESM-1b does not suffer from the same issues <ref type="bibr" target="#b4">(Anishchenko et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LOGISTIC REGRESSION WEIGHTS</head><p>In Section 5.1 we show that selecting only a sparse subset of the attention heads can yield good results for contact prediction. Overall, the L 1 -regularized logistic regression identifies 102 / 660 heads as being predictive of contacts (Figure <ref type="figure">6b</ref>). Additionally, we train separate logistic regressions to identify contacts at different ranges (local, short, medium, and long range). These regressions identify an overlapping, but non-identical set of useful attention heads. Two attention heads have the top-10 highest weights for detecting contacts at all ranges. One attention head is highly positively correlated with local contacts, but highly negatively correlated with long range contacts. Lastly, we identify a total of 104 attention heads that are correlated (positively or negatively) with contacts at only one of the four ranges, suggesting that particular attention heads specialize in detecting certain types of contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">PERPLEXITY VS. CONTACT PRECISION</head><p>Figure <ref type="figure" target="#fig_1">2</ref> explores the relationship between performance on the masked language modeling task (validataion perplexity) and contact prediction (Long Range P@L). A linear relationship exists between validation perplexity and contact precision for each model. Furthermore, for the same perplexity, the 12-layer ESM-1 model achieves the same long range P@L as the 34 layer ESM-1 model, suggesting that perplexity is a good proxy task for contact prediction. ESM-1 and ESM-1b models are trained with different masking patterns, so their perplexities cannot be directly compared, although a linear relationship is clearly visible in both. ESM-1 and ESM-1b have a similar number of parameters; the key difference is in their hyperparameters and architecture. The models shown have converged in pre-training, with minimal decrease in perplexity (or increase in contact precision) in the later epochs. This provides clear evidence that both model scale and hyperparameters play a significant role in a model's ability to learn contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">CALIBRATION, FALSE POSITIVES, AND ROBUSTNESS</head><p>One concern with large neural networks is that, while they may be accurate on average, they can also produce spurious results with high confidence. We investigate this possibility from several perspectives. First, we find that logistic regression probabilities are close to true contact probability (mean-squared error = 0.014) and can be used directly as a measure of the model's confidence (Figure <ref type="figure" target="#fig_8">12a</ref>). Second, we analyze the false positives that the model does predict. We find that these are very likely to be within a Manhattan distance of 1-4 of a true contact (Figure <ref type="figure" target="#fig_8">12b</ref>). This suggests that false positives may arise due to the way a contact is defined (Cb-Cb distance within 8 angstroms), and could be marked as true contacts under a different definition <ref type="bibr">(Zheng &amp; Grigoryan, 2017)</ref>. Further, when we explore an example where the model's predictions are not near a true contact, we see that the example in question is a homodimer, and that the model is picking up on inter-chain interactions (Figure <ref type="figure" target="#fig_9">13a</ref>). While these do not determine the structure of the monomer, they are important for its function <ref type="bibr" target="#b4">(Anishchenko et al., 2017)</ref>.</p><p>Third, we test the robustness of the model to insertions by inserting consecutive alanines at the beginning, middle, or end of 1000 randomly chosen sequences. We find that ESM-1b can tolerate up to 256 insertions at the beginning or end of the sequence and up to 64 insertions in the middle of the sequence before performance starts to significantly degrade. This suggests that ESM-1b learns a robust implicit alignment of the protein sequence. See Section A.12 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">MSA GENERATION</head><p>Wang &amp; Cho (2019) note that Transformers trained with the MLM objective can be used generatively. Here, we consider whether generations from ESM-1b preserve contact information for a given protein. The ability to generate sequences that preserve this information is a necessary condition for generation of biologically active proteins. We perform this evaluation by taking an input protein, masking out several positions, and re-predicting them. This process is repeated 10000 times to generate a pseudo-MSA for the input sequence (see Algorithm 1). We feed the resulting MSA into Gremlin to predict contacts. Over all sequences from our test set, this procedure results in a long range contact P@L of 14.5. Figure <ref type="figure" target="#fig_11">16</ref> shows one example where the procedure works well, with Gremlin on the pseudo-MSA having long range P@L of 52.2. For comparison, the standard ESM-1b pipeline achieves a contact precision of 76.7 on this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Transformer protein language models trained with an unsupervised objective learn the tertiary structure of a protein sequence in their attention maps. Residue-residue contacts can be extracted from the attention by sparse logistic regression. Attention heads are found that specialize in different types of contacts. An ablation analysis confirms that the contacts are learned in the unsupervised phase, and that the logistic regression is only necessary to extract the part of the model that represents the contacts.</p><p>These results have implications for protein structure determination and design. The initial studies proposing Transformers for protein language modeling showed that representation learning could be used to derive state-of-the-art features across a variety of tasks, but were not able to show a benefit in the fully end-to-end setting <ref type="bibr" target="#b36">(Rives et al., 2019;</ref><ref type="bibr">Rao et al., 2019;</ref><ref type="bibr" target="#b13">Elnaggar et al., 2020)</ref>. For the first time, we show that protein language models can outperform state-of-the-art unsupervised structure learning methods that have been intensively researched and optimized over decades.</p><p>Finally, we establish a link between language modeling perplexity and unsupervised structure learning. A similar scaling law has been observed previously for supervised secondary structure prediction <ref type="bibr" target="#b36">(Rives et al., 2019)</ref>, and parallels observations in the NLP community <ref type="bibr" target="#b22">(Kaplan et al., 2020;</ref><ref type="bibr">Brown et al., 2020)</ref>. Evidence of scaling laws for protein language modeling support future promise as models and data continue to grow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 NOTATION</p><p>In the figures, we report contact precision in the range of 0.0 to 1.0. In the text and in the tables, we report contact precision in terms of percentages, in the range of 0 to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 AVERAGE PRODUCT CORRECTION (APC)</head><p>In protein contact prediction, APC is commonly used to correct for background effects of entropy and phylogeny <ref type="bibr" target="#b11">(Dunn et al., 2008)</ref>. Given an L × L coupling matrix F , APC is defined as</p><formula xml:id="formula_3">F APC ij = F ij − F i F j F (3)</formula><p>Where F i , F j , and F are the sum over the i-th row, j-th column, and the full matrix respectively. We apply APC independently to the symmetrized attention maps of each head in the Transformer. These corrected attention maps are passed in as input to a logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 GREMLIN IMPLEMENTATION DETAILS</head><p>Gremlin is trained by optimizing the pseudolikelihood of W and V , which correspond to pairwise and individual amino acid propensities. The pseudolikelihood approximation models the conditional distributions of the original joint distribution and can be written:</p><formula xml:id="formula_4">log p(x d i = a|x d j =i ; W i , V i ) = log exp V ia + N j=1,j =i 20 b=1 1(x d j = b)W ijab 20 c=1 exp V ic + N j=1,j =i 20 b=1 1(x d j = b)W ijcb (4)</formula><p>subject to the constraint that W ii = 0 for all i, and that W ijab is symmetric in both sequence (i, j) and amino acid (a, b). Additionally, Gremlin uses a regularization parameter that is adjusted based on the depth of the MSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ESM-1 IMPLEMENTATION DETAILS</head><p>The original ESM-1 models were described in <ref type="bibr" target="#b36">(Rives et al., 2019)</ref>. ESM-1 is trained on Uniref50 in contrast to the TAPE model, which is trained on Pfam <ref type="bibr" target="#b15">(Finn et al., 2014)</ref> and the ProtBERT-BFD model, which is trained on Uniref100 and BFD100 <ref type="bibr" target="#b43">(Steinegger et al., 2019)</ref> segmentation fault varies depending on the input sequence). Since we see this failure for less than 1% of the dataset we choose to ignore these sequences during evaluation.</p><p>Additionally, we evaluated alternate MSAs by running Jackhmmer until a Neff of 128 was achieved (with a maximum of 8 iterations), a procedure described by <ref type="bibr" target="#b56">Zhang et al. (2020)</ref>. This resulted in very similar, but slightly worse results (average long range P@L 29.3, versus 31.3 when always using the output of the eighth iteration). We therefore chose to report results using the 8 iteration maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 RESULTS ON CASP13</head><p>In Table <ref type="table" target="#tab_3">4</ref> we report results on the 15 CASP13 Free Modeling targets for which PDBs were publicly released. The specific domains evaluated are: T0950-D1, T0957s2-D1, T0960-D2, T0963-D2, T0968s1-D1, T0968s2-D1, T0969-D1, T0980s1-D1, T0986s2-D1, T0990-D1, T0990-D3, T1000-D2, T1021s3-D1, T1021s3-D2, T1022s1-D1. ESM-1b is able to outperform Gremlin, and simply averaging the top-10 heads of ESM-1b has comparable performance to Gremlin.</p><p>In addition, we compare our logistic regression model to the bilinear contact prediction model proposed by <ref type="bibr" target="#b37">Rives et al. (2020)</ref>. This model trains two separate linear projections of the final representation layer and computes contact probabilities via the outer product of the two projections plus a bias term, which generates the following unnormalized log probability:</p><formula xml:id="formula_5">log p(contact) ∝ (xW 1 )(xW 2 ) T + b<label>(5)</label></formula><p>Here x is a sequence-length vector of features in R L×d . Each W i is a matrix in R d×k , where k is a hyperparameter controlling the projection size.</p><p>We train this model in both the limited supervision (n = 20) and full supervision (n = 14257) setting. For the limited supervision setting, we use the same 20 proteins used to train the sparse logistic regression model. For the full supervision setting we generate a 95/5% random training/validation split of the 15008 trRosetta proteins with sequence length ≤ 1024.</p><p>We performed independent grid searches over learning rate, weight decay, and hidden size for the two settings. For the n = 20 setting, we found a learning rate of 0.001, weight decay of 10.0, and projection size of 512 had best performance on the validation set. For the n = 14257 setting we found a learning rate of 0.001, weight decay of 0.01, and projection size of 512 had best performance on the validation set. All models were trained to convergence to maximize validation long range P@L with a patience of 10. The n = 20 models were trained with a batch size of 20 (i.e. 1 batch = 1 epoch) and the n = 14257 models were trained with a batch size of 128.</p><p>The bilinear model performs very poorly in the limited supervision setting, worse than simply taking the top-1 attention head. With full supervision, it moderately outperforms the logistic regression for an increase in long range P@L of 1.5 while using 700x more data.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref> we display results on the 15 FM targets colored by effective number of sequences. ESM-1b shows higher precision at L and L/5 on average, and is sometimes significantly higher for sequences with low Neff. Since ESM-1b training data was generated prior to CASP13, this suggests ESM-1b is able to generalize well to new sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 LOGISTIC REGRESSION DETAILS</head><p>Given a model with M layers, H heads, and an input sequence x of length L, let A mh be the L × L contact map from the h-th head in the m-th layer. We first symmetrize this map and apply APC and let a mhij be the coupling weight between sequence position i and j in the resulting map. Then we define the probability of a contact between positions i and j according to a logistic regression with parameters β:</p><formula xml:id="formula_6">p(c d ij ; β) = 1 1 + exp − β 0 − M m=1 H h=1 β mh a d mhij<label>(6)</label></formula><p>To fit β, let D be a set of training proteins, k be a minimum sequence separation, and λ be a regularization weight. The objective can then be defined as follows:</p><formula xml:id="formula_7">L(D; β) = d∈D L d −k i=1 L d j=i+k p(c d ij ; β)<label>(7)</label></formula><formula xml:id="formula_8">β = max β L(D; β) + 1 λ M m=1 H h=1 |β mh |<label>(8)</label></formula><p>We fit the parameters β via scikit-learn <ref type="bibr" target="#b32">(Pedregosa et al., 2011)</ref> and do not backpropagate the gradients through the attention weights. In total, our model learns M H + 1 parameters, many of which are zero thanks to the L 1 regularization.</p><p>There are three hyperparameters in our training setup: the number of proteins in our training set D, the regularization parameter λ, and the minimum sequence separation of training contacts k. We find that performance improves significantly when increasing the D from 1 protein to 10 proteins, but that the performance gains drop off when D increases from 10 to 20 (Figure <ref type="figure" target="#fig_0">1</ref>). Through a hyperparameter sweep, we determined that the optimal λ is 0.15. We find that ignoring local contacts (|i − j| &lt; 6) is also helpful. Therefore, unless otherwise specified, all logistic regressions are trained with |D| = 20, λ = 0.15, k = 6. See Figure <ref type="figure">6a</ref> for a gridsearch over the number of training proteins and regression penalty. We used 20 training proteins and 20 validation proteins for this gridsearch.</p><p>Figure <ref type="figure">6b</ref> shows the weights of the final logistic regression used for ESM-1b.</p><p>A.8 PERFORMANCE DISTRIBUTION</p><p>Figure <ref type="figure">7</ref> shows the full distribution of performance of ESM-1b compared with Gremlin. When we provide Gremlin access to Uniref100, along with metagenomic sequences, ESM-1b still consistenly outperforms Gremlin when extracting short and medium range contacts. For long range contacts, Gremlin is much more comparable, and has higher contact precision on 47% of sequences. With access to the same set of sequences, ESM-1b consistently outperforms Gremlin in detecting short, medium, and long range contacts. This suggests that ESM-1b can much better extract information from the same set of sequences and suggests that further scaling of training data may improve ESM-1b even further.</p><p>This analysis is further borne out in Figure <ref type="figure">8</ref>. Given the same set of sequences, ESM-1b outperforms Gremlin on average for short, medium, and long-range contacts regardless of the depth of the MSA generated from the ESM-1b training set.</p><p>Additionally, we find that ESM-1b can provide varying contact maps for different sequences in the MSA (Figure <ref type="figure" target="#fig_6">9</ref>). This is not possible for Gremlin, which is a family-level model. We leverage this in a fairly simple way to provide a modest boost to the contact precision of ESM-1b (Section 5.2).</p><p>A.9 SECONDARY STRUCTURE In Section 5.4 we show that some heads that detect local contacts (which often correspond to secondary structure) are actually negatively correlated with long range contacts. We test ESM-1b's ability to detect secondary structure via attention by training a separate logistic regression on the Netsurf dataset <ref type="bibr" target="#b23">(Klausen et al., 2019)</ref>. As with the logistic regression on contacts, we compute attentions and perform APC + symmetrization. To predict the secondary structure of amino acid i, we feed as input the couplings a mhij for each layer m, for each head h, and for j ∈ [i − 5, i + 5], for a total of 7260 input features. Using just 100 of the 8678 training proteins, we achieve 79.3% accuracy on 3-class secondary structure prediction on the CB513 test set <ref type="bibr" target="#b9">(Cuff &amp; Barton, 1999)</ref>. Figure <ref type="figure" target="#fig_0">10</ref> shows the importance of each layer to predicting the three secondary structure classes. There are . spikes in different layers for all three classes, indicating that particular heads within those layers are specializing in detecting specific classes of secondary structure. Figure <ref type="figure" target="#fig_0">10</ref> shows importance of each Transformer layer to predicting each of the three secondary structure classes. We see that, as with contact prediction, the most important layers are in the middle layers (14-20) and the final layers (29-33). Some layers spike more heavily on particular contact classes (e.g. layer 33 is important for all classes, but particularly important for β-strand prediction). This suggests that particular heads within these layers activate specifically for certain types of secondary structure.   A.11 MODEL CALIBRATION AND FALSE POSITIVES <ref type="bibr">Vig et al. (2020)</ref> suggested that the attention probability from the TAPE Transformer was a wellcalibrated estimator for the probability of a contact. In Figure <ref type="figure" target="#fig_8">12a</ref> we examine the same with the logistic regression trained on the ESM-1 and ESM-1b models. We note that ESM-1b, in addition to being more accurate overall than Gremlin, also provides actual probabilities.</p><p>We find that as with model accuracy, model calibration increases with larger scale and better hyperparameters. The 6, 12, and 34 layer ESM-1 models have mean-squared error of 0.074, 0.028, and 0.020 between predicted and actual contact probabilities, respectively. ESM-1b has a mean squared error of 0.014. Mean squared error is computed between contact probabilites split into 20 bins according to the scikit-learn calibration curve function. It is therefore reasonable to use the logistic regression probability as a measure of the model's confidence.</p><p>In the case of false positive contacts we attempt to measure the Manhattan distance between the coordinates of predicted contacts and the nearest true contact (Figure <ref type="figure" target="#fig_8">12b</ref>). We observe that the Manhattan distance between the coordinates of false positive contacts are often very close (Manhattan distance between 1-4) to real contacts, and that very few false positives have a Manhattan distance ≥ 10 from a true contact. With a threshold contact probability of 0.5, 83.8% of proteins have at least one predict contact with Manhattan distance &gt; 4 to the nearest contact. This drops to 71.7% with a threshold probability of 0.7, and to 52.5% with a threshold probability of 0.9.</p><p>Figure <ref type="figure" target="#fig_9">13</ref> highlights two modes for ESM-1b where signficant numbers of spurious contacts are predicted. Figure <ref type="figure" target="#fig_9">13a</ref> shows example where the model does appear to hallucinate contacts around residues 215 and 415, which do not appear in the contact map for this protein. However, this protein is a homodimer and these contacts are present in the inter-chain contact map. This suggests that some 'highly incorrect' false positives may instead be picking up on inter-chain contacts. Figure <ref type="figure" target="#fig_9">13b</ref> shows an example of a repeat protein, for which evolutionary coupling methods are known to pick up on additional 'bands' of contacts <ref type="bibr" target="#b14">(Espada et al., 2015;</ref><ref type="bibr" target="#b4">Anishchenko et al., 2017)</ref>. Multiple bands are visible in the Gremlin contact map, while only the first band, closest to the diagonal, is visible in the ESM-1b contact map. More analysis would be necessary to determine the frequency of these modes, along with additional potential modes.</p><p>A.12 ALIGNMENT One hypothesis as to the benefit of large language models as opposed to simpler Potts models is that they may be able to learn an implicit alignment due to their learned positional embedding. For Potts Model, an alignment enables a model to relate positions in the sequence given evolutionary context despite the presence of insertions or deletions. We test the robustness of the model to insertions by inserting consecutive alanines at the beginning, middle, or end of 1000 randomly chosen sequences with initial sequence length &lt; 512 (we limit initial sequence length in order to avoid out-of-memory issues after insertion). We find that ESM-1b can tolerate up to 256 insertions at the beginning or end of the sequence and up to 64 insertions in the middle of the sequence before performance starts to significantly degrade. This suggests that ESM-1b learns a robust implicit alignment of the protein sequence.</p><p>On the other hand, we find that the TAPE Transformer is less robust to insertions. On one sequence (pdbid: 1a27), we find the TAPE Transformer drops in precision by 12 percentage points after adding just 8 alanines to the beginning of the sequence, while ESM-1b sees minimal degradation until 256 alanines are inserted. We hypothesize that, because TAPE was trained on protein domains, it did not learn to deal with mis-alignments in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13 EVOLUTIONARY FINETUNING DETAILS</head><p>We finetuned each model using a learning rate of 1e-4, 16k warmup updates, an inverse square root learning rate schedule, and a maximum of 30 epochs. This resulted in a varying number of total updates depending on the size of the MSA, with larger MSAs being allowed to train for more updates. This should ideally help prevent the model from overfitting too quickly on very small MSAs. We use a variable batch size based on the length of the input proteins, fixing a maximum of 16384 tokens per batch (so for a length 300 protein this would correspond to a batch size of 54). We use MSAs from trRosetta for finetuning all proteins with the exception of avGFP, where we use the same set of sequences from <ref type="bibr" target="#b2">Alley et al. (2019)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Contact prediction pipeline. The Transformer is first pretrained on sequences from a large database (Uniref50) via Masked Language Modeling. Once finished training, the attention maps are extracted, passed through symmetrization and average product correction, then into a regression. The regression is trained on a small number (n ≤ 20) of proteins to determine which attention heads are informative. At test time, contact prediction from an input sequence can be done entirely on GPU in a single forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Language modeling validation perplexity on holdout of Uniref50 vs. contact precision over the course of pre-training. ESM-1b was trained with different masking so perplexities between the versions are not comparable. Right: Long range P@L performance distribution of ESM-1b vs. Gremlin. Each point is colored by the log of the number of sequences in the MSA used to train Gremlin.</figDesc><graphic url="image-16.png" coords="7,246.35,81.86,252.48,113.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Gremlin (trRosetta) performance binned by MSA depth. For comparison, ESM-1b performance is also shown for the sequences in each bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Logistic regression weights trained only on contacts in specific ranges: local [3, 6), short range [6, 12), medium range [12, 24), long range [24, ∞).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results on 15 CASP13 FM Domains colored by Neff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Short, medium, and long range P@L performance distribution of ESM-1b vs. Gremlin. Each point is colored by the log 2 of the number of sequences in the MSA.</figDesc><graphic url="image-20.png" coords="19,113.52,81.86,384.96,249.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Distribution of contact perplexity when evaluating different sequences from the same MSA. The x-axis shows the index of each sequence, sorted in ascending order by hamming distance the query sequence (query sequence is always index 0). The y-axis shows long range P@L. The black line indicates Gremlin performance on that MSA..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>AFigure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: L 2 norm of weights for 3-class secondary structure prediction by Transformer layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: (a) Calibrated probability of a real contact given predicted probability of contact over all test proteins. (b) Distribution of Manhattan distance between the coordinates of predicted contacts and the nearest true contact at various thresholds of minimum p(contact). A distance of zero corresponds to a true contact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Illustration of two modes for ESM-1b where significant numbers of spurious contacts are predicted. (a) Predicted contacts which do occur in the full homodimer complex, but are not present as intra-chain contacts. (b) CTCF protein contacts. A small band of contacts near the 30-residue offdiagonal is predicted by ESM-1b. This band, along with additional similar bands are also predicted by Gremlin.</figDesc><graphic url="image-25.png" coords="22,156.54,232.22,81.60,81.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Robustness of ESM-1b and TAPE models to insertions of Alanine at the beginning, middle, and end of sequence .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure15: Left: Average change in contact precision vs. number of finetuning epochs over 380 proteins. Right: Real and predicted contacts before and after evolutionary finetuning for 1a3a and avGFP. For 1a3a, long range P@L improves from 54.5 to 61.4. For avGFP, long range P@L improves from 7.9 to 11.4.</figDesc><graphic url="image-32.png" coords="24,250.69,248.38,66.84,66.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average precision on 14842 test structures for Transformer models trained on 20 structures.</figDesc><table><row><cell>Model</cell><cell>6 ≤ sep &lt; 12 L L/2 L/5</cell><cell>12 ≤ sep &lt; 24 L L/2 L/5</cell><cell>L</cell><cell>24 ≤ sep L/2</cell><cell>L/5</cell></row><row><cell cols="6">Gremlin (ESM Data) mfDCA (trRosetta Data) PSICOV 1 (trRosetta Data) 15.4 23.6 39.2 18.3 28.4 45.7 32.6 45.2 58.1 15.2 23.0 37.8 18.1 27.9 44.3 31.3 43.1 55.5 16.3 23.7 35.8 19.7 29.8 45.5 33.0 43.5 54.2 Gremlin (trRosetta Data) 17.2 26.7 44.4 21.1 33.3 52.3 39.3 52.2 62.8</cell></row><row><cell>TAPE ProtBERT-BFD ESM-1 (6 layer) ESM-1 (12 layer) ESM-1 (34 layer) ESM-1b</cell><cell cols="5">9.9 12.3 16.4 10.0 12.6 16.6 11.2 14.0 17.9 20.4 30.7 48.4 24.3 35.5 52.0 34.1 45.0 57.4 11.0 13.2 15.9 11.5 14.6 19.0 13.2 16.7 21.5 15.2 21.1 30.5 18.1 24.7 34.0 23.7 30.5 39.3 20.3 30.2 46.0 23.8 34.3 49.2 34.7 44.6 56.0 21.6 33.2 52.7 26.2 38.6 56.4 41.1 53.3 66.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc> Ablations with limited supervision and with MSA information. n is the number of logistic regression training proteins. s is the number of sequences ensembled over.</figDesc><table><row><cell>Model</cell><cell>Variant</cell><cell>6 ≤ sep &lt; 12 L L/2 L/5</cell><cell>12 ≤ sep &lt; 24 L L/2 L/5</cell><cell>L</cell><cell>24 ≤ sep L/2</cell><cell>L/5</cell></row><row><cell>Gremlin</cell><cell cols="6">ESM Data trRosetta Data 17.2 26.7 44.4 21.1 33.3 52.3 39.3 52.2 62.8 15.2 23.0 37.8 18.1 27.9 44.3 31.3 43.1 55.5</cell></row><row><cell>ESM-1b (Ablations)</cell><cell>top-1 heads top-5 heads top-10 heads n=1, s=1 n=10, s=1 n=20, s=1 MSA, s=1</cell><cell cols="5">16.8 23.4 34.8 19.8 27.6 40.2 29.3 38.1 50.0 19.2 28.5 44.5 23.3 33.8 49.0 35.0 45.2 57.3 20.0 30.1 47.4 24.7 36.0 52.2 38.5 49.4 61.1 19.4 29.7 47.1 25.1 37.1 54.0 39.2 50.6 63.0 21.4 32.9 52.3 26.1 38.5 56.4 40.8 52.9 65.7 21.6 33.2 52.7 26.2 38.6 56.4 41.1 53.3 66.1 18.4 28.1 45.5 23.9 36.1 53.7 39.9 51.3 63.0</cell></row><row><cell>ESM-1b (s seqs)</cell><cell>n=20, s=16 n=20, s=32 n=20, s=64</cell><cell cols="5">21.9 33.8 53.6 26.7 39.4 57.5 41.9 54.3 67.3 22.0 34.1 54.0 26.9 39.8 58.1 42.3 54.8 67.8 22.1 34.3 54.3 27.1 40.1 58.5 42.6 55.1 68.2</cell></row></table><note>and in particular can significantly outperform on MSAs with low effective number of sequences.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Major Architecture Differences in Protein Transformer Language Models</figDesc><table><row><cell>Name TAPE ProtBERT-BFD ESM-1 (6 layer) ESM-1 (12 layer) ESM-1 (34 layer) ESM-1b</cell><cell>Layers Hidden Size Attn Heads Parameters 12 768 12 92M 30 1024 16 420M 6 768 12 43M 12 768 12 85M 34 1280 20 670M 33 1280 20 650M</cell><cell>Dataset Pfam BFD100 Uniref50 Uniref50 Uniref50 Uniref50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>. Recently, the authors have made ESM-1b available, which is a robustly optimized version of the largest ESM-1 model. ESM-1b is the result of an extensive hyperparameter sweep that was performed on smaller 12 layer models. ESM-1b is the result of scaling up that model to 33 layers. Average metrics on 15 CASP13 FM Targets. All baselines use MSAs generated via the trRosetta MSA generation approach.</figDesc><table><row><cell>Compared to ESM-1, the main changes in ESM-1b are: higher learning rate; dropout after word embedding; learned positional embeddings; final layer norm before the output; and tied input/output word embeddings.</cell></row><row><cell>A.5 JACKHMMER DETAILS</cell></row><row><cell>We use Jackhmmer version 3.3.1 with a bitscore threshold of 27 and 8 iterations to construct MSAs</cell></row></table><note>from the ESM training set. The failures on 126 sequences noted in Section 4.4 result from a segmentation fault in hmmbuild after several iterations (the number of successful iterations before the</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">PSICOV fails to converge on</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">/ 15 targets with default parameters. We follow the procedure suggested in https://github.com/psipred/psicov to increase rho to 0.005 for those domains.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Under review as a conference paper at ICLR 2021 A.14 MSA GENERATION Result: Generated MSA input // protein sequence curr = input // optionally, the input can be repeated for batching for 0 ≤ i &lt; 10000 do masked = mask 20% of positions in curr; pred = model(masked); curr[masked positions] = pred[masked positions]; MSA.append(curr); if random() &lt; 0.1 then curr = input; end end Algorithm 1: Quickly generate a pseudo-MSA from an input sequence.</p><p>Algorithm 1 presents the algorithm used to generate pseudo-MSAs from ESM-1b. Each pseudo-MSA is passed to GREMLIN in order to evaluate the preservation of contact information (Figure <ref type="figure">16</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein residue contacts and prediction methods</title>
		<author>
			<persName><forename type="first">Badri</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4939-3572-724</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/27115648/" />
	</analytic>
	<monogr>
		<title level="m">Methods in Molecular Biology</title>
				<imprint>
			<publisher>Humana Press Inc</publisher>
			<date type="published" when="2016-08">aug 2016</date>
			<biblScope unit="volume">1415</biblScope>
			<biblScope unit="page" from="463" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DEEPCON: Protein contact prediction using dilated convolutional neural networks with dropout</title>
		<author>
			<persName><forename type="first">Badri</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Elofsson</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz593</idno>
		<ptr target="https://academic.oup.com/bioinformatics/article/36/2/470/5540673" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="470" to="477" />
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1101/589333v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/589333v1" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end differentiable learning of protein structure</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<idno type="DOI">10.1101/265231</idno>
		<ptr target="https://www.biorxiv.org/content/early/2018/08/29/265231" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">265231</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Origins of coevolution between residues distant in protein 3D structures. Proceedings of the National Academy of Sciences of the United States of America</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1702664114</idno>
		<ptr target="https://www.pnas.org/content/early/2017/08/03/1702664114https://www.pnas.org/content/early/2017/08/03/1702664114.abstract" />
		<imprint>
			<date type="published" when="2017">8 2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="9122" to="9127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning generative models for protein fold families</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langmead</forename></persName>
		</author>
		<idno type="DOI">10.1002/prot.22934</idno>
		<ptr target="http://doi.wiley.com/10.1002/prot.22934" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1078" />
			<date type="published" when="2011">4 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.08661https://arxiv.org/abs/1902.08661" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic sequence entanglement augments stability and containment of genetic information in cells</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Blazejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsing-I</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6453</biblScope>
			<biblScope unit="page" from="595" to="598" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-0134(19990301)34:4508</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/10081963/" />
	</analytic>
	<monogr>
		<title level="m">AID-PROT10 3.0.CO;2-4</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">Minnesota, 6 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Gloor</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm604</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: Using pseudolikelihoods to infer potts models</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Lövkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueheng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Aurell</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.87.012707</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevE.87.012707" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">12707</biblScope>
			<date type="published" when="2013-01">Jan 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.06225" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Capturing coevolutionary signals inrepeat proteins</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gonzalo</forename><surname>Rocío Espada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><forename type="middle">M</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">U</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName><surname>Ferreiro</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0648-3</idno>
		<ptr target="http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0648-3" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pfam: The protein families database</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jody</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penelope</forename><surname>Coggill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><forename type="middle">Y</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirstie</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liisa</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">L L</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Punta</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3965110/" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling the language of life -Deep Learning Protein Sequences</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1101/614313v3</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/614313v3" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">614313</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning protein structure with a differentiable simulator</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019, 0 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hidden Markov model speed heuristic and iterative HMM search procedure</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-11-431</idno>
		<ptr target="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-431" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2010">8 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High precision in protein contact prediction using fully convolutional neural networks and minimal sequence features</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">M</forename><surname>Kandathil</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty341</idno>
		<ptr target="https://academic.oup.com/bioinformatics/article/34/19/3308/4987145" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
			<date type="published" when="2018-10">oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Cozzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Pontil</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btr638</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btr638" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="190" />
			<date type="published" when="2011">11 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing the utility of coevolutionbased residue-residue contact predictions in a sequence-and structure-rich era</title>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1314045110</idno>
		<ptr target="https://www.pnas.org/content/110/39/15674" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="15674" to="15679" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schantz Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Closter Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamilla</forename><surname>Kjaergaard Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><forename type="middle">Isabell</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bent</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Marcatili</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25674</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.25674" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<idno type="ISSN">0887-3585</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">De novo structure prediction of globular proteins aided by sequence variation-derived contacts</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0092197</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0092197" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correlated mutations in models of protein sequences: Phylogenetic and structural effects</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><forename type="middle">G</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lonchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Stormoo</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/4356049" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes-Monograph Series</title>
		<idno type="ISSN">07492170</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="236" to="256" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Palmedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cels.2017.11.014</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/29275173/" />
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2018-01">jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ProGen: Language Modeling for Protein Generation</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Ssu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.03497" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Protein 3d structure computed from evolutionary sequence variation</title>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0028766</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0028766" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName><forename type="first">Faruck</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bertolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">N</forename><surname>Onuchic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigt</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1111471108</idno>
		<ptr target="https://www.pnas.org/content/108/49/E1293" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="E1293" to="E1301" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved de novo structure prediction in casp11 by incorporating coevolution information into rosetta</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ruei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.24974</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.24974" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<ptr target="http://scikit-learn.sourceforge.net" />
	</analytic>
	<monogr>
		<title level="m">Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine Learning in Python</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating Protein Transfer Learning with TAPE</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1101/676825</idno>
		<ptr target="https://doi.org/10.1101/676825http://arxiv.org/abs/1906.08230" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems. Cold Spring Harbor Laboratory</title>
				<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.1818</idno>
		<ptr target="http://www.nature.com/articles/nmeth.1818" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<idno type="ISSN">1548-7091</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2012">2 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep generative models of genetic variation capture the effects of mutations</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-018-0138-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1101/622803</idno>
		<ptr target="https://www.biorxiv.org/content/early/2019/04/29/622803" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1101/622803</idno>
		<ptr target="https://www.biorxiv.org/content/early/2020/08/31/622803" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An evolution-based model for designing chorismate mutase enzymes</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>William P Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Figliuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Barrat-Charlaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Socolich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Kast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Hilvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simona</forename><surname>Monasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cocco</surname></persName>
		</author>
		<author>
			<persName><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">6502</biblScope>
			<biblScope unit="page" from="440" to="445" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Seemayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu500</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1460-2059</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3128" to="3130" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<idno type="DOI">10.1093/bioinformatics/btu500</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/25064567http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4201158https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu500" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">W R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Penedones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Crossan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25834</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.25834" />
		<title level="m">Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of Protein Structure Prediction (CASP13). Proteins: Structure, Function, and Bioinformatics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1141" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">W R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Penedones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Crossan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-019-1923-7</idno>
		<ptr target="https://doi.org/10.1038/s41586-019-1923-7" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milot</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0437-4</idno>
		<ptr target="https://doi.org/10.1038/s41592-019-0437-4" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<idno type="ISSN">1548-7105</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="603" to="606" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Genomics-aided structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Sulkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Onuchic</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1207864109</idno>
		<ptr target="http://www.pnas.org/cgi/doi/10.1073/pnas.1207864109" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="10340" to="10345" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">UniRef: Comprehensive and non-redundant UniProt reference clusters</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm098</idno>
		<ptr target="http://www.uniprot.org" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>UniRef50 database licensed under (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Assessment of CASP10 contactassisted predictions</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><forename type="middle">Hsien</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byungkook</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.24367</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6961783/" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="97" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graphical models of residue coupling in protein families</title>
		<author>
			<persName><forename type="first">John</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/18451428/" />
		<imprint>
			<date type="published" when="2008-04">apr 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Co-evolutionary fitness landscapes for sequence design</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Baber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Aniana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Best</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Angewandte Chemie International Edition</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5674" to="5678" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="DOI">10.1101/2020.06.26.174417</idno>
		<ptr target="http://arxiv.org/abs/2006.15222" />
	</analytic>
	<monogr>
		<title level="j">BERTology Meets Biology: Interpreting Attention in Protein Language Models. bioRxiv</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.04094" />
		<title level="m">BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate de novo prediction of protein contact map by ultra-deep learning model</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1005324</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1005324" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Identification of direct residue contacts in protein-protein interaction by message passing</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Szurmant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Hoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Hwa</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0805923106</idno>
		<ptr target="https://www.pnas.org/content/106/1/67https://www.pnas.org/content/106/1/67.abstract" />
		<imprint>
			<date type="published" when="2009-01">jan 2009</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction by deep learning irrespective of co-evolution information</title>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcpartlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.10.12.336859v1https://www.biorxiv.org/content/10.1101/2020.10.12.336859v1.abstract</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2020.10.12.336859v1https://www.biorxiv.org/content/10.1101/2020.10.12.336859v1.abstract" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020-10">oct 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted inter-residue orientations</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahnbeom</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenling</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Harvard</surname></persName>
		</author>
		<idno type="DOI">10.1101/846279v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/846279v1" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">846279</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DeepMSA: constructing deep multiple sequence alignment to improve contact prediction and fold-recognition for distant-homology proteins</title>
		<author>
			<persName><forename type="first">Chengxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S M</forename><surname>Mortuza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0178272</idno>
		<idno>ISSN 19326203. doi: 10.1371/journal.pone. 0178272</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0178272" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e0178272</biblScope>
			<date type="published" when="2017-05">apr 2020. 5 2017</date>
		</imprint>
	</monogr>
	<note>PLoS ONE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
