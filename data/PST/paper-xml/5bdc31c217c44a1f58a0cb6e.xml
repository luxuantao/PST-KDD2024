<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actor-Critic Policy Optimization in Partially Observable Multiagent Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Pérolat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
							<email>bowlingm@.@google.com</email>
						</author>
						<title level="a" type="main">Actor-Critic Policy Optimization in Partially Observable Multiagent Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">27C656C15B4DA111E5486F66A81819A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero-sum games, without any domain-specific state space reductions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been much success in learning parameterized policies for sequential decision-making problems. One paradigm driving progress is deep reinforcement learning (Deep RL), which uses deep learning <ref type="bibr" target="#b51">[52]</ref> to train function approximators that represent policies, reward estimates, or both, to learn directly from experience and rewards <ref type="bibr" target="#b85">[85]</ref>. These techniques have learned to play Atari games beyond human-level <ref type="bibr" target="#b60">[60]</ref>, Go, chess, and shogi from scratch <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b81">81]</ref>, complex behaviors in 3D environments <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b36">37]</ref>, robotics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b73">73]</ref>, character animation <ref type="bibr" target="#b67">[67]</ref>, among others.</p><p>When multiple agents learn simultaneously, policy optimization becomes more complex. First, each agent's environment is non-stationary and naive approaches can be non-Markovian <ref type="bibr" target="#b58">[58]</ref>, violating the requirements of many traditional RL algorithms. Second, the optimization problem is not as clearly defined as maximizing one's own expected reward, because each agent's policy affects the others' optimization problems. Consequently, game-theoretic formalisms are often used as the basis for representing interactions and decision-making in multiagent systems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b64">64]</ref>.</p><p>Computer poker is a common multiagent benchmark domain. The presence of partial observability poses a challenge for traditional RL techniques that exploit the Markov property. Nonetheless, there has been steady progress in poker AI. Near-optimal solutions for heads-up limit Texas Hold'em were found with tabular methods using state aggregation, powered by policy iteration algorithms based on regret minimization <ref type="bibr" target="#b102">[102,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b11">12]</ref>. These approaches were founded on a basis of counterfactual regret minimization (CFR), which is the root of recent advances in no-limit, such as Libratus <ref type="bibr" target="#b15">[16]</ref> and DeepStack <ref type="bibr" target="#b61">[61]</ref>. However, (i) both required Poker-specific domain knowledge, and (ii) neither were model-free, and hence are unable to learn directly from experience, without look-ahead search using a perfect model of the environment.</p><p>In this paper, we study the problem of multiagent reinforcement learning in adversarial games with partial observability, with a focus on the model-free case where agents (a) do not have a perfect description of their environment (and hence cannot do a priori planning), (b) learn purely from their own experience without explicitly modeling the environment or other players. We show that actor-critics reduce to a form of regret minimization and propose several policy update rules inspired by this connection. We then analyze the convergence properties and present experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>We briefly describe the necessary background. While we draw on game-theoretic formalisms, we align our terminology with RL. We include clarifications in Appendix A <ref type="foot" target="#foot_0">1</ref> . For details, see <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b85">85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning and Policy Gradient Algorithms</head><p>An agent acts by taking actions a ∈ A in states s ∈ S from their policy π : s → ∆(A), where ∆(X) is the set of probability distributions over X, which results in changing the state of the environment s t+1 ∼ T (s t , a t ); the agent then receives an observation o(s t , a t , s t+1 ) ∈ Ω and reward R t . <ref type="foot" target="#foot_1">2</ref> A sum of rewards is a return G t = ∞ t =t R t , and aim to find π * that maximizes expected return E π [G 0 ]. <ref type="foot" target="#foot_2">3</ref>Value-based solution methods achieve this by computing estimates of</p><formula xml:id="formula_0">v π (s) = E π [G t | S t = s], or q π (s, a) = E π [G t | S t = s, A t =</formula><p>a], using temporal difference learning to bootstrap from other estimates, and produce a series of -greedy policies π(s, a) = /|A| + (1 -)I(a = argmax a q π (s, a )). In contrast, policy gradient methods define a score function J(π θ ) of some parameterized (and differentiable) policy π θ with parameters θ, and use gradient ascent directly on J(π θ ) to update θ.</p><p>There have been several recent successful applications of policy gradient algorithms in complex domains such as self-play learning in AlphaGo <ref type="bibr" target="#b80">[80]</ref>, Atari and 3D maze navigation <ref type="bibr" target="#b59">[59]</ref>, continuous control problems <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b20">21]</ref>, robotics <ref type="bibr" target="#b26">[27]</ref>, and autonomous driving <ref type="bibr" target="#b78">[78]</ref>. At the core of several recent state-of-the-art Deep RL algorithms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b21">22]</ref> is the advantage actor-critic (A2C) algorithm defined in <ref type="bibr" target="#b59">[59]</ref>. In addition to learning a policy (actor), A2C learns a parameterized critic: an estimate of v π (s), which it then uses both to estimate the remaining return after k steps, and as a control variate (i.e. baseline) that reduces the variance of the return estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Game Theory, Regret Minimization, and Multiagent Reinforcement Learning</head><p>In multiagent RL (MARL), n = |N | = |{1, 2, • • • , n}| agents interact within the same environment. At each step, each agent i takes an action, and the joint action a leads to a new state s t+1 ∼ T (s t , a t ); each player i receives their own separate observation o i (s t , a, s t+1 ) and reward r t,i . Each agent maximizes their own return G t,i , or their expected return which depends on the joint policy π.</p><p>Much work in classical MARL focuses on Markov games where the environment is fully observable and agents take actions simultaneously, which in some cases admit Bellman operators <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b103">103,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b69">69]</ref>. When the environment is partially observable, policies generally map to values and actions from agents' observation histories; even when the problem is cooperative, learning is hard <ref type="bibr" target="#b65">[65]</ref>.</p><p>We focus our attention to the setting of zero-sum games, where i∈N r t,i = 0. In this case, polynomial algorithms exist for finding optimal policies in finite tasks for the two-player case. The guarantees that Nash equilibrium provides are less clear for the (n &gt; 2)-player case, and finding one is hard <ref type="bibr" target="#b19">[20]</ref>. Despite this, regret minimization approaches are known to filter out dominated actions, and have empirically found good (e.g. competition-winning) strategies in this setting <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Partially observable environments require a few key definitions in order to define the notion of state. A history h ∈ H is a sequence of actions from all players including the environment taken from the start of an episode. The environment (also called "nature") is treated as a player with a fixed policy and there is a deterministic mapping from any h to the actual state of the environment. Define an information state, s t = {h ∈ H | player i's sequence of observations, o i,t &lt;t (s t , a t , s t +1 ), is consistent with h}<ref type="foot" target="#foot_3">4</ref> . So, s t includes histories leading to s t that are indistinguishable to player i; e.g. in Poker, the h ∈ s t differ only in the private cards dealt to opponents. A joint policy π is a Nash equilibrium if the incentive to deviate to a best response</p><formula xml:id="formula_1">δ i (π) = max π i E π i ,π-i [G 0,i ] -E π [G 0,i ] = 0 for each player i ∈ N ,</formula><p>where π -i is the set of i s opponents' policies. Otherwise, -equilibria are approximate, with = max i δ i (π). Regret minimization algorithms produce iterates whose average π reduces an upper bound on , measured using NASHCONV(π) = i δ i (π). Nash equilibrium is minimax-optimal in two-player zero-sum games: using one minimizes worst-case losses.</p><p>There are well-known links between learning, game theory and regret minimization <ref type="bibr" target="#b8">[9]</ref>. One method, counterfactual regret (CFR) minimization <ref type="bibr" target="#b102">[102]</ref>, has led to significant progress in Poker AI. Let η π (h t ) = t &lt;t π(s t , a t ), where h t h t is a prefix, h t ∈ s t , h t ∈ s t , be the reach probability of h under π from all policies' action choices. This can be split into player i's contribution and their opponents' (including nature's) contribution,</p><formula xml:id="formula_2">η π (h) = η π i (h)η π -i (h)</formula><p>. Suppose player i is to play at s: under perfect recall, player i remembers the sequence of their own states reached, which is the same for all h ∈ s, since they differ only in private information seen by opponent(s); as a result</p><formula xml:id="formula_3">∀h, h ∈ s, η π i (h) = η π i (h ) := η π i (s).</formula><p>For some history h and action a, we call h a prefix history h ha, where ha is the history h followed by action a; they may also be smaller, so h ha hab ⇒ h hab.</p><formula xml:id="formula_4">Let Z = {z ∈ H | z is terminal} and Z(s, a) = {(h, z) ∈ H × Z | h ∈ s, ha z}. CFR defines counterfactual values v c i (π, s t , a t ) = (h,z)∈Z(st,at) η π -i (h)η π i (z)u i (z), and v c i (π, s t ) = a π(s t , a)v c i (π, s t , a t )</formula><p>, where u i (z) is the return to player i along z, and accumulates</p><formula xml:id="formula_5">regrets REG i (π, s t , a ) = v c i (π, s t , a ) -v c i (π, s t )</formula><p>, producing new policies from cumulative regret using e.g. regret-matching <ref type="bibr" target="#b27">[28]</ref> or exponentially-weighted experts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>CFR is a policy iteration algorithm that computes the expected values by visiting every possible trajectory, described in detail in Appendix B. Monte Carlo CFR (MCCFR) samples trajectories using an exploratory behavior policy, computing unbiased estimates vc i (π, s t ) and REG i (π, s t ) corrected by importance sampling <ref type="bibr" target="#b48">[49]</ref>. Therefore, MCCFR is an off-policy Monte Carlo method. In one MCCFR variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as π -i enabling online regret minimization (player i can update their policy independent of π -i and T ).</p><p>There are two main problems with (MC)CFR methods: (i) significant variance is introduced by sampling (off-policy) since quantities are divided by reach probabilities, (ii) there is no generalization across states except through expert abstractions and/or forward simulation with a perfect model. We show that actor-critics address both problems and that they are a form of on-policy MCCFR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Most Closely Related Work</head><p>There is a rich history of policy gradient approaches in MARL. Early uses of gradient ascent showed that cyclical learning dynamics could arise, even in zero-sum matrix games <ref type="bibr" target="#b83">[83]</ref>. This was partly addressed by methods that used variable learning rates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>, policy prediction <ref type="bibr" target="#b99">[99]</ref>, and weighted updates <ref type="bibr" target="#b0">[1]</ref>. The main limitation with these classical works was scalability: there was no direct way to use function approximation, and empirical analyses focused almost exclusively on one-shot games.</p><p>Recent work on policy gradient approaches to MARL addresses scalability by using newer algorithms such as A3C or TRPO <ref type="bibr" target="#b76">[76]</ref>. However, they focus significantly less (if at all) on convergence guarantees. Naive approaches such as independent reinforcement learning fail to find optimal stochastic policies <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b31">32]</ref> and can overfit the training data <ref type="bibr" target="#b49">[50]</ref>. Much progress has been achieved for cooperative MARL: learning to communicate <ref type="bibr" target="#b50">[51]</ref>, Starcraft unit micromanagement <ref type="bibr" target="#b23">[24]</ref>, taxi fleet optimization <ref type="bibr" target="#b63">[63]</ref>, and autonomous driving <ref type="bibr" target="#b78">[78]</ref>. There has also been progress for mixed cooperative/competitive environments: using a centralized critic <ref type="bibr" target="#b57">[57]</ref>, learning to negotiate <ref type="bibr" target="#b17">[18]</ref>, anticipating/learning opponent responses in social dilemmas <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53]</ref>, and control in realistic physical environments <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. The most common methodology has been to train centrally (for decentralized execution), either having direct access to the other players' policy parameters or modeling them. As a result, assumptions are made about the other agents' policies, utilities, or learning mechanisms.</p><p>There are also methods that attempt to model the opponents <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>. Our methods do no such modeling, and can be classified in the "forget" category of the taxonomy proposed in <ref type="bibr" target="#b32">[33]</ref>: that is, due to its on-policy nature, actors and critics adapt to and learn mainly from new/current experience.</p><p>We focus on the model-free (and online) setting: other agents' policies are inaccessible; training is not separated from execution. Actor-critics were recently studied in this setting for multiagent games <ref type="bibr" target="#b68">[68]</ref>, whereas we focus on partially-observable environments; only tabular methods are known to converge. Fictitious Self-Play computes approximate best responses via RL <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, and can also be model-free. Regression CFR (RCFR) uses regression to estimate cumulative regrets from CFR <ref type="bibr" target="#b93">[93]</ref>. RCFR is closely related to Advantage Regret Minimization (ARM) <ref type="bibr" target="#b37">[38]</ref>. ARM <ref type="bibr" target="#b37">[38]</ref> shows regret estimation methods handle partial observability better than standard RL, but was not evaluated in multiagent environments. In contrast, we focus primarily on the multiagent setting.</p><p>3 Multiagent Actor-Critics: Advantages and Regrets CFR defines policy update rules from thresholded cumulative counterfactual regret:</p><formula xml:id="formula_6">TCREG i (K, s, a) = ( k∈{1,••• ,K} REG i (π k , s, a)) + ,</formula><p>where k is the number of iterations and (x) + = max(0, x). In CFR, regret matching updates a policy to be proportional to TCREG i (K, s, a).</p><p>On the other hand, REINFORCE <ref type="bibr" target="#b95">[95]</ref> samples trajectories and computes gradients for each state s t , updating θ toward ∇ θ log(s t , a t ; θ)G t . A baseline is often subtracted from the return: G t -v π (s t ), and policy gradients then become actor-critics, training π and v π separately. The log appears due to the fact that action a t is sampled from the policy, the value is divided by π(s t , a t ) to ensure the estimate is properly estimating the true expectation [85, Section 13.3], and ∇ θ π(s t , a t ; θ)/π(s t , a t , θ) = ∇ θ log π(s t , a t ; θ). One could instead train q π -based critics from states and actions. This leads to a q-based Policy Gradient (QPG) (also known as Mean Actor-Critic <ref type="bibr" target="#b4">[5]</ref>):</p><formula xml:id="formula_7">∇ QPG θ (s) = a [∇ θ π(s, a; θ)] q(s, a; w) - b π(s, b; θ)q(s, b, w) ,<label>(1)</label></formula><p>an advantage actor-critic algorithm differing from A2C in the (state-action) representation of the critics <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b96">96]</ref> and summing over actions similar to the all-action algorithms <ref type="bibr" target="#b86">[86,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>. Interpreting a π (s, a) = q π (s, a)b π(s, b)q π (s, b) as a regret, we can instead minimize a loss defined by an upper bound on the thresholded cumulative regret: k (a π k (s, a)) + ≥ ( k (a π k (s, a)) + , moving the policy toward a no-regret region. We call this Regret Policy Gradient (RPG):</p><formula xml:id="formula_8">∇ RPG θ (s) = - a ∇ θ q(s, a; w) - b π(s, b; θ)q(s, b; w) + .<label>(2)</label></formula><p>The minus sign on the front represents a switch from gradient ascent on the score to descent on the loss. Another way to implement an adaptation of the regret-matching rule is by weighting the policy gradient by the thresholded regret, which we call Regret Matching Policy Gradient (RMPG):</p><formula xml:id="formula_9">∇ RMPG θ (s) = a [∇ θ π(s, a; θ)] q(s, a; w) - b π(s, b; θ)q(s, b, w) + .<label>(3)</label></formula><p>In each case, the critic q(s t , a t ; w) is trained in the standard way, using 2 regression loss from sampled returns. The pseudo-code is given in Algorithm 2 in Appendix C. In Appendix F, we show that the QPG gradient is proportional to the RPG gradient at s:</p><formula xml:id="formula_10">∇ RPG θ (s) ∝ ∇ QPG θ (s).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis of Learning Dynamics on Normal-Form Games</head><p>The first question is whether any of these variants can converge to an equilibrium, even in the simplest case.  In Figure <ref type="figure" target="#fig_0">1</ref>, we see the similarity of the regret dynamics to replicator dynamics <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b75">75]</ref>. We also show the average policy dynamics and observe convergence to equilibrium in each game we tried, which is a known to be guaranteed in two-player zero-sum games using CFR, fictitious play <ref type="bibr" target="#b13">[14]</ref>, and continuous replicator dynamics <ref type="bibr" target="#b34">[35]</ref>. However, computing the average policy is complex <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b102">102]</ref> and potentially worse with function approximation, requiring storing past data in large buffers <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Partially Observable Sequential Games</head><p>How do the values v c i (π, s t , a t ) and q π,i (s t , a t ) differ? The authors of <ref type="bibr" target="#b37">[38]</ref> posit that they are approximately equal when s t rarely occurs more than once in a trajectory. First, note that s t cannot be reached more than once in a trajectory from our definition of s t , because the observation histories (of the player to play at s t ) would be different in each occurrence (i.e. due to perfect recall). So, the two values are indeed equal in deterministic, single-agent environments. In general, counterfactual values are conditioned on player i playing to reach s t , whereas q-function estimates are conditioned on having reached s t . So, q π,i (s t , a</p><formula xml:id="formula_11">t ) = E ρ∼π [G t,i | S t = s t , A t = a t ] = h,z∈Z(st,at) Pr(h | s t )η π (ha, z)u i (z)</formula><p>where</p><formula xml:id="formula_12">η π (ha, z) = η π (z) η π (h)π(s, a) = h,z∈Z(st,at) Pr(s t | h) Pr(h) Pr(s t ) η π (ha, z)u i (z) by Bayes' rule = h,z∈Z(st,at) Pr(h) Pr(s t ) η π (ha, z)u i (z) since h ∈ s t , h is unique to s t = h,z∈Z(st,at) η π (h) h ∈st η π (h ) η π (ha, z)u i (z) = h,z∈Z(st,at) η π i (h)η π -i (h) h ∈st η π i (h )η π -i (h ) η π (ha, z)u i (z) = h,z∈Z(st,at) η π i (s)η π -i (h) η π i (s) h ∈st η π -i (h )</formula><p>η π (ha, z)u i (z) due to def. of s t and perfect recall = h,z∈Z(st,at)</p><formula xml:id="formula_13">η π -i (h) h ∈st η π -i (h ) η π (ha, z)u i (z) = 1 h∈st η π -i (h) v c i (π, s t , a t ).</formula><p>The derivation is similar to show that v π,i (s t ) = v c i (π, s t )/ h∈st η π -i (h). Hence, counterfactual values and standard value functions are generally not equal, but are scaled by the Bayes normalizing constant B -i (π, s t ) = h∈st η π -i (h). If there is a low probability of reaching s t due to the environment or due to opponents' policies, these values will differ significantly. This leads to a new interpretation of actor-critic algorithms in the multiagent partially observable setting: the advantage values q π,i (s t , a t ) -v π,i (s t , a t ) are immediate counterfactual regrets scaled by 1/B -i (π, s t ). This then determines requirements for convergence guarantees in the tabular case.</p><p>Note that the standard policy gradient theorem holds: gradients can be estimated from samples. This follows from the derivation of the policy gradient in the tabular case (see Appendix E). When TD bootstrapping is not used, the Markov property is not required; having multiple agents and/or partial observability does not change this. For a proof using REINFORCE (G t only), see <ref type="bibr" target="#b78">[78,</ref><ref type="bibr">Theorem 1]</ref>. The proof trivially follows using G t,i -v π,i since v π,i is trained separately and does not depend on ρ.</p><p>Policy gradient algorithms perform gradient ascent on J P G (π θ ) = v π θ (s 0 ), using ∇ θ J P G (π θ ) ∝ s µ(s) a ∇ θ π θ (s, a)q π (s, a), where µ is on-policy distribution under π [85, Section 13.2]. The actor-critic equivalent is</p><formula xml:id="formula_14">∇ θ J AC (π θ ) ∝ s µ(s) a ∇ θ π θ (s, a)(q π (s, a) -b π(s, b)q π (s, b)).</formula><p>Note that the baseline is unnecessary when summing over the actions and ∇ θ J AC (π θ ) = ∇ θ J P G (π θ ) <ref type="bibr" target="#b4">[5]</ref>. However, our analysis relies on a projected gradient descent algorithm that does not assume simplex constraints on the policy: in that case, in general ∇ θ J AC (π θ ) = ∇ θ J P G (π θ ). Definition 1. Define policy gradient policy iteration (PGPI) as a process that iteratively runs θ ← θ + α∇ θ J P G (π θ ), and actor-critic policy iteration (ACPI) similarly using ∇ θ J AC (π θ ).</p><p>In two-player zero-sum games, PGPI/ACPI are gradient ascent-descent problems, because each player is trying to ascend their own score function, and when using tabular policies a solution exists due to the minimax theorem <ref type="bibr" target="#b79">[79]</ref>. Define player i's external regret over K steps as</p><formula xml:id="formula_15">R K i = max π i ∈Πi K k=1 E π i [G 0,i ] -E π k [G 0,i ]</formula><p>, where Π i is the set of deterministic policies.</p><p>Theorem 1. In two-player zero-sum games, when using tabular policies and an 2 projection</p><formula xml:id="formula_16">P (θ) = argmin θ ∈∆(S,A) θ -θ 2 , where ∆(S, A) = {θ | ∀s ∈ S, b∈A θ s,b = 1} is the space of tabular simplices, if player i uses learning rates of α s,k = k -1 2 η π k i (s)B -i (π, s t ) at s on iteration k, and θ k s,a &gt; 0 for all k and s, then projected PGPI, θ k+1 s,• ← P ({θ k s,a + α s,k ∂ ∂θ k s,a J P G (π θ k )} a ), has regret R K i ≤ 1 η min i |S i | √ K + ( √ K -1 2 )|A|(∆r) 2</formula><p>, where S i is the set of player i's states, ∆r is the reward range, and η min i = min s,k η k i (s). The same holds for projected ACPI (see appendix).</p><p>The proof is given in Appendix E. In the case of sampled trajectories, as long as every state is reached with positive probability, Monte Carlo estimators of q π,i will be consistent. Therefore, we use exploratory policies and decay exploration over time. With a finite number of samples, the probability that an estimator qπ,i (s, a) differs by some quantity away from its mean is determined by Hoeffding's inequality and the reach probabilities. We suspect these errors could be accumulated to derive probabilistic regret bounds similar to the off-policy Monte Carlo case <ref type="bibr" target="#b45">[46]</ref>.</p><p>What happens in the sampling case with a fixed per-state learning rate α s ? If player i collects a batch of data from many sampled episodes and applies them all at once, then the effective learning rates (expected update rate relative to the other states) is scaled by the probability of reaching s: η π i (s)B -i (π, s), which matches the value in the condition of Theorem 1. This suggests using a globally decaying learning rate to simulate the remaining k -1 2 . The analysis so far has concentrated on establishing guarantees for the optimization problem that underlies standard formulation of policy gradient and actor-critic algorithms. A better guarantee can be achieved by using stronger policy improvement (proof and details are found in Appendix E):</p><formula xml:id="formula_17">Theorem 2. Define a state-local J P G (π θ , s) = v π θ ,i (s), composite gradient { ∂</formula><p>∂θs,a J P G (π θ , s)} s,a , strong policy gradient policy iteration (SPGPI), and strong actor-critic policy iteration (SACPI) as in Definition 1 except replacing the gradient components with ∂ ∂θs,a J P G (π θ , s). Then, in two-player zero-sum games, when using tabular policies and projection P (θ) as defined in Theorem 1 with learning rates</p><formula xml:id="formula_18">α k = k -1 2 on iteration k, projected SPGPI, θ k+1 s,• ← P ({θ k s,a + α k ∂ ∂θ k s,a J P G (π θ , s)} a ), has regret R K i ≤ |S i | √ K + ( √ K -1 2 )|A|(∆r) 2</formula><p>, where S i is the set of player i's states and ∆r is the reward range. This also holds for projected SACPI (see appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>We now assess the behavior of the actor-critic algorithms in practice. While the analyses in the previous section established guarantees for the tabular case, ultimately we want to assess scalability and generalization potential for larger settings. Our implementation parameterizes critics and policies using neural networks with two fully-connected layers of 128 units each, and rectified linear unit activation functions, followed by a linear layer to output a single value q or softmax layer to output π. We chose these architectures to remain consistent with previous evaluations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b49">50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domains: Kuhn and Leduc Poker</head><p>We evaluate the actor-critic algorithms on two n-player games: Kuhn poker, and Leduc poker.</p><p>Kuhn poker is a toy game where each player starts with 2 chips, antes 1 chip to play, and receives one card face down from a deck of size n + 1 (one card remains hidden). Players proceed by betting (raise/call) by adding their remaining chip to the pot, or passing (check/fold) until all players are either in (contributed as all other players to the pot) or out (folded, passed after a raise). The player with the highest-ranked card that has not folded wins the pot.</p><p>In Leduc poker, players have a limitless number of chips, and the deck has size 2(n + 1), divided into two suits of identically-ranked cards. There are two rounds of betting, and after the first round a single public card is revealed from the deck. Each player antes 1 chip to play, and the bets are limited to two per round, and number of chips limited to 2 in the first round, and 4 in the second round.</p><p>The rewards to each player is the number of chips they had after the game minus before the game. To remain consistent with other baselines, we use the form of Leduc described in <ref type="bibr" target="#b49">[50]</ref> which does not restrict the action space, adding reward penalties if/when illegal moves are chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline: Neural Fictitious Self-Play</head><p>We compare to one main baseline. Neural Fictitious Self-Play (NFSP) is an implementation of fictitious play, where approximate best responses are used in place of full best response <ref type="bibr" target="#b31">[32]</ref>. Two transition buffers of are used: D RL and D M L ; the former to train a DQN agent towards a best response π i to π-i , data in the latter is replaced using reservoir sampling, and trains πi by classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Performance Results</head><p>Here we show the empirical convergence to approximate Nash equlibria for each algorithm in selfplay, and performance against fixed bots. The standard metric to use for this is NASHCONV(π) defined in Section 2.2, which reports the accuracy of the approximation to a Nash equilibrium.</p><p>Training Setup. In the domains we tested, we observed that the variance in returns was high and hence we performed multiple policy evaluation updates (q-update for ∇ QPG , ∇ RPG , and ∇ RMPG , and v-update for A2C) followed by policy improvement (policy gradient update). These updates were done using separate SGD optimizers with their respective learning rates of fixed 0.001 for policy evaluation, and annealed from a starting learning rate to 0 over 20M steps for policy improvement. (See Appendix G for exact values). Further, the policy improvement step is applied after N q policy evaluation updates. We treat N q and batch size as a hyper parameters and sweep over a few reasonable values. In order to handle different scales of rewards in the multiple domains, we used the streaming Z-normalization on the rewards, inspired by its use in Proximal Policy Optimization (PPO) <ref type="bibr" target="#b77">[77]</ref>. In addition, the agent's policy is controlled by a(n inverse) temperature added as part of the softmax operator. The temperature is annealed from 1 to 0 over 1M steps to ensure adequate state space coverage. An additional entropy cost hyper-parameter is added as is standard practice with Deep RL policy gradient methods such as A3C <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b77">77]</ref>. For NFSP, we used the same values presented in <ref type="bibr" target="#b49">[50]</ref>.</p><p>Convergence to Equilibrium. See Figure <ref type="figure" target="#fig_1">2</ref> for convergence results. Please note that we plot the NASHCONV for the average policy in the case of NFSP, and the current policy in the case of the policy gradient algorithms. We see that in 2-player Leduc, the actor-critic variants we tried are similar in performance; NFSP has faster short-term convergence but long-term the actor critics are comparable. Each converges significantly faster than A2C. However RMPG seems to plateau. Performance Against Fixed Bots. We also measure the expected reward against fixed bots, averaged over player seats. These bots, CFR500, correspond to the average policy after 500 iterations of CFR. QPG and RPG do well here, scoring higher than A2C and even beating NFSP in the long-term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we discuss several update rules for actor-critic algorithms in multiagent reinforcement learning. One key property of this class of algorithms is that they are model-free, leading to a purely online algorithm, independent of the opponents and environment. We show a connection between these algorithms and (counterfactual) regret minimization, leading to previously unknown convergence properties underlying model-free MARL in zero-sum games with imperfect information.</p><p>Our experiments show that these actor-critic algorithms converge to approximate Nash equilibria in commonly-used benchmark Poker domains with rates similar to or better than baseline model-free algorithms for zero-sum games. However, they may be easier to implement, and do not require storing a large memory of transitions. Furthermore, the current policy of some variants do significantly better than the baselines (including the average policy of NFSP) when evaluated against fixed bots. Of the actor-critic variants, RPG and QPG seem to outperform RMPG in our experiments.</p><p>As future work, we would like to formally develop the (probabilistic) guarantees of the sample-based on-policy Monte Carlo CFR algorithms and/or extend to continuing tasks as in MDPs <ref type="bibr" target="#b40">[41]</ref>. We are also curious about what role the connections between actor-critic methods and CFR could play in deriving convergence guarantees in model-free MARL for cooperative and/or potential games.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Learning Dynamics in Matching Pennies: (a) and (b) show the vector field for ∂π/∂t including example particle traces, where each point is each player's probability of their first action; (c) shows example traces of policies following a discrete approximation to t 0 ∂π/∂t.</figDesc><graphic coords="5,107.96,72.00,126.72,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical convergence rates for NASHCONV(π) and performance versus CFR agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>So, we now show phase portraits of the learning dynamics on Matching Pennies: a</figDesc><table /><note><p><p><p><p><p>two-action version of Rock, Paper, Scissors. These analyses are common in multiagent learning as they allow visual depiction of the policy changes and how different factors affect the (convergence) behavior</p><ref type="bibr" target="#b83">[83,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b94">94,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b99">99,</ref><ref type="bibr" target="#b98">98,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b89">89]</ref></p>. Convergence is difficult in Matching Pennies as the only Nash equilibrium π * = ((</p>1</p>2 , 1 2 ), ( 1 2 , 1 2 )) requires learning stochastic policies. We give more detail and results on different games that cause cyclic learning behavior in Appendix D.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Appendices are included in the technical report version of the paper; see<ref type="bibr" target="#b84">[84]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that in fully-observable settings, o(st, at, st+1) = st+1. In partially observable environments<ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b65">65]</ref>, an observation function O : S × A → ∆(Ω) is used to sample o(st, at, st+1) ∼ O(st, at).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We assume finite episodic tasks of bounded length and leave out the discount factor γ to simplify the notation, without loss of generality. We use γ(= 0.99)-discounted returns in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In defining st, we drop the reference to acting player i in turn-based games without loss of generality.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Martin Schmid, Audrūnas Gruslys, Neil Burch, Noam Brown, Kevin Waugh, Rich Sutton, and Thore Graepel for their helpful feedback and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multiagent reinforcement learning algorithm with non-linear dynamics</title>
		<author>
			<persName><forename type="first">Sherief</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="549" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Maximum a posteriori policy optimisation</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess Remi Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>CoRR, abs/1806.06920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Continuous adaptation via meta-learning in nonstationary and competitive environments</title>
		<author>
			<persName><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autonomous agents modelling other agents: A comprehensive survey and open problems</title>
		<author>
			<persName><forename type="first">Stefano</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="66" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mean actor critic</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melrose</forename><surname>Roderick Kavosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><surname>Littman</surname></persName>
		</author>
		<idno>CoRR, abs/1709.00503</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gambling in a rigged casino: The adversarial multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 36th Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emergent complexity via multi-agent competition</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evolutionary dynamics of multi-agent learning: A survey</title>
		<author>
			<persName><forename type="first">Daan</forename><surname>Bloembergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaisers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="659" to="697" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning, regret minimization, and equilibria</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Game Theory</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for computing strategies in two-player simultaneous move games</title>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Bošanský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Lisý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Čermák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Winands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convergence and no-regret in multiagent learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 (NIPS)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heads-up Limit Hold&apos;em Poker is solved</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oskari</forename><surname>Tammelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="issue">6218</biblScope>
			<biblScope unit="page" from="145" to="149" />
			<date type="published" when="2015-01">January 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiagent learning using a variable learning rate</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative solutions of games by fictitious play</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Activity Analysis of Production and Allocation</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="374" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic thresholding and pruning for regret minimization</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Kroer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Superhuman AI for heads-up no-limit poker: Libratus beats top professionals</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6385</biblScope>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergent communication through negotiation</title>
		<author>
			<persName><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations (ICLR)</title>
		<meeting>the Sixth International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expected policy gradients</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The complexity of computing a nash equilibrium</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">W</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-eighth Annual ACM Symposium on Theory of Computing, STOC &apos;06</title>
		<meeting>the Thirty-eighth Annual ACM Symposium on Theory of Computing, STOC &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR, abs/1604.06778</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR, abs/1802.01561</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with opponent-learning awareness</title>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</title>
		<meeting>the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient evolutionary dynamics with extensive-form games</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="335" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Regret minimization in non-zero-sum games with applications to building champion multiplayer computer poker agents</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gibson</surname></persName>
		</author>
		<idno>CoRR, abs/1305.0034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for robotic manipulation</title>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1610.00633</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple adaptive procedure leading to correlated equilibrium</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mas-Colell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1127" to="1150" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introduction to online convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="157" to="325" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Opponent modeling in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fictitious self-play in extensive-form games</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from self-play in imperfectinformation games</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>CoRR, abs/1603.01121</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A survey of learning in multiagent environments: Dealing with non-stationarity</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaisers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Baarslag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Munoz De Cote</surname></persName>
		</author>
		<idno>CoRR, abs/1707.09183</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Evolutionary Games and Population Dynamics</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sigmund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Time average replicator and best-reply dynamics</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Viossat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhang-Wei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Yun</forename><surname>Shann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Yi</forename><surname>Lee</surname></persName>
		</author>
		<idno>CoRR, abs/1712.07893</idno>
		<title level="m">A deep policy inference q-network for multi-agent systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Representation Learning</title>
		<meeting>the International Conference on Representation Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Regret minimization for partially observable deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>CoRR, abs/1710.11424</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Approximately optimal approximate reinforcement learning</title>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning, ICML &apos;02</title>
		<meeting>the Nineteenth International Conference on Machine Learning, ICML &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining no-regret and Q-learning</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Kash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Reinforcement Learning (EWRL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Analysis of hannan consistent selection for monte carlo tree search in simultaneous move games</title>
		<author>
			<persName><forename type="first">Vojtech</forename><surname>Kovarík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Lisý</surname></persName>
		</author>
		<idno>CoRR, abs/1509.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Extensive games and the problem of information</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Some topics in two-person games</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Game Theory</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sampling for regret minimization in extensive games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2009)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Monte Carlo Sampling and Regret Minimization for Equilibrium Computation and Decision-Making in Large Extensive Form Games</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<pubPlace>Edmonton, Alberta, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Further developments of extensive-form replicator dynamics using the sequenceform representation</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</title>
		<meeting>the Thirteenth International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monte Carlo sampling for regret minimization in extensive games</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A unified game-theoretic approach to multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-agent cooperation and the emergence of (natural) language</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Maintaining cooperation in complex social dilemmas using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno>CoRR, abs/1707.01068</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>CoRR, abs/1509.02971</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Action-dependent control variates for policy optimization via stein identity</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiagent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
	<note>OpenAI Pieter Abbeel, and Igor Mordatch</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Le</forename><surname>Fort-Piat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Puigdomènech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepstack: Expert-level artificial intelligence in heads-up no-limit poker</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Moravčík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Lisý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6362</biblScope>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An introduction to counterfactual regret minimization</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">W</forename><surname>Neller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<ptr target="http://modelai.gettysburg.edu/2013/cfr/index.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Model AI Assignments, The Fourth Symposium on Educational Advances in Artificial Intelligence (EAAI-2013)</title>
		<meeting>Model AI Assignments, The Fourth Symposium on Educational Advances in Artificial Intelligence (EAAI-2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Policy gradient with value function approximation for collective multiagent planning</title>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Duc Thien Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoong Chuin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4319" to="4329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Game theory and multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nowé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vrancx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-M</forename><forename type="middle">De</forename><surname>Hauwere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning: State-of-the-Art</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="441" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A Concise Introduction to Decentralized POMDPs</title>
		<author>
			<persName><forename type="first">Frans</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evolutionary dynamics of q-learning over the sequence form</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2034" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno>CoRR, abs/1804.02717</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Actor-critic fictitious play in simultaneous move multistage games</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics<address><addrLine>Playa Blanca, Lanzarote, Canary Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="9" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On the use of non-stationary strategies for solving two-player zero-sum markov games</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 19th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming for two-player zero-sum markov games</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Policy gradient methods for control applications</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<idno>TR-CLMC- 2007-1</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stochastic evolution dynamic of the rock-scissors-paper game based on a quasi birth and death process</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Debin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xiaoling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Qiyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28585</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods</title>
		<author>
			<persName><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1802.10264</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Using counterfactual regret minimization to create competitive multiplayer poker agents</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Risk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szafron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Autonomus Agents and Multiagent Systems (AAMAS)</title>
		<meeting>the International Conference on Autonomus Agents and Multiagent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Sandholm</surname></persName>
		</author>
		<title level="m">Population Games and Evolutionary Dynamics</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR, abs/1502.05477</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Safe, multi-agent, reinforcement learning for autonomous driving</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Shammah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno>CoRR, abs/1610.03295</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<title level="m">Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno>CoRR, abs/1712.01815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">530</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Nash convergence of gradient dynamics in general-sum games</title>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, UAI &apos;00</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence, UAI &apos;00<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Actor-critic policy optimization in partially observable multiagent environments</title>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<idno>CoRR, abs/1810.09026</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Comparing policy-gradient algorithms</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Solving heads-up limit Texas Hold&apos;em</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Oskari Tammelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<author>
			<persName><forename type="first">Jonker</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionarily stable strategies and game dynamics</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A Generalised Method for Empirical Game Theoretic Analysis</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAMAS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Random sampling with a reservoir</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Choosing samples to compute heuristic-strategy Nash equilibrium</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Agent-Mediated Electronic Commerce</title>
		<meeting>the Fifth Workshop on Agent-Mediated Electronic Commerce</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Analyzing Complex Strategic Interactions in Multi-Agent Systems</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>William E Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">O</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><surname>Kephart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Solving games with functional regret estimation</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedongs of the AAAI Conference on Artificial Intelligence</title>
		<meeting>eedongs of the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Methods for empirical game-theoretic analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference</title>
		<meeting>The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1552" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Variance reduction for policy gradient with actiondependent factorized baselines</title>
		<author>
			<persName><forename type="first">Cathy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">M</forename><surname>Bayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Training agent for first-person shooter game with actorcritic curriculum learning</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Representation Learning</title>
		<meeting>the International Conference on Representation Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Classes of multiagent q-learning dynamics with -greedy exploration</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wunder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Babes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1167" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Multi-agent learning with policy prediction</title>
		<author>
			<persName><forename type="first">Chongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="927" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Twentieth International Conference on Machine Learning (ICML-2003)</title>
		<meeting>Twentieth International Conference on Machine Learning (ICML-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<idno>CMU-CS-03-110</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Regret minimization in games with incomplete information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piccione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS 2007)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Cyclic equilibria in markov games</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05</title>
		<meeting>the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
