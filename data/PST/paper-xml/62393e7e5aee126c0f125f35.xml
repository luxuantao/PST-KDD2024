<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Better Language Model with Hypernym Class Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">He</forename><surname>Bai</surname></persName>
							<email>he.bai@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
							<email>tong.wang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<email>alsordon@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
							<email>peng.shi@uwaterloo.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Better Language Model with Hypernym Class Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the course of the past decades, language modeling (LM) has transitioned from n-gram to neural models <ref type="bibr" target="#b3">(Bengio et al., 2003;</ref><ref type="bibr" target="#b24">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b10">Devlin et al., 2019;</ref><ref type="bibr" target="#b6">Brown et al., 2020)</ref>. Performance improvement of today's neural LMs is often achieved at the cost of increased computational resources. For example, to capture long-term dependencies, various extensions of Transformerbased LMs have been proposed <ref type="bibr" target="#b8">(Dai et al., 2019;</ref><ref type="bibr" target="#b29">Rae et al., 2020)</ref>. These modifications bring about significant improvements on held-out perplexity, but training cost also increases significantly due to large GPU memory consumption and more computations at each training step.</p><p>In parallel, alternative training strategies have also been proposed <ref type="bibr" target="#b14">(Guu et al., 2020;</ref><ref type="bibr" target="#b37">Ziegler</ref>  and Rush, 2019; <ref type="bibr" target="#b9">Deng et al., 2020)</ref>. In this paper, we explore the effectiveness of class-based language models <ref type="bibr">(CLMs, Brown et al. 1992</ref>) in the context of neural LMs. CLMs group individual words into coarser-grained classes and has proven effective in alleviating context sparsity in n-gram LMs <ref type="bibr" target="#b7">(Dagan et al., 1999)</ref>. It has been also used to improve computational efficiency in neural LMs <ref type="bibr" target="#b25">(Morin and Bengio, 2005;</ref><ref type="bibr" target="#b12">Grave et al., 2017a)</ref>. More recently, <ref type="bibr" target="#b19">Levine et al. (2020)</ref> pretrain masked LMs <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> by predicting WordNet supersense labels. However, the work focuses on word-sense disambiguation tasks and doesn't provide clear evidence of gains in terms of perplexity.</p><p>In this paper, we revisit CLM and assign words to classes by leveraging hypernym relations from the WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref>. Our proposal, dubbed Hypernym Class Prediction (HCP) is simple and effective: for each batch, we substitute a subset of the tokens with their WordNet hypernyms (see Figure <ref type="figure" target="#fig_0">1</ref>). Then, we train an autoregressive LM on the resulting sentences using a mixed vocabulary composed of hypernyms and tokens. Crucially, we anneal the substitution rate during training, i.e., we gently switch from hypernym prediction to token prediction, following a curriculum learning approach. Note that this approach does not require WordNet information at inference time nor increases training time.</p><p>Our approach is motivated by two hypotheses. Firstly, mapping words to their hypernyms gives rise to a natural gradation of difficulty in the prediction task. Prior work has shown that LM benefits from training on instances of increasing difficulty <ref type="bibr" target="#b4">(Bengio et al., 2009;</ref><ref type="bibr">Press et al., 2021)</ref>. We thus postulate that, when coupled with the right curriculum, HCP can improve LM training and perplexity. Secondly, we hypothesize that HCP can improve rare word generalization through implicit context sharing. Neural models still struggle to learn reliable representations for rare words <ref type="bibr" target="#b33">(Schick and Schütze, 2020)</ref>. With CLM-based models, data sparsity for rare words can be abated, e.g., when the representation of their contexts are potentially drawn closer to those of their more frequent siblings by way of label (hypernym) sharing.</p><p>Empirically, the proposed method consistently yields about 0.6-1.9% relative reduction in perplexity over baselines on the WikiText-103 dataset <ref type="bibr" target="#b22">(Merity et al., 2016)</ref>, and 1.3-3.1% on the ARXIV dataset <ref type="bibr">(Lazaridou et al., 2021)</ref>. These improvements are observed with respect to memory-augmented <ref type="bibr" target="#b8">(Dai et al., 2019)</ref> and segmentaware <ref type="bibr" target="#b1">(Bai et al., 2021)</ref> LMs. Importantly, the proposed method improves performance for both rare and frequent words. We also observe that this is in contrast with performance improvements in regular LMs, which seem to be achieved at the cost of worsened performance on rare words.</p><p>To the best of our knowledge, this is the first work that shows how perplexity of Transformer LMs can be improved by leveraging hypernymy relationships. We provide an extensive ablation study highlighting crucial elements of HCP. Amongst those, we found particularly important to adopt a curriculum learning approach, rather than multiobjective learning or adaptive-softmax, and excluding frequent words from the hypernym prediction task. We highlight the simplicity and effectiveness of the proposed method as our main contribution, and hope this study would facilitate further exploration in this line of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer-based models are now popular language models. <ref type="bibr" target="#b8">Dai et al. (2019)</ref> propose Transformer-XL by extending the vanilla Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> with a memory segment, which can encode more context tokens to predict the next token. <ref type="bibr" target="#b29">Rae et al. (2020)</ref> extend Transformer-XL with a compressed memory segment to further encode long-time context memory. Other works explore different sparse Transformers to encode much longer sequences for LM <ref type="bibr" target="#b2">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b31">Roy et al., 2021)</ref>. <ref type="bibr" target="#b1">Bai et al. (2021)</ref> propose a segment-aware Transformer (Segatron) to encode more positional information for language modeling. Despite their effectiveness, neural models still struggle to learn reliable representations for rare words. Some approaches have been proposed to tackle this challenge by way of morphology <ref type="bibr" target="#b21">(Luong et al., 2013)</ref>, lexical similarity <ref type="bibr" target="#b16">(Khassanov et al., 2019)</ref>, context similarity <ref type="bibr" target="#b33">(Schick and Schütze, 2020;</ref><ref type="bibr" target="#b15">Khandelwal et al., 2020)</ref> and tokenization <ref type="bibr" target="#b17">(Kudo and Richardson, 2018)</ref>.</p><p>In addition to the model modifications, other work investigated curriculum learning to train LMs. <ref type="bibr" target="#b4">Bengio et al. (2009)</ref>  Related work aimed at integrating WordNet information into pretrained language models. <ref type="bibr" target="#b19">Levine et al. (2020)</ref> propose SenseBERT by adding the word sense (WordNet supersense) prediction as an additional task during BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> pre-training. SenseBERT outperforms BERT on both word supersense disambiguation <ref type="bibr" target="#b30">(Raganato et al., 2017)</ref> task and word in context (Pilehvar and Camacho-Collados, 2019) task. Recently, <ref type="bibr" target="#b27">Porada et al. (2021)</ref> use WordNet hypernymy chains as input to a Roberta <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> model to predict the plausibility of input events. In this work, our focus is to improve performance of auto-regressive LMs. We show that a multi-task strategy harms performance in this setting, and give a successful recipe to consistently boost LM performance with class-based predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Coupling class-based LM (CLM) and curriculum learning, HCP is to gradually anneal class prediction to token prediction during LM training. In this section, we first describe how we instantiate word classes by leveraging hypernym relation from the Figure <ref type="figure">2</ref>: Hypernym-paths of synsets "magnesium.n.01", "iron.n.01", and "desk.n.01", corresponding to the word magnesium, iron, and desk respectively.</p><formula xml:id="formula_0">def token2class(token2freq, d, f):</formula><p># token2freq is a dictionary whose key is the token and value is the tokens occurrences) # d is the depth, f is the occurrence threthold rtn = {} for token, freq in token2freq.items():</p><p>if freq &gt; f: continue for synset in wordnet.synsets(token):</p><p>for path in synset.hypernym_paths(): if len(path)&gt;=d and noun in path</p><formula xml:id="formula_1">[d−1]: rtn[token] = path[d−1] break if token in rtn: break return rtn</formula><p>Code 1: Pseudocode for token to class mapping.</p><p>WordNet. We then present how to incorporate the proposed Hypernym Class Prediction task into LM training via curriculum learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hypernymy as Word Classes</head><p>WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref> is a lexical database that groups words into sets of cognitive synonyms known as synsets, which are in turn organized into a directed graph by various lexical relations including the hypernymy (is-a) relation. As shown in Figure <ref type="figure">2</ref>, each vertex is a synset, labeled by the text within the box, and each edge points from the hypernym (supertype) to the hyponym (subtype). Note that a word form (spelling) may be associated with multiple synsets -each corresponding to a different sense of the word, which are sorted by the frequency of the sense estimated from a senseannotated corpus. For example, iron has 6 synsets, among which "iron.n.01" is the most common one.</p><p>Hence, if two words share the same hypernym at a certain level in their hypernym-paths (to the root in WordNet), we could say they are similar at that level. Here we use "Depth" to quantify the hypernym-path level. In Figure <ref type="figure">2</ref>, for example, at Depth 6, iron and magnesium are mapped to the same group named "metallic_element.n.01", while desk is mapped to "instrumentality.n.03". At Depth 2, all these three words share the same (indirect) hypernym "physical_entity.n.01".</p><p>In this work, we map each token in our training set into its hypernym class if this token (1) has a noun synset in the WordNet, (2) with a hypernympath longer than a given depth d, and (3) has frequency below a given threshold f in the training corpus. We only consider nouns because it is not only the most common class in the WordNet but also a difficult class for LMs to learn <ref type="bibr">(Lazaridou et al., 2021)</ref>. For tokens with multiple synsets, we iterate over the synsets in the order of sense frequency and break the loop once found. We select the most frequent synset no less than the required depth. The mapping pseudocode is illustrated in Code 1, which is a data pre-processing algorithm conducted only once before the training and takes no more than 5 minutes in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hypernym Class Prediction</head><p>We first partition the vocabulary into V x and V ¬x based on whether or not a token has a hypernym in the WordNet, and V h denotes the set of all hypernyms. The original task in a Transformer-based LM is then to predict the token w j 's probability with the output x from the last layer:</p><formula xml:id="formula_2">P (y = w j |x) = exp(x T vw j ) w k ∈Vx∪V¬x exp(x T vw k ) (1)</formula><p>where w k is the k th word in the original vocabulary and v w k is its embedding. Here we assume the output layer weights are tied with the input em- beddings. We call any training step predicted with Eq. 1 a token prediction step.</p><p>To do the Hypernym Class Prediction step, we replace all tokens in V x in a batch of training data with their corresponding hypernym classes in V h . After the replacement, only hypernym classes in V h and tokens in V ¬x can be found in that batch. Then, the LM probability prediction becomes:</p><formula xml:id="formula_3">P (y = w j |x) = exp(x T vw j ) w k ∈V h ∪V¬x exp(x T vw k ) (2)</formula><p>where w j could be either a token or a hypernym class. We called this batch step is a Hypernym Class Prediction (HCP) step.</p><p>Note that Eq. 2 is different from the multiobjective learning target, where the hypernym class would be predicted separately:</p><formula xml:id="formula_4">P (y = w j |x) = exp(x T vw j ) w k ∈V h exp(x T vw k ) (3)</formula><p>where w j is a hypernym class. We will elaborate on this difference in the experiment results part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Method</head><p>We train a LM by switching from HCP to token prediction. For the example in Figure <ref type="figure">2</ref>, our target is to teach a model to distinguish whether the next token belongs to the metallic element class or instrumentality class during the earlier stage in training, and to predict the exact word from magnesium, iron, and desk later.</p><p>Inspired by <ref type="bibr" target="#b4">Bengio et al. (2009)</ref>, we choose curriculum learning to achieve this. Curriculum learning usually defines a score function and a pacing function, where the score function maps from a training example to a difficulty score, while the pacing function determines the amount of the easiest/hardest examples that will be added into each epoch. We use a simple scoring function which treats HCP as an easier task than token prediction. Therefore, there is no need to sort all training examples. The pacing function determines whether the current training step is a HCP step, i.e. whether tokens will be substituted with their hypernyms.</p><p>Our pacing function can be defined as:</p><formula xml:id="formula_5">P (y = c|t) = b t &lt; a * N 0 t ≥ a * N<label>(4)</label></formula><p>or</p><formula xml:id="formula_6">P (y = c|t) = b − b * t a * N t &lt; a * N 0 t ≥ a * N<label>(5)</label></formula><p>where P (y = c|t) is the probability that the current step t is a hypernym class prediction step. N is the total training steps. a and b are hyper-parameters. So, Eq. 4 is a constant pacing function in the first a * N steps, while Eq. 5 is a linear decay function. We plot these two functions in Figure <ref type="figure" target="#fig_1">3</ref>. According to our experimental results Tab. 5, these two functions are both effective in improving the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on two datasets. WikiText-103 <ref type="bibr" target="#b22">(Merity et al., 2016</ref>) is a large wordlevel dataset with long-distance dependencies for language modeling. There are 103M tokens and 28K articles (3.6K tokens per article on average).</p><p>The original vocabulary size is 271121, among which we find 3383 hypernym classes for 71567 tokens with d = 6 and f = 6000 (Section 3.1).</p><p>ARXIV <ref type="bibr">(Lazaridou et al., 2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main results</head><p>Our main results are shown in Table <ref type="table" target="#tab_2">1</ref>. We can see that all architectures could benefit from HCP: Transformer-small improved 0.6 ppl, Transformerbase improved 0.5, Segatron-XL base improved 0.4, Transformer-large improved 0.5, and Segatron-XL large improved 0.1. We also plot the validation perplexities of small and large models trained with and without HCP in Figure <ref type="figure" target="#fig_2">4</ref>. In the beginning, the perplexity of the HCP models is higher due to the mixed training steps from the two tasks, but we can see that HCP perplexity goes down faster than the baseline method. And after fully switching to token prediction, HCP outperforms the baseline method quickly and the gap between these two methods remains stable. These results suggest that HCP is indeed effective in improving LM training.</p><p>For experiments on the ARXIV dataset, we first    hyper-parameters as the WikiText-103 large model but change the vocabulary to BPE sub-tokens. The final perplexity outperforms its counterparts about 0.4 and outperforms a larger model trained with 1024 input sequence length over 0.47, while our model length is 384.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalization on Rare Tokens</head><p>In addition to the overall perplexity comparison, we also conduct comparisons with frequency-stratified validation subsets, to show the perplexity of tokens that has been replaced with the hypernym classes during training. Results are shown in Figure <ref type="figure" target="#fig_3">5</ref>. We can see that, after the first 12k hypernym class prediction steps, there is a large gap between our HCP model and the baseline model as the HCP model only learn to predict the hypernym class instead of the token itself. After that, in the next 12k steps, HCP's PPL decreases faster, achieves similar PPL at 24k steps, and finally outperforms the baseline method in all frequency groups. The results show that our proposed training method can benefit the learning of the replaced tokens in various frequencies. Strikingly, we observe that, for the baseline, more training steps lead to a degradation of performance for rare tokens, a behavior that deserves investigation in future work.</p><p>We further conduct pairwise model comparisons with tokens that have been replaced during HCP training on the WikiText-103 test set. Given two models, we compare the prediction probabilities for each occurrence of a target token, and register a "win" for the model with a higher probability. We then calculate the percentage of winnings (as well as ties) for each model by tallying over all occurrences of the token. The results are then stratified by token frequency and plotted in Figure <ref type="figure">6</ref>. The better model is placed on the right in both The sub-optimal model is trained without HCP and trained with different hyperparameters, whose perplexity is increased by 0.9 compared with the baseline model.</p><p>sub-figures.</p><p>In Figure <ref type="figure">6</ref>(a), we see that HCP outperforms the baseline model on all frequency strata. Interestingly, the performance gap widens as frequency decreases, indicating that HCP is beneficial in modeling rare tokens. In Figure <ref type="figure">6</ref>(b), we compare the baseline model against an under-optimized model of identical architecture but slightly different hyperparameters. 2 Here, the (optimal) baseline outperforms the sub-optimal model on all but the least frequent stratum, suggesting the possibility that perplexity reduction (resulting from hyperparameter tuning in this case) might be achieved by improving frequent word prediction at the expense of rare words. This is inline with observations made recently in vision tasks <ref type="bibr" target="#b32">(Sagawa et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>We conduct ablation studies with WikiText-103 dataset and Transformer small model to investigate how to map words to hypernym classes, how to select curriculum learning pacing functions and to show why we use curriculum training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Hypernym-path Depth</head><p>The hypernym classes are chosen from the hypernym-paths in WordNet. Considering that a hypernym-path consists of multiple hypernyms, it 2 The sub-optimal model has batch size 128 instead of the optimal 64, and the perplexity gap between these two models is observed to be slightly larger than that between HCP and the baseline (0.9 vs 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head><p>Valid is not straightforward to tell which layer is the best. But the best depth d should be some layer in the middle. Because a small depth might map multiple distant words into the same class, while a large depth will result in too many classes which are hard for a model to learn. The extreme examples could be d = 1 and d = ∞, corresponding to mapping all candidate words into the class "Entity.n.01" and mapping each word into itself respectively. In Table 3, we show evaluation results among different depth selections. We find that depth 6th is the best choice, with the lowest valid perplexity. The results also confirm our assumption that the best one would be some middle layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Filter Frequency</head><p>In addition to the hypernym-path depth, we also investigate how to select frequency threshold f . As we mentioned above, our target is to map similar words into the same class, where predicting a hypernym class might be easier than predicting multiple different words. After the mapping process, lowfrequency words can be clustered into hypernym classes with higher frequency. Table <ref type="table" target="#tab_6">4</ref> shows the results of different f . We can see that f = 6000 achieves the best results while f = ∞ (without filter) is the worst. We hypothesize this might be due to two reasons. First, for some high-frequency common words, the model can learn them well already, while mapping them into hypernym classes may be superfluous or even harmful. Second, including frequent words skews the marginal distribution over hypernym classes, causing hypernym prediction to be more class-imbalanced, which in turn might lead to collapsed representation in the resulting LM <ref type="bibr" target="#b11">(Fang et al., 2021)</ref>. This hypothesis deserves further investigation. It should be noted that although the difference of #Rep.Tokens looks minor, the difference in the token's appearance is significant. For example, f = ∞ maps only 776 additional tokens compared with f = 8000, but each token's appearance is more than 8000, which explains the different perplexities in Table <ref type="table" target="#tab_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Pacing Function</head><p>Table <ref type="table" target="#tab_7">5</ref> shows the results of models trained with various curriculum pacing functions. We also report the validation perplexities of the tokens that have ever been replaced with hypernym class (Rep.PPL) during training and tokens without hypernym class (NonRep.PPL).</p><p>For the constant pacing function, we fix b = 1 and change the value of a, In this case, the models are always training with HCP in the first a * 100k steps and then switch to the token prediction training, which is a pre-training pacing function. We can see that all models outperform the baseline model over the validation perplexity. Rep.PPL improves from 348 to 339. The perplexity of NonRep.PPL between baseline model and HCP models are similar, except the model trained with a = 4, which indicates the pre-training should not take up too many steps.</p><p>For the linear pacing function, we choose some specific a and b to achieve the same HCP steps as the constant functions above. For simplicity, we also set a = b. In Table <ref type="table" target="#tab_7">5</ref>, we can see that the overall perplexity of the linear functions is similar to the corresponding constant functions, where the Non-Rep.PPL is slightly decreased while the Rep.PPL is slightly increased. We conduct a grid search over different pacing functions with Transformer small model and WikiText-103, and finally, use the constant function with a = 0.12 and b = 0.8 for all base models and large models.</p><p>Curriculum hyper-parameters could be transferred to the ARXIV dataset successfully. However, we tune the frequency threshold f on each dataset, because different tokenization methods change the frequency distribution. All HCP models in Table <ref type="table" target="#tab_3">2</ref> are using d = 6, f = 1000, and the constant pacing function with a = 0.12 and b = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Other Training Objectives</head><p>We also experimented with two other methods to incorporate hypernym information into LM training. Although neither method has yielded any empirical gain, we nonetheless report these methods and offer possible explanations for their failure.</p><p>Multi-objective Training Multi-objective (or multi-task) training consists in a weighted sum of token and hypernym prediction losses. We set the weight of the hypernym prediction loss to 0.2. The prediction of a token is calculated with Eq. 1. The prediction of a hypernym class is calculated with Eq. 3, where x can be the output vector from any layer in the Transformer LM. Table <ref type="table" target="#tab_8">6</ref> lists the results using the last layer and the 8th layer. Using the last layer significantly undermines the original token prediction results. Using the 8th layer is better but the final perplexity is still no better than the baseline model. Simply forcing the language model to predict the hypernym class for each token is harmful to LM performance. We also tried to replace Eq. 3 with Eq. 2, by mixing V h and V ¬w together when predicting the hypernym classes (mix vocab). This significantly improves multi-objective training. Learning to predict the hypernym class from a mixed vocabulary V h ∪ V ¬w is better than only hypernym classes V h .</p><p>Adaptive Softmax Another method is the adaptive-softmax <ref type="bibr" target="#b12">(Grave et al., 2017a)</ref>, where the model first predict the hypernym probability among V h ∪ V ¬w and then predict the token probability among the tokens with the same hypernym class.</p><p>In Table <ref type="table" target="#tab_8">6</ref>, we can see that the adaptive-softmax is no better than the multi-objective trained model. By looking into the poor perplexity of Rep.PPL, we find this method cannot improve the prediction of tokens in V w . We believe this is due to the noise of hypernym class mapping, where we choose the first synset path as the token's hypernym synset without considering the context. Such noise will affect the adaptive-softmax prediction but is not an issue for curriculum training as the final training stage is fully trained with the original text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a new LM training strategy with WordNet's super-subordinate relation and curriculum learning. Although WordNet is an external resources, it's not clear how to get lower perplexity using WordNet before this work. Consistent perplexity reduction can be observed over various models. Both rare and frequent tokens can be modeling better with our proposed method while other optimization method may sacrifice the performance on rare tokens. We'd like to address the limitations of this work: other methods to map words to classes; LM experiments with other languages; pre-training LM with our proposed method and testing on downstream tasks. We hope to investigate these directions in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AFigure 1 :</head><label>1</label><figDesc>Figure 1: An example of word prediction training text and hypernym class prediction training text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Probabilities of HCP step over training process with different pacing functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Valid perplexity curves during the training of small and large models with WikiText-103</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Frequency-stratified validation log(perplexity) of baseline model (Transformer-small) and HCP model (Transformer-small-HCP) with WikiText-103.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 6: Pairwise comparison results. The baseline model and HCP model are trained without and with hypernym class prediction respectively.The sub-optimal model is trained without HCP and trained with different hyperparameters, whose perplexity is increased by 0.9 compared with the baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on WikiText-103 dataset with different models.</figDesc><table><row><cell>is collected from</cell></row></table><note>• large model: 18 layers, 16 heads, hidden size 1024 batch size 128. The input lengths are 150 for the base model and 384 for the large model. The memory length is equal to the input length for both training and testing. The hyper-parameters used for the ARXIV dataset are as same as the WikiText-103, except the ARXIV base model's input length is 384. The number of training steps varies greatly for the large model in previous work, so we experiment on both the lower (80k) higher (350k) ends.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The improvements over the validation set and test set are 0.6 and 0.75 respectively. For the large model, we use the same model architecture and</figDesc><table><row><cell>Model</cell><cell cols="2">#Param. Valid PPL</cell><cell>Test PPL</cell></row><row><cell>Segatron-XL base</cell><cell>59M</cell><cell>22.39</cell><cell>24.21</cell></row><row><cell>+ HCP</cell><cell></cell><cell>21.79</cell><cell>23.46</cell></row><row><cell>Transformer-XL large (Lazaridou et al., 2021)</cell><cell>287M</cell><cell>-</cell><cell>23.07</cell></row><row><cell>Segatron-XL large</cell><cell>283M</cell><cell>21.28</cell><cell>22.99 (80k steps)</cell></row><row><cell>+ HCP</cell><cell>283M</cell><cell>20.93</cell><cell>22.60 (80k steps)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on ARXIV dataset with different models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Clustering words into classes with different layer's hypernym parents. The average depth is 8.03. #Classes denotes the total number of hypernym classes.</figDesc><table><row><cell></cell><cell cols="2">PPL #Classes</cell></row><row><cell>Baseline</cell><cell>34.5</cell><cell>0</cell></row><row><cell>d = 4</cell><cell>34.54</cell><cell>145</cell></row><row><cell>d = 5</cell><cell>34.29</cell><cell>1169</cell></row><row><cell>d = 6</cell><cell>34.05</cell><cell>3383</cell></row><row><cell>d = 7</cell><cell>34.37</cell><cell>6604</cell></row><row><cell>d = 8</cell><cell>34.25</cell><cell>9063</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ignoring words whose frequency more than a threshold f during hypernym class clustering. #Rep. denotes the number of tokens in the vocabulary that will mapped.</figDesc><table><row><cell cols="3">FilterFreq. Valid PPL #Rep.</cell></row><row><cell>Baseline</cell><cell>34.5</cell><cell>0</cell></row><row><cell>f = 3000</cell><cell>34.14</cell><cell>70859</cell></row><row><cell>f = 5000</cell><cell>34.50</cell><cell>71735</cell></row><row><cell>f = 6000</cell><cell>34.05</cell><cell>71971</cell></row><row><cell>f = 7000</cell><cell>34.32</cell><cell>72153</cell></row><row><cell>f = 8000</cell><cell>34.35</cell><cell>72291</cell></row><row><cell>f = ∞</cell><cell>40.10</cell><cell>73067</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Training N steps hypernym class prediction among 100k training steps with different pacing functions. NonRep.PPL denotes non-replaced tokens' perplexity, and Rep.PPL denotes replaced tokens' perplexity.</figDesc><table><row><cell cols="6">Constant Func. HCP steps Valid PPL NonRep.PPL Rep.PPL</cell></row><row><cell>a=0 b=0</cell><cell>0</cell><cell>34.5</cell><cell>22.07</cell><cell>348.87</cell></row><row><cell>a=0.1 b=1</cell><cell>10k</cell><cell>34.18</cell><cell>22.08</cell><cell>339.30</cell></row><row><cell>a=0.2 b=1</cell><cell>20k</cell><cell>34.15</cell><cell>22.07</cell><cell>339.34</cell></row><row><cell>a=0.3 b=1</cell><cell>30k</cell><cell>34.26</cell><cell>22.07</cell><cell>338.14</cell></row><row><cell>a=0.4 b=1</cell><cell>40k</cell><cell>34.39</cell><cell>22.26</cell><cell>338.31</cell></row><row><cell>Linear Func.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a=0.45 b=0.45</cell><cell>10k</cell><cell>34.14</cell><cell>22.04</cell><cell>340.55</cell></row><row><cell>a=0.64 b=0.64</cell><cell>20k</cell><cell>34.05</cell><cell>21.96</cell><cell>341.33</cell></row><row><cell>a=0.78 b=0.78</cell><cell>30k</cell><cell>34.26</cell><cell>22.05</cell><cell>346.77</cell></row><row><cell>a=0.90 b=0.90</cell><cell>40k</cell><cell>34.56</cell><cell>22.12</cell><cell>354.40</cell></row><row><cell></cell><cell></cell><cell cols="4">Valid PPL Test PPL NonRep.PPL Rep.PPL</cell></row><row><cell>Baseline</cell><cell></cell><cell>34.50</cell><cell>36.46</cell><cell>22.07</cell><cell>348.87</cell></row><row><cell>Adaptive Softmax</cell><cell></cell><cell>36.32</cell><cell>38.16</cell><cell>22.48</cell><cell>435.93</cell></row><row><cell>Multi-obj</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>last layer</cell><cell></cell><cell>46.06</cell><cell>48.49</cell><cell>27.81</cell><cell>627.23</cell></row><row><cell>8th layer</cell><cell></cell><cell>43.42</cell><cell>45.37</cell><cell>26.13</cell><cell>597.66</cell></row><row><cell>8th layer + mix vocab</cell><cell></cell><cell>35.97</cell><cell>38.02</cell><cell>22.98</cell><cell>365.27</cell></row><row><cell cols="2">Hypernym Class Prediction</cell><cell>34.05</cell><cell>35.87</cell><cell>21.96</cell><cell>341.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results obtained by alternative strategies for integrating hypernymy information into the LM: adaptive softmax and multi-objective training. Both under-perform the proposed HCP method.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://arxiv.org/help/oa/index</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segatron: Segment-aware transformer for language modeling and understanding</title>
		<author>
			<persName><forename type="first">He</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12526" to="12534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A neural probabilistic language model. The journal of machine learning research</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum Learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
				<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="480" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity-based models of word cooccurrence probabilities</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007537716579</idno>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="69" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Residual energybased models for text generation</title>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring deep neural networks via layerpeeled model: Minority collapse in imbalanced training</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2103091118</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for GPUs</title>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-04-24">2017b. 2017. April 24-26, 2017</date>
			<pubPlace>Toulon, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enriching rare word representations in neural language models by embedding matrix augmentation</title>
		<author>
			<persName><forename type="first">Yerbolat</forename><surname>Khassanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihua</forename><surname>Van Tung Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chng</forename><surname>Siong</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2019-1858</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cyprien de Masson d&apos;Autume, Sebastian Ruder, Dani Yogatama, et al. 2021. Pitfalls of static language modelling</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devang</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01951</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SenseBERT: Driving some sense into BERT</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Padnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4656" to="4667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
				<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on artificial intelligence and statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">WiC: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1267" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling event plausibility with consistent conceptual abstraction</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Porada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1732" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shortformer: Better language modeling using shorter inputs</title>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5493" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00353</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An investigation of why overparameterization exacerbates spurious correlations</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8346" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rare words: A major problem for contextualized representation and how to fix it by attentive mimicking</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">When do curricula work</title>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
