<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Lexicon-Based Graph Neural Network for Chinese NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
							<email>mlpeng16@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
							<email>zywei@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Lexicon-Based Graph Neural Network for Chinese NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to word ambiguities. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and longrange dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed model achieves significant improvements against other baseline models. c 1 : 武 (Wu) c 2 : 汉 (Han) c 3 : 市 (City) c 4 : ⻓长 (Long) c 5 : 江 (River) c 6 : ⼤大 (Big) c 7 : 桥 (Bridge) ...... w 1,2 : 武汉 (Wuhan) w 1,3 : 武汉市 (Wuhan City) w 3,4 : 市⻓长 (Major) w 4,5 : ⻓长江 (Yangzi River) w 6,7 : ⼤大桥 (Bridge) w 4,7 : ⻓长江⼤大桥 (Yangzi River Bridge</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of named entity recognition (NER) involves determining entity boundaries and recognizing categories of named entities, which is a fundamental task in the field of natural language processing (NLP). NER plays an important role in many downstream NLP tasks, including information retrieval <ref type="bibr" target="#b5">(Chen et al., 2015b)</ref>, relation extraction <ref type="bibr" target="#b1">(Bunescu and Mooney, 2005)</ref>, question answering systems <ref type="bibr" target="#b7">(Diefenbach et al., 2018)</ref>, and other applications. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest <ref type="bibr" target="#b10">(Duan and Zheng, 2011)</ref>.</p><p>One intuitive way to alleviate word boundary problems is to first perform word segmentation Figure <ref type="figure">1</ref>: Example of word character lattice with partial input. Because of the characteristic of chain structure, RNN-based methods must predict the label "度" using only previous partial sequences "印度 (India)", which may suffer from word ambiguities without global sentence semantics. and then apply word sequence labeling <ref type="bibr" target="#b45">(Yang et al., 2016;</ref><ref type="bibr" target="#b16">He and Sun, 2017)</ref>. However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities <ref type="bibr" target="#b34">(Peng and Dredze, 2015;</ref><ref type="bibr" target="#b15">He and Sun, 2016)</ref>. Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention <ref type="bibr" target="#b32">(Passos et al., 2014;</ref><ref type="bibr" target="#b53">Zhang and Yang, 2018)</ref>. In particular, <ref type="bibr" target="#b53">Zhang and Yang (2018)</ref> introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results.</p><p>However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential <ref type="bibr" target="#b37">(Shen et al., 2019)</ref>. As a result, these models would encounter serious word ambiguity problems <ref type="bibr" target="#b30">(Mich et al., 2000)</ref>. Especially in Chinese texts, the recognition of named entities with overlapping ambiguous strings is even more challenging. As shown in Figure <ref type="figure">1</ref>, the middle character of an overlapping ambiguous string can constitute words with the characters to both their left and their right <ref type="bibr" target="#b48">(Yen et al., 2012)</ref>, such as "河 流 (River)" and "流 经 (Flow through)", which share a common character "流". However, RNNbased models process characters in a strictly serial order, which is similar to reading Chinese, and a character has priority in being assigned to the word on the left <ref type="bibr" target="#b35">(Perfetti and Tan, 1999)</ref>. More seriously, RNN-based models must give the label of "度" using only previous partial sequences "印 度 (India)", which is problematic without seeing the remaining characters. Hence, <ref type="bibr" target="#b29">Ma et al. (2014)</ref> suggested that the overlapping ambiguity must be settled using sentence context and high-level information.</p><p>In this work, we introduce a lexicon-based graph neural network (LGN) that achieves Chinese NER as a node classification task. The proposed model breaks the serialization processing structure of RNNs with better interaction results between characters and words through careful connections. The lexicon knowledge connects related characters to capture the local composition. Meanwhile, a global relay node is designed to capture long-range dependency and high-level features. LGN follows a neighborhood aggregation scheme wherein the node representation is computed by recursively aggregating its incoming edges and the global relay node. Because of multiple iterations of aggregation, the model can use global context information to repeatedly compare ambiguous words for better prediction. Experimental results show that the proposed method can achieve state-of-the-art performance on four NER datasets.</p><p>The main contributions of this paper can be summarized as follows: 1) we propose the use of a lexicon to construct a graph neural network and achieve Chinese NER as a graph node classification task; 2) the proposed model can capture global context information and local compositions to tackle Chinese word ambiguity problems through recursively aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>2.1 Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods <ref type="bibr" target="#b24">(Li et al., 2014)</ref> and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type="bibr" target="#b17">(He and Wang, 2008;</ref><ref type="bibr" target="#b26">Liu et al., 2010)</ref>. Lexicon features have been widely used to better leverage word information for Chinese NER <ref type="bibr" target="#b20">(Huang et al., 2015;</ref><ref type="bibr" target="#b28">Luo et al., 2015;</ref><ref type="bibr" target="#b12">Gui et al., 2019)</ref>. Especially, <ref type="bibr" target="#b53">Zhang and Yang (2018)</ref> proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>. In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> to tackle word ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks on Texts</head><p>Graph neural networks have been successfully applied to several text classification tasks <ref type="bibr" target="#b40">(Veličković et al., 2017;</ref><ref type="bibr" target="#b47">Yao et al., 2018;</ref><ref type="bibr" target="#b52">Zhang et al., 2018b)</ref>. <ref type="bibr" target="#b33">Peng et al. (2018)</ref> proposed a GCNbased deep learning model for text classification. <ref type="bibr" target="#b54">Zhang et al. (2018c)</ref> proposed using the dependency parse trees to construct a graph for relation extraction. Recently, multi-head attention mechanisms <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref> have been widely used by graph neural networks during the fusion process <ref type="bibr" target="#b50">(Zhang et al., 2018a;</ref><ref type="bibr">Lee et al., 2018)</ref>, which can aggregate graph information by assigning different weights to neighboring nodes or associated edges. Given a set of vectors H ∈ R n×d , a query vector q ∈ R 1×d , and a set of trainable parameters W, this mechanism can be formulated as:</p><formula xml:id="formula_0">Attention(q, K, V) = sof tmax( qK √ d k )V head i = Attention(qW Q i , HW K i , HW V i ) MultiAtt(q, H) = [head 1 ; . . . ; head k ]W O . (1)</formula><p>However, very little work has explored how to use the relationship among characters to construct graphs in raw Chinese texts. The few previous studies on morphological processing in Chinese proposed a decomposed lexical structure <ref type="bibr" target="#b49">(Zhang and Peng, 1992;</ref><ref type="bibr" target="#b56">Zhou and Marslen-Wilson, 1994)</ref> in which Chinese words are represented in terms of their constituent characters. Inspired by these theoretical basis, we propose the use of graph neural networks to construct the relationship between constituent characters and words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexicon-Based Graph Neural Network</head><p>In this work, we propose the use of lexicon information to construct graph neural networks, and achieve Chinese NER as a node classification task.</p><p>The proposed model obtains better interaction among characters, words, and sentences through, aggregation → update → aggregation → . . . , an efficient graph message passing architecture <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Construction and Aggregation</head><p>We use the lexicon knowledge to connect characters to capture the local composition and potential word boundaries. In addition, we propose a global relay node to capture long-range dependency and high-level features. The implementation of the aggregation module for nodes and edges is similar to the multi-head attention mechanism in Transformer <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Construction</head><p>The whole sentence is converted into a directed graph, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, where each node represents a character and the connection between the first and last characters in a word can be treated as an edge. The state of the i-th node represents the features of the ith token in a text sequence. The state of each edge represents the features of a corresponding potential word. The global relay node is used as a virtual hub to gather the information from all the nodes and edges, and then utilizes the global information to help the node remove ambiguity. Formally, let s = c 1 , c 2 , ..., c n denote a sentence, where c i denotes the i-th character. The potential words in the lexicon that match a character subsequence can be formulated as w b,e = c b , c b+1 , ..., c e−1 , c e , where the index of the first and last letters are b and e, respectively. In this work, we propose the use of a directed graph G = (V, E) to model a sentence, where each character c i ∈ V is a graph node and E is the set of edges. Once a character subsequence matches a potential word w b,e , we construct one edge e b,e ∈ E, pointing from the beginning character c b to the ending character c e .</p><p>To capture global information, we add a global relay node to connect each character node and word edge. For a graph with n character nodes and m edges, there are n + m global connections linking each node and edge to the shared relay node. With the global connections, every two nonadjacent nodes are two-hop neighbors and receive non-local information with a two-step update.</p><p>In addition, we consider the transpose of the constructed graph<ref type="foot" target="#foot_0">1</ref> . It is another directed graph on the same set of nodes with all of the edges reversed compared to the orientation of the corresponding edges in G. We denote the transpose graph as G . Similar to the bidirectional LSTM, we compose G and G as a bidirectional graph and concatenate the node states of G and G as final outputs.</p><p>Local Aggregation Given the node features c t i 1043 and the incoming edge features E t c i = {∀ k e t k,i }, we use multi-head attention to aggregate e k,i and the corresponding predecessor nodes c k for each node c i , where intuition is that the incoming edges and predecessor nodes can effectively indicate potential word boundary information, as shown in Figure <ref type="figure" target="#fig_1">3 (a)</ref>. Formally, the node aggregation function can be formulated as follows:</p><formula xml:id="formula_1">e → c : ĉt i = MultiAtt(c t i , {∀ k [c t k ; e t k,i ]}),<label>(2)</label></formula><p>where t refers to the aggregation at the t-th step and [•; •] represents concatenation operation.</p><p>For edge aggregation, all the forces or potential energies acting on the edges should be considered <ref type="bibr" target="#b0">(Battaglia et al., 2018)</ref>. To exploit the word orthographic information, lexicons used to construct edges should consider all the character composition, as shown in Figure <ref type="figure" target="#fig_1">3</ref>  </p><p>Given the character sequence embeddings C ∈ R n×d and potential word embeddings E ∈ R m×d , we first fed C into an LSTM network to generate contextualized representations as the initial node states C 0 <ref type="bibr" target="#b54">(Zhang et al., 2018c)</ref>, and we used the word embeddings as the initial edge states E 0 . Global Aggregation The underlying structure of language is not strictly sequential <ref type="bibr" target="#b37">(Shen et al., 2019)</ref>. To capture long-range dependency and high-level features, as shown in Figure <ref type="figure" target="#fig_1">3</ref> (c), we utilized a global relay node to aggregate each character node and edge, as follows:</p><formula xml:id="formula_3">g t c = MultiAtt(g t , C t 1,n ) g t e = MultiAtt(g t , {∀e t ∈ E}) c, e → g : ĝt = [g t c ; g t e ].<label>(4)</label></formula><p>After multiple exchanges of information ( § 3.2), ĝt aggregates node vectors and edge vectors to summarize the global information, and êt b,e captures the compositional character information to form the local composition. As a result, the proposed model, with a thorough knowledge of both local and non-local composition, would contribute character nodes to distinguish ambiguous words <ref type="bibr" target="#b29">(Ma et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent-based Update Module</head><p>Node Update The effective use of sentence context to tackle the ambiguity among the potential words is still a key issue <ref type="bibr" target="#b29">(Ma et al., 2014)</ref>. For a general graph, it is common practice to apply recurrent-based modules to update hidden representations of nodes <ref type="bibr" target="#b36">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b25">Li et al., 2015)</ref>. Hence, we fused the global feature ĝ into a character nodes update module, as follows:</p><formula xml:id="formula_4">ξ t i = [c t−1 i−1 ; c t−1 i ], χ t i = [ĉ t−1 i ; ĝt−1 ] ât i = σ(W a i ξ t i + V a i χ t i + b a i ), a = {i, f , l} u t i = tanh(W cu ξ t i + V cu χ t i + b cu ) i t i , f t i , l t i = sof tmax( ît i , f t i , lt i ) c t i = l t i c t−1 i−1 + f t i c t−1 i + i t i u t i ,<label>(5)</label></formula><p>where W, V, and b are trainable parameters. ξ t i is the concatenation of adjacent vectors of a context window. The window size in our model is 2 and actually plays a role as a character bigram, which has been shown to be useful for representing characters in sequence labeling tasks <ref type="bibr" target="#b4">(Chen et al., 2015a;</ref><ref type="bibr" target="#b53">Zhang and Yang, 2018)</ref>. χ t i is the concatenation of the global information vector ĝt and the e→c aggregation result ĉt i . The gates i t i , f t i and l t i control information flow from global features to c t i , which can make further readjustment of the weights of the lexicon attention (e→c) to tackle the ambiguities at the subsequent aggregation step. Edge Update To better leverage the interaction among characters, words, and whole sentences, we not only designed a recurrent module for nodes but also for edges and the global relay node <ref type="bibr" target="#b0">(Battaglia et al., 2018)</ref>. We update the edges as follows:</p><formula xml:id="formula_5">χ t b,e = [ê t−1 b,e ; ĝt−1 ], a = {i, f } ât b,e = σ(W a i e t−1 b,e + V a i χ t b,e + b a i ) u t b,e = tanh(W eu e t−1 b,e + V eu χ t b,e + b eu ) i t b,e , f t b,e = sof tmax( ît b,e , f t b,e ) e t b,e = f t b,e e t−1 b,e + i t b,e u t b,e ,<label>(6)</label></formula><p>where χ t b,e is the concatenation of ĝt and the c→e aggregation result êt b,e . Similar to the node update function, i t b,e and f t b,e are gates that control information flow from e t−1 b,e and ĝt to e t b,e . Global Relay Node Update In terms of the global state g, recent works <ref type="bibr" target="#b52">(Zhang et al., 2018b;</ref><ref type="bibr" target="#b13">Guo et al., 2019)</ref> have shown the effectiveness of sharing useful messages across contexts. Thus, we also designed an update function for g, with the initialization g 0 = average(C, E). More formally:</p><formula xml:id="formula_6">ât = σ(W a i g t−1 + V a i ĝt−1 + b a i ), a = {i, f } u t = tanh(W gu g t−1 + V gu ĝt−1 + b gu ) i t , f t = sof tmax( ît , f t ) g t = f t g t−1 + i t u t . (<label>7</label></formula><formula xml:id="formula_7">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding and Training</head><p>A standard conditional random field (CRF) is used in the graph message passing process. Given the sequence of final node states c T 1 , c T 2 , . . . , c T n , the probability of a label sequence ŷ = l1 , l2 , . . . , ln can be defined as follows:</p><formula xml:id="formula_8">p(ŷ|s) = exp( n i=1 φ( li−1 , li , c T i )) y ∈Y(s) exp( n i=1 φ(l i−1 , l i , c T i )) ,<label>(8)</label></formula><p>where, Y(s) is the set of all arbitrary label sequences. φ(</p><formula xml:id="formula_9">l i−1 , l i , c T i ) = W (l i−1 ,l i ) c T i + b (l i−1 ,l i ) , where W (l i−1 ,l i ) and b (l i−1 ,l i )</formula><p>are the weight and bias parameters specific to the labels l i−1 and l i .</p><p>For training, we minimize the sentence-level negative log-likelihood loss as follows:</p><formula xml:id="formula_10">L = − N i=1 log(p(y i |s i )).<label>(9)</label></formula><p>For testing and decoding, we maximized likelihood to find the optimal sequence y * :</p><formula xml:id="formula_11">y * = argmax y∈Y(s) p(y|s). (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>We used the Viterbi algorithm to calculate the above equations, which can reduce the computational complexity efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section, we describe the datasets across different domains and the baseline methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN.  <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexicon</head><p>We used the lexicon over automatically segmented Chinese Giga-Word<ref type="foot" target="#foot_3">4</ref> , obtaining 704.4k words in the final lexicon. The embeddings of lexicon words were pre-trained using word2vec <ref type="bibr" target="#b31">(Mikolov et al., 2013)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head><p>We applied the character-level and word-level methods as baselines for comparison, which incorporate the bichar, softword, and lexicon features. We also compared several state-of-the-art methods on the four datasets to verify the effectiveness of our method. We used the BMES tagging scheme for both character-level and word-level NER tagging.</p><p>Character-level methods: These methods are based on character sequences. We applied the bidirectional LSTM <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997)</ref> and CNN <ref type="bibr" target="#b21">(Kim, 2014)</ref> as classic baseline methods.</p><p>Character-level methods + bichar + softword: Character bigrams are useful for capturing adjacent features and representing characters. We concatenated bigram embeddings with character embeddings to better leverage the bigram information. In addition, we added the segmentation information by incorporating segmentation label embeddings into the character representation. The BMES scheme is used for representing the word segmentation <ref type="bibr" target="#b44">(Xue and Shen, 2003)</ref>.</p><p>Word-level methods: For the datasets with gold segmentation, we directly employed wordlevel NER methods to evaluate the performance, which are denoted as Gold seg. Otherwise, we first used open source segmentation toolkit<ref type="foot" target="#foot_5">6</ref> to automatically segment the datasets. Then wordlevel NER methods are applied, which are denoted as Auto seg. The bi-directional LSTM and CNN are also applied as baselines.</p><p>Word-level methods + char + bichar: For characters in the subsequence w b,e , we first used a bi-directional LSTM to learn their hidden states and bigram states. We then augmented the wordlevel methods with the character-level features. Lattice LSTM: Lattice LSTM <ref type="bibr" target="#b53">(Zhang and Yang, 2018)</ref> incorporates word information into character-level recurrent units, which can avoid segmentation errors. This method achieved stateof-the-art performance on the four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyper-parameter Settings</head><p>We used the Adam (Kingma and Ba, 2014) as the optimizer, with a learning rate of 2e-5 for large datasets like Ontonotes and MSRA, while a rate of 2e-4 for small datasets Weibo and Resume. A densely connected structure <ref type="bibr" target="#b19">(Huang et al., 2017)</ref> was applied, which composites all hidden states from previous update steps as final inputs for aggregation modules at step t. To further reduce overfitting, we employed the Dropout <ref type="bibr" target="#b38">(Srivastava et al., 2014)</ref> with a rate of 0.5 for embeddings and a rate of 0.2 for aggregation module outputs. The embedding size and state size were both set to 50. The head number of multi-head attention was 10. The head dimension was set to 10 for small datasets like Weibo and Resume, while the head dimension was 20 for Ontonotes and MSRA.</p><p>Step number T was selected among {1, 2, 3, 4, 5, 6}, which is detailed analyzed in § 5.2. The standard Precision (P), Recall (R), and F1-score (F1) were used as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, we demonstrate the main results of LGN for the Chinese NER task across different domains. The model achieving best results on the development set was chosen for the final evaluation on the test set. We also probe the  effectiveness and interpretability of LGN by explanatory experiments.</p><formula xml:id="formula_13">Models P R F1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>OntoNotes Table <ref type="table" target="#tab_2">2</ref> <ref type="foot" target="#foot_6">7</ref> shows the results of wordlevel and character-level methods on OntoNotes with various settings. In the gold or automatic segmentation settings, the char and bichar features boost the performance of word-level methods. In particular, with the gold-standard segmentation, these methods are able to achieve competitive state-of-the-art results on the dataset <ref type="bibr" target="#b2">(Che et al., 2013;</ref><ref type="bibr" target="#b41">Wang et al., 2013)</ref>. However, the goldstandard segmentation is not always available. On the other hand, the automatic segmentation may induce word segmentation errors and result in a loss of performance for the downstream NER task. A feasible solution is applying character-level methods to avoid the need for word segmentation.</p><p>Our proposed LGN is a character-level model based on graphic structure. It outperforms lattice LSTM by 1.01% in F1 score and leads to a 3.00% increment of F1 score over the LSTM with bichar and softword features. The LGN also significantly outperforms the word-level models with automatic segmentation. MSRA/Weibo/Resume Results on the MSRA, Weibo, and Resume datasets are shown in Table 3, 4, and 5, respectively. Gold-standard segmentation is not available for the Weibo and Resume datasets and the test set of MSRA. The best classic methods leverage rich handcrafted features <ref type="bibr" target="#b3">(Chen et al., 2006;</ref><ref type="bibr" target="#b51">Zhang et al., 2006;</ref><ref type="bibr" target="#b55">Zhou et al., 2013)</ref>, embedding features <ref type="bibr" target="#b27">(Lu et al., 2016;</ref><ref type="bibr" target="#b34">Peng and Dredze, 2015)</ref>, radical features <ref type="bibr" target="#b8">(Dong et al., 2016)</ref>  supervised data <ref type="bibr" target="#b16">(He and Sun, 2017)</ref> for Chinese NER. Compared with the existing methods and the word-level and character-level methods, our</p><p>LGN model gives the best results by a large margin. Moreover, different from the lattice LSTM, which also leverages lexicon features, our</p><p>LGN model integrates lexicon information into the graph neural network in a more effective fashion.</p><p>As a result, it outperforms the lattice LSTM on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Steps of Message Passing</head><p>To investigate the influence of step number T during the update process, we analyzed the performance of LGN under different step numbers. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the variation of F1 score on the development sets<ref type="foot" target="#foot_7">8</ref> as the step number increases.</p><p>We used D-F1 to represent the F1 scores at different steps minus the best results.</p><p>The results indicate that the number of update steps is crucial to the performance of LGN, which peaks when T ≥ 3 on all four datasets. The F1 score decreases 1.20% on average against the best results when the step number is less than 3. In particular, the F1 score of the OntoNotes and Weibo datasets even suffered a serious reduction around 1.5% and 1.8%, respectively. After several rounds of updates, the model gives steady and competitive results and reveals that LGN benefits from the update process. Empirically, at each update step, graph nodes aggregate information from their neighbors and incrementally gain more information from further reaches of the graph as the process iterates <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>. In the LGN model, more valuable information can be captured through the recursive aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiments</head><p>To study the contribution of each component in LGN, we conducted ablation experiments on the four datasets and display the results in Table <ref type="table" target="#tab_7">6</ref>.</p><p>The results show that the model's performance is degraded if the global relay node is removed, indicating that global connections are useful in the graph structure. We also find that lexicons play an important role in character-level NER.</p><p>In particular, the performance of the OntoNotes, MSRA and Weibo datasets are seriously hurt by over 3.0% without lexicons. Moreover, missing both edges and the global node will cause a further performance loss.</p><p>To better illustrate the advantage of our model, we remove the CRF decoding layer and simplify the structure to a non-bidirectional version on both LGN and the lattice LSTM model. The results show that, with a single direction structure, the LGN achieves a higher F1 score by 0.77% on average than the lattice LSTM. In addition, the two models have an obvious performance gap when they get rid of the CRF layer. The F1 score of LGN decreases by 3.59% on average on the four datasets without CRF. In contrast, the lattice LSTM decreases by 6.24%. It manifests the LGN has stronger ability to model sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Against Sentence Length</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the performance of LGN and several baseline models on the OntoNotes dataset.  We split the dataset into six parts according to the sentence length. The lattice is a strong baseline that outperforms the word+char+bichar and char+bichar+softword models over different sentence lengths. However, the lattice accuracy decreases significantly as the sentence length increases. In contrast, the LGN not only gives higher results over short sentences, but also shows its effectiveness and robustness when the sentence length is more than 80 characters. It gives a higher F1 score in most cases compared to the baselines, which indicates that global sentence semantics and long-range dependency can be better captured under the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>Table <ref type="table" target="#tab_8">7</ref> illustrates an example that probes the ability of LGN to tackle the word ambiguity problems. The lattice LSTM ignores the sentence context and wrongly identifies "印 度(India)".</p><p>Removing the global relay node, LGN also makes the same mistake, which indicates that global connections are indispensable and can capture high-level information to help LGN better understand the sentence context. In contrast, with the global relay node, the LGN can correctly identify the entity boundary, even though the  graph composition states are updated for only one step. However, it gives an incorrect class of the entity "印度河(The Indus River)", which is a location entity but not a GPE (Geo-Political Entity). Because of the multi-step graph message passing process, the LGN is able to fuse the context information and finally detects the correct location entity in success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we investigated a GNN-based approach to alleviate the word ambiguity in Chinese NER. Lexicons are used to construct the graph and provide word-level features. The LGN enables interactions among different sentence compositions and can capture non-sequential dependencies between characters based on the global sentence semantics. As a result, it shows improved performance significantly on multiple datasets in different domains. The explanatory experiments also illustrate the effectiveness and interpretability of our proposed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of graph construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Aggregation in LGN. Red indicates the element that is being updated, and black indicates other elements involved in the aggregation. The aggregation results are then used in update modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b). Hence, different from the classic graph neural networks that use the features of terminal vertices to aggregate edges, we use the whole matching character subsequence C t b,e = {c t b , . . . , c t e } for the edge aggregation function, as follows:c → e : êt b,e = MultiAtt(e t b,e , C t b,e ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F1 variation under different update steps on the development sets. D-F1 represents the F1 scores at different steps minus the best results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F1 score against sentence length on the OntoNotes dateset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(GPE) O O B M M E (GPE) 印度河 (GPE) 流经 巴基斯坦 (GPE)The Indus River (GPE) flows through Pakistan (GPE).LGNB M E (LOC) O O B M M E (GPE) 印度河 (LOC) 流经 巴基斯坦 (GPE)The Indus River (LOC) flows through Pakistan (GPE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main results on OntoNotes.</figDesc><table><row><cell cols="2">Input Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>Yang et al. (2016)</cell><cell>65.59</cell><cell cols="2">71.84 68.57</cell></row><row><cell></cell><cell>Yang et al. (2016)* †</cell><cell>72.98</cell><cell cols="2">80.15 76.40</cell></row><row><cell></cell><cell>Che et al. (2013)*</cell><cell>77.71</cell><cell cols="2">72.51 75.02</cell></row><row><cell>Gold</cell><cell>Wang et al. (2013)*</cell><cell>76.43</cell><cell cols="2">72.32 74.32</cell></row><row><cell>seg.</cell><cell>Word-level LSTM</cell><cell>76.66</cell><cell cols="2">63.60 69.52</cell></row><row><cell></cell><cell>+char+bichar</cell><cell>78.62</cell><cell cols="2">73.13 75.77</cell></row><row><cell></cell><cell>Word-level CNN</cell><cell>66.84</cell><cell cols="2">62.99 64.86</cell></row><row><cell></cell><cell>+char+bichar</cell><cell>68.22</cell><cell cols="2">72.37 70.24</cell></row><row><cell></cell><cell>Word-level LSTM</cell><cell>72.84</cell><cell cols="2">59.72 65.63</cell></row><row><cell>Auto</cell><cell>+char+bichar</cell><cell>73.36</cell><cell cols="2">70.12 71.70</cell></row><row><cell>seg.</cell><cell>Word-level CNN</cell><cell>54.62</cell><cell cols="2">55.20 54.91</cell></row><row><cell></cell><cell>+char+bichar</cell><cell>64.69</cell><cell cols="2">65.09 64.89</cell></row><row><cell></cell><cell>Char-level LSTM</cell><cell>68.79</cell><cell cols="2">60.35 64.30</cell></row><row><cell></cell><cell>+bichar+softword</cell><cell>74.36</cell><cell cols="2">69.43 71.89</cell></row><row><cell>No</cell><cell>Char-level CNN</cell><cell>56.78</cell><cell cols="2">60.99 58.81</cell></row><row><cell>seg.</cell><cell>+bichar+softword</cell><cell>59.60</cell><cell cols="2">65.14 62.25</cell></row><row><cell></cell><cell>Lattice LSTM</cell><cell>76.35</cell><cell cols="2">71.56 73.88</cell></row><row><cell></cell><cell>LGN</cell><cell cols="3">76.13 73.68 74.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Main results on MSRA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>, cross-domain, and semi-Main results on Weibo.</figDesc><table><row><cell>Models</cell><cell>NE</cell><cell>NM</cell><cell>All</cell></row><row><cell>Peng and Dredze (2015)</cell><cell cols="3">51.96 61.05 56.05</cell></row><row><cell cols="4">Peng and Dredze (2015)* 55.28 62.97 58.99</cell></row><row><cell>He and Sun (2016)</cell><cell cols="3">50.60 59.32 54.82</cell></row><row><cell>He and Sun (2017)*</cell><cell cols="3">54.50 62.17 58.23</cell></row><row><cell>Word-level LSTM</cell><cell cols="3">36.02 59.38 47.33</cell></row><row><cell>+char+bichar</cell><cell cols="3">43.40 60.30 52.33</cell></row><row><cell>Char-level LSTM</cell><cell cols="3">46.11 55.29 52.77</cell></row><row><cell>+bichar+softword</cell><cell cols="3">50.55 60.11 56.75</cell></row><row><cell>Lattice LSTM</cell><cell cols="3">53.04 62.25 58.79</cell></row><row><cell>LGN</cell><cell cols="3">55.34 64.98 60.21</cell></row><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">Word-level LSTM 93.72 93.44 93.58</cell></row><row><cell>+char+bichar</cell><cell cols="3">94.07 94.42 94.24</cell></row><row><cell>Char-level LSTM</cell><cell cols="3">93.66 93.31 93.48</cell></row><row><cell cols="4">+bichar+softword 94.53 94.29 94.41</cell></row><row><cell>Lattice LSTM</cell><cell cols="3">94.81 94.11 94.46</cell></row><row><cell>LGN</cell><cell cols="3">95.28 95.46 95.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Main results on Resume.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>An ablation study of LGN. F1 scores were evaluated on the development sets.</figDesc><table><row><cell>Models</cell><cell></cell><cell>Onto-Notes</cell><cell cols="4">MSRA Weibo Resume</cell></row><row><cell>LGN</cell><cell></cell><cell>71.96</cell><cell></cell><cell>93.46</cell><cell cols="2">62.42</cell><cell>94.43</cell></row><row><cell>-global</cell><cell></cell><cell>71.26</cell><cell></cell><cell>92.99</cell><cell cols="2">62.30</cell><cell>94.31</cell></row><row><cell cols="3">-edge /lexicon 65.88</cell><cell></cell><cell>89.63</cell><cell cols="2">59.19</cell><cell>94.05</cell></row><row><cell cols="2">-edge-global</cell><cell>65.34</cell><cell></cell><cell>89.47</cell><cell cols="2">58.62</cell><cell>94.09</cell></row><row><cell cols="2">-bidirection</cell><cell>67.52</cell><cell></cell><cell>90.98</cell><cell cols="2">59.67</cell><cell>94.23</cell></row><row><cell>-crf</cell><cell></cell><cell>66.37</cell><cell></cell><cell>91.13</cell><cell cols="2">57.73</cell><cell>92.70</cell></row><row><cell cols="2">Lattice LSTM</cell><cell>71.62</cell><cell></cell><cell>93.18</cell><cell cols="2">61.64</cell><cell>93.64</cell></row><row><cell cols="2">-bidirection</cell><cell>66.63</cell><cell></cell><cell>90.72</cell><cell cols="2">58.75</cell><cell>93.21</cell></row><row><cell>-crf</cell><cell></cell><cell>61.74</cell><cell></cell><cell>85.38</cell><cell cols="2">55.71</cell><cell>92.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LGN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Lattice LSTM</cell></row><row><cell></cell><cell>80.0</cell><cell></cell><cell></cell><cell cols="3">Word + char + bichar Char + bichar + softword</cell></row><row><cell>F1 Score (%)</cell><cell>75.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20 &lt;</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>&gt;100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sentence Length</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>An example with overlapping ambiguity. Contents with red and blue colors represent incorrect and correct entities, respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://en.wikipedia.org/wiki/Transpose graph</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.weibo.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://finance.sina.com.cn/stock/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://catalog.ldc.upenn.edu/LDC2011T13</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/jiesutd/LatticeLSTM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/lancopku/PKUSeg-python</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">In Table2, 3, 4 and 5, the models with * use external labeled data for semi-supervised learning. Those with † also use discrete features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">Since the MSRA dataset does not have the development set, we used the test set instead.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&amp;D Program (No.2018YFC0831105, 2018YFB1005104, 2017YFB1002104), National Natural Science Foundation of China (No. 61976056, 61532011, 61751201), Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), STCSM (No.16JC1420401, 17JC1420200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition with conditional probabilistic models</title>
		<author>
			<persName><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Core techniques of question answering systems over knowledge bases: a survey</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Diefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Maret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KAIS</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="569" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterbased lstm-crf with radical-level features for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Chuanhai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Understanding and Intelligent Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A study on features of the crfs-based chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Huanzhong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">IJAI</biblScope>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cnn-based chinese ner with lexicon rethinking</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09113</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Startransformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">F-score driven max margin neural network for named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04234</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition and word segmentation based on character</title>
		<author>
			<persName><forename type="first">Jingzhou</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Sixth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne Levow ; Haibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2014</date>
			<biblScope unit="page" from="2532" to="2536" />
		</imprint>
	</monogr>
	<note>LREC</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition with a sequence labeling approach: based on characters</title>
		<author>
			<persName><forename type="first">Zhangxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="634" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiprototype chinese character embedding</title>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word segmentation of overlapping ambiguous strings during chinese reading</title>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingshan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1046</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ambiguity measures in requirement engineering</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Mich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Garigliano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Theory and Practice. ICS</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution. CoNLL-2014</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
				<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The constituency model of chinese word identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Perfetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Hai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Effective bilingual constraints for semi-supervised learning of named entity recognizers</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Seventh AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Ontonotes release 4.0</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Philadelphia</forename><surname>Ldc2011t03</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penn</forename></persName>
		</author>
		<imprint>
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as lmr tagging</title>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second SIGHAN workshop on Chinese language processing</title>
				<meeting>the second SIGHAN workshop on Chinese language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="176" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Combining discrete and neural features for sequence labeling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05679</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Usage of statistical cues for word boundary in reading chinese sentences</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Miao-Hsuan Yen</surname></persName>
		</author>
		<author>
			<persName><surname>Radach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-L</forename><surname>Ovid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie-Li</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reading and writing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1007" to="1029" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decomposed storage in the chinese lexicon</title>
		<author>
			<persName><forename type="first">Biyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danling</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="131" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Word segmentation and named entity recognition for sighan bakeoff3</title>
		<author>
			<persName><forename type="first">Suxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sentence-state lstm for text representation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02474</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02023</idno>
		<title level="m">Chinese ner using lattice lstm</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10185</idno>
		<imprint>
			<date type="published" when="2018">2018c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition via joint identification and categorization</title>
		<author>
			<persName><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese journal of electronics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Words, morphemes and syllables in the chinese mental lexicon</title>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Marslen-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
