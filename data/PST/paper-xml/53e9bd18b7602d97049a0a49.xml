<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust visual tracking based on online learning sparse representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-06-07">7 June 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<email>h.yao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of Electronics</orgName>
								<orgName type="department" key="dep2">Communications and Information Technology</orgName>
								<orgName type="institution">Queen&apos;s University Belfast</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust visual tracking based on online learning sparse representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-06-07">7 June 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">8FD5C7C770CDB4219CDA72AC48DA4415</idno>
					<idno type="DOI">10.1016/j.neucom.2011.11.031</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visual tracking Sparse representation Online learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Handling appearance variations is a very challenging problem for visual tracking. Existing methods usually solve this problem by relying on an effective appearance model with two features: (1) being capable of discriminating the tracked target from its background, (2) being robust to the target's appearance variations during tracking. Instead of integrating the two requirements into the appearance model, in this paper, we propose a tracking method that deals with these problems separately based on sparse representation in a particle filter framework. Each target candidate defined by a particle is linearly represented by the target and background templates with an additive representation error. Discriminating the target from its background is achieved by activating the target templates or the background templates in the linear system in a competitive manner. The target's appearance variations are directly modeled as the representation error. An online algorithm is used to learn the basis functions that sparsely span the representation error. The linear system is solved via ' 1 minimization.</p><p>The candidate with the smallest reconstruction error using the target templates is selected as the tracking result. We test the proposed approach using four sequences with heavy occlusions, large pose variations, drastic illumination changes and low foreground-background contrast. The proposed approach shows excellent performance in comparison with two latest state-of-the-art trackers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The purpose of visual tracking is to estimate the state of the tracked target in a video. It has wide applications such as intelligent video surveillance, advanced human computer interaction, robot navigation and so on. It is usually formulated as a search task where an appearance model is first used to represent the target and then a search strategy is utilized to infer the state of the target in current frame. Therefore, how to effectively model the appearance of the target and how to accurately infer the state from all candidates are two key steps for a successful tracking system. Although a variety of tracking algorithms have been proposed in the last decades, visual tracking still cannot meet the requirements of practical applications. The main difficulty of visual tracking is designing a powerful appearance model which should not only discriminate the target from its surrounding background but also be robust to its appearance variations. For the former issue, some promising progresses have been achieved recently by considering visual tracking as a two-class classification or detection problem. Many elegant features in the field of pattern recognition can be used to discriminate the target from its background. However, the latter is very difficult to achieve since there are a large number of un-predictive appearance variations over time such as pose changes, shape deformation, illumination changes, partial occlusion and so on.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), the candidate marked by the blue rectangle is a ''bad'' candidate for tracking because there are a large number of background pixels inside it. An effective appearance model should not consider this candidate as the tracking result. In contrast, although the candidate marked by the red rectangle is partially occluded by the book, it is still a ''good'' candidate and should be considered as the tracking result. Existing appearance models in visual tracking cannot meet these requirements. For example, HSV histogram was widely used to model the target's appearance <ref type="bibr" target="#b0">[1]</ref>. We show the HSV histograms of the target template and two candidates in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Although modeling appearance with a HSV histogram can reflect the difference between the target template and the ''bad'' candidate, it also models the difference between the target template and the ''good'' candidate as shown in the tails of the three histograms. Furthermore, when certain similarity measure between two histograms is used as the data likelihood, the tracker may wrongly find the ''bad'' candidate as the final tracking result. For example, when Battacharyya coefficients are adopted to measure the similarity between two histograms, the similarity between the target template and the ''bad'' candidate is 0.852, which is larger than 0.829-the similarity between the target template and the ''good'' candidate. Therefore, traditional tracking methods that resort to an effective appearance model to achieve robust tracking are not always feasible.</p><p>Recently, sparse representation has attracted much attention in the field of computer vision <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b1">[2]</ref>, a robust face recognition method was proposed and the robustness to occlusions was achieved by introducing an error vector in the sparse representation model. Motivated by this idea, Mei and Ling proposed a robust visual tracking method based on sparse representation <ref type="bibr" target="#b2">[3]</ref>. In their method, partial occlusion, appearance variances and other challenging issues were considered as the error vector represented by a set of trivial templates. We realized that <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> used column vectors of the identity matrix as basis functions to linearly represent the error vector. These methods have some drawbacks. First, the sparsity cannot be met. Each basis function used in their method can model whether one pixel position is occluded or not. It assumes that the occluded pixels occupy a relatively small portion of the entire image and the error vector has sparse nonzero entries. However, this assumption may not hold in real world especially when the target is severely occluded during tracking. For example, as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, the candidate marked by the red rectangle has almost half of the face occluded by the book. In this case, the representation coefficients are not sparse as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. Second, there are not basis vectors corresponding to background region. For example, the candidate marked by the blue rectangle in Fig. <ref type="figure" target="#fig_0">1</ref>(a) contains some background pixels, its representation coefficients will also not be sparse as shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>). Finally, Mei's method uses a fixed basis to represent the error vector during the entire tracking. However, in practice, the error vector may change with the environment over time. Therefore, the fixed basis determined before tracking begins cannot effectively adapt the error vector to the environment's changes.</p><p>In this paper, motivated by the online basis learning for sparse coding <ref type="bibr" target="#b4">[5]</ref>, we proposed a novel tracking framework based on online learning sparse representation, which overcomes the aforementioned problems and achieves more robust results. The key idea of the proposed method is to simultaneously achieve discriminating the target from its background and being robust to its appearance variations separately based on sparse representation in a particle filter framework. Specifically, we sparsely represent each target candidate defined by a particle using target templates, background templates and error basis. The introduction of both the target and background templates in sparse representation is capable of discriminating the target from its background. For example, for the candidate corresponding to the target region, only target templates play key roles in the linear representation. In contrast, for the candidate corresponding to the background region, only background templates play roles in the linear representation. the target's appearance variants are modeled as the error in the linear representation. The error is spanned by a set of basis functions which are learned online when new observations are available over time. The representation coefficients are computed via ' 1 minimization. Each candidate is weighted in the particle filter framework based on the residuals when it is projected on the target templates. The tracking result is the weighted mean of all particles. The rationalities behind the proposed method are two-folds: First, although appearance variations are unpredictable during tracking, they can still be compactly represented in certain subspace. Using the learned basis to represent the appearance variations can assure that the representation coefficients are sparse. Second, online basis learning can adaptively model unpredictable appearance variations during tracking, therefore improve the robustness of the tracking method.</p><p>The rest of the paper is organized as follows. In Section 2 related methods on visual tracking and sparse representation are summarized. Section 3 details the tracking algorithm based on online learning sparse representation. Experimental results on four sequences are reported in Section 4. We conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual tracking</head><p>In this subsection, in order to clarify the motivations of the proposed method, we reviewed the various appearance models and inference methods for visual tracking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Appearance modeling</head><p>Appearance modeling is used to represent the tracked target using information extracted from the target region. In the literature, widely used features are color <ref type="bibr" target="#b5">[6]</ref>, shape <ref type="bibr" target="#b6">[7]</ref>, texture <ref type="bibr" target="#b7">[8]</ref>, combination of color and texture <ref type="bibr" target="#b8">[9]</ref>, SIFT features <ref type="bibr" target="#b9">[10]</ref>, combination of color and SIFT <ref type="bibr" target="#b10">[11]</ref> and attentional features <ref type="bibr" target="#b11">[12]</ref>. For many tracking situations, color is a good choice for representing the tracked target because of its descriptive power and the fact that color information is readily accessible in image. Since color histogram was first proposed in <ref type="bibr" target="#b12">[13]</ref>, it has been successfully used to represent the target in visual tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> due to its simplicity, efficiency, and robustness. However, an evident limitation of color histogram is that it models the color distribution in a region but ignores the spatial relationship among pixels. Adding spatial information into color histogram improves the representation power. Along this line, color spatiogram <ref type="bibr" target="#b14">[15]</ref> and color correlogram <ref type="bibr" target="#b15">[16]</ref> are also employed for visual tracking. These methods model the target's appearance from a global perspective. When the target is locally occluded, the representation ability will significantly degrade. In order to overcome this shortcoming, in <ref type="bibr" target="#b16">[17]</ref>, a target candidate was divided into multiple patches and each one was represented by a color histogram. The robustness to occlusions was achieved by combining the vote maps of the multiple patches.</p><p>The appearance models mentioned above are fixed before tracking begins, which cannot effectively handle the target's appearance variations because such variations usually incur during tracking. Jepson and colleagues <ref type="bibr" target="#b17">[18]</ref> proposed a framework to learn an adaptive appearance model, which adapts to the changing appearance over time. In <ref type="bibr" target="#b18">[19]</ref>, an online feature selection method was proposed to select features that are able to discriminate the target from its background. Since the feature selection is online when new observations are available, the selected features adapt to environment changes and achieve superior tracking results especially when the intensity difference between target and background is very small. In <ref type="bibr" target="#b19">[20]</ref>, a tracking method was proposed to incrementally learn a low-dimensional subspace representation, which efficiently adapts online to appearance changes. When the target suffered large changes in pose, scale and illumination, the method still accurately tracked the target. In <ref type="bibr" target="#b20">[21]</ref>, Kue et al. proposed an AdaBoost based algorithm for learning a discriminative appearance model for multi-target tracking, which allows the models to adapt to target variations over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Inference methods</head><p>Given the appearance model of the target template, how to infer the target's state in current frame? Existing methods can be roughly classified into two classes: optimization and approximations. The first class performs a direct optimization procedure. The widely used optimization procedures are iterative gradient based search <ref type="bibr" target="#b21">[22]</ref> and linear program <ref type="bibr" target="#b22">[23]</ref>. The significant advantage of this method is its efficiency. However, the performance of optimization is not very stable especially in complex scenes. For example, once the tracker fails, it cannot recover from the lost. The second class uses probabilistic approximation to perform inference. Seminal work includes Kalman filter <ref type="bibr" target="#b23">[24]</ref> and particle filter <ref type="bibr" target="#b0">[1]</ref>. Particle filter has been successfully used in many tracking systems due to its efficiency and effectiveness.</p><p>In recent years, some novel tracking frameworks were proposed with impressive tracking performances. In <ref type="bibr" target="#b24">[25]</ref>, gait analysis was introduced to improve the tracking performance. In <ref type="bibr" target="#b25">[26]</ref>, visual tracking was embedded into a metric learning framework. Tracking-by-detection method <ref type="bibr" target="#b26">[27]</ref> formulated visual tracking as a detection problem that detects whether the target appears or not in each candidate region. Many sophisticated machine learning technologies were introduced in this framework, e.g., semisupervised classifier <ref type="bibr" target="#b27">[28]</ref> and combining of supervised and semisupervised classifiers <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse representation</head><p>In this section, we review related work including applications of sparse representation in computer vision and the dictionary learning for sparse representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Applications of sparse representation</head><p>Recently sparse representation has been widely used in computer vision tasks, including face recognition <ref type="bibr" target="#b1">[2]</ref>, denoising and inpainting <ref type="bibr" target="#b29">[30]</ref> and visual tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In these applications, the most related methods to us are <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. In <ref type="bibr" target="#b1">[2]</ref>, a robust face recognition method was proposed, in which each test sample was linearly Instead of representing each target candidate with target and trivial templates <ref type="bibr" target="#b2">[3]</ref>, in <ref type="bibr" target="#b3">[4]</ref> a novel sparse representation framework was also proposed for visual tracking, which represents the target template with target candidates. The candidate with the largest coefficient was considered as tracking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Dictionary learning</head><p>Dictionary learning in sparse representation means to learn a dictionary from training data instead of using a predefined one. Since the learned dictionary can reflect the underlying structure of the data, it is more effective than the predefined one. The successful applications of learned dictionary in many visual systems <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> have verified its excellent performance. Discriminative learning method was also used in dictionary learning for face recognition <ref type="bibr" target="#b32">[33]</ref>. The learned dictionary cannot only sparsely represent each sample but also discriminate samples from different classes. In <ref type="bibr" target="#b4">[5]</ref>, an online dictionary learning method was proposed, which processes a small subset of the training set at a time. The computation cost is very low, which is particularly important in the context of image and video processing <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tracking based on online learning sparse representation</head><p>In this section, we give the details of the proposed tracking method based on online learning sparse representation. We briefly review the particle filter based tracking framework, and then formulate visual tracking as the sparse representation problem which simultaneously models the target and background templates as well as the error basis in a linear system, The online learning of the error basis and the template update are introduced, followed by a speed up strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Particle filter tracking framework</head><p>The proposed algorithm is built upon a widely used tracking framework based on particle filter <ref type="bibr" target="#b0">[1]</ref>, which uses a set of weighted particles to approximate the posterior probability distribution of target's state. Due to its efficiency and robustness, excellent performance is obtained both in single object <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref> and multiple target tracking <ref type="bibr" target="#b35">[36]</ref>. Let x t denote the target's state at time t. For example x t can be a vector that consists of the coordinate and size of the tracked target <ref type="bibr" target="#b0">[1]</ref>. Given a set of observed images z 1:t ¼ fz 1 ,z 2 , . . . ,z t g, particle filter can be used to infer the posterior probability pðx t 9z 1:t Þ using a set of weighted samples fx i t ,p i t g i ¼ 1:N where x i t is the i-th particle at time t and p i t is the corresponding weights. The tracking result can be denoted as the weighted average of all particles ð1=NÞ P N i ¼ 1 p i t x i t or the particle with the largest weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simultaneously modeling target, background and error in linear system</head><p>For each particle x i t , we can obtain an image region using the coordinate and size in the particle. We then normalize the region to the predefined sizes w Â h. Gray values of all pixels in this region can be formed a 1D vector y i t A R d ðd ¼ w Â hÞ. In this paper, motivated by the success of sparse representation in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we linearly represent each target candidate by a set of target and background templates. The target templates are normalized regions corresponding to the initially tracked target and updated during the tracking. The background templates are normalized regions around the initially tracked target and also updated over time. Specifically, at time t, given template set P t ¼ ½p t,1 , . . . p t,n A R dÂn , containing n f target templates and nÀn f background templates, a target candidate y i t can be approximately represented by P t as</p><formula xml:id="formula_0">y i t % a i t,1 p t,1 þ a i t,2 p t,2 þ Á Á Á þa i t,n p t,n ¼ P t a i t ,<label>ð1Þ</label></formula><p>where a i t ¼ ða i t,1 ,a i t,2 , . . . ,a i t,n Þ T A R n is the coefficient vector of i-th candidate at time t. It should be noted that our linear system (Eq. ( <ref type="formula" target="#formula_0">1</ref>)) is different from <ref type="bibr" target="#b2">[3]</ref> which only used target templates in the linear system. In our system, both target and background templates are used, which is able to handle more complex appearance variations. Intuitively, for a ''good'' target candidate, only target templates will be activated in the linear representation system. Similarly, for a ''bad'' target candidate, the coefficients corresponding to target templates tend to be zeros. However, in practical scenarios, target's appearance will change drastically due to heavy occlusions, large pose variations, and drastic illumination changes. Previous methods focus on developing an effective appearance models that are robust to these variations. In this paper, we just use a simple appearance model (pixel intensity). The robustness to appearance variations is obtained by modeling appearance variations as the representation error as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> </p><formula xml:id="formula_1">y i t ¼ P t a i t þ e i t ,<label>ð2Þ</label></formula><p>where e i t is the representation error of the i-th candidate at time t.</p><p>In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, they used identity matrix I A R d as the basis set to span the error vector e i t . In other words, they assume that the e i t can be represented in the pixel coordinates and this representation is sparse. We argue that this assumption is not valid when the appearance variations are significant. For example, when the target is occluded severely, the error e i t will not be sparse. The reason is that a larger proportion of pixels are occluded, accordingly, a larger proportion of basis vectors will be activated. Instead of using the identity matrix as the basis set, we adopt an online learned basis to span the error e i t . Let E t ¼ ½e t,1 , . . . ,e t,d A R dÂd be the basis set learned at time t using online basis learning for sparse coding <ref type="bibr" target="#b4">[5]</ref>, then e i t can be sparsely represented by</p><formula xml:id="formula_2">e i t % b i t,1 e t,1 þb i t,2 e t,2 þ Á Á Á þb i t,d e t,d ¼ E t b i t ,<label>ð3Þ</label></formula><formula xml:id="formula_3">where b i t ¼ ½b i t,1 ,b i t,2 , . . . ,b i t,d T A R d is the sparse coefficient vector.</formula><p>Then the target candidate can be represented as</p><formula xml:id="formula_4">y i t ¼ P t a i t þ E t b i t ¼ ½P t , E t a i t b i t " # ¼ B t c i t ,<label>ð4Þ</label></formula><formula xml:id="formula_5">where B t ¼ ½P t E t A R dÂðn þ dÞ is the basis set and c i t ¼ ½ a i t b i t A R n þ d is</formula><p>the coefficient vector. It also should be noted that although the form of Eq. ( <ref type="formula" target="#formula_4">4</ref>) is similar with the one in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, the intrinsic meanings are different because we use the online learned basis instead of the column vectors of the identity matrix. Since n 5 d and b i t is sparse, the coefficient vector c i t is sparse and can be obtained by ' 1 minimization:</p><formula xml:id="formula_6">ĉi t ¼ arg min c i t Jc i t J 1 subject to y i t ¼ B t c i t :<label>ð5Þ</label></formula><p>Fig. <ref type="figure" target="#fig_1">2(c</ref>) and (d) is the obtained coefficient vectors with size of 190 of the ''good'' and ''bad'' candidates in our method. Their numbers of nonzero coefficients are 65 and 70, respectively, which shows that the sparsity of the coefficient vector obtained by our learned basis is guaranteed. In contrast, if the identity matrix is used as the basis set in <ref type="bibr" target="#b2">[3]</ref>, the obtained coefficient vectors are not sparse as shown in Fig. <ref type="figure" target="#fig_1">2</ref> Instead of projecting the candidate only on the target templets as in <ref type="bibr" target="#b2">[3]</ref>, we compute the residual of the candidate y i t after it was projected on the target templates as well as error basis:</p><formula xml:id="formula_7">rðy i t Þ ¼ Jy i t ÀP t d f ða i t ÞÀE t b i t J 2 ,<label>ð6Þ</label></formula><p>where d f ða i t Þ is a new vector whose only nonzero entries are the entries in a i t that are associated with target templates. The ''best'' target candidate is the one that has the smallest residual with index I ¼ arg min i rðy i t Þ. The ''worst'' target candidate is the one that has the largest residual with index @ ¼ arg max i rðy i t Þ. Then the tracking result at current time is the particle x I t which corresponds to the ''best'' target candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Template update and online basis learning</head><p>In this paper, we adopt a simple weight based update strategy. We weight all target template according to their roles in representing the ''best'' target candidate. Similarly, all background templates are weighted according to their roles in representing the ''worst'' target candidate. Specifically, we weight j-th target template with its coefficient a I t,j , j ¼ 1, . . . ,n f when representing the ''best'' target candidate. Then we sort all target templates in descending order according to their weights. We weight j-th background template with its coefficient a @ t,j þ n f ,j ¼ 1, . . . ,nÀn f when represent the ''worst'' target candidate. All background templates are sorted in descending order according to their weights. The target template with the lowest weight is updated using the ''best'' candidate as</p><formula xml:id="formula_8">y n f t ¼ y n f t þð1ÀZÞy I t ,<label>ð7Þ</label></formula><p>where Z is the update rate parameter. The background template with the lowest weight is updated using the ''worst'' candidate as</p><formula xml:id="formula_9">y n t ¼ y n t þð1ÀZÞy @ t :<label>ð8Þ</label></formula><p>We model the target's appearance variations by introducing an error vector in Eq. ( <ref type="formula" target="#formula_1">2</ref>). In order to assure that the coefficient vector b i t is sparse and adaptive to target's appearance variations over time, we use the online basis learning algorithm <ref type="bibr" target="#b4">[5]</ref> to learn the error basis set E t . Specifically, at time t, the target template with the largest weight is p 1 t . We collect training samples fs i t ¼ y i t Àp 1 t g i ¼ 1:N . The i-th sample is represented by all column vectors of E tÀ1 and the representation coefficient vector a i t can be computed by sparse coding model <ref type="bibr" target="#b36">[37]</ref>. The current basis set E t can be obtained by minimizing the cost function:</p><formula xml:id="formula_10">CðE t Þ ¼ X N i ¼ 1 Js i t ÀE t a i t J 2 2 þlJa i J 1 :<label>ð9Þ</label></formula><p>The detailed steps of solving this minimization problem can be found in <ref type="bibr" target="#b4">[5]</ref>. An illustration of some learned basis vectors on the face sequence are shown in Fig. <ref type="figure" target="#fig_3">3</ref>, from which we can see that some basis vectors (e.g., marked by white dashed rectangle) reflect how the face is occluded by the book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reducing computation time</head><p>In practical tracking application, the linear system Eq. ( <ref type="formula" target="#formula_4">4</ref>) is very large. For instance, if the normalized size of the target is 64 Â 64, the dimension of the feature vector y i t is on the order of 10 3 . Although solving Eq. ( <ref type="formula" target="#formula_6">5</ref>) relies on scalable methods such as linear programming, the computation cost is still beyond the capability of normal computers. In <ref type="bibr" target="#b37">[38]</ref>, a rapid face recognition based on sparse representation is proposed, where the computation time reduction is achieved by applying a hash matrix to both sides of the linear system. In this paper, we also utilize the hash matrix to reduce the computation cost of the proposed method.</p><p>Let s A f1, . . . ,Sg be the seed, we can define a hash function h s ðj,dÞ : N-f1, . . . ,dg. The hash matrix H ¼ ðH ij Þ is then defined as in <ref type="bibr" target="#b37">[38]</ref> H ij ¼ 2h s ðj,2ÞÀ3, h s ðj,dÞ ¼ i,8s A f1, . . . ,Sg,</p><formula xml:id="formula_11">0 otherwise: (<label>ð10Þ</label></formula><p>Applying H to both sides of Eq. ( <ref type="formula" target="#formula_4">4</ref>) yields</p><formula xml:id="formula_12">Hy i t ¼ HB t c i t ,<label>ð11Þ</label></formula><p>which can be solved the following ' 1 minimization:</p><formula xml:id="formula_13">ĉi t ¼ arg min c i t Jc i t J 1 subject to Hy i t ¼ HB t c i t :<label>ð12Þ</label></formula><p>The detailed algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Tracking based on online learning sparse representation</head><p>Input: t as the tracking result Et þ 1 'Et Resample particle set according to their weights and transit particles to time t þ 1 by sampling from a Gaussian distribution</p><formula xml:id="formula_14">x n 0 A R 6 (initial state), y n 0 A R d (initiate template), E 0 ¼ IA R dÂd (</formula><formula xml:id="formula_15">x i t þ 1 $ N ðx i t ,S n Þ 21 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In order to validate the effectiveness of the proposed method, we conduct experiments on four challenging sequences which involve severe appearance variations including heavy occlusions, large pose variations, and drastic illumination changes as well as low foreground background contrast. The proposed method is compared with two latest state-of-the-art methods named Incremental Visual Tracking (IVT) <ref type="bibr" target="#b19">[20]</ref> and ' 1 minimization tracking (L1) <ref type="bibr" target="#b2">[3]</ref>. Note that for all test sequences, we set main parameters as: Z ¼ 0:7, w 0 ¼0.05, N¼200, n f ¼10 and n¼20. Noted that all these parameters are set by hand tuning with some prior knowledge. Taking Z as an example, this parameter controls how much the new date impacts the template. We think the new date can only contribute a small part to the template, so we set the parameter to 0.7. The comparison is performed from both visual evaluation and quantitative evaluation in Sections 4.1 and 4.2, respectively. The performance explanations are given in Section 4.3. We also test the running speed of the proposed method in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visual evaluation</head><p>The first sequence is the doll sequence with 430 frames obtained from <ref type="bibr" target="#b19">[20]</ref>. An animal doll moves with significant pose, lighting and scale variation in a cluttered scene. The normalization size is 14 Â 14. In order to analyze the details of the proposed method, we show some intermediate results of the 50th frame of this sequence. As shown in Fig. <ref type="figure" target="#fig_8">8</ref>, some candidates are similar to the tracked targets. However, others have significant difference with the tracked target. A good tracking algorithm should distinguish these candidates and find the most similar candidate as the tracking result. In Fig. <ref type="figure">9</ref>, we show all foreground and background templates. It should be noted that in this work, we mean the background templates as the image regions containing some background pixels rather than those background regions. The reason is that in a particle filter framework, although most particles locate at the target region, there are still some particles around the target region, which will causes candidates corresponding to these particles will contain background pixels. Some samples of the final tracking results are shown in Fig. <ref type="figure" target="#fig_4">4</ref>. The frame indices are 50, 100, 200, 300, 396 and 430. From Fig. <ref type="figure" target="#fig_4">4</ref>, we can see that our tracker is capable of tracking the doll all the time even when it changes pose drastically. The IVT tracker also achieves comparable performance. However, the L1 tracker fails to track the target in the fifth index frame and does not recover later.</p><p>The second test sequence is the face sequence <ref type="bibr" target="#b16">[17]</ref> with 300 frames. The face of the women is severely occluded by a book. The normalization size is 15 Â 12. Six representative frames with indices 20, 47, 89, 140, 180, 280 are shown in Fig. <ref type="figure" target="#fig_5">5</ref> where rows 1, 2 and 3 are for our proposed tracker, L1 tracker and IVT tracker, respectively. Due to the presence of occlusions, IVT tracker drifts to the un-occluded face region. The L1 tracker obtains the similar performance compared with the proposed method before 150th frame. However, it loses the target soon after. In contrast, the proposed method successfully track the face in the entire sequence even with severe occlusions by the book.</p><p>In order to evaluate our tracker in outdoor environments, where lighting conditions often change drastically, we conduct experiment on the head sequence with 240 frames from <ref type="bibr" target="#b19">[20]</ref>. In it, a person walks underneath a trellis covered by vines, resulting in significant appearance variations of his head due to cast shadows. Frames with indices 50, 100, 150, 183, 200 and 218 are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. It can be observed that our tracker is able to track the target accurately. However, the L1 and IVT trackers drift apart when the significant illumination happens in the 150th frame.  In addition to appearance variations, low foreground-background contrast is also very difficult for visual tracking. We test whether the proposed method can overcome this difficulty or not on the PkTest02 sequence from VIVID benchmark dataset <ref type="bibr" target="#b38">[39]</ref>. It is an infrared image sequence where the target-to-background contrast is very low. We give the tracking results on six representative frames with indices 20, 41, 60, 80, 100 and 120 in Fig. <ref type="figure" target="#fig_7">7</ref>, which shows that our tracker is capable of tracking the car all the time even with severe occlusions by the trees on the roadside. In comparison, the L1 tracker loses the car when it is first occluded by a tree in the 20th frame. The IVT tracker is superior to the L1 tracker and successfully track the car in the 20th frame. However, it finally fails to track the car in the 80th frame due to more severe occlusions by a bigger tree.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative evaluation</head><p>In order to evaluate the overall performance of the proposed method, we also perform quantitative evaluation which is based on the tracking error e in each frame. We use a simple measure e ¼ E=d, where E is the offset of the center of the tracking result from the ground truth and d is the diagonal length of the tracking result. The smaller e is, the better the tracking performance is obtained. In Fig. <ref type="figure" target="#fig_0">10</ref>, we present the tracking error-time curves of the four test sequences. We can see that the overall performance of our tracker is significant superior to the L1 and IVT trackers on all sequences, which validate the robustness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance explanations</head><p>The experimental results above show that our tracking algorithm is superior to both L1 and IVT trackers. Here, we will provide further discussions. The IVT tracker incrementally learns a low-dimensional subspace used to represent the target. This representation is online updated as new observations arrive. The tracker can adapt to the target's appearance variations to some extent. However, when the target's appearance changes drastically, the subspace learned from the historic observations cannot effectively represent the current target. Comparably, the L1 tracker separates the appearance modeling from the variation representation. The appearance model consists of a set of target templates. Variations are represented by a set of trivial templates. Since the trivial templates are columns of the identity matrix, the representation coefficients can be obtained correctly via ' 1 minimization when their variations are minor. However, when the variations become severe, the sparsity of coefficients cannot be guaranteed. So coefficients obtained by ' 1 minimization in this case cannot estimate whether a candidate is the target template or not, leading to the tracking failure. Unlike the IVT and L1 trackers, our tracker overcomes the weaknesses aforementioned: First, instead of using a fixed basis like the L1 tracker, we learn the basis online which assures that this representation can more efficiently adapt to appearance changes than the L1 tracker. In this aspect, our method is also significantly different with IVT tracker although both of them online learn basis. The IVT tracker learns the basis used to represent the target's appearance. However, our tracker learns the basis used to represent the appearance's variations. Intuitively, our tracker can be more effective than IVT tracker when handling target's appearance variations. Second, both the IVT and L1 trackers cannot effectively discriminate the target from the background and do not consider the background information in their models. In contrast, we introduce appropriate background templates in our sparse representation. The background templates and the target templates coherently represent each candidate, whilst the representation coefficients are obtained by ' 1 minimization. Our tracker can discriminate the target from the background well due to the different distributions of the target and background templates when used to represent the candidate. In summary, our tracker has two features: (1) online learning of the target's appearance variations and (2) discrimination capability of detecting the target from the background, which ensure the outstanding performance of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Processing speed</head><p>The proposed method is implemented in Matlab on a Pentium-IV 3 GHz PC with 1G RAM. We test the processing speed of our tracker on face sequence which consists of 300 frames of size 352 Â 288. The average processing speed of the our tracker is about 5 frames per second, which will be further improved with code optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we propose a robust visual tracking algorithm based on online learning sparse representation. The main contribution is that we integrate two requirements of an expected appearance model, discriminating the tracked target from background and being robust to appearance variations, into a linear representation system that each target candidate is represented by target templates, background templates and online learned error basis. To the best of our knowledge, it is the first time that these two requirement are achieved separately based on sparse representation. Experimental results of four challenging sequences validate that the proposed method is significantly superior to two latest state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustrations of template and candidates. (a) The template is marked by white rectangle on the top. Two candidates are shown in the bottom, in which the''good'' candidate and ''bad'' candidates are marked by red and blue rectangles, respectively. (b) The HSV histograms of the template, ''good'' candidate and ''bad'' candidate, respectively. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="2,42.80,70.85,157.40,258.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Coefficient examples. (a) The coefficient vector of the ''good'' candidate using identity matrix as error basis. (b) The coefficient vector of the ''bad'' candidate using identity matrix as error basis. (c) The coefficient vector of the ''good'' candidate using online learning error basis. (d) The coefficient vector of the ''bad'' candidate using online learning error basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) and (b) where the number of nonzero coefficients of the two coefficient vectors are 156 and 160, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. In this figure, we show 32 basis vectors out of total of 190 basis vectors learned on the face sequence by our method. Some basis vectors (marked by white dashed rectangle) reflect how the face is occluded by the book. Note that all vectors are scaled for visualization. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="5,89.84,58.61,158.84,132.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The tracking results of the doll sequence with significant pose, lighting and scale variation in a cluttered scene. The first, second and third rows are results obtained by the proposed method, L1 tracker and IVT tracker, respectively.</figDesc><graphic coords="6,119.97,402.60,345.60,131.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The tracking results of the face sequence with severe occlusions. The first, second and third rows are results obtained by the proposed method, L1 tracker and IVT tracker, respectively.</figDesc><graphic coords="6,119.97,578.92,345.60,143.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The tracking results of the head sequence with significant pose, lighting and scale variation in a cluttered scene. The first, second and third rows are results obtained by the proposed method, L1 tracker and IVT tracker, respectively.</figDesc><graphic coords="7,87.78,58.64,429.84,163.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The tracking results of the PkTest02 sequence with significant pose, lighting and scale variation in a cluttered scene. The first, second and third rows are results obtained by the proposed method, L1 tracker and IVT tracker, respectively.</figDesc><graphic coords="7,87.78,259.91,429.84,172.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. All candidates corresponding to 200 particles at time 50 on doll sequence.</figDesc><graphic coords="7,129.78,471.69,345.60,172.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. All foreground and background templates at time 50 on doll sequence. The top row shows the 10 foreground templates and the bottom row shows the 10 background templates.</figDesc><graphic coords="8,119.97,58.64,345.60,66.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Àp 1 t g i ¼ 1:N using Eq: ð9Þ Compute index of the ''best'' candidate as I ¼ arg min i rðy i t Þ Compute index of the ''worst'' candidate as @ ¼ arg max i rðy i t Þ Weight jÀth target template with âI t,j , j ¼ 1, . . . ,n f Weight jÀth background template with â@ t,j þ nf , j ¼ 1, . . . ,nÀn f Sort target and background templates according to their weights; respectively</figDesc><table><row><cell></cell><cell>f (number</cell></row><row><cell cols="2">of target templates)</cell></row><row><cell>1</cell><cell>Initialize particles set: fx i 0 ¼ x n 0 ,p i 0 ¼ 1 N g i ¼ 1:N and template set fp 0,i ¼ y n 0 g i ¼ 1:n</cell></row><row><cell>2</cell><cell>for t¼1 to T do</cell></row><row><cell>3</cell><cell>for i ¼ 1 to N do</cell></row><row><cell>4</cell><cell>Extract target candidate y i t</cell></row><row><cell>5</cell><cell>Compute sparse coefficients a i t and b i t by solving Eq: ð12Þ via '1 minimization</cell></row><row><cell>6</cell><cell>Compute residual rðy i t Þ ¼ Jy i t ÀPt d f ða i t ÞÀEt b i t J2</cell></row><row><cell>7</cell><cell>Weight iÀth particle with p i t ¼ expðÀlrðy i t ÞÞ</cell></row><row><cell>8</cell><cell>Collect sample y i t Àp 1 t</cell></row><row><cell>9</cell><cell>end</cell></row><row><cell>10 11 12 13 14 15 16</cell><cell>Update basis Et with samples fs i t ¼ y i t Update target template as y nf t ¼ y nf t þð1ÀZÞy î t , with initial weight w0</cell></row><row><cell>17</cell><cell>Update background template as y n t ¼ y n t þð1ÀZÞy ĩ t , with initial weight w0</cell></row><row><cell>18</cell><cell>return x I</cell></row><row><cell>19</cell><cell></cell></row><row><cell>20</cell><cell></cell></row></table><note><p>initial basis set), Sn (Gaussian covariance), Z (template update rate), w 0 (initial weight), T (length of sequence), N (number of particles), n (number of templates), n</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by the National Natural Science Foundation of China (Grant no.: 61071180 and Key Program Grant no.: 61133003). Shengping Zhang is also supported by funding of Ph.D. student short-term visiting abroad from HIT. Huiyu Zhou is currently supported by UK EPSRC Grant EP/ G034303/1 and Invest NI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Color-based probabilistic tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pe ´rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="661" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust visual tracking using L1 minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Computer Vision</title>
		<meeting>the 12th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1436" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust object tracking based on sparse representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE International Conference on Visual Communications and Image Processing</title>
		<meeting>SPIE International Conference on Visual Communications and Image Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="77441N" to="77441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computer vision face tracking as a component of a perceptual user interface</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Technol. J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contour tracking by stochastic propagation of conditional density</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th European Conference on Computer Vision</title>
		<meeting>the 4th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="343" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast texture-based tracking and delineation using texture entropy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Partial occlusion robust object tracking using an effective appearance model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE International Conference on Visual Communications and Image Processing</title>
		<meeting>SPIE International Conference on Visual Communications and Image Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="77442U" to="77443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object tracking using sift features and mean shift</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="352" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust object tracking combining color and scale invariant features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE International Conference on Visual Communications and Image Processing</title>
		<meeting>SPIE International Conference on Visual Communications and Image Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="77442R" to="77443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust visual tracking using feature-based visual attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1150" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="577" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sriram</surname></persName>
		</author>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1158" to="1163" />
		</imprint>
	</monogr>
	<note>Spatiograms versus histograms for region-based tracking</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object tracking using color correlogram</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop Performance Evaluation of Tracking and Surveillance</title>
		<meeting>IEEE Workshop Performance Evaluation of Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust online appearance models for visual tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ei-Maraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1296" to="1311" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On-line selection of discriminative tracking features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1631" to="1643" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-target tracking by on-line learned discriminative appearance models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision</title>
		<meeting>IEEE Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking targets using adaptive Kalman filtering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Velger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerospace Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="691" to="699" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient tracking and ego-motion recovery using gait analysis, Signal Process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2367" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative tracking by metric learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision</title>
		<meeting>the 11th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="200" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision</title>
		<meeting>the 10th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="234" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond semi-supervised tracking: tracking should be as simple as detection, but not simpler than recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stalder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Workshop</title>
		<meeting>IEEE Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference</title>
		<meeting>IEEE Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative K-SVD for dictionary learning in face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image sequence denoising via sparse and redundant representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual tracker using sequential Bayesian learning: discriminative, generative, and hybrid</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1578" to="1591" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Part B: Cybern.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminatively trained particle filters for complex multiobject tracking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: a strategy employed by V1?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rapid face recognition using hashing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An open source tracking testbed and evaluation web site, in: IEEE Workshop Performance Evaluation of Tracking and Surveillance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
