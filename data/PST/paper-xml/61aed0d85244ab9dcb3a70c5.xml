<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-06">6 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Suzhen</forename><surname>Wang</surname></persName>
							<email>wangsuzhen@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Netease Fuxi AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
							<email>lilincheng@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Netease Fuxi AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
							<email>dingyu01@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Netease Fuxi AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
							<email>xin.yu@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-06">6 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.02749v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio-driven one-shot talking face generation methods are usually trained on video resources of various persons. However, their created videos often suffer unnatural mouth shapes and asynchronous lips because those methods struggle to learn a consistent speech style from different speakers. We observe that it would be much easier to learn a consistent speech style from a specific speaker, which leads to authentic mouth movements. Hence, we propose a novel one-shot talking face generation framework by exploring consistent correlations between audio and visual motions from a specific speaker and then transferring audio-driven motion fields to a reference image. Specifically, we develop an Audio-Visual Correlation Transformer (AVCT) that aims to infer talking motions represented by keypoint based dense motion fields from an input audio. In particular, considering audio may come from different identities in deployment, we incorporate phonemes to represent audio signals. In this manner, our AVCT can inherently generalize to audio spoken by other identities. Moreover, as face keypoints are used to represent speakers, AVCT is agnostic against appearances of the training speaker, and thus allows us to manipulate face images of different identities readily. Considering different face shapes lead to different motions, a motion field transfer module is exploited to reduce the audio-driven dense motion field gap between the training identity and the one-shot reference. Once we obtained the dense motion field of the reference image, we employ an image renderer to generate its talking face videos from an audio clip. Thanks to our learned consistent speaking style, our method generates authentic mouth shapes and vivid movements. Extensive experiments demonstrate that our synthesized videos outperform the state-of-the-art in terms of visual quality and lip-sync.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Synthesizing audio-driven photo-realistic portraits is of great importance to various applications, such as digital human animation <ref type="bibr" target="#b18">(Ji et al. 2021;</ref><ref type="bibr" target="#b47">Zhu et al. 2021)</ref>, visual dubbing in movies <ref type="bibr" target="#b24">(Prajwal et al. 2020;</ref><ref type="bibr" target="#b14">Ha et al. 2020</ref>) and fast short video creation <ref type="bibr" target="#b44">(Zhou et al. 2021;</ref><ref type="bibr" target="#b39">Zeng et al. 2020)</ref>. One-shot talking face generation methods are designed to animate video portraits for unseen speakers and voice. When Figure <ref type="figure">1</ref>: Illustration of our proposed talking head generation framework. Our approach takes one-shot reference image as input and generates audio-driven talking faces with rhythmic head motions, natural mouth shapes and accurate lip synchronization. Although our audio-visual correlation model is trained on a specific speaker, our framework supports arbitrary one-shot reference image and voice as input and renders the photo-realistic talking face videos.</p><p>watching a synthetic talking head video, humans are mainly affected by three aspects: visual quality (clear and jitterfree), natural head motions, and synced lip movements. Existing one-shot methods <ref type="bibr" target="#b3">(Chen et al. 2019;</ref><ref type="bibr" target="#b24">Prajwal et al. 2020;</ref><ref type="bibr" target="#b45">Zhou et al. 2020</ref><ref type="bibr" target="#b44">Zhou et al. , 2021;;</ref><ref type="bibr" target="#b20">Lahiri et al. 2021</ref>) are usually trained on video resources of various persons, and their results suffer unnatural lip shapes and bad lip-sync. This is mainly because their networks trained on multiple speech styles try to fit the common style among different identities while treating personalized variations as noise. Therefore, it is very challenging to synthesize natural and synchronous lip movements for one-shot speakers.</p><p>We observe that it is much easier to learn a consistent speech style from a specific speaker. Motivated by this, we propose a new one-shot talking face generation framework, which first learns a consistent speaking style from a single speaker and then animates vivid videos of arbitrary speak-ers with new speech audio from the learned style. The key insight of the proposed method is to utilize the advantage of authentic lip movements and rhythmic head motions learned from an individual, and then to seek migration from an individual to multiple persons. Put differently, our method explores a consistent speech style between audio and visual motions from a specific speaker and then transfers audiodriven keypoint based motion fields to a reference image for talking face generation.</p><p>Towards this goal, we firstly develop a speakerindependent Audio-Visual Correlation Transformer (AVCT) to obtain keypoint-base dense motion fields <ref type="bibr" target="#b24">(Siarohin et al. 2019</ref>) from audio signals. To eliminate the timbre effect among different identities, we adopt phonemes to represent audio signals. With the input phonemes and head poses, the encoder is expected to establish the latent pose-entangled audio-visual mapping. Considering vivid mouth movements are closely related to audio signals (e.g., the mouth amplitudes are affected by the fierce tones), we use the embedded acoustic features as the query of the decoder to modulate mouth shapes for more vivid lip movements. Moreover, due to the keypoint representation of a reference image, AVCT is agnostic against appearances of the training speaker, allowing us to manipulate face images regardless of different identities. Furthermore, considering different faces have diverse shapes, these variations would lead to different facial motions. Thus, a relative motion transfer module <ref type="bibr" target="#b24">(Siarohin et al. 2019</ref>) is employed to reduce the motion gap between the training identity and the one-shot reference. Once obtaining the dense motion field, we generate talking head videos by an image renderer.</p><p>Thanks to our learned consistent speaking style, our method is able to not only produce talking face videos of the training speaker on par with speaker-specific methods, but also animate vivid portrait videos for unseen speakers with more accurate lip synchronization and more natural mouth shapes than previous one-shot talking head approaches. Remarkably, our method can also address talking faces with translational and rotational head movements whereas prior arts usually handle rotational head motions. Extensive experimental results on widely-used VoxCeleb2 and HDTF demonstrate the superiority of our proposed method.</p><p>In summary, our contributions are three-fold:</p><p>• We propose a new audio-driven one-shot talking face generation framework, which establishes the consistent audio-visual correlations from a specific speaker instead of learning from various speakers as in prior arts.</p><p>• We design an audio-visual correlation transformer that takes phonemes and facial keypoint based motion field representations as input, thus allowing it to be easily extended to any other audio and identities.</p><p>• Although the audio-visual correlations are only learned from a specific speaker, our method is able to generate photo-realistic talking face videos with accurate lip synchronization, natural lip shapes and rhythmic head motions from a reference image and a new audio clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Animating talking faces from audio or text has received more attention in the field of artificial intelligence. As there exists a considerable audio-visual gap, early works <ref type="bibr" target="#b9">(Edwards et al. 2016;</ref><ref type="bibr" target="#b27">Taylor et al. 2017;</ref><ref type="bibr">Pham, Cheung, and Pavlovic 2017;</ref><ref type="bibr" target="#b19">Karras et al. 2017;</ref><ref type="bibr" target="#b46">Zhou et al. 2018;</ref><ref type="bibr" target="#b7">Cudeiro et al. 2019)</ref> focus on driving animations of 3D face models. With the development of image generation <ref type="bibr" target="#b36">(Yu and Porikli 2016;</ref><ref type="bibr" target="#b39">Yu et al. 2019b;</ref><ref type="bibr" target="#b22">Li, Yu, and Yang 2021;</ref><ref type="bibr" target="#b35">Yu et al. 2019a)</ref>, an increasing number of works have been proposed for 2D photo-realistic talking face generation. These methods can mainly be divided into two categories, speakerspecific methods and speaker-arbitrary methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker-specific Talking Face Generation</head><p>For a given new speaker, speaker-specific methods retrain part or all of their models on the videos of that speaker. Most works (Suwajanakorn, Seitz, and Kemelmacher-Shlizerman 2017; <ref type="bibr" target="#b25">Song et al. 2020;</ref><ref type="bibr" target="#b33">Yi et al. 2020;</ref><ref type="bibr" target="#b10">Fried et al. 2019;</ref><ref type="bibr" target="#b28">Thies et al. 2020;</ref><ref type="bibr">Li et al. 2021;</ref><ref type="bibr" target="#b20">Lahiri et al. 2021;</ref><ref type="bibr" target="#b18">Ji et al. 2021;</ref><ref type="bibr">Zhang et al. 2021a,b;</ref><ref type="bibr" target="#b20">Lahiri et al. 2021</ref>) synthesize photo-realistic talking head videos guided by 3D face models. Suwajanakorn, Seitz, and Kemelmacher-Shlizerman (2017) synthesize videos from audio in the region around the mouth. Several methods <ref type="bibr" target="#b28">(Thies et al. 2020;</ref><ref type="bibr">Li et al. 2021</ref>) consist of speaker-independent components relying on 3D face models and speak-specific rendering modules. <ref type="bibr" target="#b10">Fried et al. (2019)</ref> present a framework for text based video editing. <ref type="bibr" target="#b13">Guo et al. (2021)</ref> propose the audiodriven neural radiance fields for talking head generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker-arbitrary Talking Face Generation</head><p>Speaker-arbitrary methods aim to build a single universal model for various subjects. Some works <ref type="bibr" target="#b4">(Chung, Jamaludin, and Zisserman 2017;</ref><ref type="bibr" target="#b3">Chen et al. 2018;</ref><ref type="bibr" target="#b26">Song et al. 2018;</ref><ref type="bibr" target="#b43">Zhou et al. 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019;</ref><ref type="bibr" target="#b30">Vougioukas, Petridis, and Pantic 2019;</ref><ref type="bibr" target="#b8">Das et al. 2020</ref>) focus on learning a mapping from audio to the cropped faces, but their fixed poses and cropped faces in the videos are unnatural for human observations. Other works <ref type="bibr" target="#b32">(Wiles, Koepke, and Zisserman 2018;</ref><ref type="bibr" target="#b2">Chen et al. 2020;</ref><ref type="bibr" target="#b24">Prajwal et al. 2020;</ref><ref type="bibr" target="#b45">Zhou et al. 2020;</ref><ref type="bibr" target="#b42">Zhang et al. 2021c;</ref><ref type="bibr" target="#b31">Wang et al. 2021;</ref><ref type="bibr">Zhou et</ref>    <ref type="formula">2021</ref>) create videos with translational and rotational head movements while keeping background still in generated videos. However, since all these methods are trained on the corpus of multiple speakers, they often struggle to learn a consistent speaking style, and their results suffer unnatural mouth shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>We propose a new talking face generation framework to make audio-driven portrait videos for arbitrary speakers by learning audio-visual correlations on a specific speaker. Giving a reference image I r and an audio clip A, our method creates talking face images y=I 1:T . The whole pipeline is shown in Figure <ref type="figure" target="#fig_0">2</ref>. Our pipeline consists of four modules:</p><p>(1) a head motion predictor E h estimates the head motion sequence h 1:T (h i ∈ R 6 includes the 3D rotation and the 3D We extract the audio channels from the training videos and transform them into audio features and phonemes as pre-processing. To be consistent with videos at 25 fps, we extract acoustic features a i ∈ R 4×41 and one phoneme label p i ∈ R per 40ms. The acoustic features include Mel Frequency Cepstrum Coefficients (MFCC), Mel-filterbank energy features (FBANK), fundamental frequency and voice flag. The phoneme is extracted by a speech recognition tool 1 .</p><p>1 https://cmusphinx.github.io/wiki/phonemerecognition/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio-visual Correlation Transformer (AVCT)</head><p>The core of the proposed method is to build accurate audiovisual correlations which can be extended to any other audio and identities. Such correlations are learned via a speakerindependent audio-visual correlation transformer. Considering the high temporal coherence, E avct takes the assembled features in a sliding window as input. Specifically, for the ith frame, E avct takes the paired conditioning input c i = {f r , a i−n:i+n , h i−n:i+n , p i−n:i+n } and outputs the keypoints k i ∈ R N ×2 and their corresponding Jacobian j i ∈ R N ×2×2 . f r is the latent representation of keypoints from the reference image I r through the keypoint detector E kd . n indicates the window length and is set to 5 in our experiments. N is the number of keypoints and is set to 10. The paired (k i , j i ) represents the dense motion field <ref type="bibr" target="#b24">(Siarohin et al. 2019)</ref>. The head motions h 1:T are extracted by OpenFace <ref type="bibr">(Baltrusaitis et al. 2018)</ref>.</p><p>The proposed AVCT is able to aggregate dynamic audiovisual information within the temporal window, thus creating more accurate lip movements. To model the correlations among different modalities, we employ Transformer <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref> as the backbone of AVCT due to its powerful attention mechanism. For a better extension to any other audio and identities, we carefully design the Encoder and the Decoder of E avct as follows.</p><p>Encoder. We employ the phoneme labels as input instead of acoustics features to bridge the timbre gap between the specific speaker and arbitrary ones. We establish a latent pose-entangled audio-visual mapping by encoding the input phonemes p i−n:i+n and poses h i−n:i+n in the encoder. The attentions between sequential frames 2n+1 allow for obtaining the refined latent mouth motion representation at frame i. Specifically, we employ a 256-dimension word embedding (Levy and Goldberg 2014) to represent the phoneme label, then reshape and upsample it as f p i ∈ R 1×64×64 . h i is converted to the projected binary image f h i ∈ R 1×64×64 as in <ref type="bibr" target="#b31">(Wang et al. 2021)</ref>. Then, the concatenated features {f p i , f h i } are fed into a residual convolution network consisting of 5 2× downsampling ResNet blocks <ref type="bibr" target="#b15">(He et al. 2016)</ref> in order to obtain the assembled feature f</p><formula xml:id="formula_0">(p,h) i ∈ R 1×512 .</formula><p>The input to the encoder is the sequential features f (p,h) ∈ R (2n+1)×512 by concatenating all the frame features along the temporal dimension. Since the architecture of the transformer is permutation-invariant, we supplement f (p,h) with fixed positional encoding <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref>.</p><p>Decoder. Practically, the mouth amplitude is affected by the loudness and energy of the audio in addition to the phoneme. To create more subtle mouth movements, we employ the acoustics features in the decoding phase for capturing energy changes. We extract audio features f a i ∈ R 32×64×64 from a i using an upsampling convolution network. To reduce the dependency on the identities, we cannot directly take the reference image as input but employ the latent representation, f r ∈ R 32×64×64 , of the keypoints of the reference image I r . f r is extracted from the pretrained keypoint detector E kd . It mainly retains the pose-based structural information of body, face and background, weakening identity-related information. Such initial structural information dominates the low-frequency holistic layout in the generated dense motion fields.</p><p>f r is repeated by 2n+1 times. Then the concatenation of f r i and f a i is fed into another residual convolutional network to obtain the embedding f (r,a) i</p><p>. Similarly, we acquire the features f (r,a) ∈ R (2n+1)×512 by concatenation, and supplement it with positional encodings. f (r,a) is used as the initial query of the decoder to modulate the layout of the body, head and background, and to refine the subtle mouth shape. Following the standard transformer, the decoder creates 2n+1 embeddings. Only the i-th embedding is taken and projected to keypoints k i and Jacobians j i with two different linear projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batched Sequential Training</head><p>Since AVCT generates the dense motion fields of each frame individually, we develop a batched sequential training strategy to improve the temporal consistency. The training samples on each batch consist of the T successive conditional inputs c i:T of T successive images from the same video. Then, we generate image sequence Î1:T in parallel in each batch. This design allows us to apply constraints on the image sequence within each batch rather than on single images. We call the above strategy as Batched Sequential Training (BST). In addition to the pixel loss on each frame image, the sequential constraint is imposed by a temporal discriminator D seq . Besides, as the common pixel reconstruction loss is insufficient to supervise the lip-sync, we employ another lip-sync discriminator D sync to improve the lip-sync.</p><p>Temporal Discriminator. D seq follows the structure of PatchGAN <ref type="bibr">(Goodfellow et al. 2014;</ref><ref type="bibr" target="#b17">Isola et al. 2017;</ref><ref type="bibr">Yu and Porikli 2017a,b;</ref><ref type="bibr" target="#b34">Yu et al. 2018)</ref>. We stack the T successive image frames along the channel dimension as the input of D seq . D seq tries to distinguish whether the input is natural or synthetic. D seq and E avct are learned jointly with the generative-adversarial learning.</p><p>Lip-sync Discriminator. D sync employs the structure of SyncNet <ref type="bibr" target="#b6">(Chung and Zisserman 2016)</ref> in Wav2Lip <ref type="bibr" target="#b24">(Prajwal et al. 2020</ref>). D sync is trained to discriminate the synchronization between audio and video by randomly sampling an audio window that is either synchronous or asynchronous with a video window. The discriminated frame lies in the middle of the window, and the window size is set to 5. D sync computes the visual embedding e v from an image encoder and the audio embedding e a from an audio encoder. We adopt the cosine-similarity to indicate the probability whether e v and e a are synchronous:</p><formula xml:id="formula_1">P sync = e v • e a max( e v 2 • e a 2 , )</formula><p>.</p><p>(1)</p><p>Loss Function. Based on the batched sequential training, the loss function for each batch image sequence is defined as:</p><formula xml:id="formula_2">L total = L seq ( Î1:T ) + λ sync T − 4 T −2 i=3 L sync ( Îcrop i−2:i+2 )+ + 1 T T i=1 (λ v L mul vgg ( Îi , I i ) + λ P eq L K eq ( ki ) + λ J eq L J eq ( ĵi ))<label>(2)</label></formula><p>where L seq is the GAN loss of D seq . L sync is the lipsync loss from the pretrained D sync and is defined as −log(P sync ). Note that Îcrop means the cropped mouth area and that we ignore the boundary frames to fit the temporal input of D sync . We crop the mouth region in each training iteration dynamically according to the detected bounding boxes of real videos. L mul vgg is the multi-layer perceptual loss that relies on a pretrained VGG network. L K eq and L J eq are the equivariance constraint loss <ref type="bibr" target="#b24">(Siarohin et al. 2019)</ref> to ensure the consistency of estimated keypoints and jacobians. In our experiments, T is set to 24 (on RTX 3090), λ sync ,λ v ,λ P eq and λ J</p><p>eq are set to 10,1,10,10 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Motion Predictor</head><p>The head motion predictor E h , developed to generates h 1:T in the inference stage, is also trained on the specific speaker. E h adopts the network structure of the head motion predictor of Audio2Head <ref type="bibr" target="#b31">(Wang et al. 2021</ref>) but has two differences. First, instead of being trained on a large number of identities, E h is trained on a specific speaker. Therefore, in order to avoid overfitting to the appearance of the specific speaker, we replace the input reference image of Au-dio2Head with the projected binary pose image. Secondly, for the convenience of the relative motion transfer (see below), the starting point of the predicted head pose sequence should be the same as the head pose of the reference image. Hence, we add an L1 loss term between the head poses  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Motion Transfer</head><p>As the generated motion fields are inevitably entangled with the specific speaker, we adopt the relative motion transfer <ref type="bibr" target="#b24">(Siarohin et al. 2019</ref>) in the inference stage to reduce the motion gap between the training identity and the one-shot reference image. We transfer the relative motion between ( k1 , ĵ1 ) and ( k1:T , ĵ1:T ) to (k r , j r ). (k r , j r ) are detected from the reference image. This operation is defined by:</p><formula xml:id="formula_3">k i = ki − k1 + k r , ĵ i = ĵi ĵ−1 1 j r .<label>(3)</label></formula><p>Then, we use ( k 1:T , ĵ 1:T ) to render the output videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Dataset. We collect Obama Weekly Address videos from Youtube. Since we aim to generate talking face videos with translational and rotational head movements, we crop and resize the original videos to 256×256 pixels as in FOMM (Siarohin et al. 2019) without aligning speakers' noses across frames. After processing all the crowdsourcing videos, we obtain 20 hours of videos of Obama. Although our audio-visual correlation transformer is only trained on Obama, we employ two in-the-wild audio-visual datasets to evaluate our method, HDTF <ref type="bibr" target="#b42">(Zhang et al. 2021c</ref>) and Vox-Celeb2 <ref type="bibr" target="#b5">(Chung, Nagrani, and Zisserman 2018)</ref>.</p><p>Evaluation Metrics. We conduct quantitative evaluations on several metrics that have been wildly used in previous methods. As the generated videos have different head motions from ground truth, we use the Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">(Heusel et al. 2017</ref>) and the Cumulative Probability of Blur Detection (CPBD) <ref type="bibr" target="#b23">(Narvekar and Karam 2009)</ref> to evaluate the image quality. To evaluate the mouth shape and lip synchronization, we adopt the Landmark Distance (LMD) <ref type="bibr" target="#b3">(Chen et al. 2019</ref>) around mouths and audio- We select the frames that pronounce the same phonemes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>We compare our method with recent state-of-the-art methods, including Wav2Lip <ref type="bibr" target="#b24">(Prajwal et al. 2020)</ref>, MakeitTalk <ref type="bibr" target="#b45">(Zhou et al. 2020)</ref>, Audio2Head <ref type="bibr" target="#b31">(Wang et al. 2021)</ref>, FGTF <ref type="bibr" target="#b42">(Zhang et al. 2021c)</ref>, and PC-AVS <ref type="bibr" target="#b44">(Zhou et al. 2021)</ref>. The samples of each method are generated using their released codes with the same reference image and audio. The reference images are specially cropped to fit the input of PC- AVS. Since Wav2Lip and PC-AVS cannot obtain head poses from audio, the head poses are fixed in their generated videos. For other methods, the head poses are controlled separately by each method. The quantitative results are reported in Table <ref type="table" target="#tab_2">1</ref>. Our method achieves the best performance under most of evaluation metrics on HDTF and VoxCeleb2. As Wav2Lip only edits the mouth regions and keeps most parts of the reference image unchanged, it reaches the highest CPBD score on HDTF, but their synthetic mouth areas are noticeably blurry, as visible in Figure <ref type="figure" target="#fig_1">3</ref>. These results validate that our method achieves high-quality videos, even though our audio-visual correlation transformer is learned from a specific speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>Comparisons with Speaker-arbitrary Methods. We first compare our method with speaker-arbitrary (one-shot) methods qualitatively. The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. Among all methods, our method creates the most accurate mouth shape and preserves the best identity (see our demo video for more clearly comparisons). Only Wav2Lip and PC-AVS achieve similar lip-sync to ours, but their mouth shapes look mechanical and unnatural because these methods struggle to produce consistent talking styles. Moreover, both of them cannot obtain head poses from audio and PC-AVS can only deal with aligned faces. Notably, PC-AVS alters the identity information of the reference image in Figure <ref type="figure" target="#fig_1">3</ref>. While MakeitTalk creates subtle head motions, its results are obviously out of sync. Audio2Head, FGTF and our method are able to produce talking face videos with moving head poses. However, Audio2Head still suffer inferior lip-sync while our method produce authentic lip-sync. In addition, FGTF produces distorted mouth shapes in its results while our generated mouths look very natural.</p><p>Comparisons with Speaker-specific Methods. We compare our method with two speaker-specific methods, i.e., SynObama (Suwajanakorn, Seitz, and Kemelmacher-Shlizerman 2017) and Write-a-speaker <ref type="bibr">(Li et al. 2021)</ref>. We first extract the reference image and audio from their demo videos and then generate our results. The comparisons are shown in Figure <ref type="figure" target="#fig_2">4</ref>. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, our method synthesizes comparable videos of Obama with the methods that are customized for Obama. SynObama synthesizes the region around the mouth from audio, and uses compositing techniques to borrow the rest regions from real videos. This composition sometimes results in visible artifacts around the mouth. Our method is an end-to-end approach without requiring additional editing, and achieves accurate mouth shapes than Write-a-speaker. Please see our supplementary video for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To evaluate the effectiveness of each component in our framework, we conduct an ablation study with 7 variants:</p><p>(1) remove phonemes from the encoder (w/o Pho), (2) remove audio features from the decoder (w/o Aud), (3) remove the keypoint features in the decoder (w/o Dec kp ), (4) replace the extracted keypoint features from E kd with the reference image (w/o Kp), (5) remove the temporal discriminator D seq and lip-sync discriminator D sync (w/o BST), (6) only remove D sync (w/o D sync ), and (7) our full model (Full). For evaluation, we replace generated head motions with poses extracted from real videos to create the samples. The numerical results on HDTF are shown in Table <ref type="table" target="#tab_3">2</ref>. As all the variants employ the same pretrained image renderer, most of them achieve similar FID and CPBD scores. However, it can be seen that the image quality drops dramatically when removing the reference image or replacing the keypoint features with the reference image. Without the supervision of D seq and D sync , the results show poor temporal stability and bad lip synchronization. The model w/o Pho fails to extend to the unseen timbre by removing the input of phonemes, producing out-of-sync videos. In Table <ref type="table" target="#tab_3">2</ref>, the model w/o Aud obtains good quantitative results without using audio features. However, as seen in Figure <ref type="figure">5</ref>, the audio features indeed affect the vivid mouth movements.</p><p>We also conduct another ablation study to evaluate the superiority of the proposed method. We compare our method with the combination of a speaker-specific work (i.e., Syn-Obama) and a expression transfer work (i.e., FOMM). Specifically, we transfer the expressions in videos created by SynObama to the reference image using FOMM. The results are shown in Figure <ref type="figure">6</ref>. Since FOMM requires the initial poses of the reference image and one-shot to be similar while the pose of the first frame from SynObama is possibly different from that of the reference one, the combination of SynObama and FOMM would lead to inferior results. In contrast, our model is able to preserve the initial pose and thus generates satisfactory facial motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Study</head><p>We conduct a user study of 19 volunteers base on their subjective perception of talking head videos. We create 3 videos for each method with the same input and obtain 21 videos in total. We adopt the questions used in the user study of PC-AVS <ref type="bibr" target="#b44">(Zhou et al. 2021)</ref>, and participants are asked to give their ratings (1-5) of each video on three questions: (1) the Lip sync quality, (2) the naturalness of head movements, and (3) the realness of results. The mean scores are listed in Table 3. Note that we do not offer reference poses for Wav2Lip and PC-AVS, so their scores on head movements and video realness are reasonably low. Our method outperforms competing methods in all the aspects, demonstrating the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel framework for one-shot talking face generation from audio. Particularly, our method learns consistent audio-visual correlations from a single speaker, and then transfers the talking styles to different subjects. Differs from prior one-shot talking face works, our method provides a new perspective to address this task and achieves vivid videos of arbitrary speakers. The extensive quantitative and qualitative evaluations illustrate that our method is able to generate photo-realistic talking-face videos with accurate lip synchronization, natural lip shapes and rhythmic head motions from a reference image and a new audio clip. Besides face photography, we can animate talking head videos for non-photorealistic paintings, demonstrating a promising generalization ability of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Illustration of our pipeline. We first extract an initial pose h r from a reference image, and extract acoustic features a 1:T and phoneme labels p 1:T from the audio. The latent representation of keypoints of the reference image f r is extracted by the keypoint detector E kd . The head motion predictor E h predicts the head motion sequence h 1:T from the input {a 1:T , h r }.Then {a 1:T , p 1:T , h 1:T , f r } are sent to the audio-visual correlation transformer E avct to generate pose-aware keypoint-based dense motion fields. Finally, the image renderer E r renders the output videos. We also use a temporal discriminator D seq and a lip-sync discriminator D sync to improve the temporal stability and lip synchronization respectively in training. Particularly, in the inference stage, the generated dense motions are refined with a relative motion transfer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comparisons with the speaker-arbitrary methods. We select the frames that pronounce the same phonemes marked in red. Please see our demo videos for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparisons with the speaker-specific methods.We select the frames that pronounce the same phonemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>visual metrics (AVOff and AVConf) proposed in SyncNet<ref type="bibr" target="#b6">(Chung and Zisserman 2016)</ref>. Note that we calculate the normalized relative landmark distance instead of the absolute landmark distance to avoid the influence of both head poses and image resolution.Implementation Details. All models are implemented by PyTorch, and we adopt Adam<ref type="bibr" target="#b20">(Kingma and Ba 2014)</ref> optimizer for all experiments. Before training our AVCT E avct , we train the keypoint detector and image renderer on the combination of VoxCeleb (Nagrani, Chung, and Zisserman 2017) and Obama datasets to obtain the pretrained E kd and E r . D sync is trained with a fixed learning rate 1e-4 on videos of Obama. E kd , E r and D sync are frozen when training E avct on the videos of Obama. E avct is trained on 4 GPU for about 5 days using the batched sequential training mechanism, with an initial learning rate of 2e-5 and a weight decay of 2e-7. E h is trained on a single GPU for about 12 hours with a learning rate of 1e-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Samples are generated from different representations of the same audio clip, i.e., phoneme features and the combination of phoneme and audio features. Here, the mouth shapes are further controlled by the intensity of the pronunciation in our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The quantitative results on HDTF and VoxCeleb2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>HDTF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VoxCeleb2</cell><cell></cell></row><row><cell>Method</cell><cell cols="10">FID↓ CPBD↑ LMD↓ AVOff(→0) AVConf↑ FID↓ CPBD↑ LMD↓ AVOff(→0) AVConf↑</cell></row><row><cell>Wav2Lip</cell><cell>0.180</cell><cell>0.790</cell><cell>0.289</cell><cell>-2.92</cell><cell>6.97</cell><cell>0.203</cell><cell>0.541</cell><cell>0.273</cell><cell>-2.92</cell><cell>6.65</cell></row><row><cell>MakeitTalk</cell><cell>0.210</cell><cell>0.694</cell><cell>0.546</cell><cell>-2.93</cell><cell>4.87</cell><cell>0.230</cell><cell>0.550</cell><cell>0.482</cell><cell>-2.83</cell><cell>4.38</cell></row><row><cell cols="2">Audio2Head 0.176</cell><cell>0.732</cell><cell>0.483</cell><cell>0.33</cell><cell>3.90</cell><cell>0.224</cell><cell>0.532</cell><cell>0.314</cell><cell>0.50</cell><cell>2.47</cell></row><row><cell>FGTF</cell><cell>0.187</cell><cell>0.738</cell><cell>0.387</cell><cell>1.05</cell><cell>4.24</cell><cell>0.212</cell><cell>0.559</cell><cell>0.283</cell><cell>0.491</cell><cell>4.54</cell></row><row><cell>PC-AVS</cell><cell>0.238</cell><cell>0.725</cell><cell>0.318</cell><cell>-3.00</cell><cell>7.18</cell><cell>0.276</cell><cell>0.514</cell><cell>0.251</cell><cell>-3.00</cell><cell>6.83</cell></row><row><cell>Ground Truth</cell><cell>0</cell><cell>0.827</cell><cell>0</cell><cell>0.15</cell><cell>8.58</cell><cell>0</cell><cell>0.612</cell><cell>0</cell><cell>-2.33</cell><cell>7.16</cell></row><row><cell>Ours</cell><cell>0.172</cell><cell>0.751</cell><cell>0.271</cell><cell>-0.33</cell><cell>7.09</cell><cell>0.194</cell><cell>0.564</cell><cell>0.252</cell><cell>-0.08</cell><cell>6.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of ablation study on HDTF.</figDesc><table><row><cell>Method</cell><cell cols="4">FID↓ CPBD↑ LMD↓ AVOff(→0) AVConf↑</cell></row><row><cell cols="2">w/o Pho 0.155 0.740</cell><cell>0.376</cell><cell>-0.503</cell><cell>6.13</cell></row><row><cell cols="2">w/o Aud 0.151 0.747</cell><cell>0.293</cell><cell>-0.538</cell><cell>6.59</cell></row><row><cell cols="2">w/o Dec kp 0.184 0.707</cell><cell>0.418</cell><cell>-0.308</cell><cell>5.24</cell></row><row><cell>w/o Kp</cell><cell>0.210 0.704</cell><cell>0.461</cell><cell>-0.170</cell><cell>4.92</cell></row><row><cell cols="2">w/o BST 0.151 0.747</cell><cell>0.388</cell><cell>-0.743</cell><cell>4.55</cell></row><row><cell cols="2">w/o Dsync 0.150 0.741</cell><cell>0.385</cell><cell>-0.385</cell><cell>5.53</cell></row><row><cell>Full</cell><cell>0.148 0.740</cell><cell>0.274</cell><cell>-0.417</cell><cell>7.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of user study. Participants rate each video from 1 to 5. Large scores indicate better visual quality. Here, the average scores across 21 videos are reported.</figDesc><table><row><cell>Method</cell><cell cols="7">Wav2Lip MakeitTalk Audio2Head FGTF PC-AVS Ground Truth Ours</cell></row><row><cell>Lip Sync Quality</cell><cell>3.80</cell><cell>2.65</cell><cell>2.07</cell><cell>2.43</cell><cell>3.74</cell><cell>4.96</cell><cell>4.32</cell></row><row><cell>Head Movement Naturalness</cell><cell>1.21</cell><cell>2.04</cell><cell>3.79</cell><cell>3.74</cell><cell>1.79</cell><cell>4.89</cell><cell>4.11</cell></row><row><cell>Video Realness</cell><cell>1.68</cell><cell>1.70</cell><cell>3.21</cell><cell>3.08</cell><cell>2.14</cell><cell>4.89</cell><cell>3.86</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
	</analytic>
	<monogr>
		<title level="m">13th IEEE international conference on automatic face &amp; gesture recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Talking-head generation with rhythmic head motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3D speaking styles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10101" to="10111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech-driven facial animation using cascaded gans for learning of motion and texture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">JALI: an animator-centric viseme model for expressive lip synchronization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note>Generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11078</idno>
		<title level="m">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10893" to="10900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-driven emotional video portraits</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14080" to="14089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Neural word embedding as implicit matrix factorization. Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2021. 2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1911" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08863</idno>
		<idno>arXiv:1706.08612</idno>
	</analytic>
	<monogr>
		<title level="m">Voxceleb: a large-scale speaker identification dataset</title>
				<imprint>
			<date type="published" when="2017">2021. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A no-reference perceptual image sharpness metric based on a cumulative probability of blur detection</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2017</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
	<note>2009 International Workshop on Quality of Multimedia Experience</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7137" to="7147" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05201</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04786</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Synthesizing obama: learning lip sync from audio</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realistic speech-driven facial animation with gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Au-dio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Audio-driven talking face video generation with learning-based personalized head pose</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10137</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="217" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic face hallucination: Super-resolving very lowresolution face images with supplementary attributes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Face hallucination with tiny unaligned images by transformative discriminative neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hallucinating very lowresolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="3760" to="3768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019b. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12757" to="12764" />
		</imprint>
	</monogr>
	<note>Can we see more? joint frontalization and hallucination of unaligned tiny faces</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d talking face with personalized pose dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021c</date>
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audiovisual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4176" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MakeltTalk: speaker-aware talking-head animation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visemenet: Audio-driven animator-centric speech animation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep audio-visual learning: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
