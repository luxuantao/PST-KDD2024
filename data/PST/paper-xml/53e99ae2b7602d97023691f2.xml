<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Instruction Fetch Streaming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Architecture Lab (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Parallel Systems Architecture Lab (PARSA)</orgName>
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Advanced Computer Architecture Lab (ACAL)</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Architecture Lab (CALCM)</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Parallel Systems Architecture Lab (PARSA)</orgName>
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Parallel Systems Architecture Lab (PARSA)</orgName>
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Instruction Fetch Streaming</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>instruction streaming</term>
					<term>fetch-directed</term>
					<term>caching</term>
					<term>prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L1 instruction-cache misses pose a critical performance bottleneck in commercial server workloads. Cache access latency constraints preclude L1 instruction caches large enough to capture the application, library, and OS instruction working sets of these workloads. To cope with capacity constraints, researchers have proposed instruction prefetchers that use branch predictors to explore future control flow. However, such prefetchers suffer from several fundamental flaws: their lookahead is limited by branch prediction bandwidth, their accuracy suffers from geometrically-compounding branch misprediction probability, and they are ignorant of the cache contents, frequently predicting blocks already present in L1. Hence, L1 instruction misses remain a bottleneck.</p><p>We propose Temporal Instruction Fetch Streaming (TIFS)-a mechanism for prefetching temporally-correlated instruction streams from lower-level caches. Rather than explore a program's control flow graph, TIFS predicts future instruction-cache misses directly, through recording and replaying recurring L1 instruction miss sequences. In this paper, we first present an informationtheoretic offline trace analysis of instruction-miss repetition to show that 94% of L1 instruction misses occur in long, recurring sequences. Then, we describe a practical mechanism to record these recurring sequences in the L2 cache and leverage them for instruction-cache prefetching. Our TIFS design requires less than 5% storage overhead over the baseline L2 cache and improves performance by 11% on average and 24% at best in a suite of commercial server workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L1 instruction-cache misses pose a critical performance bottleneck in commercial server workloads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. Commercial server workloads span multiple binaries, shared libraries, and operating system code and exhibit large instruction working sets that overwhelm L1 instruction caches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Although total on-chip cache capacity is growing, low access latency and high fetch bandwidth requirements preclude enlarging L1 instruction caches to fit commercial application working sets. As a result, commercial server workloads incur instruction-related delays in the memory system that account for as much as 25%-40% of execution time <ref type="bibr" target="#b11">[12]</ref>.</p><p>Instruction-cache misses are particularly expensive because they contribute to system performance loss in multiple ways. First, instruction fetch stalls directly prevent a core from making forward progress because instructions are not available for dispatch. Unlike data accesses, which can be overlapped through out-of-order execution, instruction fetch is on the crit-ical path of program execution, and nearly the entire latency of an L1 instruction miss is exposed. Second, due to lower average ROB occupancy, instruction-fetch stalls reduce the number of load instructions simultaneously present in the ROB, thereby indirectly reducing performance through a decrease in memory-level parallelism.</p><p>To improve fetch unit performance despite limited instruction cache capacity, researchers have proposed a variety of hardware prefetching schemes. The widely-implemented nextline instruction prefetcher <ref type="bibr" target="#b28">[29]</ref>, despite its simplicity, substantially reduces L1 instruction-cache misses in commercial server workloads <ref type="bibr" target="#b16">[17]</ref>, but is only effective for straight-line code. More advanced instruction prefetchers can predict discontinuous control flow by using the branch predictor to explore a program's control flow graph ahead of the fetch unit <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, prefetching the instruction-cache blocks encountered along the predicted path into the L1 cache.</p><p>Although state-of-the-art hardware prefetchers eliminate many instruction misses, they suffer from four fundamental flaws that limit their coverage and lookahead. First, the branch predictors at the core of these prefetchers predict only a few branches per cycle, limiting instruction prefetch lookahead. Second, branch predictors issue predictions at basic block rather than cache block granularity. Substantial long-range prediction efficiency is lost when exploring control flow within an instruction-cache block and around inner loops. Third, misprediction probability compounds geometrically, limiting prefetch accuracy. Finally, branch predictors are ignorant of cache contents, predicting all future control flow. As a result, the vast majority of predicted blocks are already present in L1, incurring prediction overhead or requiring filtering and lookup mechanisms.</p><p>Rather than exploring a program's control flow graph, we propose to predict the L1 instruction-cache miss sequences directly. Our key observation, inspired by recent studies of data prefetching <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>, is that repetitive control flow graph traversals lead to recurring instruction-cache miss sequences, much like repetitive data-structure traversals lead to recurring data-cache miss sequences. We show that nearly all instruction-cache misses, including control transfers to noncontiguous instructions, occur as part of such recurring sequences, which we call temporal instruction streams. In contrast to the instruction streams in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, which comprise a sequence of contiguous basic blocks, temporal instruction streams comprise recurring sequences of instruction-cache blocks that span discontinuities (taken branches) within the instruction address sequence. We record temporal instruction streams as they are encountered, and replay them to predict misses, eliminating the inefficiencies that arise through reconstructing the sequences indirectly with branch predictions. Our hardware design, Temporal Instruction Fetch Streaming (TIFS), embeds the storage required to track temporal streams in the L2 tag and data arrays, imposing less than 5% storage overhead. Through a combination of information-theoretic analysis of repetition in instruction-cache miss sequences, trace-driven hardware simulation, and cycle-accurate modeling of a CMP system running commercial server workloads, we demonstrate:</p><p>• Recurring streams of instruction misses. We demonstrate that 94% of instruction-cache misses repeat a prior miss stream, with a median stream length of over 20 cache blocks.</p><p>• Lookahead limits of branch-predictor-directed prefetch. We find that, for approximately one-third of instructioncache misses, more than 16 non-inner-loop branches must be correctly predicted to achieve a four-cache-miss lookahead. TIFS lookahead is not limited by the branch predictor.</p><p>• I-streaming effectiveness exceeding the state-of-the-art. TIFS operates on instruction-cache misses rather than the basic-block sequence, improving accuracy, bandwidth efficiency, and lookahead of instruction-cache prefetching. TIFS improves commercial workload performance by 5% on average and 14% at best over a system with fetch-directed prefetching <ref type="bibr" target="#b23">[24]</ref> and by 11% on average and 24% at best over one with next-line instruction prefetching.</p><p>The remainder of this paper is organized as follows. We demonstrate the need for improvement over next-line instruction prefetching in Section 2. We analyze examples of code patterns that can lead to improved performance with TIFS in Section 3. In Section 4, we present an information-theoretic opportunity study to characterize repetition in instruction-miss streams. Based on this study, we develop a hardware design for TIFS in Section 5. We evaluate the effectiveness of TIFS on real workloads in Section 6. In Section 7, we discuss related work, and in Section 8 we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE NEED FOR IMPROVED INSTRUCTION PREFETCHING</head><p>To motivate the need for further improvements to instruction prefetching, we begin with a preliminary performance study of the sensitivity of commercial server workloads to instruction prefetch accuracy beyond what is obtained with the next-line instruction prefetchers in current processors. To measure workload performance sensitivity, we simulate a simple probabilistic prefetching model and vary its coverage. For each L1 instruction miss (also missed by the next-line instruction prefetcher), if the requested block is available on chip, we determine randomly (based on the desired prefetch coverage) if the request should be treated as a prefetch hit. Such hits are instantly filled into the L1 cache. If the block is not available on chip (i.e., this is the first time the instruction is fetched), the miss proceeds normally. A probability of 100% approximates a perfect and timely instruction prefetcher. We include three commercial server workload classes in our study: online transaction processing (OLTP), decision support (DSS), and web serving (Web). We provide complete details of our workloads in Section 4.1 and our simulation infrastructure and performance measurement methodology in Section 6.1.</p><p>We plot results of this study in Figure <ref type="figure">1</ref>. As these results are derived from sampled simulations and are subject to sample variability (see Section 6.1), we plot linear regressions to illustrate their trend. Our results demonstrate that both online transaction processing (OLTP) workloads and Web-Apache are highly sensitive to instruction prefetching-their performance improves by over 30% with a perfect instruction prefetching mechanism. In decision support system (DSS) queries, and in Web-Zeus, the instruction working set is smaller, yielding less sensitivity. Overall, it is clear that augmenting next-line instruction prefetching with a more capable mechanism can have substantial performance impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DRAWBACKS OF EXISTING INSTRUCTION PREFETCHERS</head><p>To provide better intuition for the drawbacks of existing instruction prefetch mechanisms, we perform an analysis of a few common-case scenarios where a processor equipped with next-line and branch-predictor-directed prefetchers may experience stalls due to instruction-cache misses. We perform this investigation by extracting call-stack traces at instruction-cache misses of interest, and then use application binary and kernel symbol tables to map the call stack to function names. The mapping enables us to locate and analyze the source code (for Apache and Solaris) at these misses.</p><p>The specific examples we describe here do not by themselves account for a majority of processor front-end stalls (though frequent, they still represent a small fraction of the overall miss profile). Rather, each example illustrates a common-case behavior that arises in many program contexts. Analysis of these cases offers a deeper look at the drawbacks of existing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unpredictable sequential fetch</head><p>Surprisingly, both the next-line and the branch-predictordirected prefetchers sometimes fail to hide instruction-cache miss latency even in functions with sequential instruction flow. For a next-line instruction prefetcher, each fetch discontinuity can lead to such stalls. Upon a control transfer at the discontinuity, the fetch unit retrieves the target instruction, and the next-line instruction prefetcher initiates a prefetch of the next few cache blocks. Often, execution of instructions in the first cache block completes before prefetches of the subsequent blocks complete, and some instruction-fetch latency is exposed. It is this scenario that motivates the discontinuity prefetcher <ref type="bibr" target="#b30">[31]</ref>. The discontinuity prefetcher and more sophisticated fetch-directed prefetchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> have the potential to eliminate these stalls if they provide sufficient lookahead to predict the critical discontinuity. However, in branch-heavy code, these prefetchers cannot run far enough ahead of the fetch unit to hide all instruction-fetch latency.</p><p>The Solaris kernel scheduling code is critical to commercial server workload performance. An example of a frequentlyinvoked helper function from the scheduler code is highbit(), which calculates the index of the most-significant asserted bit in a double-word through a series of mask and arithmetic operations organized in a sequence of branch hammocks (shown in Figure <ref type="figure" target="#fig_0">2 (a)</ref>). Although highbit() spans only a few consecutive instruction-cache blocks, and execution always proceeds through all blocks, the last block incurs an instruction-cache miss in a large fraction of invocations, even with a fetch-directed prefetcher. The complex control flow in the scheduling code preceding the invocation of highbit() and the dense branch hammocks within it exceed the prefetchers' lookahead capabilities. Moreover, because the scheduling code begins with synchronization instructions that drain the processor's reorder buffer, the instruction-cache miss in highbit() is fully exposed and incurs a large penalty.</p><p>The sequence of instruction-cache misses experienced while executing highbit() is always the same, and there are only a limited number of call sites from which this function is invoked. It is therefore possible for a hardware mechanism to record the sequences of misses that lead up to and include the highbit() function. Whenever a sequence of cache misses that includes the highbit() function is detected within the scheduler code, the previously-recorded sequence can be used to predict all of the needed cache blocks before any of the instructions of highbit() are executed. Therefore, unlike next-line and branch-predictor-directed prefetchers whose lookahead is limited by the number of branches that must be traversed, temporal instruction streaming can properly prefetch the function's instruction-cache misses long before their use, eliminating all of highbit()'s instruction-cache stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Re-convergent hammocks</head><p>Predictor-directed prefetchers are not effective in code regions that exhibit series of if-then-else statements that reconverge in the program's control flow (hammocks). The datadependent branches in such regions cause frequent branch mispredictions, which thwart branch-predictor-directed prefetch methods. The core_output_filter() function found in the Apache 2.x web server illustrates this behavior.</p><p>Because of its central nature, core_output_filter() is invoked from numerous call sites and contains a multitude of highly data-dependent code paths and loops. The complexity of core_output_filter() results in a large instruction working set, over 2.5KB for this function alone, in addition to a variety of helpers called from within its body. The "if" statements in core_output_filter() serve as an example of conditional branches that frequently cause processor stalls at the re-convergence points. Because branch outcomes are datadependent and change with each invocation of the function, branch predictors cannot look beyond the loops, conditional paths, and sub-routine calls within branch hammocks in this code. Branch mispredictions impede correct prefetches by a fetch-directed mechanism; predicted prefetch paths are discarded and the fetch-directed prefetcher restarts its controlflow exploration each time a branch resolves incorrectly. Hence, despite a fetch-directed prefetcher, execution of core_output_filter() exhibits frequent instruction-cache misses at the hammock re-converge points (Figure <ref type="figure" target="#fig_0">2 (b)</ref>).</p><p>The hammocks in core_output_filter() eventually re-converge on the same path. While the specific branch outcomes are unpredictable, having a global view of the previously encountered cache-miss sequence enables prediction of future misses at the hammock re-convergence points. In fact, because the re-convergence points are present in all previously recorded miss-address sequences, correct predictions can be made by using any one of the sequences. Furthermore, the complex, data-dependent computations that are present in core_output_filter() are slow, allowing sufficient time for a temporal instruction-stream prefetcher to fully hide all instruction-miss latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPPORTUNITY FOR TEMPORAL INSTRUCTION FETCH STREAMING</head><p>Our analysis of the performance losses from instructioncache misses demonstrates the need to address on-chip instruction access stalls such as those described in Section 3. The key idea of TIFS is to record streams of L1 instruction misses, predict when these streams recur, and then stream instructions from lower-level caches prior to explicit fetch requests. In this section, we describe an information-theoretic analysis of repetition in instruction-cache misses to determine the potential to eliminate misses with TIFS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Opportunity study methodology</head><p>TIFS relies on the repetition in L1 instruction miss sequences. To determine the opportunity to eliminate misses with TIFS, we study the amount of repetition in traces of the L1 instruction-cache misses of commercial server workloads. Table <ref type="table" target="#tab_1">I</ref> enumerates the details of our workload suite. We collect traces with the FLEXUS full-system simulation infrastructure <ref type="bibr" target="#b37">[38]</ref>. We trace the DSS queries in their entirety, and four billion instructions (one billion per core) from the remaining workloads. Our traces include all user and OS instruction fetches. We study a 4-core single-chip CMP with split 64 KB 2-way setassociative L1 caches and 64-byte blocks, and a next-line instruction prefetcher that continually prefetches two cache blocks ahead of the fetch unit (other model parameters do not affect trace collection). A "miss" is an instruction fetch that can't be satisfied by the L1 instruction cache or next-line instruction prefetcher.</p><p>We apply an information-theoretic approach to quantify miss-stream repetition. Like similar studies of repetitive streams in L1 data accesses <ref type="bibr" target="#b5">[6]</ref>, off-chip data misses <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, and program paths <ref type="bibr" target="#b15">[16]</ref>, we use the SEQUITUR <ref type="bibr" target="#b9">[10]</ref> hierarchical data compression algorithm to identify repetitive subsequences within the miss-address traces. SEQUITUR produces a grammar whose production rules correspond to repetitions in its input. Hence, grammar production rules correspond to recurring miss-address streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Miss repetition</head><p>We apply SEQUITUR to quantify the maximum opportunity for TIFS. Figure <ref type="figure">3</ref> shows a categorization of instruction misses based on whether or not they repeat a prior temporal miss stream. Non-repetitive misses do not occur as part of a repetitive stream (i.e., they never occur twice with the same preceding or succeeding miss address). On the first occurrence of a repetitive stream, we categorize the misses as New. On subsequent occurrences, the first miss in the stream is categorized as Head and the remaining misses as Opportunity. We distinguish Head and New from Opportunity because our hardware mechanisms are not be able to eliminate these misses due to training and prediction-trigger mechanisms. An example temporal miss stream and categorization appear in Figure <ref type="figure">4</ref>.</p><p>Our SEQUITUR analysis reveals that nearly all instruction misses, on average 94%, are part of a recurring stream of instruction-cache misses that may be eliminated by TIFS. Opportunity is highest in OLTP workloads, which have the largest instruction working sets among our workloads. This offline trace analysis demonstrates the highly repetitive nature of instruction execution-the same execution paths incur the same temporal miss streams over and over.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stream length</head><p>The length of repetitive access sequences directly translates to prefetch coverage that a memory streaming mechanism can achieve by predicting that sequence. The ability to follow arbitrarily long streams by periodically requesting additional addresses distinguishes temporal streaming <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> from prefetching approaches that only retrieve a constant number of blocks in response to a miss (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>). Without this ability, the system would incur one miss for each group of prefetched blocks. Furthermore, long streams improve prefetch timeliness-after a stream head is detected, a simple ratematching mechanism enables the prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type="bibr" target="#b36">[37]</ref>.</p><p>Figure <ref type="figure">5</ref> shows the cumulative distribution of recurring stream lengths as identified by SEQUITUR. The stream lengths shown in Figure <ref type="figure">5</ref> reflect only non-sequential instruction block references; all sequential misses are removed from the traces to simulate the effect of a perfect next-line instruction prefetcher (stream lengths roughly double when sequential blocks are not removed). The SEQUITUR analysis confirms that instructionmiss streams are long-for example, in OLTP-Oracle, the median length is 80 perfectly-repeating non-sequential blocks. In comparison, prior work reports that off-chip temporal datamiss streams exhibit a median length of 8 to 10 blocks <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Transaction Processing (TPC-C)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oracle</head><p>Oracle 10g Enterprise Database Server, 100 warehouses (10 GB), <ref type="bibr" target="#b15">16</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stream lookup heuristics</head><p>The SEQUITUR opportunity study identifies all recurring streams present in the miss sequence. Under some circumstances, such as divergent control flow, multiple distinct repetitive streams begin with the same head address. SEQUITUR always identifies the best match among these alternatives-it measures the upper bound of repetition, and hence represents perfect stream lookup. In contrast, a practical hardware streaming mechanism must use a heuristic to determine which previously-seen stream among all distinct prior alternatives should be fetched upon a subsequent miss to the head address.</p><p>To guide the design of our streaming hardware, we consider several possible heuristics for choosing the correct stream. The results of our comparison are shown in Figure <ref type="figure">6</ref>. The First policy associates a head address to the first stream (earliest in program order) that SEQUITUR identifies as starting with that address. Digram uses the second address, in addition to the head address, to identify which stream to follow. The Recent heuristic continually re-associates a miss address with the most-recently encountered stream headed by that address. Finally, Longest associates a miss address with the longest stream to ever occur following that address. Opportunity is the bound on repetition established in Figure <ref type="figure">3</ref>.</p><p>Our study reveals that the Longest policy is most effective. Unfortunately, we are unaware of a practical implementation of this heuristic, because hardware mechanisms can only discover stream length after the fact, by counting successful prefetches. Hence, like past designs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as it is easy to implement in hardware while still yielding high predictor coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN</head><p>We separate our presentation of the TIFS hardware design into two sections. For clarity of presentation, in Section 5.1 we present the logical structure and operation of TIFS while omitting the details of the hardware implementation. In Section 5.2 we present the physical organization of these structures in TIFS, placing emphasis on a low hardware-cost implementation. Where possible, we virtualize the TIFS storage structures <ref type="bibr" target="#b3">[4]</ref>-that is, predictor storage is allocated within the L2 cache rather than in dedicated SRAM. Virtualization allows software to control and vary structure size allocation and partitioning on a per-application or per-core basis, up to and including disabling TIFS entirely to reclaim L2 capacity in the event it is not effective for a particular workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Logical hardware overview</head><p>Figure <ref type="figure">7</ref> illustrates the logical structure and operation of TIFS. The basic operation of TIFS mirrors prior address-correlated prefetching proposals that target data accesses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. TIFS adds three logical structures to the chip: a set of Streamed Value Buffers (SVBs), one per L1-I cache; a set of Instruction Miss Logs (IMLs), one per L1-I cache; and a single Index Table <ref type="table">.</ref> As in prior stream buffer proposals <ref type="bibr" target="#b13">[14]</ref>, the SVB holds streamed blocks that have not yet been accessed and maintains the information necessary to continue fetching a stream. Each IML structure maintains a log of all instruction-cache miss addresses from the corresponding L1-I cache. The shared Index Table, used to locate streams, maintains a pointer from each address to the most recent global occurrence of that address in any IML. Because the Index Table is shared among all IMLs, an Index Table pointer is not limited to a particular IML, enabling SVBs to locate and follow streams logged by other cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Logging miss-address streams</head><p>As instructions retire, L1-I fetch-miss addresses are logged in an IML. Addresses are logged only at instruction retirement to reduce the impact of out-of-order and wrong-path execution. To facilitate logging, ROB entries are marked if the instruction misses in the L1-I cache during the fetch stage. IMLs record physical addresses to avoid the need for address translation during prefetch. As an address is appended to an IML, an entry in the Index Table is created/updated with a pointer to indicate the location inside the IML at which the address was logged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Instruction streaming</head><p>The SVB tracks the current state of a stream flowing into its L1 cache and serves as the temporary storage for streamed blocks that have not yet been accessed by the processor core. On every L1-I cache miss, the core checks if the accessed block is present in the SVB. The SVB check is performed after the L1 access to avoid circuit complexity that might impact the fetch critical path. Upon an SVB hit, the block is immediately allocated in the L1 cache and the stream's next block is prefetched. SVB hits are also logged in the appropriate IML, ensuring that the block will be fetched during the next stream traversal.</p><p>If the requested block is not found in the SVB, a new stream is allocated to prefetch subsequent blocks. The SVB consults the Index Table to find a pointer to the IML location where the miss address was most recently observed. If found, a stream pointer in the SVB is set to the corresponding IML location, and the SVB begins reading the IML and prefetching blocks according to the logged stream. As further blocks in the stream are accessed, the SVB continues advancing the stream pointer and prefetching additional blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">End of stream detection</head><p>In prior stream buffers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, in the stride predictors of commercial systems <ref type="bibr" target="#b12">[13]</ref>, and in the simplest TIFS design, stream buffers make no effort to guess where a stream might end. However, as shown in Figure <ref type="figure">5</ref>, streams vary drastically in length, from two to thousands of blocks. Stopping too early may result in lost coverage, while stopping too late may result in erroneous prefetches and wasted bandwidth.</p><p>The SVB terminates a stream by remembering where the stream ended the last time it was followed. As addresses are logged in an IML, an additional bit is stored to indicate if the address is recorded as the result of an SVB hit. When following a stream, the SVB immediately fetches blocks if this bit is set (indicating a correct prior prediction), but pauses after fetching the first block where the bit is cleared (indicating a potential stream end). If the block is accessed as a result of an L1-I miss, the SVB resumes stream fetch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hardware implementation</head><p>Figure <ref type="figure">8</ref> depicts our proposed implementation of TIFS in the context of a four-core chip multiprocessor. The physical implementation differs from the logical description of Section 5.1 in three respects. First, the SVB hardware simultaneously maintains several stream pointers and small queues of upcoming prefetch addresses to allow for multiple parallel inprogress streams. Multiple streams may arise because of traps, context switches, or other interruptions to the usual control flow. Second, although our logical design calls for a dedicated storage structure for each IML, we implement TIFS with minimal hardware overhead by virtualizing IMLs and storing their contents inside the L2 data array as proposed in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Finally, rather than implementing the Index Table as a separate structure, we embed the IML pointers in the L2 cache as additional bits in the tag array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Streamed value buffers</head><p>Figure <ref type="figure">9</ref> depicts the anatomy of the SVB. Our SVB design is adapted from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. The SVB contains a small fully-associative buffer for temporary storage of streamed blocks. Each entry includes a valid bit, tag, and contents of the instruction block. Upon an SVB hit, the block is transferred to the L1 Icache and the SVB entry is freed. If the SVB becomes full, entries are replaced using an LRU policy. To manage prefetching for in-flight streams, the SVB maintains FIFO queues of addresses awaiting prefetch and pointers into the IML indicating the continuation of each active stream. As a FIFO drains, further addresses are read from the IML and the IML pointer is advanced. In this fashion, the SVB can follow a stream of arbitrary length.</p><p>The SVB logic ensures that streams are followed (i.e., blocks are requested) sufficiently early to hide their retrieval latency. SVB matches the rate that blocks are fetched to the rate at which the L1 cache consumes these blocks. The SVB attempts to maintain a constant number of streamed-but-notyet-accessed blocks for each active stream. Across workloads, we find that four blocks per stream is sufficient. Hence, the SVB capacity can remain small (2 KB per core).</p><p>The key purpose of the SVB is to avoid polluting the cache hierarchy if a retrieved stream is never used. By making multiple blocks from a stream available simultaneously in a</p><formula xml:id="formula_0">L1-I Miss C Index Table P Q R C D E F G Instruction Miss Log L1-I { E F G . . . . . L2 Streamed Value Buffer D (6) (1)<label>(2)</label></formula><p>(3) (4)</p><p>(5) fully-associative buffer, the SVB also serves as a window to mitigate small (e.g., a few cache blocks) deviations in the order of stream accesses that may arise due to minor data-dependent control flow irregularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Instruction miss logs</head><p>TIFS requires an allocation of on-chip storage to record cache-miss addresses. We virtualize the IML storage structures, placing cache-miss addresses directly in the L2 cache. From the cache's point of view, IML entries exist in a private region of the physical address space. IML reads and writes are issued to L2 as accesses to this address region, and are performed at cache-block granularity (i.e., 64-byte cache blocks containing twelve recorded miss addresses are read or written at a time).</p><p>The Index Table is realized as a part of the L2 tag array, as an additional IML pointer field per tag; the width of this pointer field places an upper bound on virtual IML size. Stream lookups are performed on every L1-I fetch miss. Co-locating the Index Table with the L2 tags provides a "free" lookup mechanism performed in parallel with the L2 access. When an access hits in the L2 tag array, the IML pointer is returned to the corresponding SVB to initiate stream fetch. The stream fetch proceeds in parallel with the L2 data-array access.</p><p>Each time an address is appended to an IML (at instruction retirement), the IML pointer stored as part of the L2 tag must be updated. IML pointer updates are queued separately from other cache requests and are scheduled to the tag pipelines with the lowest priority. If back-pressure results in full queues, updates are discarded. Because of the time delay between cache access and instruction retirement, IML pointer updates occasionally fail to find a matching address in the tag arrays; such updates are silently dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We evaluate TIFS in the context of a four-core chip multiprocessor. We study a system with aggressive, out-of-order cores and decoupled front-end instruction fetch units <ref type="bibr" target="#b24">[25]</ref> which partially hide instruction-fetch latency in commercial server workloads. We expect the impact of TIFS to be even higher in simpler cores without these mechanisms, where all instruction-fetch latency is exposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>We study TIFS using a combination of trace-based and cycle-accurate simulation. We model a four-core CMP with private L1 instruction and data caches and a shared, unified L2 cache. The L2 is divided into 16 banks with independentlyscheduled tag and data pipelines. The L1 caches are connected to L2 banks with a full, non-blocking crossbar. Each bank's data pipeline may initiate a new access at most once every four cycles. The minimum total L2 hit latency is 20 cycles, but accesses may take longer due to bank conflicts or queueing delays. A total of at most 64 L2 accesses, L1 peer-to-peer transfers, and off-chip misses may be in flight at any time. We configure our core model to resemble the Intel Core 2 microarchitecture, and base main-memory latency and bandwidth parameters on the IBM Power 6. Further configuration details appear in Table <ref type="table" target="#tab_1">II</ref>.</p><p>Our base system includes a next-line instruction prefetcher in each instruction fetch unit, and a stride prefetcher at L2 for retrieving data from off chip. We account TIFS hits only in excess of those provided by the next-line instruction prefetcher (i.e., next-line hits are counted as L1 hits; we do not include these in TIFS coverage even if TIFS could also prefetch the blocks.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Lookahead limitations of fetch-directed prefetching</head><p>A key advantage of TIFS is that it provides far greater lookahead than other instruction prefetchers. TIFS lookahead is bounded by temporal instruction stream length, and each IML lookup provides twelve prefetch addresses. In contrast, fetchdirected prefetching lookahead is limited by the number of branches that can be predicted accurately between instructioncache misses.</p><p>To assess the lookahead limits of fetch-directed prefetching, we analyzed the number of branches that must be predicted correctly to prefetch four instruction-cache misses ahead of the fetch unit. We exclude backwards branches in inner-most loops, as a simple filter could detect such loops and prefetch along the fall-through path. The results of our analysis appear in Figure <ref type="figure">10</ref>. For roughly a quarter of all instruction-cache misses, a fetch-directed prefetcher must traverse more than 16 non-inner-loop branches to achieve a lookahead of just four misses. With a branch predictor that can make only one or two predictions per cycle, fetch-directed prefetchers fall far short of the lookahead possible with TIFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hardware requirements</head><p>Although they are large relative to L1 caches, application instruction working sets are small relative to L2, which allows TIFS to capture temporal instruction streams in IML storage that is a small fraction of L2. Figure <ref type="figure">11</ref> shows the TIFS predictor coverage as a function of IML storage capacity (for this analysis, we assume a perfect, dedicated Index Table <ref type="table">)</ref>. Our result confirms that a relatively small number of hot execution traces account for nearly all execution. For peak coverage, we find that each core's IML must record roughly 8K instruction block addresses, for an aggregate storage cost of 156 KB (8K entries / core, 38 physical address bits + 1 hit bit / entry, 4 cores; Finally, each core's SVB requires only minimal hardware: a 2KB buffer for instruction-cache blocks and several small FIFOs and registers for prefetch addresses and IML pointers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Traffic overhead</head><p>TIFS increases L2 traffic in two ways: (1) it prefetches blocks which are never accessed by the CPU, which we call discards; and (2) it reads and writes IML blocks. Correctly prefetched blocks replace misses that occur in the base system, thus they cause no increase in traffic.</p><p>We first consider discards. A discard occurs whenever TIFS prefetches a block, and that block is replaced in the SVB by another prefetch (i.e., the block is not fetched). Discards occur most frequently when TIFS prefetches past the end of a repetitive stream; our end-of-stream detection mechanism mitigates this (Section 5.1). Figure <ref type="figure" target="#fig_3">12</ref> (left) shows TIFS coverage and discards for a predictor with previously-cited IML sizes. The coverage and discards are normalized to L1 fetch misses. TIFS traffic overhead also includes reads and writes of the virtualized IMLs. In the best case, if all instruction-cache misses are streamed by TIFS, the IML read and write traffic are each 1/12th (&lt;8.5%) of the fetch traffic. Discards and short streams increase these overheads. In Figure <ref type="figure" target="#fig_3">12</ref> (right), we show the total L2 traffic overhead as a fraction of the base L2 traffic (reads, fetches, and writebacks). TIFS increases L2 traffic on average by 13%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Performance evaluation</head><p>We compare TIFS performance to state-of-the-art prefetching mechanisms and investigate the performance cost of virtualizing TIFS hardware structures in Figure <ref type="figure" target="#fig_4">13</ref>. All bars are normalized to the performance of the next-line instruction prefetcher included in our base system design. FDIP is a stateof-the-art fetch-directed instruction prefetcher <ref type="bibr" target="#b23">[24]</ref>. We make several minor adjustments to the original FDIP design to tune it for our workloads. First, the FDIP authors propose several optimizations to minimize FDIP's impact on L1 tag-port bandwidth. To simplify our experiments, we omit these optimizations, but provide FDIP unlimited tag bandwidth (i.e., no impact on fetch). Second, the authors recommend allowing FDIP to proceed roughly 30 instructions ahead of the fetch unit. To maximize FDIP effectiveness in our environment, we increase this depth to 96 instructions, but at most 6 branches. Finally, for fairness, we assume a fully-associative rather than FIFO prefetch buffer, as the SVB is fully-associative. In our environment, these changes strictly improve FDIP performance. The three TIFS bars represent our TIFS design with unbounded IMLs, 156 KB of dedicated IML storage, and 156 KB of virtualized IML storage in the L2 data array, respectively. Finally, Perfect indicates the upper bound achieved by a perfect instruction streaming mechanism.</p><p>The TIFS performance improvements match the coverages reported in Figure <ref type="figure" target="#fig_3">12</ref> and the sensitivity reported in Figure <ref type="figure">1</ref>. TIFS outperforms FDIP on all studied workloads except DSS  DB2 Qry17, where instruction prefetching provides negligible benefit. TIFS provides the largest benefit in the OLTP workloads, which have the largest instruction working sets. Limiting IML capacity to 156 KB has no effect on performance. Virtualizing the IML marginally reduces performance in OLTP-DB2 because of a slight increase in L2 bank contention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>The importance of instruction prefetching has long been recognized by computer architects. Sequential instruction (next-line) prefetching was implemented in the IBM System 360 Model 91 in the late 1960's <ref type="bibr" target="#b1">[2]</ref>, and prefetching into caches was analyzed by Smith in the late 1970's <ref type="bibr" target="#b28">[29]</ref>. Although computer architectures and workloads have evolved drastically since then, simple next-line instruction prefetching remains critical to the performance of modern commercial server workloads <ref type="bibr" target="#b16">[17]</ref>. More recent work on instruction-stream prefetching generalizes the notion of the next-line instruction prefetcher to arbitrary-length sequences of contiguous basic blocks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>A greater challenge lies in prefetching at fetch discontinuities-interruptions in the sequential instruction-fetch sequence from procedure calls, taken branches, and traps. The discontinuity predictor <ref type="bibr" target="#b30">[31]</ref> maintains a table of such fetch discontinuities. As a next-line instruction prefetcher explores ahead of the fetch unit, it consults the discontinuity table with each block address and, upon a match, prefetches both the sequential and discontinuous paths. Although it is simple and requires minimal hardware, the discontinuity predictor can bridge only a single fetch discontinuity; recursive lookups to explore additional paths result in an exponential growth in the number of prefetched blocks.</p><p>Rather than use a dedicated table, branch-predictor-directed prefetchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> reuse existing branch predictors to explore the future control flow of a program and identify cache blocks for prefetch. Pre-execution and speculative threading mechanisms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> and Runahead Execution <ref type="bibr" target="#b19">[20]</ref> similarly use branch prediction to enable speculative control-flow exploration and prefetching. TIFS differs from these approaches in that it records and replays instruction-cache miss sequences rather than branch outcomes, predicting directly the anticipated future sequence of fetch discontinuities, which improves prefetch timeliness. Furthermore, unlike TIFS, neither next-line nor branch-predictor-directed mechanisms can ensure timely prefetch of sequential blocks that follow a discontinuity. TIFS is not affected by the behavior of unpredictable data-dependent branches, relying only on actual instruction-cache miss sequences that occurred in the past for future predictions. We compared the performance of TIFS with Fetch Directed Instruction Prefetching (FDIP) <ref type="bibr" target="#b23">[24]</ref> in Section 6.5. We quantified the relationship between branch prediction bandwidth and prefetch lookahead in Section 6.2.</p><p>Software approaches to reduce instruction-fetch bottlenecks relocate rarely-executed code and increase the number of sequentially executed instructions <ref type="bibr" target="#b21">[22]</ref>, improve cache line reuse through gang-scheduling or data batching <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39]</ref>, add compiler-inserted instruction prefetches <ref type="bibr" target="#b18">[19]</ref>, or perform call graph prefetching <ref type="bibr" target="#b2">[3]</ref>. These approaches target the same bottlenecks as TIFS, but all require detailed software analysis and modification, and some are application-specific.</p><p>The TIFS design is based on recent proposals for addresscorrelated prefetch of recurring temporal data streams <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>. These prefetchers target primarily off-chip data references and require large off-chip tables to capture prefetcher meta-data. As instruction working sets are orders-of-magnitude smaller than data working sets, TIFS meta-data fits on chip (see Section 6.3). To further reduce the hardware overhead of TIFS, we employ a variant of predictor virtualization <ref type="bibr" target="#b3">[4]</ref>, a technique for storing prefetcher meta-data in the L2 cache (see <ref type="bibr">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type="bibr" target="#b36">[37]</ref>, refers to extended sequences of data references that recur over the course of program execution. Similar repetition in the sequence of basic blocks visited by a program has been reported by Larus <ref type="bibr" target="#b15">[16]</ref> and underlies trace scheduling <ref type="bibr" target="#b8">[9]</ref> and trace caches <ref type="bibr" target="#b25">[26]</ref>. Temporal instruction streams differ from previously-defined instruction streams <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> in two key respects: (1) temporal instruction streams are defined at cache-block rather than basic-block granularity, and (2) temporal instruction streams span fetch discontinuities. TIFS efficiently records and exploits long recurring temporal instruction streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>L1 instruction misses are a critical performance bottleneck in commercial server workloads. In this work, we made the observation that instruction-cache misses repeat in long recurring streams. We employed this observation to construct Temporal Instruction Fetch Streaming (TIFS)-a mechanism for prefetching temporally-correlated instruction streams from lower-level caches. Unlike prior fetch-directed prefetchers, which explore a program's control flow graph, TIFS predicts future instruction-cache misses directly, through recording and replaying recurring L1 instruction-miss sequences. Through full-system simulation of commercial server workloads, we demonstrated that TIFS improves performance by 11% on average and 24% at best. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Instruction fetch example. Examples of cache misses incurred by next-line and branch-predictor-directed prefetching with unpredictable sequential fetch (a) and re-convergent hammocks (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Opportunity. The opportunity to eliminate misses through TIFS as revealed by our information-theoretic analysis of miss-address repetition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .Figure 9 .</head><label>789</label><figDesc>Figure 7. TIFS Operation. An L1-I miss to address C consults the index table<ref type="bibr" target="#b0">(1)</ref>, which points to an IML entry (2). The stream following C is read from the IML and sent to the SVB (3). The SVB requests the blocks in the stream from L2 (4), which returns the contents<ref type="bibr" target="#b4">(5)</ref>. Later, on a subsequent L1-I miss to D, the SVB returns the contents to the L1-I (6).</figDesc><graphic url="image-2.png" coords="6,367.97,72.36,96.30,75.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Coverage, Discards, and Traffic Overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. TIFS Performance Comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>clients, 1.4 GB SGA COMMERCIAL SERVER WORKLOAD PARAMETERS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Opportunity</cell><cell>Head</cell><cell>New</cell><cell>Non-repetitive</cell></row><row><cell></cell><cell></cell><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DB2</cell><cell>IBM DB2 v8 ESE, 100 warehouses (10 GB), 64 clients, 2 GB buffer pool</cell><cell>% L1 Misses</cell><cell>20% 40% 60% 80%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Decision Support (TPC-H on DB2 v8 ESE) Qry 2 Join-dominated, 480 MB buffer pool Qry 17 Balanced scan-join, 480 MB buffer pool Web Server (SPECweb99) Apache Apache HTTP Server 2.0, 4K connections, FastCGI,</cell><cell></cell><cell>0%</cell><cell>DB2 OLTP Oracle</cell><cell></cell><cell>Qry2 DSS DB2 Qry17</cell><cell>Apache</cell><cell>Web</cell><cell>Zeus</cell></row><row><cell></cell><cell>worker threading model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zeus</cell><cell>Zeus Web Server v4.3, 4K connections, FastCGI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The Index Table stores a mapping from addresses to IML pointers and is embedded in the L2 tag array. Each L2 tag is extended with a 15-bit IML pointer, for an aggregate storage cost of 240 KB (&lt;3% increase in L2 storage). Storage costs for a dedicated Index Table are comparable, as a dedicated table requires fewer entries, but each entry must contain a tag as well as the IML pointer. Index Table storage requirements are independent of the number of cores, and instead scale with the number of L2 tags.</figDesc><table><row><cell></cell><cell cols="3">TABLE II. SYSTEM PARAMETERS.</cell></row><row><cell>Cores</cell><cell>UltraSPARC III ISA,</cell><cell>Main</cell><cell>3 GB total memory</cell></row><row><cell></cell><cell>Four 4.0 GHz OoO cores</cell><cell>Memory</cell><cell>28.4 GB/s peak bw</cell></row><row><cell></cell><cell>4-wide dispatch / retirement</cell><cell></cell><cell>45ns access latency</cell></row><row><cell></cell><cell>96-entry ROB, LSQ</cell><cell></cell><cell>64-byte transfer unit</cell></row><row><cell>L1-D</cell><cell>64KB 2-way</cell><cell>L2</cell><cell>8MB 16-way</cell></row><row><cell>Cache</cell><cell>2-cycle load-to-use</cell><cell>Shared</cell><cell>20-cycle access latency</cell></row><row><cell></cell><cell>3 ports, 32 MSHRs</cell><cell>Cache</cell><cell>16 banks, 64 MSHRs</cell></row><row><cell></cell><cell>64-byte lines</cell><cell></cell><cell>64-byte lines</cell></row><row><cell>I-Fetch</cell><cell>64KB 2-way L1-I cache</cell><cell>Stride</cell><cell>Next-line I-prefetcher</cell></row><row><cell>Unit</cell><cell>16-entry pre-dispatch queue</cell><cell>Prefetch</cell><cell>32-entry D-stream buffer</cell></row><row><cell></cell><cell>Hybrid branch predictor</cell><cell></cell><cell>Up to 16 distinct strides</cell></row><row><cell></cell><cell>16K gShare &amp; 16K bimodal</cell><cell></cell><cell></cell></row><row><cell>in total, less than 2% of the L2 cache capacity). Total IML</cell><cell></cell><cell></cell><cell></cell></row><row><cell>capacity requirements scale with core count (~40KB per core).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Brian T. Gold, Nikolaos Hardavellas, Stephen Somogyi, and the anonymous reviewers for their feedback on drafts of this paper. This work was partially supported by grants and equipment from Intel, two Sloan research fellowships, an NSERC Discovery Grant, an IBM faculty partnership award, and NSF grant CCR-0509356.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBMSs on a modern processor: Where does time go?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The IBM system/ 360 model 91: Machine philosophy and instruction handling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sparacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Tomasulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="24" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Call graph prefetching for database applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="412" to="444" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictor virtualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int&apos;l Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>13th Int&apos;l Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instruction prefetching using branch prediction information</title>
		<author>
			<persName><forename type="first">I-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l Conference on Computer Design</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient representations and abstractions for quantifying and exploiting data reference locality</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGPLAN &apos;01 Conference on Programming Language Design and Implementation</title>
				<meeting>SIGPLAN &apos;01 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-cost epoch-based correlation prefetching for commercial applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual Int&apos;l Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Last-touch correlated data streaming. Int&apos;l Symposium on Performance Analysis of Systems and Software</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trace scheduling: A technique for global microcode compaction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="478" to="490" />
			<date type="published" when="1981-07">Jul. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying hierarchical structure in sequences: A linear-time algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mancheril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<title level="m">Database servers on Chip Multiprocessors: Limitations and Opportunities. 3rd Biennial Conference on Innovative Data Systems Research</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Steps towards cache-resident transaction processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int&apos;l Conference on Very Large Data Bases (VLDB&apos;04)</title>
				<meeting>30th Int&apos;l Conference on Very Large Data Bases (VLDB&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Optimizing application performance on Intel Core microarchitecture using hardware-implemented prefetchers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hedge</surname></persName>
		</author>
		<ptr target="http://www.intel.com/cd/ids/developer/asmo-na/eng/298229.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>17th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Performance characterization of a quad Pentium Pro SMP using OLTP workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>25th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Whole program paths</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGPLAN &apos;99 Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>SIGPLAN &apos;99 Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of database workload performance on simultaneous multithreaded processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>25th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tolerating memory latency through software-controlled preexecution in simultaneous multithreading processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>28th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annual Int&apos;l Symposium on Microarchitecture</title>
				<meeting>31st Annual Int&apos;l Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Runahead execution: an effective alternative to large instruction windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="2003-12">Nov./Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Symposium on High-Performance Computer Architecture</title>
				<meeting>10th Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Code layout optimizations for transaction processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>28th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fetching instruction streams. 35th Annual Int&apos;l Symposium on Microarchitecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fetch-directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annual Int&apos;l Symposium on Microarchitecture</title>
				<meeting>32nd Annual Int&apos;l Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizations enabled by a decoupled front-end architecture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Computers</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trace cache: a low latency approach to high bandwidth instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annual Int&apos;l Symposium on Microarchitecture</title>
				<meeting>29th Annual Int&apos;l Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enlarging instruction streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1342" to="1357" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Predictor-directed stream buffers. 33rd Annual Int&apos;l Symposium on Microarchitecture</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>29th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int&apos;l Symposium on High-Performance Computer Architecture</title>
				<meeting>11th Int&apos;l Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Branch history guided instruction prefetching. 7th Int&apos;l Symposium on High-Performance Computer Architecture</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A detailed comparison of two transaction processing workloads</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int&apos;l Workshop on Workload Characterization</title>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Slipstream processors: improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int&apos;l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IX)</title>
				<meeting>9th Int&apos;l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IX)</meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<title level="m">Temporal Memory Streaming</title>
				<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal streams in commercial server applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symposium on Workload Characterization</title>
				<meeting>IEEE Int&apos;l Symposium on Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2008-09">Sept. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>32nd Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SimFlex: statistical sampling of computer system simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006-08">Jul.-Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Buffering database operations for enhanced instruction cache performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Execution-based prediction using speculative slices</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Annual Int&apos;l Symposium on Computer Architecture</title>
				<meeting>28th Annual Int&apos;l Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
