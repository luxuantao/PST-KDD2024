<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abstract-Semantic segmentation of remote sensing images plays an important role in land resource management, yield estimation, and economic assessment. U-Net is a sophisticated encoder-decoder architecture which has been frequently used in medical image segmentation and has attained prominent performance. And asymmetric convolution block can enhance the square convolution kernels using asymmetric convolutions. In this paper, based on U-Net and asymmetric convolution block, we incorporate multi-scale features generated by different layers of U-Net and design a multi-scale skip connected architecture, MACU-Net, for semantic segmentation using high-resolution remote sensing images. Our design has the following advantages: (1) The multi-scale skip connections combine and realign semantic features contained both in low-level and high-level feature maps with different scales; (2) the asymmetric convolution block strengthens the representational capacity of a standard convolution layer. Experiments conducted on two remote sensing image datasets captured by separate satellites demonstrate that the performance of our MACU-Net transcends the U-Net, SegNet, DeepLab V3+, and other baseline algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><forename type="middle">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
							<email>syzheng@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Abstract-Semantic segmentation of remote sensing images plays an important role in land resource management, yield estimation, and economic assessment. U-Net is a sophisticated encoder-decoder architecture which has been frequently used in medical image segmentation and has attained prominent performance. And asymmetric convolution block can enhance the square convolution kernels using asymmetric convolutions. In this paper, based on U-Net and asymmetric convolution block, we incorporate multi-scale features generated by different layers of U-Net and design a multi-scale skip connected architecture, MACU-Net, for semantic segmentation using high-resolution remote sensing images. Our design has the following advantages: (1) The multi-scale skip connections combine and realign semantic features contained both in low-level and high-level feature maps with different scales; (2) the asymmetric convolution block strengthens the representational capacity of a standard convolution layer. Experiments conducted on two remote sensing image datasets captured by separate satellites demonstrate that the performance of our MACU-Net transcends the U-Net, SegNet, DeepLab V3+, and other baseline algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>high-resolution remote sensing images</term>
					<term>asymmetric convolution block</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>emantic segmentation using remote sensing images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, plays a critical role in wide range of application scenarios such as land resource management, yield estimation, and economic assessment <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Hitherto the remote sensing community has tried to design assorted classifiers from diverse perspectives, from orthodox methods such as distance measure <ref type="bibr" target="#b5">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type="bibr" target="#b6">[7]</ref> and random forest (RF) <ref type="bibr" target="#b7">[8]</ref>. However, the high dependency on hand-crafted visual features or mid-level semantic features restricts the flexibility and adaptability of these methods.</p><p>More recently, Convolutional Neural Networks (CNN) <ref type="bibr" target="#b8">[9]</ref> have demonstrated its powerful capacity of automatically capture nonlinear and hierarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type="bibr" target="#b9">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, and DeepLab <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> have become the frequently-used schemes. Generally, feature maps generated by the encoder comprise low-level and fine-grained detailed information, while feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type="bibr" target="#b14">[15]</ref>. And skip connections, which combine the low-level and high-level feature maps, are an effective method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type="bibr" target="#b14">[15]</ref>, plain skip connections are substituted by nested and dense skip connections, which enhance the ability of skip connections and narrow the semantic gap between the encoder and decoder. To make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type="bibr" target="#b15">[16]</ref>. However, the design philosophy of full-scale skip connections impliedly indicates that all channels of feature maps generated by different layers share equal weights, and the computational complexity of U-Net 3+ is tremendous. On the contrary, the features generated by different stages own different levels of discrimination. To cope with this issue, in our MACU-Net, we propose a multi-scale skip connection which not only takes full advantages of the multi-scale features, but also realigns channel-wise features responses adaptively.</p><p>Asymmetric convolution blocks (ACB), which own branches with square, horizontal and vertical kernels, could capture refined features by summing up the outputs of three convolutions, which just cause finite increasing in additional computational complexity <ref type="bibr" target="#b16">[17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type="bibr" target="#b16">[17]</ref>, image denoising <ref type="bibr" target="#b17">[18]</ref>, and medical image segmentation <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, only the convolutional layers of encoder are replaced by ACB. Aiming at the task of high-resolution remote sensing image segmentation, we thoroughly incorporate ACB with U-Net by substitute all convolutional layers to enhance the representational ability of a standard square-kernel layer.</p><p>Based on the above-mentioned insight and progress, we design a multi-scale connected architecture deep network, MACU-Net with asymmetric convolution blocks. To verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, DeepLab V3+ <ref type="bibr" target="#b13">[14]</ref>, FC-DenseNet57 <ref type="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</ref>. The major C. Duan is with the State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan 430079, China; chenxiduan@whu.edu.cn (e-mail: chenxiduan@whu.edu.cn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MACU-Net: Semantic Segmentation from High-Resolution Remote Sensing Images</head><p>Rui Li*, Chenxi Duan*, and Shunyi Zheng S contributions of this letter could be listed as follows:</p><p>1) To take full advantages of the multi-scale features, we design a multi-scale skip connection which can realign channel-wise features responses adaptively. 2) We substitute all convolutional layers of U-Net for asymmetric convolution blocks substitute to enhance the representational ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Asymmetric Convolution Block</head><p>Objects may be captured by satellites in any possible orientations. Thus, objects in remote sensing images may be presented at any perspective, which can be summarized as rotation. In order to extract available information contained in remote sensing images, the neural network should be robust to rotation and renders consistent results in different rotations. As reported in <ref type="bibr" target="#b16">[17]</ref>, different asymmetric convolutions are robust with different rotation objects. As can be seen from Fig. <ref type="figure" target="#fig_2">2</ref>, 3Ã—1 kernel is insensitive to horizontal flipping. Hence, asymmetric convolutions are viable schemes to boost the rotation robustness of the neural network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type="bibr" target="#b16">[17]</ref> and design an asymmetric convolution block (ACB) to capture features from different receptive fields, which can be seen from Fig. <ref type="figure" target="#fig_0">3</ref>. There are three branches in ACB, i.e. 3Ã—3 convolution, a 1Ã—3 convolution, and a 3Ã—1 convolution, to get different information.</p><p>The 3Ã—3 convolution captures features by a relatively large receptive field, while the 1Ã—3 and 3Ã—1 convolutions curb the rotation sensibility of vertical and horizontal flipping separately. Meanwhile, ACB expand the width of the network. The feature maps generated by three branches are added up to obtain fusion result. Finally, batch norm (BN) and ReLU are used to boost the numerical stability and activate the result in a nonlinear manner. The formulation of ACB can be described as:</p><formula xml:id="formula_0">ğ’™ğ’™ ï¿½ ğ‘–ğ‘– = ğ¹ğ¹ 3Ã—3 (ğ’™ğ’™ ğ‘–ğ‘–âˆ’1 ) + ğ¹ğ¹ 1Ã—3 (ğ’™ğ’™ ğ‘–ğ‘–âˆ’1 ) + ğ¹ğ¹ 3Ã—1 (ğ’™ğ’™ ğ‘–ğ‘–âˆ’1 )<label>(1)</label></formula><formula xml:id="formula_1">ğ’™ğ’™ ğ‘–ğ‘– = Ïƒ ï¿½ğ›¾ğ›¾ ğ‘–ğ‘– ğ’™ğ’™ ï¿½ ğ‘–ğ‘– âˆ’ ğ¸ğ¸(ğ’™ğ’™ ï¿½ ğ‘–ğ‘– ) ï¿½ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ’™ğ’™ ï¿½ ğ‘–ğ‘– ) + ğœ–ğœ– ğ‘–ğ‘– + ğ›½ğ›½ ğ‘–ğ‘– ï¿½<label>(2)</label></formula><p>where ğ’™ğ’™ ğ‘–ğ‘– is the output of the ACB, and ğ’™ğ’™ ğ‘–ğ‘–âˆ’1 is the input of the ACB. ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(â€¢) and ğ¸ğ¸(â€¢) represent the variance function and   ACB is used to capture and refine the features of objects in each layer of the encoder, and is attached after each transposed convolution of decoder to avoid the checkerboard pattern and generate smooth image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Scale Skip Connections</head><p>As information from multi-scales are not fully exploit both by plain connections of U-Net and nested and dense connections of U-Net++, we design the multi-scale skip connections to capture the interplay between the encoder and decode and extract both fine-grained detailed information and coarse-grained semantic information.</p><p>Taking ğ‘‹ğ‘‹ ğ·ğ·ğ·ğ· 3 as an example, Fig. <ref type="figure" target="#fig_5">4</ref> demonstrates how to generated the feature maps. Firstly, the feature maps of the same-level encoder layer, i.e. ğ‘‹ğ‘‹ ğ¸ğ¸ğ¸ğ¸ 3 , are directly connected. Secondly, the fine-grained detailed information contained in lower-level encoder layers, i.e. ğ‘‹ğ‘‹ ğ·ğ·ğ·ğ· 4 and ğ‘‹ğ‘‹ ğ·ğ·ğ·ğ· 5 , are delivered by transposed convolution and asymmetric convolution block. Thirdly, the coarse-grained semantic information contained in higher-level encoder layers, i.e. ğ‘‹ğ‘‹ ğ¸ğ¸ğ¸ğ¸ 1 and ğ‘‹ğ‘‹ ğ¸ğ¸ğ¸ğ¸ 2 , are transmitted by max-pooling and asymmetric convolution block. The above procedure can be formulated as: </p><formula xml:id="formula_2">ğ‘‹ğ‘‹ ğ·ğ·ğ·ğ· ğ‘–ğ‘– = â© âª â¨ âª â§ ğ‘‹ğ‘‹ ğ¸ğ¸ğ¸ğ¸ ğ‘–ğ‘– , ğ‘–ğ‘– = ğ‘ğ‘ ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ ï¿½ï¿½ğ¶ğ¶(ğ·ğ·(ğ‘‹ğ‘‹ ğ¸ğ¸ğ¸ğ¸ ğ‘˜ğ‘˜ ) ğ‘˜ğ‘˜=1 ğ‘–ğ‘–âˆ’1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel Attention Block</head><p>With five feature maps with identical size and resolution in hand, we need to further decrease the prodigious number of channels, as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b22">[23]</ref>, we design the channel attention block (CAB) to reweighting the channel-wise features, which can be seen from the right of Fig. <ref type="figure" target="#fig_5">4</ref>. The aim of CAB is to learn a 1-D weight ğ‘Šğ‘Š ğ‘†ğ‘† âˆˆ ğ‘…ğ‘… ğ¶ğ¶Ã—1Ã—1 which weights the channels of input feature map ğ¹ğ¹ âˆˆ ğ‘…ğ‘… ğ¶ğ¶Ã—ğ»ğ»Ã—ğ‘Šğ‘Š , where ğ¶ğ¶, ğ‘Šğ‘Š, and ğ»ğ» indicate the number of channels, the height, and the width of the feature map. By multiplying ğ‘Šğ‘Š ğ‘†ğ‘† and ğ¹ğ¹, CAB enhances the discriminative channels and restrains the indiscriminative channels.</p><p>First of all, we use a 1Ã—1 convolution with 128 filters to reduce the number of channels. Then, the spatial dimension is squeezed by the operation of an average-pooling and a maxpooling simultaneously. By two convolution layers with 8 filters and ReLU activation functions, the channels of squeezed feature maps are compressed to one-sixteenth of its original number, and then channels are reinstated using two convolution layers with 128 filters. Finally, the sum of two layers is activated by sigmoid and then multiply by the output of the first convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>This section first introduces the datasets and experimental settings to verify the effectiveness of MACU-Net, and then compares the performance between different frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and Gaofen Image Dataset (GID) <ref type="bibr">[30]</ref>. WHDLD contains 4940 RGB images in the size of 256 Ã— 256 captured by Gaofen 1 Satellite and ZY-3 Satellite over Wuhan urban area. By image fusion and resampling, the images resolution is reach to 2m/pixel. The images contained in WHDLD are labeled with six classes, i.e. bare soil, building, pavement, vegetation, road, and water.</p><p>GID contains 150 RGB images in the size of 7200 Ã— 6800 captured by Gaofen 2 Satellite over 60 cities in China. Each image covering a geographic region of 506 ğ‘˜ğ‘˜ğ‘˜ğ‘˜ 2 . The images contained in GID are labeled with six classes, i.e. build-up, forest, farmland, meadow, water, and others. We just select 15 images contained in GID. The principle of selection is to cover whole six classes. And the serial number of the selected images will be released with our open source code 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type="bibr" target="#b10">[11]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, DeepLab V3 <ref type="bibr" target="#b12">[13]</ref>, DeepLab V3+ <ref type="bibr" target="#b13">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type="bibr" target="#b19">[20]</ref>, Attention U-Net <ref type="bibr" target="#b20">[21]</ref>, FGC <ref type="bibr" target="#b21">[22]</ref>, MSFCN, and U-Net++ <ref type="bibr" target="#b14">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaining methods are all improved version of U-Net.</p><p>All of the models are implemented with PyTorch, and the optimizer is set as Adam with 0.0001 learning rate and 16 batch size. All the experiments are implemented on a single NVIDIA GeForce RTX 2080ti GPU with 11 GB RAM. The crossentropy loss function is used as quantitative evaluation and backpropagation index to measure the disparity between the obtained 2D segmentation maps and ground truth.</p><p>For WHDLD, we randomly select 60% images as training set, 20% images as validation set, and the rest 20% images as test set. For GID, we separately partition each image into nonoverlap patch sets with the size of 256 Ã— 256, and just discard the pixels on the edges which cannot be divisible by 256. Thus, 10920 patches are obtained. Then we randomly select 60% patches as training set, 20% patches as validation set, and the rest 20% patches as test set.</p><p>For each dataset, the overall accuracy (OA), average accuracy (AA), Kappa coefficient (K), mean Intersection over Union (mIoU), Frequency Weighted Intersection over Union (FWIoU), and F1-score (F1) are adopted as evaluation indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on WHDLD and GID</head><p>The experimental results of different methods on WHDLD and GID are demonstrated in Table <ref type="table">â… </ref> and Table <ref type="table">â…¡</ref>. The performance of proposed MAC-UNet transcends other algorithms in all quantitative evaluation indexes, which can be seen from tables. For WHDLD, the proposed MACU-Net brings near 0.8% improvements on mIoU compared with U-Net++. And for GID dataset, the improvements are more than 1.4% in mIoU and nearly 1% in F1-score, respectively. Some visual results generated by our method and U-Net are provided in Fig. <ref type="figure" target="#fig_7">5</ref> and Fig. <ref type="figure" target="#fig_6">6</ref>, which manifest that the proposed MACU-Net can capturn refined features.</p><p>What is more, the number of parameters and the consumptions of calculation are also significant to assess the merit of a framework. The comparison of parameters and 1 https://github.com/lironui/U-Net-with-Multi-Scale-Skip-Connections-and-Asymmetric-Convolution-Blocks. computational complexity between different algorithms are reported in Table <ref type="table" target="#tab_0">â…¢</ref>, where 'M' is the abbreviation of million, the unit of parameter number, and 'G' is the abbreviation of Gillion (thousand million), the unit of floating point operations. And the comparison demonstrates that the design of MACU-Net is efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this letter, to implement semantic segmentation of highresolution remote sensing images, we combine multi-scale features generated by different layers of U-Net and design a multi-scale skip connected architecture, MACU-Net. Using multi-scale skip connections and channel attention blocks, semantic features generated by different layers of U-Net are combined and refined. Meanwhile, the representational capacity of the standard convolution layer is enhanced by the asymmetric convolution block. Experiments conducted on two large-scale datasets manifest the performance of our MACU-Net transcends nine baseline algorithms.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 )</head><label>3</label><figDesc>Fig.1 gives graphical overviews of U-Net, U-Net++ and the proposed MACU-Net. Compared with U-Net and U-Net++, the multi-scale features of MACU-Net are combined by redesigning skip connections.</figDesc><graphic url="image-3.png" coords="2,328.95,388.10,221.25,153.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of (a) U-Net, (b) U-Net++ï¼Œ and proposed (c) MACU-Net 3+. The depth of each node is presented below the circle.</figDesc><graphic url="image-1.png" coords="2,46.10,53.05,509.75,200.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison between 3Ã—3 kernel (red) and 3Ã—1 kernel (yellow). The 3Ã—1 kernel obtains the same results on horizontal flipped input, but 3Ã—3 kernel attains different result.</figDesc><graphic url="image-2.png" coords="2,314.00,274.40,246.77,77.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The structure of asymmetric convolution block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ( 3 )</head><label>13</label><figDesc>ğ‘–ğ‘–+1) ğ‘¡ğ‘¡â„ ~ğ‘ğ‘ğ‘¡ğ‘¡â„ ï¿½ï¿½ , ğ‘–ğ‘– = 1, â€¦ , ğ‘ğ‘ âˆ’ where ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ indicates channel attention block which realigns channel-wise features, and ğ¶ğ¶(â€¢) denotes asymmetric convolution block. ğ·ğ·(â€¢) and ğ‘ˆğ‘ˆ(â€¢) represent down-sampling using max-pooling layer and up-sampling using transposed convolution respectively, and [â€¢] represents the operation of concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of how to construct the multi-scale aggregated feature map of ğ‘‹ğ‘‹ ğ·ğ·ğ·ğ· 3 .</figDesc><graphic url="image-4.png" coords="3,47.05,49.25,514.90,241.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of results on GID.</figDesc><graphic url="image-5.png" coords="5,46.75,279.30,252.00,204.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of results on WHDLD.</figDesc><graphic url="image-6.png" coords="5,46.80,50.60,252.00,205.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE â…¢</head><label>â…¢</label><figDesc>THE COMPARISON OF PARAMETERS AND COMPUTATIONAL COMPLEXITY. .787 71.403 52.940 68.876 66.529 DeepLab V3 79.611 67.515 71.001 54.992 68.409 68.804 U-Net 82.864 68.730 75.736 57.410 73.449 70.278 Tiramisu 82.188 70.712 74.903 58.167 72.243 71.276 U-NetAtt 82.602 69.738 75.484 56.918 73.474 69.622 FGC 82.975 68.855 75.927 57.368 73.540 70.274 MSFCN 84.168 72.081 77.558 60.366 74.892 73.031 DeepLab V3+ 83.568 73.053 76.740 61.194 73.894 74.047</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TABLE â… </cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">THE EXPERIMENTAL RESULTS ON WHDLD DATASET.</cell><cell></cell></row><row><cell>Method</cell><cell>OA</cell><cell>AA</cell><cell>K</cell><cell>mIoU</cell><cell>FWIoU</cell><cell>F1</cell></row><row><cell cols="7">SegNet 80.229 63U-Net++ 84.067 74.004 77.430 61.799 74.496 74.633</cell></row><row><cell>MACU-Net</cell><cell cols="6">84.623 74.568 78.233 62.600 75.231 75.245</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE â…¡</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">THE EXPERIMENTAL RESULTS ON GID DATASET.</cell><cell></cell></row><row><cell>Method</cell><cell>OA</cell><cell>AA</cell><cell>K</cell><cell>mIoU</cell><cell>FWIoU</cell><cell>F1</cell></row><row><cell>SegNet</cell><cell cols="6">80.035 82.396 74.612 70.962 67.420 82.290</cell></row><row><cell>DeepLab V3</cell><cell cols="6">80.072 83.182 74.741 71.348 67.632 82.672</cell></row><row><cell>U-Net</cell><cell cols="6">79.754 81.677 74.219 70.535 67.129 82.113</cell></row><row><cell>Tiramisu</cell><cell cols="6">79.467 84.808 74.377 69.032 65.627 80.716</cell></row><row><cell>U-NetAtt</cell><cell cols="6">80.919 83.838 75.878 70.930 68.539 82.511</cell></row><row><cell>FGC</cell><cell cols="6">81.180 84.716 76.270 72.067 68.859 83.240</cell></row><row><cell>MSFCN</cell><cell cols="6">82.520 85.042 77.897 73.637 71.047 85.378</cell></row><row><cell cols="7">DeepLab V3+ 82.667 85.065 78.012 74.109 71.409 84.678</cell></row><row><cell>U-Net++</cell><cell cols="6">83.221 84.942 78.723 74.105 71.973 84.725</cell></row><row><cell>MACU-Net</cell><cell cols="6">84.062 86.136 79.819 75.587 73.290 85.700</cell></row><row><cell>Method</cell><cell cols="5">input shape Parameters (M) Complexity (G)</cell><cell></cell></row><row><cell>SegNet</cell><cell></cell><cell></cell><cell>29.45</cell><cell></cell><cell>40.29</cell><cell></cell></row><row><cell>DeepLab V3</cell><cell></cell><cell></cell><cell>58.16</cell><cell></cell><cell>18.63</cell><cell></cell></row><row><cell>U-Net</cell><cell></cell><cell></cell><cell>10.86</cell><cell></cell><cell>13.94</cell><cell></cell></row><row><cell>Tiramisu</cell><cell></cell><cell></cell><cell>1.38</cell><cell></cell><cell>11.92</cell><cell></cell></row><row><cell>U-NetAtt FGC</cell><cell cols="2">3Ã—256Ã—256</cell><cell>2.17 2.19</cell><cell></cell><cell>12.75 8.4</cell><cell></cell></row><row><cell>MSFCN</cell><cell></cell><cell></cell><cell>2.67</cell><cell></cell><cell>9.66</cell><cell></cell></row><row><cell cols="2">DeepLab V3+</cell><cell></cell><cell>59.46</cell><cell></cell><cell>23.98</cell><cell></cell></row><row><cell>UNet++</cell><cell></cell><cell></cell><cell>9.05</cell><cell></cell><cell>29.94</cell><cell></cell></row><row><cell>MACU-Net</cell><cell></cell><cell></cell><cell>5.28</cell><cell></cell><cell>7.43</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundations of China (No. 41671452).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation of aerial images with an ensemble of CNSS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="473" to="480" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved maize cultivated area estimation over a large scale combining MODIS-EVI time series data and crop phenological information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="102" to="113" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint Deep Learning for land cover and land use classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote sensing of environment</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical mapping of annual global land cover 2001 to present: The MODIS Collection 6 Land Cover product</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sulla-Menashe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Abercrombie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Remote sensing of environment</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="183" to="194" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel texture-preceded segmentation algorithm for high-resolution imagery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2818" to="2828" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An SVM ensemble approach combining spectral, structural, and semantic features for the classification of high-resolution remotely sensed imagery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="257" to="272" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic classification of urban buildings combining VHR image and GIS data: An improved random forest approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1911" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on real image denoising: Dataset, methods and results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="496" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial Enhanced Rotation Aware Network for Breast Mass Segmentation in Digital Mammogram</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning discriminative spatiotemporal features for precise crop classification from multi-temporal satellite images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3162" to="3174" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance evaluation of singlelabel and multi-label remote sensing image retrieval using a dense labeling dataset</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">964</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilabel Remote Sensing Image Retrieval Based on Fully Convolutional Network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="318" to="328" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
