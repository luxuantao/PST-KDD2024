<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"> A novel framework enables NN analysis in medical applications involving small datasets  An accurate model for trabecular bone strength estimation in severe osteoarthritis is developed  Model enables non-invasive patient-specific prediction of hip fracture risk  Method of multiple runs mitigates sporadic fluctuations in NN performance due to small data  Surrogate data test is used to account for random effects due to small test data Handling limited datasets with neural networks in medical applications: a small-data approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Torgyn</forename><surname>Shaikhina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Natalia</forename><forename type="middle">A</forename><surname>Khovanova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Warwick Coventry</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main"> A novel framework enables NN analysis in medical applications involving small datasets  An accurate model for trabecular bone strength estimation in severe osteoarthritis is developed  Model enables non-invasive patient-specific prediction of hip fracture risk  Method of multiple runs mitigates sporadic fluctuations in NN performance due to small data  Surrogate data test is used to account for random effects due to small test data Handling limited datasets with neural networks in medical applications: a small-data approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E18BC1CF6ED405CCAADEADF7EB31F3B</idno>
					<idno type="DOI">10.1016/j.artmed.2016.12.003</idno>
					<note type="submission">Received date: 12-5-2016 Revised date: 21-11-2016 Accepted date: 28-12-2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>author id=&quot;aut0005&quot; author-id=&quot;S0933365716301749-f78a11c67c6ef73794c3dfab5028c6de&quot;&gt; Torgyn Shaikhina&lt;ce:author id=&quot;aut0010&quot; author-id=&quot;S0933365716301749-52fb79698154ee09e0dfc44d97fb4771&quot;&gt; Natalia A. Khovanova Predictive modelling</term>
					<term>Small data</term>
					<term>Regression neural networks</term>
					<term>Osteoarthritis</term>
					<term>Compressive strength</term>
					<term>Trabecular bone</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation:</head><p>Single-centre studies in medical domain are often characterised by limited samples due to the complexity and high costs of patient data collection. Machine learning methods for regression modelling of small datasets (less than 10 observations per predictor variable) remain scarce. Our work bridges this gap by developing a novel framework for application of artificial neural networks (NNs) for regression tasks involving small medical datasets.</p><p>Methods: In order to address the sporadic fluctuations and validation issues that appear in regression NNs trained on small datasets, the method of multiple runs and surrogate data analysis were proposed in this work. The approach was compared to the state-of-the-art ensemble NNs; the effect of dataset size on NN performance was also investigated.</p><p>Results: The proposed framework was applied for the prediction of compressive strength (CS) of femoral trabecular bone in patients suffering from severe osteoarthritis. The NN model was able to estimate the CS of osteoarthritic trabecular bone from its structural and biological properties with a standard error of 0.85 MPa. When evaluated on independent test samples, the NN achieved accuracy of 98.3%, outperforming an ensemble NN model by 11%. We reproduce this result on CS data of another porous solid (concrete) and demonstrate that the proposed framework allows for an NN modelled with as few as 56 samples to generalise on 300 independent test samples with 86.5% accuracy, which is comparable to the performance of an NN developed with 18 times larger dataset (1030 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>The significance of this work is two-fold: the practical application allows for non-destructive prediction of bone fracture risk, while the novel methodology extends beyond the task considered in this study and provides a general framework for application of regression NNs to medical problems characterised by limited dataset sizes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>IN recent decades, a surge of interest in Machine learning within the medical research community has resulted in an array of successful data-driven applications ranging from medical image processing and the diagnosis of specific diseases, to the broader tasks of decision support and outcome prediction <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. The focus of this work is on predictive modelling for applications characterised by small datasets and realnumbered continuous outputs. Such tasks are normally approached by using conventional multiple linear regression models. These are based on the assumptions of statistical independence of the input variables, linearity between dependent and independent variables, normality of the residuals, and the absence of endogenous variables <ref type="bibr" target="#b3">[4]</ref>. However, in many applications, particularly those involving complex physiological parameters, those assumptions are often violated <ref type="bibr" target="#b4">[5]</ref>. This necessitates more sophisticated regression models based, for instance, on Machine learning. One such approachpredictive modelling using feedforward backpropagation artificial neural networks (NNs)is considered in this work. NN is a distributed parallel processor which resembles a biological brain in the sense that it learns by responding to the environment and stores the acquired knowledge in interneuron synapses <ref type="bibr" target="#b5">[6]</ref>. One striking aspect of NNs is that they are universal approximators. It has been proven that a standard multilayer feedforward NN is capable of approximating any measurable function and that there are no theoretical constraints for the success of these networks <ref type="bibr" target="#b6">[7]</ref>. Even when conventional multiple regression models fail to quantify a nonlinear relationship between causal factors and biological responses, NNs retain their capacity to find associations within high-dimensional, nonlinear and multimodal medical data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Despite their superior performance, accuracy and versatility, NNs are generally viewed in the context of the necessity for abundant training data. This, however, is rarely feasible in medical research, where the size of datasets is constrained by the complexity and high cost of large-scale experiments. Applications of NNs for regression analysis and outcome prediction based on small datasets remain scarce and thus require further exploration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For the purposes of this study, we define small data as a dataset with less than ten observations (samples) per predictor variable.</p><p>NNs trained with small datasets often exhibit unstable behaviour in performance, i.e. sporadic fluctuations due to the sensitivity of NNs to initial parameter values and training order <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. NN initialisation and backpropagation training algorithms commonly contain deliberate degrees of randomness in order to improve convergence to the global minimum of the associated cost function <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. In addition, the order with which the training data is fed to the NN can affect the level of convergence and produce erratic outcomes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Such inter-NN volatility limits both the reproducibility of the results and the objective comparison between different NN designs for future optimisation and validation. Previous attempts <ref type="bibr" target="#b14">[15]</ref> to resolve the stability problems in NNs demonstrated the success of k-fold cross-validation and ensemble methods for a medical classification problem; the dataset comprised 53 features and 1355 observations, which corresponds to 25 observations per predictor variable. To the best of our knowledge, effective strategies for regression tasks on small biomedical datasets have not been considered, thus necessitating the establishment of a framework for application of NNs to medical data analysis.</p><p>One important biomedical application of NNs in hard tissue engineering was considered in our previous work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, where a NN was applied for correlation analysis of 35 trabecular bone samples from male and female specimens of various ages suffering from severe osteoarthritis (OA) <ref type="bibr" target="#b16">[17]</ref>. OA is common degenerative joint disease associated with damaged cartilage <ref type="bibr" target="#b17">[18]</ref>. Unlike in osteoporosis, where decreasing bone mineral density (BMD) decreases bone compressive strength (CS) and increases bone fracture risk, the BMD in OA was seen to increase <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. There is further indication that higher BMD does not protect against bone fracture risk in OA <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. The mathematical relationship between BMD and CS observed in healthy patients does not hold for patients with OA, necessitating development of a CS model for OA.</p><p>In the current work, we consider the application of NNs to osteoarthritic hip fracture prediction for noninvasive estimation of bone CS from structural and physiological parameters. For this particular application there are two commonly used computational techniques: quantitative computed tomography-based finite element analysis <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and the indirect estimation of local properties of bone tissue through densitometry <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Yet, subject-specific models for hip fracture prediction from structural parameters of trabecular bone in patients affected by degenerative bone diseases have not been developed. An accurate patient data driven model for CS estimation based on NNs could offer a hip fracture risk stratification tool and provide valuable clinical insights for the diagnosis, prevention and potential treatment of OA <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The aim of this research is to develop subject-specific models for hip fracture prediction in OA and a general framework for the application of regression NNs to small datasets. In this work we introduce the method of multiple runs to address the inter-NN volatility problem caused by small data conditions. By generating a large set (1000+) of NNs, this method allows for consistent comparison between different NN designs. We also propose surrogate data test in order to account for the random effects due to small datasets. The use of surrogate data was inspired by their successful application in nonlinear physics, neural coding, and time series analysis <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>.</p><p>The utility of the proposed framework was explored by considering a larger dataset. Due to the unavailability of a large number of bone samples, a different CS dataset, that of 1030 samples of concrete, was used <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. We designed and trained regression NNs for several smaller subsets of the data and demonstrated that smalldataset (56 samples) NNs developed using our framework can achieve a performance comparable to that of the NNs developed on the entire dataset (1030 samples).</p><p>The structure of this article is as follows. Section 2 describes the data used for analysis, NN model design, and introduces the new framework. In section 3, the role of data size on NN performance and generalisation ability is explored to demonstrate the utility of the proposed framework. In section 4 we apply our framework for prediction of osteoarthritic trabecular bone CS and demonstrate the superiority of the approach over established ensemble NN methods in the context of small data. Section 5 discusses both the methodological significance of the proposed framework and the medical application of the NN model for prediction of hip fracture risk. Additional information on NN outcomes and datasets is provided in the Appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Porous solids: data</head><p>Compressive strength of trabecular bone. Included in this study are 35 patients who suffered from severe OA and underwent total hip arthroplasty (Table <ref type="table" target="#tab_0">1</ref>, Appendix A1). The original dataset <ref type="bibr" target="#b16">[17]</ref> obtained from trabecular tissue samples taken from the femoral head of the patients contained five predictor features (a 5-D input vector for the NN): patients" age and gender, tissue porosity (BV/TV), structure model index (SMI), trabecular thickness factor (tb.th), and one output variable, the CS (in MPa). The dataset was divided at random into training (60%), validation (20%) and testing (20%) subsets, i.e. 22, 6 and 7 samples, respectively.</p><p>Compressive strength of concrete. The dataset <ref type="bibr" target="#b30">[31]</ref> of 1030 samples was obtained from a publically available repository <ref type="bibr" target="#b31">[32]</ref> and contained the following variables: compressive strength (CS) of concrete samples (in MPa), the amounts of 7 components in the concrete mixture (in kg/m 3 ): cement, blast furnace slag, fly ash, water, superplasticizer, coarse and fine aggregates, and the duration of concrete aging (in days). The CS of concrete is a highly nonlinear function of its components and the duration of aging, yet an appropriately trained NN can effectively capture this complex relationship between the CS and the other 8 variables. A successful application of NNs to CS prediction based on 700 concrete samples has been demonstrated in an original study by Yeh <ref type="bibr" target="#b30">[31]</ref>. For the purposes of our NN modelling, the samples were divided at random into training (60%), validation (10%) and testing (30%). Thus, out of 1030 available samples, 630 were used for NN training, 100 for validation and 300 were reserved for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NN design for CS prediction in porous solids</head><p>Considering the size and nature of the available data, a feedforward backpropagation NN with one hidden layer, input features and one output was chosen as the base for the CS model (Fig. <ref type="figure" target="#fig_0">1</ref>). The neurons in the hidden layer is characterised by a hyperbolic tangent sigmoid transfer function <ref type="bibr" target="#b32">[33]</ref>, while the output neuron relates the CS output to the input by using a simple linear transfer function (Fig. <ref type="figure" target="#fig_0">1</ref>). The -by-input weights matrix , -by-1 layer weights column vector ̅̅̅̅̅ , and the corresponding biases ̅̅̅̅̅ for each layer were initialised according to the Nguyen-Widrow method <ref type="bibr" target="#b33">[34]</ref> in order to distribute the active region of each neuron in the layer evenly across the layer's input space.</p><p>The NNs were trained using the Leverberg-Marquardt backpropagation algorithm <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. The cost function was defined by the mean squared error (MSE) between the output and actual CS values. Early stopping on an independent validation cohort was implemented in order to avoid NN overtraining and increase generalisation <ref type="bibr" target="#b37">[38]</ref>. The validation subset was sampled at random from the model dataset for each NN, ensuring a diversity among the samples. The resulting NN model mapped the output (in MPa) to the input vector ̅ is:</p><formula xml:id="formula_0">[ ̅ ̅̅̅̅̅ ] ̅̅̅̅̅ (1)</formula><p>The final values of the weights and bias parameters in (1) for the trained bone data NN are provided in Table <ref type="table" target="#tab_2">3</ref> in Appendix A3.</p><p>Note, parameter estimation for the optimal network structure, size, training duration, training function, neural transfer function and cost function was conducted at the preliminary stage following an established textbook practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. Assessment and comparison of various NN designs were carried out using the multiple runs technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Method of multiple runs</head><p>In order to address the small dataset problem we introduce the method of multiple runs in which a large number of NNs of the same design are trained simultaneously. In other words, the performance of a given NN design is assessed not on a single NN instance, but repeatedly on a set (multiple run) of a few thousands NNs. Identical in terms of their topology and neuron functions, NNs within each such run differ due to the 3 sources of randomness deliberately embedded in the initialisation and training routines: (a) the initial values of the layer weights and biases, (b) the split between the training and validation datasets (test samples were fixed), and (c) the order with which the training and validation samples are fed into the NN. In every run, several thousand NNs with various initial conditions are generated and trained in parallel, producing a range of successful and unsuccessful NNs evaluated according to criteria set in section 2.7. Subsequently, their performance indicators are reported as collective statistics across the whole run, thus allowing consistent comparisons of performance among runs despite the limited size of the dataset. This helps to quantify the varying effects of design parameters, such as the NN"s size and the training duration during the iterative parameter estimation process. Finally, the highest performing instance of the optimal NN design is selected as the working model. This strategy principally differs from NN ensemble methods (as discussed below in section 2.6) in the sense that only the output of a single best performing NN is ultimately selected as the working (optimal) model.</p><p>In summary, the following terminology applies throughout the paper:</p><p> design parameters are NN size, neuron functions, training functions, etc.  individual NN parameters are weights and biases  optimal NN design is based on estimation of appropriate NN size, topology, training functions, etc.  working (optimal) model is the highest performing instance selected from a run of the optimal NN design. The choice of the number of NNs per run is influenced by the balance between the required precision of the statistical measures and computational efficiency, as larger runs require more memory and time to simulate. It was found that for the bone CS application considered in this study, 2000 NNs maintained most performance statistics, such as mean regression between NN targets and predictions, consistent to 3 decimal places, which was deemed sufficient. For inter-run consistency each 2000 NN run was repeated 10 times, yielding 20000 NNs in total. The average simulation time for instantiating and training a run of 2000 NNs on a modern PC (Intel® Core™ i7-3770 CPU @3.40GHz, 32 GB RAM) was 280 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Surrogate data test</head><p>Where a sufficient number of samples is available, the efficiency of learning by NN of the interrelationships in the data is expected to correlate with its test performance. With small datasets, however, the efficiency of learning is decreased and even poorlydesigned NNs can achieve a good performance on test samples at random. In order to avoid such situation and to evaluate NN performance in the presence of random effects, a surrogate data test is proposed in this study. Surrogate data mimics the statistical properties of the original dataset independently for each component of the input vector. While resembling the statistical properties of the original data, the surrogates do not retain the intricate interrelationships between the various components of the real dataset. Hence, the NN trained and tested on surrogates is expected to perform poorly.</p><p>Numerous surrogate data NNs are generated using method of multiple runs described in section 2.3. The highest performing surrogate NN instance defines as the lowest performance threshold for real data models. To pass the surrogate data test, real data NNs must outperform this threshold.</p><p>The surrogate samples can be generated using a variety of methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. In this study two approaches were used. For trabecular bone data, all continuous input variables were normally distributed according to the Kolmogorov-Smirnov statistical test <ref type="bibr" target="#b3">[4]</ref>. Thus surrogates were generated from random numbers to match the truncated normal distributions, e.g. mean and standard deviation estimated from the original data, as well as the range and size of the original tissue samples (Table <ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_0">Appendix A1</ref>). For the concrete data, where vector distributions were not normal, random permutations <ref type="bibr" target="#b3">[4]</ref> of the original vectors were applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Summary of the proposed framework</head><p>Combined, the method of multiple runs and surrogate data test comprise a framework for application of regression NNs to small datasets, as summarised in Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multiple runs enable (i) consistent comparison of various NN designs during design parameter estimation, (ii) comparison between surrogate data and real data</head><p>NNs during surrogate data test, and (iii) selection of the working model among the models of optimal design. Fig. <ref type="figure" target="#fig_10">2</ref>. Proposed framework for application of regression neural networks to small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Assessing NN generalisation</head><p>In the context of ML, generalising performance is a measure of how well a model predicts an outcome based on independent test data with which the NN was not previously presented. In recent decades considerable efforts in ML have been dedicated to improving the generalisation of NNs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. A data-driven predictive model has little practical value if it is not able to form accurate predictions on new data. Yet in small datasets, where such test data are scarce, the simple task of assessing generalisation becomes impractical. Indeed, reserving 20% of the bone data for independent testing leaves us with only 7 samples. The question of whether the NN model would generalise on a larger set of new samples cannot be illustrated with such limited test data. This poses a major obstacle for small medical datasets in general, thus the effect of dataset size on NN performance must be considered. We investigate the effect of the model dataset size on the generalisation ability of the NN models developed with our framework on a large dataset of concrete CS samples described in section 2.1. The findings are presented in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Performance criteria</head><p>In order to assess the performance of an individual NN, including the best performing, the linear regression coefficients R between the actual output (target) and predicted output were calculated.</p><p>In particular, regression coefficients were calculated for the entire dataset ( , and separately for training ( , validation ( , and testing ( . can take values between 0 and 1, where 1 corresponds to the highest model predictive performance (100% accuracy) with equal target and prediction values.</p><p>greater than 0.6 defines statistically significant performance, i.e. and <ref type="bibr" target="#b10">[11]</ref>. The root mean squared error ( across the entire dataset was also assessed.</p><p>presents the same information regarding model accuracy as the regression coefficient , but in terms of the absolute difference between NN predictions and targets. RMSE helps to visualise the predictive error since it is expressed in the units of the output variable, i.e. in MPa for CS considered in this work.</p><p>The collective performance of the NNs within a multiple run was evaluated based on the following statistical characteristics:  mean µ and standard deviation σ of and averaged across all NNs in the run,  the number of NNs that are statistically significant,  the random effect threshold set by the highest performing surrogate NN, in terms of and . In order to select the best performing NN in a run, we considered both and . Commonly the validation subset is used for model selection <ref type="bibr" target="#b8">[9]</ref>, however under small-data conditions, is unreliable. On the other hand, although does not indicate the NN performance on new samples, it gives a useful estimation of the highest expected NN performance. It is expected that is higher than for a trained NN. Subsequently, when selecting the best performing NN, we disregard models with &gt; and from the remaining models we choose the one with the highest . Note that should not be involved in the model selection as it reflects the generalising performance of NN models on new data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Alternative model: NN ensemble methods</head><p>Ensemble methods refer to powerful ML models based on combining predictions of a series of individual ML models, such as NNs, trained independently <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>. The principle behind a good ensemble is that its constituent models are diverse and are able to generalise over different subsets of an input space, effectively offsetting mutual errors. The resulting ensemble is often more robust than any of its constituent models and has superior generalisation accuracy <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>. We compared the NN ensemble performance with that of a single NN model developed within the proposed multiple runs framework for both the concrete and bone applications.</p><p>In an ensemble, the constituent predictor models can be diversified by manipulating the training subset, or by randomising their initial parameters <ref type="bibr" target="#b44">[44]</ref>. The former comprises boosting and bagging techniques, which were disregarded as being impractical for the small datasets, as they reduced already scarce training samples. We utilised the latter ensembling strategy, where each constituent NN was initialised with random parameters and trained with the complete training set, similar to the multiple runs strategy described in section 2.3. Optiz &amp; Maclin showed that this ensemble approach was "surprisingly effective, often producing results as good as Bagging" <ref type="bibr" target="#b43">[43]</ref>. The individual predictions of the constituent NNs were combined using a common linear approach of simple averaging <ref type="bibr" target="#b45">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Statistical analysis</head><p>A non-parametric Wilcoxon rank sum test, also known as the Mann-Whitney U test, for medians was utilised for comparing the performances of any two NN runs <ref type="bibr" target="#b46">[46]</ref>. The null-hypothesis of no difference between the groups was tested at the 5% significance level and this is presented by p-values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Investigations of the effect of data size on NN performance: concrete CS models</head><p>In this section, we utilise a large dataset on concrete CS, described in section 2.1, to investigate the role of dataset size on NN performance and generalising ability. It is demonstrated that for a larger number of samples the optimal NN coefficients can be derived without involving the proposed framework, yet the importance of the framework increases as the data size is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collective NN performance (per run)</head><p>First, a large-dataset NN model was developed on a complete dataset of 1030 samples, out of which 30% (300 samples) were reserved for tests. The NN was designed as in Fig. <ref type="figure" target="#fig_0">1</ref>, with =8 inputs and k=10 neurons in hidden layer. In a multiple run of 1000, all large-data NNs performed with statistically significant regression coefficients (R &gt; 0.6). As expected with large data, the collective performance was highly accurate, with μ( =0.95 and μ( =0.94 when averaged across the multiple run of 1000 NNs. (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">a</ref>)</p><p>Secondly, a NN was applied to a smaller subset of the original dataset (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">b</ref>). Out of 1030 concrete samples, 100 samples were sampled at random and without replacement <ref type="bibr" target="#b3">[4]</ref>. The proportions for training, validation and testing subsets, as well as the training and initialisation routines, were analogous to those used for the large concrete dataset NN with an exception to the following adjustments:</p><p>-2000 and not 1000 NNs were evaluated per run to ensure inter-run repeatability, -the number of neurons in the hidden layer was reduced from 10 to 5 and the number of maximum fails for early stopping was decreased from 10 to 6 to account for a dataset size reduction. Finally, an extreme case with even smaller subset of the data was considered (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">c</ref>). From the concrete CS dataset with 8 predictors, 56 samples were selected at random to yield the same ratio of the number of observations per predictor variable as in the bone CS dataset (35 samples and 5 predictors). The small-dataset NN based on 56 concrete samples was modelled on 41 samples and initially tested on 15 samples.  In comparison to the large-dataset NNs (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">a</ref>), the distributions of the regression coefficients along x-axis for smaller dataset NNs (Fig. <ref type="figure" target="#fig_3">3, b-c</ref>) were within much wider ranges. The standard deviations σ also increased substantially for NN modes based on smaller datasets compared with the initial large-dataset model (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">a</ref>). Distributions of the regression coefficients achieved by the 2000 NN instances within the same run (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">c</ref>) demonstrate higher intra-run variance when compared to the large-dataset NNs (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">a</ref>). Over half of the NNs did not converge and only 762 NNs produced statistically significant predictions.</p><p>The mean regression coefficients across the run decreased to μ( =0.719, and μ( =0.542 (Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">c</ref>). When considering only statistically significant NNs (R &gt; 0.6), the mean performance of all samples was μ( =0.839 and individually for tests μ( =0.736. Despite higher volatility, an undesirable distribution spread and lower mean performance, the maximal R values for the small-dataset NNs were comparable with those for the large-dataset NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Surrogate data test: interpretation for various dataset sizes</head><p>As expected, NNs trained on the real concrete data consistently outperformed surrogate NNs. Fig. <ref type="figure" target="#fig_4">4</ref> demonstrates how the difference in performance between the real and surrogate NNs increased with the dataset size.</p><p>For the large-dataset NN developed with 1030 samples (Fig. <ref type="figure" target="#fig_4">4,</ref><ref type="figure">a</ref>), the surrogate and real-data NN distributions did not overlap. In fact, the surrogate NNs in this instance achieved approximately zero mean performance, which signifies that random effects would not have an impact on NN learning with a dataset of this size.</p><p>The 100-sample and 56-sample surrogate NNs had a non-zero mean performance of μ( = 0.219 (Fig. <ref type="figure" target="#fig_4">4,</ref><ref type="figure">b</ref>) and μ( =0.187 (Fig. <ref type="figure" target="#fig_4">4,</ref><ref type="figure">c</ref>), respectively. They were also characterised by a higher standard deviation of and compared to large-dataset NNs (</p><p>The nonzero mean performance of NNs suggests that random effects cannot be disregarded with small datasets and require quantification offered by the proposed surrogate data test. For 56-sample datasets (Fig. <ref type="figure" target="#fig_4">4,</ref><ref type="figure">c</ref>), the surrogate NNs performed with an average regression of μ( =0.187, as opposed to μ( =0.715 for real-data NNs. None of the 2000 surrogate small-dataset NNs achieved a statistically significant performance (R≥0.6). The surrogate threshold for the 56sample NN was considered: the highest performing surrogate NN achieved =0.791. This was largely due to overtraining, as its corresponding performance on test samples was poor ( = 0.515).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Individual NN performance</head><p>This subsection compares performance of individual NNs: a large-dataset NN (1030 samples) and a smalldataset NN (56 samples) developed using the proposed framework. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>,a, all large-data NNs performed with high accuracy and small variance, thus one of them could be selected as a working model without the need for multiple runs. The performance of one of 1000 large-data NN from the run in Fig. <ref type="figure" target="#fig_2">3</ref>,a is demonstrated in Fig. <ref type="figure" target="#fig_5">5</ref>. This NN achieved ( =0.944 and generalised with ( =0.94 on 300 independent test samples (Fig. <ref type="figure" target="#fig_5">5,</ref><ref type="figure">d</ref>). This large-dataset model provides an indication of NN performance achieved with abundant training samples. For small datasets, we are now concerned with NNs that perform above the surrogate data threshold of =0.791 established in section 3.3. Among the 2000 small-dataset (56-sample) NNs, the bestperforming NN was selected using the performance criteria in section 2.7. This model achieved regression coefficients of ( =0.92 on the entire dataset, and separately: ( =0.96, ( =0.92 and ( =0.90 on 15-sample test (Fig. <ref type="figure" target="#fig_6">6, a-d</ref>). In comparison, the largedataset NN developed with 1030 samples performed only 2.12% higher. The values were well above the surrogate threshold, indicating that high performance of the small-data NN was not due to luck. This result was confirmed when the small-data NN was subjected to the generalisation assessment on new test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalising performance of the small-dataset NN</head><p>In order to assess generalisation, 300 new test samples were randomly selected from the available dataset of 1030-56=974 samples not previously seen by the NN. Modelled with only 41 samples, the NN was able to predict CS on 300 new test samples with =0.865 (Fig. <ref type="figure" target="#fig_6">6,</ref><ref type="figure">e</ref>); the corresponding RMSE was 9.5 MPa. This constitutes a 7.5% decrease in generalising performance compared to the specimen large-dataset NN tested with the same number of independent samples (Fig. <ref type="figure" target="#fig_5">5,</ref><ref type="figure">c</ref>).</p><p>In other words, using the proposed framework we were able to develop an 86.5% accurate NN model with an 18 times smaller dataset than the original one, which demonstrates superiority of the suggested methodology and its applicability to the problems characterised by restricted dataset sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison of the small-dataset NN with the ensemble model for the concrete CS data</head><p>Firstly, an NN ensemble was designed by combining the outputs of 1000 NNs trained with the complete dataset of concrete samples (analogous to the largedataset NNs described in section 3.1 and presented in Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">a</ref>). As anticipated, this NN ensemble was able to achieve a superior generalisation accuracy of = 0.96 when tested on 300 independent samples.</p><p>The second NN ensemble was designed by combining the 2000 56-sample NNs (analogous to the small-dataset NNs in section 3.1 and Fig. <ref type="figure" target="#fig_3">3,</ref><ref type="figure">c</ref>). This ensemble achieved = 0.81 on 15 independent test samples. In comparison, our small-dataset concrete NN model developed with the multiple runs technique achieved = 0.903 on the same test samples. Subsequently the generalising ability of this ensemble was assessed on 300 additional concrete samples. The ensemble was able to retain its generalising ability with the accuracy of = 0.81, proving its robustness, irrespective of the test sample size.</p><p>Despite such striking consistency, the accuracy of the ensemble model was decreased by over 8% when compared with the generalising performance of the single NN model, developed using method of multiple runs ( = 0.865, section 3.3). These results demonstrate that a NN ensemble can achieve a remarkable performance on predictive tasks with sufficient data, but is unable to perform as well as the multiple runs model on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results: bone CS model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NN design configuration</head><p>The NN design described in section 2.2 for bone CS data comprised 5 input parameters. The heterogeneous 1x input vector, ̅ , was stacked in the following order: = morphology (SMI), = level of interconnectivity (tb.th), = porosity (BV/TV), = age and = gender. Following a standard parameter estimation routine, but with the help of multiple runs, the NN design was configured to 4 neurons in the hidden layer (Appendix A2). The number of permissible consecutive validation iterations during which the NN performance fails to improve and which directly influences duration of NN training, was set to 9 (Appendix A2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Surrogate data test</head><p>Performances of the NNs trained with real and surrogate data were compared by assessing 10 runs of 2000 NNs, i.e. a total of 20000 NNs. The real dataset NNs consistently outperformed the surrogate NNs with, on average, a 35% performance increase (Fig. <ref type="figure" target="#fig_7">7,</ref><ref type="figure">a</ref>).</p><p>Wilcoxon rank sum tests for median and across 20000 NNs revealed significant statistical difference (p = 0) between the groups, with median =0.38 for surrogates versus median =0.78 for the real dataset (Fig. <ref type="figure" target="#fig_7">7,</ref><ref type="figure">b</ref>). Similar differences in the distributions of and were observed for tests samples (Fig. <ref type="figure" target="#fig_7">7, c-d</ref>). The surrogate threshold was = 0.87 which indicated the lower performance threshold for the real dataset NN. Overall, the surrogate test signified that the accurate results yielded by the bone NN model are not due to random effects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimal bone CS model</head><p>Among the run of 2000 NNs of optimal design, the best-performing NN was capable of predicting trabecular tissue CS with RMSE = 0.85 MPa on the test samples. The linear regression coefficients between targets and predictions achieved by the NN were: individually for =0.999, =0.991, =0.983 and =0.993 (Fig. <ref type="figure" target="#fig_8">8, a-d</ref>). This indicates a very high accuracy of predictions despite the limited dataset of 35 samples. The final values of weights and biases of this fullytrained network are provided in Appendix A3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with ensemble NN</head><p>The NN ensemble achieved = 0.882, which is 11% lower than the accuracy of the proposed multiple run NN model ( = 0.983) and only marginally higher than the surrogate threshold = 0.87 established in section 4.2 for the bone dataset. This result further confirms that the NN ensembles, when tasked with small-dataset applications, were unable to realise their full predictive potential and were inferior to NNs designed within a multiple runs framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Significance of the proposed methodology</head><p>A framework for the application of regression NNs to medical datasets has been developed in order to mitigate the small dataset problem. NNs trained with small datasets exhibit sporadic fluctuations in the performance due to degrees of randomness inherent in the NN initialisation and training routines. This raises the problem of consistent comparisons between various NN models. Another problem is the evaluation of NN performance in the presence of random effects when the test data are scarce. The limitations of small datasets have been overcome in this work by using a novel framework comprising: (1) a multiple runs strategy for monitoring the performance measures collectively across a large set of NNs, and (2) surrogate data analysis for model validation. The proposed surrogate data approach provided a mechanism for NN model validation where no additional test samples were available. A large-scale study involving 20000 NNs confirmed that NNs trained on real bone data significantly outperform the NNs trained on surrogate data.</p><p>The framework has been evaluated via a comparative study that predicted concrete CS using both large (1030 samples) and small (56 samples) datasets. Using the proposed framework it was possible to develop a smalldataset NN with performance =0.923 comparable with that of a large-dataset NN =0.944). This demonstrates that a drastic 18 times reduction in the required dataset size corresponds to only a small decrease in accuracy of 2.12% -a compromise to be considered in single-center studies where datasets are often limited.</p><p>When applied to 35 osteoarthritic specimens, our methodology yielded a reliable predictive NN tool for non-destructive estimation of bone compressive strength. The optimised NN achieved a high generalising accuracy of 98.3%. Additionally, by quantifying random effects specific to the dataset, the surrogate data approach allowed us to define a performance threshold of =0.87 for successful NNs. The successful application of the proposed methodology confirms that the size of datasets does not necessarily limit the utility of NNs in the medical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Practical significance of the bone CS model</head><p>In cellular solids, CS is an exponential function of the apparent density, BV/TV, raised to the power of 3/25 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">47]</ref>. Although such an exact relationship has not been established specifically for osteoarthritic trabecular tissue, this power model, with a bivariate regression coefficient = 0.906 is the best existing fit to the data <ref type="bibr" target="#b16">[17]</ref>. The generalising NN performance =0.983 achieved in our study exceeded by 8.5%. The proposed NN model yields substantially more accurate predictions by considering variable interrelations within multi-dimensional medical datasets and successfully capturing the complex physiological phenomena in patients suffering from severe OA.</p><p>The high accuracy of the proposed CS model enables prediction of bone fracture risk based on the structural and physiological parameters that can be derived without invasive tests on the patient. Hence, by predicting how CS correlates with the bone volume fraction, trabecular thickness and structure model index for patients of various age and gender groups, the NN model can provide a decision support tool for hard tissue engineers and clinicians alike <ref type="bibr" target="#b25">[26]</ref>. To our best knowledge, the NN presented in this work is the only existing patient-specific model for prediction of CS in trabecular bone affected by OA.</p><p>The potential practical applications include: the estimation of bone fracture risk in osteoarthritic patients from CT-scans and basic physiological data, load modelling of synthetic bioscaffolds that mimic natural trabecular bone damaged by OA, and the tailoring of bioscaffold designs for an individual patient to match the damaged trabecular tissue at the site of implantation.</p><p>The predictive NN model can be adapted to larger datasets and to other degenerative bone disorders, such as osteoporosis and metastatic cancer, with marginal increase in design effort and cost <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Such scalability is inherent in the underlying ML algorithms, which enable NNs to learn and improve their performance with new data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>Appendix A1 -Trabecular bone data: real vs surrogate samples   For statistically significant NNs the distributions of and were compared for various neuron configurations. The highest was achieved in NN designs with 3 and 4 neurons. The Wilkinson rank sum test was used to assess the inter-run volatility for the two candidate designs. Based on comparison of the 50 pairwise p-values at 5% confidence level, NNs with 4 neurons were established to be more stable than those with 3 neurons. Following careful evaluation of the largest number of statistically significant NNs produced, the highest and performance, and adequate inter-run stability, NN with 4 neurons in a hidden layer was chosen as the final NN design for the next stage in parameter estimation.</p><p>Another way to identify optimal NN size is by integrating a parameter regularisation into a training process. A weight decay procedure penalises large weights forcing the NN parameters to shrink. Larger networks have more parameters to start with, but regularisation prevents some of this "excessive capacity" from being trained unnecessarily. The effective number of parameters in a NN trained with regularisation can serve as an indication of how well the NN utilises its capacity. We investigated the number of effective parameters for NNs of varying hidden layer size (from 1 to 20 neurons) trained by Bayesian regularization backpropagation (Fig. <ref type="figure" target="#fig_11">A2-2</ref>). The number of effective parameters rose in NN configurations with 1 to 4 neurons and fell in configurations with 5 neurons and above, indicating that the NN with 4 neurons was most effective. This was further confirmed by considering validation performance across 20 runs, which was highest for the NNs with 4 neurons.   Table <ref type="table" target="#tab_2">3</ref> shows the final weight and bias parameters for the trained bone NN: the input weights matrix , the layer weights column vector ̅̅̅̅̅ , and the corresponding biases ̅̅̅̅̅ and . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Neural network model topology and layer configuration represented by a -dimensional input, -neuron hidden layer and 1 output variable.</figDesc><graphic coords="5,313.20,484.29,251.98,90.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distributions of regression coefficients and across a run of neural networks: (a) large-dataset model (1030 samples), (b) intermediate 100 sample model, and (c) small-dataset model (56 samples). The inset shows the enlarged area highlighted in (a).</figDesc><graphic coords="9,87.95,50.40,169.70,347.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 illustrates the changes to the regression coefficient distributions as the size of the dataset decreased from (a) 1030 to (b) 100, and to (c) 56 samples.In comparison to the large-dataset NNs (Fig.3, a), the distributions of the regression coefficients along x-axis for smaller dataset NNs (Fig.3, b-c) were within much wider ranges. The standard deviations σ also increased substantially for NN modes based on smaller datasets compared with the initial large-dataset model (Fig.3, a). Distributions of the regression coefficients achieved by the 2000 NN instances within the same run (Fig.3, c) demonstrate higher intra-run variance when compared to the large-dataset NNs (Fig.3, a). Over half of the NNs did not converge and only 762 NNs produced statistically significant predictions.The mean regression coefficients across the run decreased to μ( =0.719, and μ( =0.542 (Fig.3, c). When considering only statistically significant NNs (R &gt; 0.6), the mean performance of all samples was μ( =0.839 and individually for tests μ( =0.736. Despite higher volatility, an</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distributions of regression coefficients achieved by smalldataset neural networks for surrogates (green) and real concrete data (navy) for (a) large-dataset model (1030 samples), (b) intermediate 100 sample model, and (c) small-dataset model (56 samples).</figDesc><graphic coords="10,46.80,50.40,234.44,578.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Linear regression between target and predicted compressive strength achieved by the specimen large-data (1030 samples) concrete neural network model. Values are reported individually for (a) training (blue), (b) validation (green), (c) testing (red), and (d) the entire dataset (black).</figDesc><graphic coords="10,364.00,342.21,159.78,170.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Linear regression between target and predicted compressive strength achieved by the small-dataset (56 samples) optimised concrete neural network. Values are reported individually for (a) training (blue), (b) validation (green) and (c) testing (red), (d) the entire dataset (black), and (e) for 300 independent test samples (purple).</figDesc><graphic coords="11,46.80,338.82,261.32,170.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Distributions (a) of regression coefficients achieved by neural networks for surrogates (light blue) and real bone data (navy) and (b) Wilcoxon rank sum test for medians across all samples. Distributions and Wilcoxon test results across test samples are reported in (c) and (d).</figDesc><graphic coords="12,46.80,103.53,251.98,251.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Linear regression between the target and predicted compressive strength (in MPa) achieved by the bone neural network. Values were reported individually for a) training (blue), b) validation (green) and c) testing (red), and d) the entire dataset (black).</figDesc><graphic coords="12,323.30,50.40,202.09,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. A2- 1 .</head><label>1</label><figDesc>Fig. A2-1. Number of statistically significant NNs per run for various number of neurons in the hidden layer.</figDesc><graphic coords="16,46.80,321.52,251.98,176.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. A2- 2 .</head><label>2</label><figDesc>Fig.A2-2. Distributions of the effective number of parameters in regularised neural networks for various number of neurons in the hidden layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 .</head><label>2</label><figDesc>Effects of the training duration Training duration stipulates the balance between the NN training performance and generalisation. Although extended training can lead to exceptional performance on the training dataset, it often results in poor generalisation on the test data that the NNs had not seen before. Early stopping helps to avoid NN over-fitting upon reaching the maximum number of validation checks. The number, , of consecutive validation iterations during which the NN performance fails to decrease plays key role in controlling the quality of NN training. It also affects computational efficiency of the training algorithm, which deteriorates with the increasing . When investigated on 20 runs of 2000 NNs, corresponding to from 1 to 10 in the increments of 1 and 10 to 100 in the increments of 10, the effect of on the NN performance was marginal. No statistical difference was established between the distributions of R (neither nor ) for various in any possible pair of Wilkinson rank sum comparisons at 5% significance level. Thus, any configuration that yielded the highest values of and was a suitable candidate for the final NN. Based on the above considerations, the value of 9 allowed for maximum performance across all samples while maintaining adequate simulation efficiency. Appendix A3 -Values of weights and biases of the final NN model for trabecular bone data The small-dataset bone CS NN was trained using the Levenberg Marquardt backpropagation algorithm [37]. During each iteration (epoch), the performance of the NN on training, validation and test samples was monitored in terms of its cost function expressed by MSE. Fig. A3-1 shows how the NN error on the training set was monotonically decreasing with each epoch. The errors on the validation and test samples were sporadic until the 14 th epoch. At the 31 st epoch the validation error failed to decrease for 9 consecutive iterations and the early stopping criterion was reached. The weights and biases were then reverted by 9 epochs to the state at which the validation error was least, i.e. the final state of the trained NN weights and biases corresponded to the 22 nd epoch. Notably, this is not the state that minimises cost function for the test samples, as these independent test samples were not involved in the model training; their corresponding cost function is provided for illustrative purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. A3- 1 .</head><label>1</label><figDesc>Fig. A3-1. Neural network cost function dynamics during the 30 epoch of training (blue), validation (green) and testing (red). Upon reaching the 9 th validation check at 22 nd epoch (green circle), the</figDesc><graphic coords="17,46.80,470.07,251.97,217.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Real bone data</figDesc><table><row><cell>Sample no.</cell><cell>SMI</cell><cell>tb.th</cell><cell>BV/TV</cell><cell>Age (years)</cell><cell>Gender (F=1)</cell><cell>CS (MPa)</cell></row><row><cell>1</cell><cell>0.06</cell><cell>243</cell><cell>32.5</cell><cell>41.8</cell><cell>1</cell><cell>20.9</cell></row><row><cell>2</cell><cell>1.42</cell><cell>224</cell><cell>21.5</cell><cell>52.0</cell><cell>1</cell><cell>6.91</cell></row><row><cell>3</cell><cell>0.48</cell><cell>239</cell><cell>26.6</cell><cell>57.0</cell><cell>1</cell><cell>18.2</cell></row><row><cell>4</cell><cell>-0.82</cell><cell>212</cell><cell>43.5</cell><cell>63.9</cell><cell>1</cell><cell>9.46</cell></row><row><cell>5</cell><cell>1.22</cell><cell>419</cell><cell>17.9</cell><cell>64.0</cell><cell>1</cell><cell>23.1</cell></row><row><cell>6</cell><cell>0.64</cell><cell>223</cell><cell>27.6</cell><cell>67.1</cell><cell>1</cell><cell>19.4</cell></row><row><cell>7</cell><cell>2.10</cell><cell>197</cell><cell>9.82</cell><cell>68.1</cell><cell>1</cell><cell>2.76</cell></row><row><cell>8</cell><cell>0.38</cell><cell>367</cell><cell>26.9</cell><cell>71.5</cell><cell>1</cell><cell>18.9</cell></row><row><cell>9</cell><cell>0.80</cell><cell>218</cell><cell>15.4</cell><cell>74.9</cell><cell>1</cell><cell>6.49</cell></row><row><cell>10</cell><cell>0.54</cell><cell>314</cell><cell>25.0</cell><cell>76.0</cell><cell>1</cell><cell>17.8</cell></row><row><cell>11</cell><cell>0.30</cell><cell>326</cell><cell>32.4</cell><cell>87.0</cell><cell>1</cell><cell>24.2</cell></row><row><cell>12</cell><cell>-0.17</cell><cell>287</cell><cell>30.4</cell><cell>41.7</cell><cell>0</cell><cell>21.5</cell></row><row><cell>13</cell><cell>-0.31</cell><cell>284</cell><cell>37.0</cell><cell>47.9</cell><cell>0</cell><cell>16.4</cell></row><row><cell>14</cell><cell>0.04</cell><cell>265</cell><cell>38.7</cell><cell>49.8</cell><cell>0</cell><cell>11.1</cell></row><row><cell>15</cell><cell>0.82</cell><cell>241</cell><cell>22.7</cell><cell>49.8</cell><cell>0</cell><cell>26.5</cell></row><row><cell>16</cell><cell>-0.23</cell><cell>303</cell><cell>37.6</cell><cell>65.8</cell><cell>0</cell><cell>28.8</cell></row><row><cell>17</cell><cell>1.77</cell><cell>219</cell><cell>25.3</cell><cell>68.0</cell><cell>0</cell><cell>4.91</cell></row><row><cell>18</cell><cell>1.33</cell><cell>261</cell><cell>17.4</cell><cell>72.9</cell><cell>0</cell><cell>9.81</cell></row><row><cell>19</cell><cell>0.04</cell><cell>307</cell><cell>29.7</cell><cell>73.9</cell><cell>0</cell><cell>23.7</cell></row><row><cell>20</cell><cell>0.36</cell><cell>271</cell><cell>31.6</cell><cell>81.8</cell><cell>0</cell><cell>24.4</cell></row><row><cell>21</cell><cell>0.31</cell><cell>252</cell><cell>33.8</cell><cell>60.9</cell><cell>1</cell><cell>20.5</cell></row><row><cell>22</cell><cell>0.70</cell><cell>283</cell><cell>22.5</cell><cell>62.9</cell><cell>1</cell><cell>12.2</cell></row><row><cell>23</cell><cell>1.59</cell><cell>247</cell><cell>13.7</cell><cell>72.6</cell><cell>1</cell><cell>1.93</cell></row><row><cell>24</cell><cell>0.45</cell><cell>257</cell><cell>27.4</cell><cell>45.7</cell><cell>0</cell><cell>19.6</cell></row><row><cell>25</cell><cell>0.44</cell><cell>266</cell><cell>27.5</cell><cell>62.9</cell><cell>0</cell><cell>18.5</cell></row><row><cell>26</cell><cell>0.15</cell><cell>270</cell><cell>32.1</cell><cell>77.8</cell><cell>0</cell><cell>22.2</cell></row><row><cell>27</cell><cell>1.08</cell><cell>193</cell><cell>19.4</cell><cell>87.0</cell><cell>0</cell><cell>9.12</cell></row><row><cell>28</cell><cell>1.93</cell><cell>154</cell><cell>9.68</cell><cell>49.0</cell><cell>1</cell><cell>8.22</cell></row><row><cell>29</cell><cell>0.92</cell><cell>263</cell><cell>25.3</cell><cell>66.0</cell><cell>1</cell><cell>15.4</cell></row><row><cell>30</cell><cell>-0.43</cell><cell>299</cell><cell>39.7</cell><cell>69.9</cell><cell>1</cell><cell>23.2</cell></row><row><cell>31</cell><cell>1.04</cell><cell>239</cell><cell>21.0</cell><cell>73.9</cell><cell>1</cell><cell>8.15</cell></row><row><cell>32</cell><cell>-0.05</cell><cell>288</cell><cell>35.6</cell><cell>46.8</cell><cell>0</cell><cell>24.3</cell></row><row><cell>33</cell><cell>0.39</cell><cell>246</cell><cell>26.6</cell><cell>64.9</cell><cell>0</cell><cell>19.3</cell></row><row><cell>34</cell><cell>0.71</cell><cell>178</cell><cell>12.2</cell><cell>68.0</cell><cell>0</cell><cell>14.0</cell></row><row><cell>35</cell><cell>0.70</cell><cell>234</cell><cell>21.8</cell><cell>84.9</cell><cell>0</cell><cell>13.3</cell></row></table><note><p><p><p>Bone data were extracted from the original study from</p><ref type="bibr" target="#b16">[17]</ref> </p>using a Plot Digitiser tool.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Surrogates</figDesc><table><row><cell>Sample no.</cell><cell>SMI</cell><cell>tb.th</cell><cell>BV/TV</cell><cell>Age (years)</cell><cell>Gender (F=1)</cell><cell>CS (MPa)</cell></row><row><cell>1</cell><cell>1.00</cell><cell>260</cell><cell>32.3</cell><cell>66.8</cell><cell>1</cell><cell>17.43</cell></row><row><cell>2</cell><cell>0.58</cell><cell>217</cell><cell>38.0</cell><cell>54.0</cell><cell>0</cell><cell>16.21</cell></row><row><cell>3</cell><cell>0.73</cell><cell>260</cell><cell>40.5</cell><cell>82.7</cell><cell>1</cell><cell>6.95</cell></row><row><cell>4</cell><cell>0.13</cell><cell>209</cell><cell>19.3</cell><cell>57.4</cell><cell>0</cell><cell>19.89</cell></row><row><cell>5</cell><cell>0.53</cell><cell>185</cell><cell>17.6</cell><cell>80.4</cell><cell>1</cell><cell>28.51</cell></row><row><cell>6</cell><cell>1.72</cell><cell>314</cell><cell>30.5</cell><cell>55.9</cell><cell>0</cell><cell>13.48</cell></row><row><cell>7</cell><cell>0.67</cell><cell>269</cell><cell>16.0</cell><cell>60.9</cell><cell>1</cell><cell>26.99</cell></row><row><cell>8</cell><cell>0.63</cell><cell>336</cell><cell>26.8</cell><cell>49.6</cell><cell>1</cell><cell>13.33</cell></row><row><cell>9</cell><cell>0.12</cell><cell>287</cell><cell>26.9</cell><cell>68.9</cell><cell>0</cell><cell>23.77</cell></row><row><cell>10</cell><cell>0.58</cell><cell>271</cell><cell>35.0</cell><cell>54.2</cell><cell>0</cell><cell>15.24</cell></row><row><cell>11</cell><cell>0.80</cell><cell>306</cell><cell>26.0</cell><cell>46.6</cell><cell>1</cell><cell>14.52</cell></row><row><cell>12</cell><cell>0.90</cell><cell>320</cell><cell>24.1</cell><cell>71.6</cell><cell>1</cell><cell>10.76</cell></row><row><cell>13</cell><cell>0.42</cell><cell>376</cell><cell>29.9</cell><cell>60.1</cell><cell>0</cell><cell>8.40</cell></row><row><cell>14</cell><cell>0.37</cell><cell>155</cell><cell>31.5</cell><cell>69.6</cell><cell>1</cell><cell>12.63</cell></row><row><cell>15</cell><cell>1.93</cell><cell>317</cell><cell>26.7</cell><cell>61.3</cell><cell>0</cell><cell>20.02</cell></row><row><cell>16</cell><cell cols="2">-0.49 275</cell><cell>23.1</cell><cell>68.5</cell><cell>1</cell><cell>21.19</cell></row><row><cell>17</cell><cell>1.38</cell><cell>378</cell><cell>18.0</cell><cell>44.5</cell><cell>1</cell><cell>19.40</cell></row><row><cell>18</cell><cell>1.47</cell><cell>264</cell><cell>28.1</cell><cell>79.5</cell><cell>0</cell><cell>2.93</cell></row><row><cell>19</cell><cell>1.14</cell><cell>258</cell><cell>21.1</cell><cell>74.4</cell><cell>1</cell><cell>22.59</cell></row><row><cell>20</cell><cell cols="2">-0.23 304</cell><cell>13.5</cell><cell>72.4</cell><cell>1</cell><cell>24.92</cell></row><row><cell>21</cell><cell>0.18</cell><cell>224</cell><cell>31.9</cell><cell>74.9</cell><cell>1</cell><cell>20.93</cell></row><row><cell>22</cell><cell>0.32</cell><cell>261</cell><cell>20.6</cell><cell>61.2</cell><cell>1</cell><cell>5.17</cell></row><row><cell>23</cell><cell>0.90</cell><cell>326</cell><cell>25.1</cell><cell>68.2</cell><cell>1</cell><cell>13.90</cell></row><row><cell>24</cell><cell cols="2">-0.11 270</cell><cell>30.3</cell><cell>66.9</cell><cell>0</cell><cell>19.60</cell></row><row><cell>25</cell><cell>0.98</cell><cell>312</cell><cell>23.3</cell><cell>65.6</cell><cell>0</cell><cell>20.09</cell></row><row><cell>26</cell><cell cols="2">-0.20 293</cell><cell>31.4</cell><cell>57.8</cell><cell>0</cell><cell>10.90</cell></row><row><cell>27</cell><cell>0.86</cell><cell>272</cell><cell>24.1</cell><cell>56.8</cell><cell>1</cell><cell>11.85</cell></row><row><cell>28</cell><cell>0.59</cell><cell>227</cell><cell>30.2</cell><cell>63.3</cell><cell>1</cell><cell>19.02</cell></row><row><cell>29</cell><cell>1.10</cell><cell>283</cell><cell>30.6</cell><cell>56.1</cell><cell>1</cell><cell>15.62</cell></row><row><cell>30</cell><cell>0.97</cell><cell>194</cell><cell>25.1</cell><cell>74.6</cell><cell>0</cell><cell>18.13</cell></row><row><cell>31</cell><cell>1.44</cell><cell>277</cell><cell>11.7</cell><cell>85.3</cell><cell>1</cell><cell>11.17</cell></row><row><cell>32</cell><cell>1.39</cell><cell>282</cell><cell>22.7</cell><cell>45.3</cell><cell>0</cell><cell>9.80</cell></row><row><cell>33</cell><cell>0.32</cell><cell>292</cell><cell>24.7</cell><cell>65.8</cell><cell>0</cell><cell>22.25</cell></row><row><cell>34</cell><cell>0.94</cell><cell>323</cell><cell>21.3</cell><cell>49.6</cell><cell>0</cell><cell>20.85</cell></row><row><cell>35</cell><cell>0.04</cell><cell>367</cell><cell>19.4</cell><cell>74.5</cell><cell>1</cell><cell>25.36</cell></row></table><note><p>Surrogate data were synthesised as a random normal distribution with the mean and standard deviation of the real bone data within the same range.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 -</head><label>3</label><figDesc>Weights and biases</figDesc><table><row><cell></cell><cell>0.887</cell><cell>2.382</cell><cell>-0.888</cell><cell>-3.584</cell></row><row><cell></cell><cell>1.301</cell><cell>-1.586</cell><cell>0.904</cell><cell>-3.841</cell></row><row><cell></cell><cell>-3.268</cell><cell>0.632</cell><cell>-1.342</cell><cell>-0.144</cell></row><row><cell></cell><cell>-1.216</cell><cell>-2.153</cell><cell>-1.380</cell><cell>-3.000</cell></row><row><cell></cell><cell>-0.620</cell><cell>1.592</cell><cell>-0.379</cell><cell>-1.169</cell></row><row><cell>̅̅̅̅̅</cell><cell>-0.698</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.151</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.349</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-1.501</cell><cell></cell><cell></cell><cell></cell></row><row><cell>̅̅̅̅̅</cell><cell>0.268</cell><cell>-0.006</cell><cell>-1.224</cell><cell>-4.972</cell></row><row><cell></cell><cell>0.623</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by EPSRC UK (EP/K02504X/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A2 -NN design parameter estimation for bone CS data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Effects of the number of neurons in hidden layer</head><p>Limited availability of the training samples necessitates careful selection of the size of the hidden layer in order to achieve well-generalising NNs. The effect of increasing number of neurons in the hidden layer from 1 to 13 was investigated in the series of experiments that involved 10 runs of 2000 NNs for each neuron, i.e. 260000 NNs in total were analysed for enhanced repeatability.</p><p>Reported in Fig. <ref type="figure">A2</ref>-1 is the number of statistically significant NNs, i.e. NNs that exhibited performance of across the entire dataset, as well as individually for the training, validation and test datasets. Despite the inter-run volatility in the results, on average the highest performing NNs had 2, 3, 4, and 5 neurons in hidden layer with 890, 878, 873 and 851 statistically significant NNs per run, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine Learning Methodology in Bioinformatics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<editor>N. Kasabov</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="185" to="206" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from Little: Comparison of Classifiers Given Little Training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc PKDD</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="161" to="172" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Inza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Armañanzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: An Indispensable Tool in Bioinformatics</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Matthiesen</surname></persName>
		</editor>
		<imprint>
			<publisher>Humana Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">593</biblScope>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probability and Statistics for Computer Science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Whiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Woolson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Clarke</surname></persName>
		</author>
		<title level="m">Statistical Methods for the Analysis of Biomedical Data</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Artificial neural networks in medical diagnosis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Peña-Méndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vaňhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hampl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Havel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Biomed</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Neural networks and artificial intelligence for biomedical engineering</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Artificial Neural Networks and Predictive Medicine: a Revolutionary Paradigm Shift</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grossi</surname></persName>
		</author>
		<editor>K. Suzuki</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>InTech</publisher>
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural networks for analysis of trabecular bone in osteoarthritis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Khovanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaikhina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinspired, Biomim. Nanobiomaterials</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bootstrap evaluation of the effect of data splitting on financial time series</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lebaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="213" to="220" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal division of data for neural network models in water resources applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resour. Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Wasserman</surname></persName>
		</author>
		<title level="m">Neural computing: theory and practice</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand-Reinhold</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stability problems with artificial neural networks and the ensemble solution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="225" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Artificial Neural Networks in Hard Tissue Engineering: Another Look at Age-Dependence of Trabecular Bone Properties in Osteoarthritis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaikhina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khovanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mallick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE EMBS Int. Conf. Biomed. Heal. Informatics</title>
		<imprint>
			<biblScope unit="page" from="622" to="625" />
			<date type="published" when="2014">2014</date>
			<publisher>IEEE</publisher>
			<pubPlace>Valencia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structural parameters and mechanical strength of cancellous bone in the femoral head in osteoarthritis do not depend on age</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baleani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baruffaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viceconti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bone</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="760" to="768" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Osteoarthritis: Diagnosis and Treatment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sinusas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Fam. Physician</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bone mineral density in osteoarthritis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Rheumatol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="464" to="467" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bone Mineral Density in Osteoarthritis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Živković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">.</forename><surname>Stamenković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nedović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Fac. Medicae Naissensis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="135" to="141" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bone mineral density and association of osteoarthritis with fracture risk</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Center</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Eisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Osteoarthritis Cartilage</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1251" to="1258" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prediction of the strength and fracture location of the femoral neck by CT-based finite-element method: A preliminary study on patients with hip fracture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bessho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kominami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Orthop. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="545" to="550" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prediction of femoral fracture load using automated finite element modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Keyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Skinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bone compressive strength: the influence of density and strain rate</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1174" to="1176" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mathematical relationships between bone density and mechanical properties: A literature review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Helgason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Taddei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viceconti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Biomech</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Geris</surname></persName>
		</author>
		<title level="m">Computational Modeling in Tissue Engineering</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Osteoarthritis : diagnosis and treatment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sinusas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Fam. Physician</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Testing a neural coding hypothesis using surrogate data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Blenkinsop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved Surrogate Data for Nonlinearity Tests</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="635" to="638" />
			<date type="published" when="1996-07">Jul. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Testing for nonlinearity in time series: the method of surrogate data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eubank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Longtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galdrikian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Doyne</forename><surname>Farmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D Nonlinear Phenom</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="77" to="94" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling of strength of high-performance concrete using artificial neural networks</title>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cem. Concr. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1797" to="1808" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UCI Machine Learning Repository: Concrete Compressive Strength Data Set</title>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Yeh</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning Repository</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2007-01">2007. Jan-2015</date>
		</imprint>
		<respStmt>
			<orgName>University of California Irvine, Center of Machine Learning and Intelligent Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparing Sigmoid Transfer Functions for Neural Network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yonaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Anctil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fortin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Hydrol. Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Jt. Conf. Neural Networks</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Method for the Solution of Certain Non-linear Problems in Least-Squares</title>
		<author>
			<persName><forename type="first">K</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An Algorithm for Least-Squares Estimation of Nonlinear Parameters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Soc. Ind. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Levenberg-Marquardt algorithm: Implementation and theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>More</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lect. Notes Math.</title>
		<imprint>
			<biblScope unit="volume">630</biblScope>
			<biblScope unit="page" from="105" to="116" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Estimation of prediction error by using K-fold crossvalidation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fushiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Power of surrogate data testing with respect to nonstationarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Timmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5153" to="5156" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using mega-trenddiffusion and artificial samples in small data set learning for early flexible manufacturing system scheduling knowledge</title>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-I</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="982" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The generalization complexity measure for continuous input data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Cannas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Osenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jerez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. World J</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">815156</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Surveying the methods of improving ANN generalization capability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><surname>Int</surname></persName>
		</author>
		<author>
			<persName><surname>Conf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Cybern., Xian: IEEE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1259" to="1263" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Popular Ensemble Methods: An Empirical Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comparison of different methods for combining multiple neural networks models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 Int. Joint Conf. Neural Networks</title>
		<meeting>of the 2002 Int. Joint Conf. Neural Networks<address><addrLine>Honolulu</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="828" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hollander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wolfe</surname></persName>
		</author>
		<title level="m">Nonparametric statistical methods</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cellular materials in nature and medicine</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Harley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low bone mineral density and its predictors in type 1 diabetic patients evaluated by the classic statistics and artificial neural network analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eller-Vainicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhukouskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tolkachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Koritko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cairoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beck-Peccoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chiodini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Shepelkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2186" to="2191" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Toward the scalability of neural networks through feature selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peteiro-Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bolon-Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso-Betanzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guijarro-Berdinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sanchez-Marono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2807" to="2816" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
