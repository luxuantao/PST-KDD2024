<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capacity of the Gaussian Arbitrarily Varying Channel</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Imre</forename><surname>Csiszhr</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IEEE International Symposium on Information Theory</orgName>
								<address>
									<addrLine>January 14-19</addrLine>
									<postCode>1990</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Prakash</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IEEE International Symposium on Information Theory</orgName>
								<address>
									<addrLine>January 14-19</addrLine>
									<postCode>1990</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hungary</forename><forename type="middle">P</forename><surname>Narayan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering Department and the Systems Research Center</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Capacity of the Gaussian Arbitrarily Varying Channel</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39301DF1F37ACD2477491BB211CCC51C</idno>
					<note type="submission">received April 25. 1989; revised February 13, 1990. I.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Arbitrarily varying channel</term>
					<term>Gaussian</term>
					<term>capacity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Gaussian arbitrarily varying channel with input constraint r and state constraint 2 admits input sequences x = ( x l , , x , ~) of real numbers with Cxf 5 nT and state sequences s = ( 5 ,. , T , ~) of real numbers with L Y ~ 5 H A ; the output sequence x + s + V , where V = ( VI, , y,) is a sequence of independent and identically distributed Gaussian random variables with mean 0 and variance c'. It is proved that the capacity of this arbitrarily varying channel for deterministic codes and the average probability of error criterion equals l o g ( l + r/( 2 t c 2 ) )</p><p>if Z &lt; r and is 0 otherwise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>RBITRARILY varying channels (AVC's) were intro-A duced by <ref type="bibr">Blackwell et al. [5]</ref> to model communication channels with unknown parameters that may vary with time in an arbitrary and unknown manner during the transmission of a codeword. In this paper, attention is restricted to AVC's without memory; further, it is assumed that the sequence of channel states is selected arbitrarily subject to a constraint specified later, and possibly depending on the codebook but independently of the codeword actually sent.</p><p>AVC's exhibit various mathematical complexities even in the case of discrete alphabets (cf. Csiszir-Korner [6, Section 2.61). In particular, their capacity may depend on whether or not random codes are permitted, and whether the average or maximum probability of error criterion is used. The random coding capacity admits a simple characterization as a min-max of mutual information, a result dating back to Blackwell et al. <ref type="bibr">[5]</ref>. In contrast, the problem of capacity for deterministic codes is much harder. In particular, for the maximum probability of error criterion, a single-letter capacity formula is known only under certain conditions on the structure of the AVC (cf. Ahlswede <ref type="bibr" target="#b1">[2]</ref> and Csiszir-Korner <ref type="bibr" target="#b6">[7]</ref>).</p><p>Unless stated otherwise, the term capacity will hereafter always refer to capacity for deterministic codes and the ui'erage probability of error criterion. In the absence of state constraints, Ahlswede <ref type="bibr">[l]</ref> proved that this capacity was either equal to the random coding capacity or otherwise to zero. The necessary and sufficient condition for positive capacity, as well as capacity under a state constraint, have been determined by Csiszhr-Narayan <ref type="bibr" target="#b7">[8]</ref>; it was further shown that Ahlswede's alternatives do not necessarily obtain under a state constraint.</p><p>Less attention has been bestowed in the literature on the capacity of AVC's with continuous alphabets. Presumably motivated by random coding capacity, there have been game-theoretic considerations concentrating on the min-max of mutual information (cf. McEliece <ref type="bibr">[l 11</ref>). Hughes-Narayan [lo] have used a geometric approach to determine the random coding capacity of the Gaussian AVC defined formally in the following paragraph. Blachman <ref type="bibr" target="#b3">[4]</ref> has provided lower and upper bounds on capacity in a communication situation differing from ours in that the interference (i.e., state sequence) could depend on the actual codeword transmitted. Our incomplete understanding of his paper seems to indicate that he, too, considered random coding capacity. To our knowledge, Ahlswede's <ref type="bibr" target="#b2">[3]</ref> is the only paper treating the capacity of a continuous alphabet AVC for deterministic codes. His AVC (a Gaussian channel with the noise variance arbitrarily varying but not exceeding a given bound) allowed a very simple approach, which may not be extendable to other cases of interest.</p><p>In this paper, we determine the capacity of the Gaussian AVC formally defined as follows. Let the input and output alphabets, and the set of states, be the real line. For any input sequence x = ( x , ; . ., x,i) and state sequence s = (si;.., s,?), let the output be x + s + V , where V = ( V , ; . ., V,,) is a sequence of independent and identically distributed (i.i.d.1 Gaussian random variables with mean 0 and variance (T'. We adopt an input constraint r and state constraint A , namely the permissible input sequences of length n are those satisfying (1.2)</p><formula xml:id="formula_0">i = l</formula><p>A code of block-length n comprises a set of codewords xi;. ',x,,,, each in R", and a decoder cp: R" +{O;.., N ) . The average probability of error of this code, used on the Gaussian AVC as above when the state sequence is s, equals 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ( s ) = -</head><formula xml:id="formula_1">P r { c p ( x , + s + V ) # i } . (1.3) N , = I I 'j</formula><p>The capacity C of the Gaussian AVC with input constraint r and state constraint A is the largest number with the property that for every 6 &gt; 0 and sufficiently large n, there exist codes with N 2 exp{n(C -a)} codewords, each satisfying (l.l), such that the supremum of F(s) subject to (1.2) converges to 0 as n +m.</p><p>Our main result is the following. Thus, in this case Ahlswede's alternatives do obtain. Yet a proof of the theorem above by the elegant "elimination technique" of Ahlswede [1] is not apparent. Rather, we shall use the straightforward but more computational method of CsiszAr-Narayan [SI. Suitable approximation arguments would enable a derivation of our theorem directly from the results of [SI. Instead, we prefer to present a more transparent and direct proof, which will also serve to keep this paper self-contained. We also determine the capacity of the noiseless additive AVC whose output is r + s rather than r + s + V . The capacity of this AVC is defined similarly to that of the Gaussian AVC with the exception that (1.3) is now replaced by 1</p><formula xml:id="formula_2">N F(s) = -I(i: cp(x, + s) # i}I.</formula><p>Theorem 2: The capacity of the noiseless additive AVC with input constraint r and state constraint A is if T s A .</p><p>Whereas this result is not a formal special case of Theorem 1, both theorems can be proved by the same method.</p><p>We shall prove the simpler Theorem 2 first so that the reader may better understand the key ideas. Observe that Theorem 1 requires a separate proof only in the case A + u 2 2 r. In fact, since (1.2) implies for an arbitrary E &gt; 0 that 11s + VJ12 I n(A + u 2 + E ) with probability arbitrarily close to 1 if n is sufficiently large, in the case A + u 2 &lt; the assertion of Theorem 1 follows immediately from that of Theorem 2.</p><p>Actually, we shall show that the capacity as claimed in Theorems 1 and 2 can be achieved using the minimum-distance decoder, namely</p><formula xml:id="formula_3">( 1 5 ) if I I ~ -.rill2 &lt; ~l y -xjl12, if no such 1 I i I N exists. for j + i d(Y) = { i , 0,</formula><p>It is worth pointing out that the result of Theorem 2 with this decoder provides a solution to a weakened version of the unsolved sphere-packing problem. This problem seeks the exponential rate of the maximum number of nonintersecting spheres of radius JT? in R" with centers in a sphere of radius n. In our case, the spheres may intersect but for any given s in R" of norm I &amp; , only for a vanishingly small fraction of sphere centers xi can x i + s be closer to another sphere center than to x i . The number C in Theorem 2 then gives the exponential rate of the maximum number of spheres satisfying this condition. A similar weakened version of the sphere-packing problem in Hamming space was solved in <ref type="bibr" target="#b7">[8]</ref> as a special case of the coding theorem for the binary adder AVC.</p><p>11. PROOF OF THE MAIN RESULT The proof of the converse parts of Theorems 1 and 2 being standard, is relegated to the Appendix. The essential contribution of this paper consists in the direct part of coding Theorems 1 and 2.</p><p>Our goal is to show that, when r &gt; A, for all sufficiently large n there exist N = exp(nR) codewords xI; . ., x,,, in R" satisfying Ilx,l12 I nT, i = 1; . ., N, with R arbitrarily close to the asserted capacity value, such that for a suitable decoder cp the average probability of error F(s) is arbitrarily small uniformly subject to llsl12 5 nA.</p><p>Using the minimum distance decoder 4 of (1.5) for the noiseless AVC, (1.4) becomes and for the Gaussian case, (1.3)</p><formula xml:id="formula_4">gives i N Xjl12 I 11s + VI12,</formula><p>for some j z i}.</p><p>We can assume without any loss of generality that  + -({i: (x,,s) + ( x i , x i ) &gt; 1 -7 , for some j z i) 1.</p><formula xml:id="formula_5">( 2 . 2 ) = 1, 0 &lt; A &lt; 1.</formula><formula xml:id="formula_6">-a k ) / f i ) , k = 1;. ., K , the condition a' + p2 &gt; 1 + 7 - exp(-2R) of 2) of</formula><formula xml:id="formula_7">1 1 c = -l o g 1 + -= --l o g 1-- (2.6) 2 ( i) 2 ( 1;Aj'</formula><p>The first term of the sum in (2.6) can be bounded by Lemma I(i). In fact, letting U be the unit vector such that the inequality (2.9) will be satisfied if 77 is sufficiently small. The proof for the Gaussian case (Theorem 1) is similar but bounding (2.4) is not as easy. We first present two simple technical lemmas.</p><p>Lemma 2: Let the r.v. U be uniformly distributed on the unit n-sphere. Then for every vector U on this sphere and any 0 &lt; a &lt; 1, we have (2.10)</p><p>Further, for any pair of constants a,p, Ilau + pull2 I ( a 2 + p2)(1 + 77).</p><p>(2.11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof: Let u ' = ( u -( u , u ) u ) / I I u -(u,u</head><p>)ull be the unit vector orthogonal to U such that span(u,u'} = span(u,u). Then <ref type="figure">I ( u ,</ref><ref type="figure">u ) ( u ,</ref><ref type="figure">x</ref>  Continuing with the proof of the direct part of Theorem 1, note that on account of (2.8) it suffices to consider only those terms in (2.4) for which I(x,, u)l I 77, where U is a unit vector satisfying s = ullsll. We shall bound these terms using </p><formula xml:id="formula_8">IIx, + s + v -x,1I2 = 11x, 112 + 11s + v1t2 + llx,112 + 2(x,, s) + 2( X I , -2( x,, x,) -2( x,, s) -2(x,, v). (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=(x,,v"'")+(xy",v).</head><p>(2.13) Since V = (VI,. . ., V,,) is a sequence of i.i.d. Gaussian random variables with mean 0 and variance a 2 / n , we have as n --fm that Pr {I( x , , ~) I &gt; 77) -, 0, P r ( IIv"~~II &gt; 77) + o uniformly in i and U. This along with (2.12), (2.13), implies that Pr {llx, + s + vx,l12 I 11s + v1l2, for some j z i ) = Pr ((x,,x,) + (x,,s) + ( x y + c , ~) &gt; 1 + ( x , , ~)</p><formula xml:id="formula_9">+ (x,, V ) -(x;"i U , v ) , for some j + i 1 I pr (( xy'., V ) &gt; 1 -377 -(( x,, x,) I -l(x,, s) 1 ,</formula><p>for some j # i + E , <ref type="bibr">(2.14</ref>)</p><formula xml:id="formula_10">)</formula><p>for all sufficiently large n, whenever Kx,, u)l I 17.</p><p>uniformly subject to llsl12 I A , it suffices to prove that Then the expression (2.15) is 1</p><formula xml:id="formula_11">I c $ i : W 4 I l ( k , l ) E G</formula><p>As the first term above goes to zero uniformly in U as n +CO by 2) of Lemma 1, it remains to consider only the second term. Recalling again that V = (V,, . . ., V,) is an i.i.d. sequence of N ( 0 , u 2 / n ) random variables, we note that Pr(llVl12 &gt; u2 + v} -+ 0 as n + W . Therefore, it suffices to prove that</p><formula xml:id="formula_12">- Pr{ U ( I I V I I ~~~~+ ~, 1 i : K x , , ~) l s ~ ( k , l ) E G iEF,A/</formula><p>where w = ( a k x j + p p ) / IIakxi + ppll. By Lemma 3 (for Kxi,u)l I q), I l a k x j + ppll I J(ai + p:)( 1 + 77) , so that by Lemma 1( i) for all n sufficiently large, where we can assume that a: + p : &lt; 1 + q (2.18) (as otherwise F,kl = 4).  <ref type="figure">( x y ' ,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">~</ref>) &gt; 1 -5 7 -ahp,fi&gt;</p><formula xml:id="formula_13">(2.19)</formula><p>where U is a r.v. distributed uniformly on the unit n-sphere and U ' is any fixed unit vector in R". Together with (2.17), this implies that (2.16) is overbounded by C, <ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">A',</ref><ref type="figure">;</ref><ref type="figure">'</ref>, where Hence it suffices to show that A$;)+ 0 as n + C O for every ( k , l ) E G. Since ( k , I ) E G, there are two cases to consider: a)</p><p>a; + P: We first observe that in both cases</p><formula xml:id="formula_14">&gt; 1 + 17 -exp ( -<label>2</label></formula><formula xml:id="formula_15">1 -a h -p / f i -j V &gt; 0 (2.21) provided that 7 is chosen sufficiently small. Indeed, in case a), the expression in (2.21) is 2 1 -fi -677. In case b), the assumption R &lt; C -6 = + log(1 + 1/(A + u 2 ) ) -6 implies that 1 a: + p : 5 1 + 77- 1 exp(26) l + y A + u 11 &lt; 1 + 77 --exp(26). 1 + A Since a h + P / f i 5 d( a; + p f ) ( 1 + 1 1 )</formula><p>(as can be directly verified by squaring both sides), this yields 1 -a , -p/fi-577 &gt; 1 -5 7 -41 -A(exp(26) -1) + v( 1 + A ) &gt; 0 if 77 is sufficiently small. Now, in case a) we obtain, using Lemma 2, that if E and 77 are chosen small enough.</p><p>using Lemma 2 we obtain from (2.20) that</p><p>In case b), we have R + + log(1-ai -P: + 77) &gt; 0. Then, a ; -p ; + q)+ e )</p><p>1 1 i ( l -a , &lt; -P / f i -5 7 7 ) * ( v 2 + q ) ( l -ai -P,? +47)</p><formula xml:id="formula_16">1 J I /</formula><p>Evaluating the maximum of</p><formula xml:id="formula_17">(1 -a -p a -5 q ) * 2 -p 2 +477 - y ( a , P ) = l -a &gt; U 2 f V</formula><p>we obtain by differentiation that the maximum is attained at and the value of the maximum is</p><formula xml:id="formula_18">(1 -w2 l+477- Thus, in case b), Ai;) + 0 if -E . (2.22) ( 1 -5 ~) ~ l + A + a 2 + v</formula><p>Obviously, if C -2 6 &lt; R &lt; C -6 for any fixed 6 &gt; 0, where the inequality (2.22) will be satisfied if 7 and E are sufficiently small. This completes the proof. A co,nparison of the Gaussian AVC with the discrete case treated in <ref type="bibr" target="#b7">[8]</ref>, [9] indicates that the former is simpler in that it does not call for a complex decoding rule. Indeed, simple minimum-distance decoding suffices to achieve capacity. On the other hand, since the powerful and intuitive method of types is unavailable, the computations are less lucid and appear to rely, to a degree, on analytical artifices.</p><p>One generalization of Theorem 1 is immediate. Namely, if in the representation x + s + V of the channel output, the variances of the independent, zero-mean Gaussian components of V are allowed to vary arbitrarily subject to a, s a , j = 1,. . . , N, the capacity remains unaltered. Indeed the only change necessitated in the error-bounding is the-replacement of IlVll' I a' + q in (2.15) and (2.18) by llVl12 s a 2 + 77, where V = ( V , a / a , ; . ., V,,u/a,,) has i.i.d.-components, followed by the observation that (x;, V ) = (i;, V I , where lli,'Il&lt; 1 since i; is obtained by multiplying the components of the unit vector x; by U, / U I 1, i = 1; . ., n.</p><p>A further generalization with arbitrarily varying noise variances subject only to (l/n)C:'=Iu,2 I U', when we believe the capacity to yet remain unchanged, does not yield to such a simple artifice. On the contrary, it apparently requires more complex calculations, including a generalization of Lemma 1. Indeed for more general AVC models with continuous alphabets, the direct approach may well become unmanageable, thus necessitating recourse to the method of approximations by discrete AVC's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We shall first prove the converse parts of Theorems 1 and 2, followed by the proof of Lemma 1 (cf. Section 11).  for n 2 no(€, A').</p><p>Here the left side is the average probability of error of the given code on the "ordinary" memoryless channel with additive Gaussian noise of variance A'+ u2. Hence, by the converse to the coding theorem for such channels, it follows that 1 r C r C ' = -l o g l+-2</p><p>( A ' + u 2 ) ' Since A' &lt; A was arbitrary, this completes the proof of the converse part of Theorem 1 (and, with the obvious changes, also that of Theorem 2). We now prove Lemma 1. We shall show that N = exp(nR) randomly selected unit vectors will possess, with probability arbitrarily close to 1, all the properties states in Lemma 1. Its proof entails Chernoff bounding applied to dependent random variables, which is provided by the following Lemma A I : Let Z,;. ., Z, be arbitrary r.v.'s and f , ( Z , ; . ., Z,) be arbitrary with 0 5 f , I 1, i = 1; . ., N. Then the condition implies that</p><p>Proof: This lemma is the same as Lemma A1 of <ref type="bibr" target="#b7">[8]</ref>, with the exponentials and logarithms to the base 2 in the latter replaced by natural exponentials and logarithms.</p><p>Proof of Lemma I : Throughout this proof "for large n" will mean "for all n larger than some threshold n,, depending only on E , 77, and K." Let 2,; . ., 2, be independent r.v.'s each uniformly distributed on the unit n-sphere. First, fix a unit vector U in R" The inequality ( A l l would then follow if we showed that the term within the square brackets, denoted h ( n , R , E , a ) , was bounded below by i e x p ( n ~/ 2 ) for large n. There are two cases to consider. . (A6)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and the permissible state sequences are those satisfying n Ilsll'= z s , ? ~n A , ( A &gt; O ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>N</head><label></label><figDesc>Lemma 1. (The condition a 2 7 is clearly satisfied provided 7 &lt; min{+,(l-6 ) / 2 } . ) Differentiation shows that a 2 + ( 1 -2 7 -a ) ' / A is minimized by a = (1 -277)/1+ A, and the minimum equals (1 -2 ~) ~/ 1 + A. Thus, the condition to be satisfied is 1 Obviously, if C -26 &lt; R &lt; C -6 for any fixed 6 &gt; 0, where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(- 7 . 8 )</head><label>78</label><figDesc>x,. s) I -77 implies by the assumption A &lt; 1 that (x,, U) I Thus if R &gt; -log(1-v2), we get thats exp { n ( e -z)) -+ 0, as n -+ W . (2.The second term of the sum in (2.6) can be bounded using 2) of Lemma 1 by suitably partitioning the set of possible values of the inner product (x,,x,). To this end, let a l = 1 -77 -f i &lt; a 2 &lt; . .. &lt; a , = 1 -2 ~, with a r + I -a k ~q , k =1; . ., K -1.Then (x,, s ) + (x,, x,) &gt; 1 -77 implies that (x,, x,) 1 a I , and if a L I ( x I , x,) I + I then necessarily (x,, s) &gt; 1-27 -a h . The latter, in turn, implies by (2.7) that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Denote the angle between the unit vectors U and U by O(U,u). Then by Shannon [12, (28)1, With a = cos$, it follows that 1 i f a 2 -+zz The proof is completed by observing that Pr{(U, U ) I -a } = Lemma 3: Let U and U be unit vectors with I ( u , u ) ~~ 7. of x orthogo-Pr{(U, U) 2 a). U Then for any unit vector x, the component x nal to span {U, v ) has norm I I X ~I I * I 1 -( u , x ) 2 -( U , x ) 2 + 4 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 I</head><label>2</label><figDesc>(u,x)I -277. Since J)x )I2 = 1 -(U, xI2 -(U', x)', this implies (2.10). Finally ((au + pull2 = a 2 + p2 + 2 a p ( u , u ) I ( a 2 + P')(l+ 771, as 12ap/(a2 + p2)I I 1, thereby proving (2.11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>12 )</head><label>12</label><figDesc>Decomposing x, and V into components in M,,u = s p a n b , , U} and in Mi*:, we have (x,, v) = (xy U ) U ) + [ xy=, V".'.')</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-</head><label></label><figDesc>Hence, in order to show that Z'(s) in (2.4) goes to 0 1 Kx,,u)I&lt;q -l ( x , , u ) l f i , forsome j # i )(2.15)    converges to 0 uniformly for unit vectors U E R", as n * m.T o this end, we partition the set of all possible values of the inner products (x,, x,) and (x,, U). Let a1 = 0 &lt; "2 &lt; ' . . &lt; a K = l and p I = 0 &lt; p 2 &lt; . .. &lt; p , = l . w i t h a A + l -ah 4 17, k = l ; . . , K -l , and / 3 / + l -p / ~~, I = l ; . . , L -l . Further let F , ~/ = { j : j + i , a,, 5 ((x,,x,)I I a k + l , P l I 1(x,J4 15 &amp; + I ) and G = {( k , l ) : 1 I k I K , 1 I 1 I L , a k 2 17, a f + p : &gt; l + ~~-e x p ( -2 R ) } .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Further, by Lemma 3 ,</head><label>3</label><figDesc>if x i = x , ' ( i , u ) represents the unit vector in the direction of x y ' 1 , for j E F,h/ we have IIxy , z I I I J1-(x,,x,)'-(x,,u)'+477 I J l -a : -p f + 4 7 7 if IKx,, u)l I 7 . Hence Pr(llVII' 5 u 2 + 77,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>p,? I 1 + 77 -exp( -2 ~) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>DlscusslONWe have established the capacity of the Gaussian AVC with input constraint r, state constraint A, and noise power a 2 , for deterministic codes and the average probability of error criterion. It is 0 if A 2 r, and equals the capacity of an ordinary memoryless channel with additive Gaussian noise of power A + u 2 , for the same input constraint r, if A &lt; r. The limiting value of this capacity as u 2 --f 0 is, as expected, the capacity of the noiseless AVC with input constraint r and state constraint A. The previous result solves a weakened version of the problem of determining the exponential rate of the tightest sphere-packing in R" as n +m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>The fact that r I A implies C = 0 follows by a well-known argument of Blackwell rr al.[5]. NamPly, let x I ; . . , x N , N 2 2, be arbitrary codewords in R" satisfying (l.l), -23 and-assuming r I A-consider the state sequences sI = xI;. ' , s N = x N . Then for any decoder cp, whenever i # j. Hence 1 N ( N -1 ) 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 I</head><label>1</label><figDesc>and a , P in [O, 13. The main step of the proof consists in asserting the doubly exponential probability bounds that for large n {i: I ( z , , z , ) I &gt; ~, I ( z , , I ( ) ( ~~,f o r s o m e j # i } / } &gt; exp( -n e )To establish (AI), (A3), we shall apply Lemma A1 to 2,; . ., 2, for two different choices of the functions f,.Observe that (All holds trivially if 4 log(1a 2 ) + ~/ 2 &gt; 0, i.e., if a &lt; d l -exp( -E ) . Hence, restricting attention toa 2 4 -&gt; 0, letThe hypothesis of Lemma A1 is then satisfied by Lemma 2 with a = 2(1 -a 2 ) c ' f p ' ) / 2 for large n. Thus with t = ( 1 / N ) e x p ( n ( l R + ~l o g ( l -a 2 ) I + + e / 2 ) } in Lemma A l , we get 1 exp (-[ N ( exp ( n ( lR + log( 1 -a 2 ) I ++ :))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>--log (1 -a 2 ) then 2 exp ( I) ne .log2 -2exp R (by A5) with the bounding completed as in case a). Turning to (A3) next, define A ; = { j : j &lt; i, 1(Zj,u)12 P } and A , = \ 4 , otherwise. First note that (2, f A , for some i) c(lA,I &gt; exp(n(lR + 4 log(1-P2)1+ + ~/2)}}, which by (Al) has probability less than exp( -e x p ( n ~/ 2 ) ) for large n. Next, let I , i f I ( Z , , Z , ) I &gt; a f o r s o m e j E 2 , i 0, otherwise. f , ( Z , , . ' . , Z , ) = Then for large n Pr -{i: 1 ( ~, , ~, ) 1 2 a , 1 ( ~, , u ) l 2 p , f o r s o m e j &lt; i ) l (AI &gt; exp( -n e )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and consequently 2 ( s j ) 2 f for at least one j E {l; . ., N).</figDesc><table><row><cell>2 --' N2</cell><cell>2</cell><cell>'4'</cell></row><row><cell cols="3">Next, to prove that C I 4 log(1 + r/(a2 + A)), consider an</cell></row><row><cell>i.i.d. sequence S</cell><cell></cell><cell></cell></row></table><note><p>= (SI; . * , S,) of Gaussian random variables (also independent of V ) with mean 0 and variance A'&lt; A. Given any code with codewords xI; . ., x N , and decoder cp, the expectation of Z(S) (cf. (1.3)) equals while E2( S) I max 2( s) + Pr (IIS112 &gt; n A } . s: llsll2 5 n A Since varSi = A ' &lt; A, it holds that Pr{llSI12 &gt; n A } + 0 and, therefore, if maxIls,,~ Z(s) &lt; E , we have r N</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>exp( -n e ) for all U E .&lt; and a E A , p E B satisfying (A2) will be arbitrarily close to 1. We complete the proof of the lemma by observing that for an appropriate choice of .-&lt;, A and B as above, if xI; . ., x N are unit vectors satisfying</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Csiszir was supported by the Hungarian National Foundation Scientific Research Grant No. 1806. P. Narayan was supported by the Systems Research Center at the University of Maryland under NSF Grant OIR-85-00108. This work was</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The second term on the right side of (A6) will be bounded using Lemma Al. To this end, we introduce the event (I(Z;,u)l 2 q/4} and note from Lemma 2 that its probability is less than 2(1-q2/16)("-"/2 for large n. Also, writing Z; = (Z;, u)u + Z,', we see that (Zj, Z;) = (Z;, uXZj, U ) + (Zj, Z :  Then if I;r is any fixed unit vector in R", we obtain for large n that by Lemma 2 if a -q / 4 &lt; d q ; otherwise the probability is trivially zero. Since lA,l I exp(n(IR + ilog(1-p2)I+ + ~/ 2 ) } , we obtain from (A7) and (A8) that the hypothesis of Lemma A1 is satisfied with if aq / 4 &lt; d q , and otherwise with</p><p>) Observe that for large tz (using the hypothesis &gt; 8 6 ) establishing (A12) when a -q / 4  for large n , using the condition of the lemma that 7 &gt; 8 6 .</p><p>Thus, in both cases, the term in (A141 is less than exp( -ne). This, together with (A13), establishes (A12). By symmetry, the same bound holds if "for some j &lt; i" is replaced by "for some j &gt; i," thereby validating the claim in (A31.</p><p>The doubly exponential bounds in (Al), (A31 imply that for any finite set 4 of unit vectors in R" with lc&lt;l increasing exponentially in n , and any finite subsets A and B of [0,1], the probability of the joint occurrence of the events for all U E 9'' and a E A , and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Elimination of correlation in random codes for arbitrarily varying channels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z . Wuhr.~cheinlichkeitstheorie Verw. Gebiete</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A method of coding and an application to arbitrarily varying channels</title>
	</analytic>
	<monogr>
		<title level="j">J . Combinat., Inform. Sysi. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The capacity of a channel with arbitrarily varying additive Gaussian channel probability functions</title>
	</analytic>
	<monogr>
		<title level="m">Trans. Skth Prague Conf. Inform. Theor), Siaiistical Decision Functions, Random Processes</title>
		<imprint>
			<date type="published" when="1971-09">Sept. 1971</date>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the capacity of a band-limited channel perturbed by statistically dependent interference</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Blachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="55" />
			<date type="published" when="1962-01">Jan. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The capacities of certain channel classes under random coding</title>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">D</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Thomasian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="558" to="567" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Information Theory: Coding Theorems for Discrete Memoryless Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszlr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiirner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the capacity of the arbitrarily varying channel for maximum probability of error</title>
	</analytic>
	<monogr>
		<title level="j">Z. Wahrscheinlichkeitsrheorie Venu. Gebiete</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="87" to="101" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The capacity of the arbitrarily varying channel revisited: Positivity, constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="1988-03">Mar. 1988</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Inform. Theory</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gaussian arbitrarily varying channels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="267" to="284" />
			<date type="published" when="1987-03">Mar. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Communication in the presence of jamming-An information theory approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mceliece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Secure Digital Communications</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Longo</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probability of error for optimal codes in a Gaussian channel</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="752" to="769" />
			<date type="published" when="1959-05-10">May 1959. 10-35, 1980. July 1989. 1983</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
