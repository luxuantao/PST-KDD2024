<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
						</author>
						<title level="a" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2022.3154319</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2022.3154319, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural networks</term>
					<term>neural network expressivity</term>
					<term>graph isomorphism</term>
					<term>graph classification</term>
					<term>graph substructures</term>
					<term>network analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often intimately related to downstream tasks. To this end, we propose "Graph Substructure Networks" (GSN), a topologically-aware message passing scheme based on substructure encoding. We theoretically analyse the expressive power of our architecture, showing that it is strictly more expressive than the WL test, and provide sufficient conditions for universality. Importantly, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We perform an extensive experimental evaluation on graph classification and regression tasks and obtain state-of-the-art results in diverse real-world settings including molecular graphs and social networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE field of graph representation learning has undergone a rapid growth in the past few years. In particular, Graph Neural Networks (GNNs), a family of neural architectures designed for irregularly structured data, have been successfully applied to problems ranging from social networks and recommender systems <ref type="bibr" target="#b0">[1]</ref> to bioinformatics <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, chemistry <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and physics <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, to name a few. Most GNN architectures are based on message passing <ref type="bibr" target="#b4">[5]</ref>, where the representation of each vertex is iteratively updated by aggregating information from its neighbours.</p><p>A crucial difference from traditional neural networks operating on grid-structured data is the absence of canonical ordering of the vertices in a graph. To address this, the aggregation function is constructed to be invariant to neighbourhood permutations and, as a consequence, to graph isomorphism. This kind of symmetry is not always desirable and thus different inductive biases that disambiguate the neighbours have been proposed. For instance, in geometric graphs, such as 3D molecular graphs and meshes, directional biases are usually employed in order to model the positional information of the vertices <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>; for proteins, ordering information is used to disambiguate amino-acids at different positions in the sequence <ref type="bibr" target="#b13">[14]</ref>; in multi-relational knowledge graphs, a different aggregation is performed for each relation type <ref type="bibr" target="#b14">[15]</ref>.</p><p>The structure of the graph itself does not usually explicitly take part in the aggregation function. In fact, most • G.Bouritsas and S.Zafeiriou are with the Department of Computing, Imperial College London, London SW7 2AZ, UK. E-mail: {g.bouritsas, s.zafeiriou}@imperial.ac.uk • F.Frasaca and M.Bronstein are with the Department of Computing, Imperial College London, SW7 2AZ, UK and also with Twitter Cortex, London W1B 5DL, UK. E-mail: {ffrasca, mbronstein}@twitter.com models rely on multiple message passing steps as a means for each node to discover the global structure of the graph. However, since message-passing GNNs are at most as powerful as the Weisfeiler Leman test (WL) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, they are limited in their abilities to adequately exploit the graph structure, e.g. by counting substructures <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. This uncovers a crucial limitation of GNNs, as substructures have been widely recognised as important in the study of complex networks. For example, in molecular chemistry, functional groups and rings are related to a plethora of chemical properties, while cliques are related to protein complexes in Protein-Protein Interaction networks and community structure in social networks, respectively <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Therefore, three major questions arise when designing GNN architectures: (a) How to go beyond isotropic, i.e., locally symmetric, aggregation functions ? (b) How to ensure that GNNs are aware of the structural chatacteristics of the graph? (c) How to achieve the above two without sacrificing invariance to isomorphism and hence the ability of GNNs to generalise?</p><p>In this work we attempt to simultaneously provide an answer to the above. We propose to break local symmetries by introducing structural information in the aggregation function, hence addressing (a) and (b). In particular, the contribution of each neighbour (message) is transformed differently depending on its structural relationship with the central node. This relationship is expressed by counting the appearance of certain substructures. Since substructure counts are vertex invariants, i.e. they are invariant to vertex permutations, it is easy to see that the resulting GNN will be invariant to isomorphism, hence also addressing (c). Moreover, by choosing the substructures, one can provide the model with different inductive biases, based on the graph distribution at hand.</p><p>We characterise the expressivity of our message-passing scheme, coined as Graph Substructure Network (GSN), showing that GSN is strictly more expressive than traditional GNNs for the vast majority of substructures, while retaining the locality of message passing, as opposed to higher-order methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> that follow the WL hierarchy (see Section 2). In the limit, our model can yield a unique representation for every isomorphism class and is thus universal. We provide an extensive experimental evaluation on hard instances of graph isomorphism testing (strongly regular graphs), as well as on real-world networks from the social and biological domains, including the recently introduced large-scale benchmarks <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. We observe that when choosing the structural inductive biases based on domain-specific knowledge, GSN achieves state-of-theart results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Let G = (V G , E G ) be a graph with vertex set V G and edge set E G , directed or undirected, with n vertices and m edges. A subgraph</p><formula xml:id="formula_0">G S = (V G S , E G S ) of G is any graph with V G S ⊆ V G , E G S ⊆ E G . When E G S includes all the edges of G with endpoints in V G S , i.e., E G S = E G ∩ V G S × V G S ,</formula><p>the subgraph is said to be induced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Isomorphism &amp; Automoprhism</head><p>Two graphs G, H are isomorphic (denoted H G), if there exists an adjacency-preserving bijective mapping (isomor-</p><formula xml:id="formula_1">phism) f : V G → V H , i.e., (v, u) ∈ E G iff (f (v), f (u)) ∈ E H .</formula><p>Given some small graph H, the subgraph isomorphism problem amounts to finding a subgraph G S of G such that G S</p><p>H. An automorphism of H is an isomorphism that maps H onto itself. The set of all automorphisms forms a group, the automorphism group of the graph, denoted as Aut(H), which contains all possible graph symmetries.</p><p>The automorphism group yields a partition of the vertices into disjoint subsets of V H called orbits. Intuitively, this concept allows us to group the vertices based on their structural roles, e.g. the endpoint vertices of a path, or all the vertices of a cycle (see Figure <ref type="figure" target="#fig_0">1</ref>). Formally, the orbit of a vertex v ∈ V H is the set of vertices to which it can be mapped via an automorphism:</p><formula xml:id="formula_2">Orb(v) = {u ∈ V H | ∃g ∈ Aut(H) s.t. g(u) = v}, and the set of all orbits V H /Aut(H) = {Orb(v) | v ∈ V H }</formula><p>is called the quotient of the action of the automorphism group on V H . We denote the elements of this set as {O V H,1 , . . . , O V H,d H }, where d H is the cardinality of the quotient.</p><p>Analogously, we define edge structural roles via edge automorphisms, i.e., bijective mappings from the edge set onto itself, that preserve edge adjacency (two edges are adjacent if they share a common endpoint). In particular, every vertex automorphism g induces an edge automorphism by mapping each edge (u, v) to (g(u), g(v)). 1 In the same way as before, we construct the edge automorphism group, from which we deduce the partition of the edge set in edge orbits</p><formula xml:id="formula_3">{O E H,1 , O E H,2 , . . . , O E H,d H }.</formula><p>1. Note that the edge automorphism group is larger than that of induced automorphisms, but strictly larger only for 3 trivial cases <ref type="bibr" target="#b26">[27]</ref>. However, induced automorphisms provide a more natural way to express edge structural roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weisfeiler-Leman tests</head><p>The Weisfeiler-Leman graph-isomorphism test <ref type="bibr" target="#b27">[28]</ref>, also known as naive vertex refinement, 1-WL, or just WL), is a fast heuristic to decide if two graphs are isomorphic or not. The WL test proceeds as follows: every vertex v is initially assigned a colour c 0 v that is later iteratively refined by aggregating neighbouring information:</p><formula xml:id="formula_4">c t+1 v = HASH c t v , c t u u∈Nv ,<label>(1)</label></formula><p>where • denotes a multiset (a set that allows element repetitions) and N (v) is the neighbourhood of v. The WL algorithm terminates when the colours stop changing, and outputs a histogram of colours. Two graphs with different histograms are non-isomorphic; if the histograms are identical, the graphs are possibly, but not necessarily, isomorphic. Note that the neighbour aggregation in the WL test is a form of message passing, and GNNs are the learnable analogue.</p><p>A series of works on improving GNN expressivity mimic the higher-order generalisations of WL, known as k-WL and k-Folklore WL (WL hierarchy) and operate on k-tuples of vertices (see Appendix B.1). The (k + 1)-FWL is strictly stronger than k-FWL, k-FWL is as strong as (k + 1)-WL and 2-FWL is strictly stronger than the simple 1-WL test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH SUBSTRUCTURE NETWORKS</head><p>Graphs consist of vertices (or edges) with repeated structural roles. Thus, it is natural for a neural network to treat them in a similar manner, akin to weight sharing between local patches in CNNs for images <ref type="bibr" target="#b28">[29]</ref> or positional encodings in language models for sequential data <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Nevertheless, GNNs are usually unaware of the vertices' different structural roles, since all vertices are treated equally when performing local operations. Despite the initial intuition that the neural network would be able to discover these roles by constructing deeper architectures, it has been shown that GNNs are ill-suited for this purpose and are blind to the existence of structural properties, e.g. triangles or larger cycles <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>To this end, we propose to explicitly encode structural roles as part of message passing, in order to capture richer topological properties. Our method draws inspiration from <ref type="bibr" target="#b32">[33]</ref>, where it was shown that GNNs become universal when the vertices in the graph are uniquely identified, i.e when they are equipped with different features. However, it is not clear how to choose these identifiers in a permutation equivariant way. Structural roles, when treated as identifiers, although not necessarily unique, are permutation equivariant, and also more amenable to generalisation due to their repetition across different graphs. Thus, they can constitute a trade-off between uniqueness and generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural features</head><p>Structural roles are encoded into features by counting the appearance of certain substructures. Let H = {H 1 , H 2 . . . H K } be a set of small (connected) graphs, for example cycles of fixed length or cliques. For each graph H ∈ H, we first find its isomorphic subgraphs in G denoted as G S . For each vertex v ∈ V G S we infer its role w.r.t. H by obtaining the orbit of its mapping f (v) in H, Orb H (f (v)). By counting all the possible appearances of different orbits in v, we obtain the vertex structural feature x V H (v) of v, defined as follows. For all i ∈ {1, . . . , d H }:</p><formula xml:id="formula_5">x V H,i (v) = G S H v ∈ V G S , f (v) ∈ O V H,i } . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>Note that there exist |Aut(H)| different functions f that can map a subgraph G S to H, but any of those can be used to determine the orbit mapping of each vertex v. By combining the counts from different substructures in H and different orbits, we obtain the feature vector</p><formula xml:id="formula_7">x V v = [x V H1 (v), . . . , x V H K (v)] ∈ N D×1 of dimension D = Hi∈H d Hi .</formula><p>Similarly, we can define edge structural features x E H,i (u, v) by counting occurrences of edge automorphism orbits:</p><formula xml:id="formula_8">x E H,i (u, v) = G S H (u, v) ∈ E G S , (f (u), f (v)) ∈ O E H,i ,<label>(3)</label></formula><p>and the combined edge features</p><formula xml:id="formula_9">x E u,v = [x E H1 (u, v), . . . , x E H K (u, v)</formula><p>]. An example of vertex and edge structural features is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure-aware message passing</head><p>The key building block of our architecture is the graph substructure layer, defined in a general manner as a Message Passing Neural Network (MPNN) <ref type="bibr" target="#b4">[5]</ref>, where now the messages from the neighbouring vertices also contain the structural information. In particular, each vertex v updates its state h t v by combining its previous state with the aggregated messages:</p><formula xml:id="formula_10">h t+1 v = UP t+1 h t v , m t+1 v (4) m t+1 v =            M t+1 (h t v , h t u , x V v , x V u , e u,v ) u∈N (v) (GSN-v) or M t+1 (h t v , h t u , x E u,v , e u,v ) u∈N (v) (GSN-e),<label>(5)</label></formula><p>where UP t+1 is an arbitrary function approximator (e.g. a MLP), M t+1 is the neighborhood aggregation function, i.e. an arbitrary function on multisets (e.g., of the form</p><formula xml:id="formula_11">u∈N (v) MLP(•)</formula><p>) and e u,v are the edge features. The two variants, named GSN-v and GSN-e, correspond to vertex-or edge-counts, respectively, which are analogous to absolute and relative positional encodings in language models <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> that obtain universality at the expense of permutation equivariance (since the identifiers are arbitrarily chosen with the sole requirement of being unique), GSNs retain this property, hence they are by construction ivariant to isomorphism. This stems from the fact that the process generating our structural identifiers (i.e. subgraph isomorphism) is permutation equivariant itself (proof provided in the Appendix A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How powerful are GSNs?</head><p>We now turn to the expressive power of GSNs in comparison to MPNNs and the WL tests, a key tool for the theoretical analysis of the expressivity of graph neural networks so far. Since GSN is a generalisation of MPNNs, it is easy to see that it is at least as powerful. Importantly, GSNs have the capacity to learn functions that traditional MPNNs cannot learn. The following observation derives directly from the analysis of the counting abilities of the 1-WL test <ref type="bibr" target="#b17">[18]</ref> and its extension to MPNNs <ref type="bibr" target="#b18">[19]</ref> (for proofs, see Appendices A.2-A.4).</p><p>Theorem 3.1. GSN is strictly more powerful than MPNN and the 1-WL test when one of the following holds:</p><p>• H is any graph except for star graphs of any size, and structural features are inferred by subgraph matching, i.e. we count all subgraphs G S ∼ = H for which it holds that E G S ⊆ E G . Or, • H is any graph except for single edges and single vertices, and structural features are inferred by induced subgraph matching, i.e. we count all subgraphs G S ∼ = H for which it holds that</p><formula xml:id="formula_12">E G S = E G ∩ V G S × V G S .</formula><p>Proof. It is easy to see that GSN model class contains MPNNs, and is thus at least as expressive. We can also show that GSN is at least as expressive as 1-WL by repurposing the proof of Theorem 3 in <ref type="bibr" target="#b15">[16]</ref> (see Appendix A.2) Given the first part of the proposition, in order to show that GSNs are strictly more expressive than the 1-WL test, it suffices to show that GSN can distinguish a pair of graphs that 1-WL deems isomorphic. <ref type="bibr" target="#b17">[18]</ref> showed that 1-WL, and consequently MPNNs, can count only forests of stars. Thus, if the subgraphs are required to be connected, then they can only be star graphs of any size (note that this contains single vertices and single edges). In addition, <ref type="bibr" target="#b18">[19]</ref> showed that 1-WL, and consequently MPNNs, cannot count any connected induced subgraph with 3 or more vertices, i.e. any connected subgraph apart from single vertices and single edges.</p><p>If H is a substructure that 1-WL cannot learn to count, i.e. the ones mentioned above, then there is at least one pair of graphs with different number of counts of H, that 1-WL deems isomorphic. Thus, by assigning counting features to the vertices/edges of the two graphs based on appearances of H, a GSN can obtain different representations for G 1 and G 2 by summing up the features. Hence, G 1 , G 2 are deemed non-isomorphic. An example is depicted in Figure <ref type="figure">2</ref> (left), Fig. <ref type="figure">2:</ref> (Left) Decalin and Bicyclopentyl: Non-isomorphic molecular graphs than can be distinguished by GSN, but not the by the WL test <ref type="bibr" target="#b33">[34]</ref> (vertices represent carbon atoms and edges chemical bonds). (Right) Rook's 4x4 graph and the Shrikhande graph: the smallest pair of strongly regular non-isomorphic graphs with the same parameters SR <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2)</ref>. GSN can distinguish them with 4-clique counts, while 2-FWL fails.</p><p>where the two non-isomorphic graphs are distinguishable by GSN via e.g. cycle counting, but not by 1-WL.</p><p>Universality. A natural question that emerges is what are the sufficient conditions under which GSN can solve graph isomorphism. This would entail that GSN is a universal approximator of functions defined on graphs <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. To address this, we can examine whether there exists a specific substructure collection that can completely characterise each graph. As of today, we are not aware of any results in graph theory that can guarantee the reconstruction of a graph from a smaller collection of its subgraphs. However, the Reconstruction Conjecture <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, states that a graph with size n ≥ 3 can be reconstructed from its vertex-deleted subgraphs (proven for n ≤ 11 <ref type="bibr" target="#b41">[42]</ref>). Consequently, (proof in the Appendix A.3): Corollary 3.2. If the Reconstruction Conjecture holds and the substructure collection H contains all graphs of size k = n − 1, then GSN can distinguish all non-isomorphic graphs of size n and is therefore universal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GSN-v vs GSN-e.</head><p>We can also examine the expressive power of the two proposed variants. A crucial observation that we make is that for each graph H in the collection, the vertex structural identifiers can be reconstructed by the corresponding edge identifiers. Thus, we can show that for every GSN-v there exists a GSN-e that can simulate the behaviour of the former (proof in the Appendix A.4). Theorem 3.3. For a given subgraph collection H, let C V the set of functions that can be expressed by a GSN-v with arbitrary depth and with, and C E the set of functions that can be expressed by a GSN-e with the same properties. Then, it holds that C E ⊇ C V , or in other words GSN-e is at least as expressive as GSN-v.</p><p>Comparison with higher-order WL tests. Finally, the expressive power of GSN can be compared to higher-order versions of the WL test. In particular, for each k-th order Folklore WL test in the hierarchy, it is known that there exists a family of graphs that will make the test fail. These are known in the literature as k-isoregular graphs <ref type="bibr" target="#b42">[43]</ref>, and the most well-known example is the Strongly Regular (SR) graph family, for k = 2 (more details in Appendix B).</p><p>Hence, if we can find a substructure collection that allows GSN to distinguish certain pairs from these families, then this guarantees that the corresponding k-FWL test is no stronger than GSN. In this work, we identify such counterexamples for the 2-FWL test. Formally: Proposition 3.4. There exist substructure families with O(1) size, i.e., independent of the size of the graph n, such that 2-FWL is no stronger than GSN.</p><p>We provide numerous counterexamples that prove this claim. Figure <ref type="figure">2</ref> (right) provides a typical pair of SR graphs that can be distinguished with a 4-clique, while in section 5.1 this is extended to a large-scale study, where other constant size substructures (paths, cycles and cliques) can achieve similar results.</p><p>Although it is not clear if there exists a certain substructure collection that results in GSNs that align with the WL hierarchy, we stress that this is not a necessary condition in order to design more powerful GNNs. In particular, despite the increase in expressivity, k-WL tests are not only more computationally involved, but they also process the graph in a non-local fashion. However, locality is presumed to be a strong inductive bias of GNNs and key to their excellent performance in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">How to choose the substructures?</head><p>Expressivity. The Reconstruction Conjecture provides a sufficient, albeit impractical condition for universality. This motivates us to analyse the constant size case k = O(1) for practical scenarios, similar to the argument put forward for hard instances of graph isomorphism (Proposition 3.4).</p><p>In particular, one can count only the most discriminative subgraphs, i.e. the ones that can achieve the maximum possible vertex disambiguation, similarly to identifier-based approaches. Whenever these subgraph counts can provide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type="bibr" target="#b32">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the number of subgraphs needed for unique, or near-unique identification, are far fewer than those dictated by Corollary 3.2. This is consistent with our experimental findings, where we observed that certain small substructures such as paths and trees, significantly improve vertex disambiguation, compared to the initial vertex features (see Figure <ref type="figure" target="#fig_2">4</ref> (left) and Table <ref type="table" target="#tab_1">2</ref> in the appendix). As expected this allows for better fitting of the training data, which validates our claim that GNN expressivity improves.</p><p>Generalisation. However, none of the above claims can guarantee good generalisation in unseen data. For example, in Figure <ref type="figure" target="#fig_2">4</ref>, we observe that the test set performance does not follow the same trend with train performance when choosing substructures with strong vertex disambiguation. Aiming at better generalisation, it is desirable to make use of substructures for which there is prior knowledge of their importance in certain network distributions and have been observed to be intimately related to various properties. For example, small substructures (graphlets) have been extensively analysed in protein-protein interaction networks <ref type="bibr" target="#b43">[44]</ref>, triangles and cliques characterise the structure of ego-nets and social networks in general <ref type="bibr" target="#b19">[20]</ref>, simple cycles (rings) are central in molecular distributions, directed and temporal motifs have been shown to explain the working mechanisms of gene regulatory networks, biological neural networks, transportation networks and food webs <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>In Figure <ref type="figure" target="#fig_2">4</ref> (right), we showcase the importance of these inductive biases: a cycle-based GSN predicting molecular properties achieves smaller generalisation gap compared to a traditional MPNN, while at the same time generalising better with less training data. Choosing the best substructure collection is still an open problem that does not admit a straightforward solution due to its combinatorial nature. Alternatively, various heuristics can be used, e.g., motif frequencies or feature selection strategies. Answering this question is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity</head><p>The complexity of GSN comprises two parts: precomputation (substructure counting) and training/testing. The key appealing property is that training and inference are linear w.r.t the number of edges, O(|E|), as opposed to higherorder methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref> with O(n k ), and <ref type="bibr" target="#b47">[48]</ref> with O(n 2 ) training complexity and relational pooling <ref type="bibr" target="#b48">[49]</ref> with O(n!) training complexity in absence of approximations.</p><p>The worst-case complexity of subgraph enumeration for arbitrary subgraph patterns of size k is O(n k ), which follows from naively enumerating all vertex sets of size k in the graph. However, this bound can be substantially improved for specific instances of the problem, taking into account the following: (1) The properties of the target graph, e.g., its sparsity. <ref type="bibr" target="#b1">(2)</ref> The structure of the subgraph of interest, e.g., cycles and cliques. These substructure families are of particular importance in real-world graphs, social networks and molecules respectively, as will be discussed in the experimental section. (3) The number of occurrences of the patterns in the target graph (output sensitive algorithms), e.g., the algorithm in <ref type="bibr" target="#b49">[50]</ref> enumerates connected induced subgraphs in O c(G)nm time, where c(G) the number of occurrences of the subgraph of interest. In the Appendix C.1 we provide an overview of these specialised algorithms and their complexities.</p><p>In addition, arbitrary subgraph counting/enumeration is widely studied, and many general-purpose algorithms <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> provide practical implementations that achieve significant speed-ups using heuristics. As a side note, approximate counting algorithms are also widely used, especially for counting frequent network motifs <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and can provide a considerable speed-up. Furthermore, recent neural approaches <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> provide fast approximate counting.</p><p>In our experiments, we used a general-purpose subgraph isomorphism algorithm. We benchmarked the VF2 algorithm <ref type="bibr" target="#b52">[53]</ref>, and its recently improved version, the VF3 <ref type="bibr" target="#b53">[54]</ref> in our real-world networks and, as expected, in most of the cases, the run-time was considerably smaller than that of the naive enumeration. As discussed above, specialised algorithms are expected to yield further improvements, however this comparison is beyond the scope of our work. In the Appendix C.2, we report the average and total runtimes for various datasets and both algorithms (Tables <ref type="table" target="#tab_1">1 and  2</ref>) as well as the empirical scaling w.r.t. n and k (Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Expressive power of GNNs</head><p>WL hierarchy. The seminal results in the theoretical analysis of the expressivity of GNNs <ref type="bibr" target="#b15">[16]</ref> and k-GNNs <ref type="bibr" target="#b16">[17]</ref> established that traditional message passing-based GNNs are at most as powerful as the 1-WL test. <ref type="bibr" target="#b38">[39]</ref> showed that graph isomorphism is equivalent to universal invariant function approximation. Higher-order Invariant Graph Networks (IGNs) have been studied in a series of works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, establishing connections with the WL hierarchy, similarly to <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b65">[66]</ref>. The main drawbacks of these methods are the training and inference time complexity and memory requirements of O(n k ) and the superexponential number of parameters (for linear IGNs) making them impractical, as well as their non-local nature making them more prone to overfitting. Finally, <ref type="bibr" target="#b66">[67]</ref> also analysed the expressive power of MPNNs and other more powerful variants and provided generalisation bounds.</p><p>Unique identifiers. From a different perspective, <ref type="bibr" target="#b67">[68]</ref> and <ref type="bibr" target="#b32">[33]</ref> showed the connections between GNNs and distributed local algorithms <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref> and suggested more powerful alternatives based on either local orderings or unique global identifiers (in the form of random features in <ref type="bibr" target="#b36">[37]</ref>) that make GNNs universal. Similarly, <ref type="bibr" target="#b37">[38]</ref> propose to use random colorings in order to uniquely identify the vertices. However, these methods lack a principled permutation equivariant way to choose orderings/identifiers. To date this is an open problem in graph theory called graph canonisation and it is at least as hard as solving graph isomorphism itself. A possible workaround is proposed in <ref type="bibr" target="#b48">[49]</ref>, where the authors take into account all possible vertex permutations. However, obviously this quickly becomes intractable (O(n!)) even when considering small-sized graphs.</p><p>More expressive permutation equivariant GNNs. Concurrently with our work, other more expressive GNNs have been proposed using equivariant message passing. In <ref type="bibr" target="#b71">[72]</ref>, the authors propose to linearly transform each message with a different kernel based on the local isomorphism class of the corresponding edge (similar to our definition of structural roles). However, as also noted by the authors, taking into account all possible local isomorphism classes leads to insufficient weight sharing and hence to overfitting. In contrast, in GSN, usually the substructure collection is small (∼5-10 graphs) and the substructures are repetitive in the graph distribution, and as a result generalisation improves. Vignac et al. <ref type="bibr" target="#b47">[48]</ref> propose a message passing scheme where matrices of order equal to the size of the graph are propagated instead of vectors. This can be perceived as a practical unique identification scheme, but the neural network complexity becomes quadratic in the number of vertices. Finally, <ref type="bibr" target="#b72">[73]</ref> and <ref type="bibr" target="#b73">[74]</ref> enhance the aggregation function with distance encodings (a strategy more relevant for vertex-level tasks) and graph eigenvectors respectively as alternative symmetry breaking mechanisms. In the experimental section, GSN is compared against these methods in real-world scenarios.</p><p>Quantifying expressivity. Solely quantifying the expressive power of GNNs in terms of their ability to distinguish nonisomorphic graphs does not provide the necessary granularity: even the 1-WL test can distinguish almost all (in the probabilistic sense) non-isomorphic graphs <ref type="bibr" target="#b74">[75]</ref>. As a result, there have been several efforts to analyse the power of k-WL tests in comparison to other graph invariants <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, while recently <ref type="bibr" target="#b18">[19]</ref> approached GNN expressivity by studying their ability to count substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Substructures in Complex Networks.</head><p>The idea of analysing complex networks based on smallscale topological characteristics dates back to the 1970's and the notion of triad census for directed graphs <ref type="bibr" target="#b78">[79]</ref>. The seminal paper of <ref type="bibr" target="#b44">[45]</ref> coined the term network motifs as overrepresented subgraph patterns that were shown to characterise certain functional properties of complex networks in systems biology. The closely related concept of graphlets <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>, different from motifs in being induced subgraphs, has been used to analyse the distribution of realworld networks and as a topological signature for network similarity. Our work is similar in spirit with the graphlet degree vector (GDV) <ref type="bibr" target="#b79">[80]</ref>, a vertex-wise descriptor based on graphlet counting.</p><p>Substructures have been also used in the context of ML. In particular, subgraph patterns have been used to define Graph Kernels (GKs) <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, with the most prominent being the graphlet kernel <ref type="bibr" target="#b83">[84]</ref>. Motifbased vertex embeddings <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref> and diffusion operators <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref> that employ adjacency matrices weighted according to motif occurrences, have recently been proposed for graph representation learning. Our formulation provides a unifying framework for these methods and it is the first to analyse their expressive power. Finally, GNNs that operate in larger induced neighbourhoods <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref> or higher-order paths <ref type="bibr" target="#b94">[95]</ref> have prohibitive complexity since the size of these neighbourhoods typically grows exponentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In the following section we evaluate GSN in comparison to the state-of-the-art in a variety of datasets from different domains. We are interested in practical scenarios where the collection of subgraphs, as well as their size, are kept small. Depending on the dataset domain we experimented with typical substructure families (cycles, paths and cliques) and maximum substructure size k (note that for each setting, our substructure collection consists of all the substructures of the family with size ≤ k). In particular, for networks with community structure (moderate and large clustering coefficient) we used cliques, while for molecular graphs we used cycles, due to the large frequency of ring structures that are also known to strongly influence molecular properties.</p><p>We also experimented with both graphlets and motifs and observed similar performance in most cases. To showcase that structural features can be used as an off-the-shelf strategy to boost GNN performance, we usually choose a base message passing architecture and minimally modify it into a GSN. Unless otherwise stated, the base architecture is a general-purpose MPNN with MLPs used in the message and update functions. Additional implementation details can be found in the Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Graph Isomorphism test</head><p>We tested the ability of GSNs to decide if two graphs are non-isomorphic on a collection of Strongly Regular graphs of size up to 35 vertices, attempting to disambiguate pairs with the same number of vertices (for different sizes the problem becomes trivial). As we are only interested in the bias of the architecture itself, we use GSN with random weights to compute graph representations. Two graphs are deemed isomorphic if the Euclidean distance of their representations is smaller than a predefined threshold . Figure <ref type="figure" target="#fig_1">3</ref> shows the failure percentage of our isomorphism test when using different graphlet substructures (cycles, paths, and cliques) of varying size k. Interestingly, the number of failure cases of GSN decreases rapidly as we increase k; cycles and paths of maximum length k = 6 are enough to tell apart all the graphs in the dataset. Note that the performance of cliques saturates, possibly because the largest clique in our dataset has 5 vertices. Observe also the discrepancy between GSN-v and GSN-e. In particular, vertex-wise counts do not manage to distinguish all graphs, although missing only a few instances, which is in accordance with Theorem 3.3. Finally, 1-WL <ref type="bibr" target="#b15">[16]</ref> and 2-FWL <ref type="bibr" target="#b23">[24]</ref> equivalent models demonstrate 100% failure, as expected from theory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TUD Graph Classification Benchmarks</head><p>We evaluate GSN on datasets from the classical TUD benchmarks. We use seven datasets from the domains of bioinformatics and computational social science and compare against various GNNs and Graph Kernels. The base architecture that we used is GIN <ref type="bibr" target="#b15">[16]</ref>. We follow the same evaluation protocol of <ref type="bibr" target="#b15">[16]</ref>, performing 10-fold cross-validation and then reporting the performance at the epoch with the best average accuracy across the 10 folds. Table <ref type="table" target="#tab_0">1</ref> lists all the methods evaluated with the split of <ref type="bibr" target="#b100">[101]</ref>. We select our model by tuning architecture and optimisation hyperparameters and substructure related parameters, that is: (i) k, (ii) motifs against graphlets. Following domain evidence we choose the following substructure families: cycles for molecules, cliques for social networks. Best performing substructures both for GSN-e and GSN-v are reported. As can be seen, our model obtains state-of-the-art performance in most of the datasets, with a considerable margin against the main GNN baselines in some cases. GCN <ref type="bibr" target="#b103">[104]</ref> 0.469±0.002 -GIN <ref type="bibr" target="#b15">[16]</ref> 0.408±0.008 -GraphSage <ref type="bibr" target="#b104">[105]</ref> 0.410±0.005 -GAT <ref type="bibr" target="#b105">[106]</ref> 0.463±0.002 -MoNet <ref type="bibr" target="#b9">[10]</ref> 0.407±0.007 -GatedGCN <ref type="bibr" target="#b106">[107]</ref> 0.422±0.006 0.363±0.009 MPNN 0.254±0.014 0.209±0.018 MPNN-r 0.322±0.026 0.279±0.023 PNA <ref type="bibr" target="#b107">[108]</ref> 0.320±0.032 0.188±0.004 DGN <ref type="bibr" target="#b73">[74]</ref> 0.219±0.010 0.168±0.003 GNNML <ref type="bibr" target="#b108">[109]</ref> 0.1612 ± 0.006 -HIMP <ref type="bibr" target="#b109">[110]</ref> -0.151±0.006 SMP <ref type="bibr" target="#b47">[48]</ref> 0.219± 0.138± GSN 0.140±0.006 0.115±0.012</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ZINC Molecular graphs</head><p>We evaluate GSN on the task of regressing the "penalized water-octanol partition coefficient -logP" (see <ref type="bibr" target="#b110">[111]</ref>, <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref> for details) of molecules from the ZINC database <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b113">[114]</ref>. We use structural features obtained with kcycle counting and report the result of the best performing substructure w.r.t. the validation set. As dictated by the evaluation protocol of <ref type="bibr" target="#b24">[25]</ref>, the total number of parameters of the model is approximately 100K, which is achieved by selecting an appropriate network width (a GSN with 500K params attains 0.101 ± 0.010 MAE.). The data split is obtained from <ref type="bibr" target="#b24">[25]</ref> and the evaluation metric is the Mean Absolute Error (MAE). We compare against a variety of baselines, ranging from traditional message passing NNs to recent more expressive architectures <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref> and a molecular-specific one (HIMP) which is based on the junction-tree molecular decomposition <ref type="bibr" target="#b109">[110]</ref>. Wherever possible, we compare two variants, one that does not take edge features into account and one that does. In both cases, GSN achieves state-of-theart results outperforming all the baseline architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">OGB graph property prediction datasets</head><p>We use three graph property prediction datasets from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b25">[26]</ref>: ogbg-molhiv and ogbg-molpcba are molecular datasets where the task is graph-level binary classification. In ogbg-molhiv the task is to predict if a molecule inhibits HIV replication or not, while ogbg-molpcba has multiple tasks that we need to optimise for simultaneously. We also evaluate GSN on ogbg-ppa, a dataset of protein-protein association networks, where the task is to predict the taxonomic group of each graph (37-way classification) The underlying distribution of this dataset is significantly different from the other two, since it contains larger and denser graphs with community structure.</p><p>In Table <ref type="table" target="#tab_2">3</ref> we compare against the following baselines: GCN <ref type="bibr" target="#b103">[104]</ref> and GIN <ref type="bibr" target="#b15">[16]</ref>, two conventional GNNs, PNA <ref type="bibr" target="#b107">[108]</ref> and DGN <ref type="bibr" target="#b73">[74]</ref> two modern GNNs with provably increased expressivity and strong empirical performance, and HIMP, a molecule-specific GNN <ref type="bibr" target="#b109">[110]</ref>. There is a plethora of other methods that have reported results in these datasets, sometimes outperforming GSN (see the OGB public leaderboard 2 ). Note that many of them are either domain-specific, or use various tricks which are orthogonal to our work. Hence, an exhaustive comparison is beyond the scope of this experimental section, which is to highlight that GSN can be used as a plug-and-play method and boost the performance of a base architecture without any hyperparameter tuning (except for the maximum substructure size k), rather than to achieve state-of-the-art results. Following this rationale, we choose a base architecture and modify it into a GSN variant by introducing structural features in the aggregation function: cycle counts for the molecular datasets and triangle counts for ogbg-ppa. We use the following base architectures: (a) GIN and (b) GIN-VN, a variation of GIN that allows for edge features and is extended with a virtual node, i.e. a node connected to every node in the graph. For ogbg-molhiv we also used (c) DGN <ref type="bibr" target="#b73">[74]</ref> as base, a GNN that propagates messages in an anisotropic manner, based on a predefined graph vector field. Observe that the vector field is an alternative way to break local symmetries. The authors of DGN use vector fields defined by the eigenvectors of the graph, while in our case the vector field is defined by graph substructures. More information can be found in the supplementary material.</p><p>Using the evaluators provided by the authors, we report the relevant metric at the epoch with the best validation performance (substructure size is also chosen based on the validation set). By examining the results in Table <ref type="table" target="#tab_2">3</ref> the following observations can be made, (a) GSN seamlessly improves the performance of the base architecture, both in the test set and the validation set, sometimes significantly, or, in only one case, is on par. (b) Cyclical substructures are a good inductive bias when learning on molecules, confirming our results on the ZINC dataset, while the same holds for triangles in PPA networks. The latter agrees with our intuition that tasks defined on graphs with community structure correlate with the presence of triangles (or cliques), as was the case for social networks in the TU Datatsets experiments. (c) General purpose GNNs benefit from symmetry breaking mechanisms, either in the form of eigenvectors (DGN) or in the form of substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Comparison between substructure collections</head><p>In Figure <ref type="figure" target="#fig_2">4</ref> (left), we compare the training and test error for different substructure families (cycles, paths and nonisomorphic trees -for each experiment we use all the substructures of size ≤ k in the family). Additionally, we measure the "uniqueness" of the identifiers each substructure yields as follows: for each graph G in the dataset, we measure the number of unique vertex features u G (input vertex features concatenated with vertex structural identifiers for GSN-v). Then, we sum them up over the entire training set and divide by the total number of vertices, yielding the disambiguation score</p><formula xml:id="formula_13">δ = G u G G |V G | .</formula><p>The disambiguation scores for the different types of substructures are illustrated as horizontal bars in Figure <ref type="figure" target="#fig_2">4</ref> (the exact values can be found in Appendix D.4, Table <ref type="table" target="#tab_3">4</ref>).</p><p>A first thing to notice is that the training error is tightly related to the disambiguation score. As identifiers become more discriminative, the model gains expressive power. On the other hand, the test error is not guaranteed to decrease when the identifiers become more discriminative. For example, although cycles have smaller disambiguation scores, they manage to generalise much better than the other substructures, the performance of which is similar to the baseline architecture (MPNN with MLPs). This is also observed when comparing against <ref type="bibr" target="#b36">[37]</ref> (MPNN-r method in Table <ref type="table" target="#tab_1">2</ref>), where, akin to unique identifiers, random features are used to strengthen the expressivity of GNN architectures. This approach also fails to improve the baseline architecture in terms of the performance in the test set. This validates our intuition that unique identifiers can be hard to generalise when chosen in a non-permutation equivariant way and motivates once more the importance of choosing the identifiers not only based on their discriminative power, but also in a way that allows incorporating the appropriate inductive biases. Finally, we observe a substantial jump in performance when using GSN with cycles of size k ≥ 6. This is not surprising, as cyclical patterns of such sizes (e.g. aromatic rings) are very common in organic molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Generalisation</head><p>We repeat the experimental evaluation on ZINC using different fractions of the training set and compare the vanilla MPNN model against GSN. In Figure <ref type="figure" target="#fig_2">4</ref> (right), we plot the training and test errors of both methods. Regarding the training error, GSN consistently performs better, following our theoretical analysis on its expressive power. More importantly, GSN manages to generalise much better even with a small fraction of the training dataset. Observe that GSN requires only 20% of the samples to achieve approximately the same test error that MPNN achieves when trained on the entire training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Structural Features &amp; Message Passing:</head><p>We perform an ablation study on the abilities of the structural features to predict the task at hand, when given as input to a graph-agnostic network. In particular, we compare our best performing GSN with a DeepSets model <ref type="bibr" target="#b114">[115]</ref> that treats the input features and the structural identifiers as a set. For fairness of evaluation the same hyperparameter search is performed for both models (see Appendix D.6). Interestingly, as we show in Table <ref type="table" target="#tab_3">4</ref>, our baseline attains particularly strong performance across a variety of datasets and often outperforms other traditional message passing baselines. This demonstrates the benefits of these additional features and motivates their introduction in GNNs, which are unable to compute them. As expected, we observe that applying message passing on top of these features, brings performance improvements in the vast majority of the cases, sometimes considerably, as in the ZINC dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a novel way to design structureaware graph neural networks. Motivated by the limitations of traditional GNNs to capture important topological properties of the graph, we formulate a message passing scheme enhanced with structural features that are extracted by subgraph isomorphism. We show both theoretically and empirically that our construction leads to improved expressive power and attains state-of-the-art performance in realworld scenarios. In future work, we will further explore the expressivity of GSNs as an alternative to the k-WL tests, as well as their generalisation capabilities. Another important direction is to infer prominent substructures directly from the data and explore the ability of graph neural networks to compose substructures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Vertex (left) and edge (right) induced subgraph counting for a 3-cycle and a 3-path. Counts are reported for the blue vertex on the left and for the blue edge on the right. Different colors depict orbits.</figDesc><graphic url="image-1.png" coords="3,48.01,43.69,252.00,87.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: SR graphs isomorphism test (log scale, smaller values are better). Different colours indicate different substructure sizes.</figDesc><graphic url="image-4.png" coords="6,312.00,440.39,252.00,157.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (Left) Train (dashed) and test (solid) MAEs for Path-, Tree-and Cycle-GSN-EF as a function of the maximum substructure size k. Vertical bars indicate standard deviation; horizontal bars depict disambiguation scores δ. (Right) Train (dashed) and test (solid) MAEs for GSN-EF(blue) and MPNN-EF(red) as a function of the dataset fraction used for training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Graph classification accuracy on TUD Dataset. First, Second, Third best methods are highlighted. For GSN, the best performing structure is shown. * Graph Kernel methods.</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>Proteins</cell><cell>NCI1</cell><cell>Collab</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell>RWK* [96]</cell><cell>79.2±2.1</cell><cell>55.9±0.3</cell><cell>59.6±0.1</cell><cell>&gt;3 days</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GK* (k=3) [84]</cell><cell>81.4±1.7</cell><cell>55.7±0.5</cell><cell>71.4±0.31</cell><cell>62.5±0.3</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PK* [97]</cell><cell>76.0±2.7</cell><cell>59.5±2.4</cell><cell>73.7±0.7</cell><cell>82.5±0.5</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WL kernel* [98]</cell><cell>90.4±5.7</cell><cell>59.9±4.3</cell><cell>75.0±3.1</cell><cell>86.0±1.8</cell><cell>78.9±1.9</cell><cell>73.8±3.9</cell><cell>50.9±3.8</cell></row><row><cell>GNTK* [99]</cell><cell>90.0±8.5</cell><cell>67.9±6.9</cell><cell>75.6±4.2</cell><cell>84.2±1.5</cell><cell>83.6±1.0</cell><cell>76.9±3.6</cell><cell>52.8±4.6</cell></row><row><cell>DCNN [100]</cell><cell>N/A</cell><cell>N/A</cell><cell>61.3±1.6</cell><cell>56.6±1.0</cell><cell>52.1±0.7</cell><cell>49.1±1.4</cell><cell>33.5±1.4</cell></row><row><cell>DGCNN [101]</cell><cell>85.8±1.8</cell><cell>58.6±2.5</cell><cell>75.5±0.9</cell><cell>74.4±0.5</cell><cell>73.8±0.5</cell><cell>70.0±0.9</cell><cell>47.8±0.9</cell></row><row><cell>IGN [22]</cell><cell>83.9±13.0</cell><cell>58.5±6.9</cell><cell>76.6±5.5</cell><cell>74.3±2.7</cell><cell>78.3±2.5</cell><cell>72.0±5.5</cell><cell>48.7±3.4</cell></row><row><cell>GIN [16]</cell><cell>89.4±5.6</cell><cell>64.6±7.0</cell><cell>76.2±2.8</cell><cell>82.7±1.7</cell><cell>80.2±1.9</cell><cell>75.1±5.1</cell><cell>52.3±2.8</cell></row><row><cell>PPGNs [24]</cell><cell>90.6±8.7</cell><cell>66.2±6.6</cell><cell>77.2±4.7</cell><cell>83.2±1.1</cell><cell>81.4±1.4</cell><cell>73.0±5.8</cell><cell>50.5±3.6</cell></row><row><cell>Natural GN [72]</cell><cell>89.4±1.60</cell><cell>66.8±1.79</cell><cell>71.7±1.04</cell><cell>82.7±1.35</cell><cell>N/A</cell><cell>74.8±2.01</cell><cell>51.3±1.50</cell></row><row><cell>WEGL [102]</cell><cell>N/A</cell><cell>67.5±7.7</cell><cell>76.5±4.2</cell><cell>N/A</cell><cell>80.6±2.0</cell><cell>75.4±5.0</cell><cell>52.3±2.9</cell></row><row><cell cols="2">GIN+GraphNorm [103] 91.6 ± 6.5</cell><cell>64.9 ± 7.5</cell><cell>77.4 ± 4.9</cell><cell>82.7 ± 1.7</cell><cell>80.2 ± 1.0</cell><cell>76.0 ± 3.7</cell><cell>N/A</cell></row><row><cell>GSN-e</cell><cell>90.6±7.5</cell><cell>68.2±7.2</cell><cell>76.6±5.0</cell><cell>83.5± 2.3</cell><cell>85.5±1.2</cell><cell>77.8±3.3</cell><cell>54.3±3.3</cell></row><row><cell></cell><cell>6 (cycles)</cell><cell>6 (cycles)</cell><cell>4 (cliques)</cell><cell>15 (cycles)</cell><cell>3 (triangles)</cell><cell>5 (cliques)</cell><cell>5 (cliques)</cell></row><row><cell>GSN-v</cell><cell>92.2±7.5</cell><cell>67.4±5.7</cell><cell>74.59±5.0</cell><cell>83.5±2.0</cell><cell>82.7±1.5</cell><cell>76.8±2.0</cell><cell>52.6±3.6</cell></row><row><cell></cell><cell>12 (cycles)</cell><cell>10 (cycles)</cell><cell>4 (cliques)</cell><cell>3 (triangles)</cell><cell>3 (triangles)</cell><cell>4 (cliques)</cell><cell>3 (triangles)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>MAE in ZINC.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>MAE (EF)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>2. https://ogb.stanford.edu/docs/leader graphprop/ Test and Validation performance metrics for 3 OGB graph property prediction datasets. First, Second, Third</figDesc><table><row><cell>Method</cell><cell cols="2">ogbg-molhiv Test ROC-AUC Val. ROC-AUC</cell><cell cols="2">ogbg-molpcba Test AP Val. AP</cell><cell cols="2">ogbg-ppa Test Accuracy Val. Accuracy</cell></row><row><cell>GCN [104]</cell><cell>0.7606 ± 0.0097</cell><cell>0.8204 ± 0.0141</cell><cell>0.2020 ± 0.0024</cell><cell>0.2059 ± 0.0033</cell><cell>0.6839 ± 0.0084</cell><cell>0.6497 ± 0.0034</cell></row><row><cell>HIMP [110]</cell><cell>0.7880 ± 0.0082</cell><cell>N/A</cell><cell>0.2739 ± 0.0017</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PNA [108]</cell><cell>0.7905 ± 0.0132</cell><cell>0.8519 ± 0.0099</cell><cell cols="2">0.2838 ± 0.0035 0.2926 ± 0.0026</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GIN [16]</cell><cell>0.7558 ± 0.0140</cell><cell>0.8232 ± 0.0090</cell><cell>0.2266 ± 0.0028</cell><cell>0.2305 ± 0.0027</cell><cell>0.6892 ± 0.0100</cell><cell>0.6562 ± 0.0107</cell></row><row><cell>GIN+VN [16]</cell><cell>0.7707 ± 0.0149</cell><cell>0.8479 ± 0.0068</cell><cell>0.2703 ± 0.0023</cell><cell>0.2798 ± 0.0025</cell><cell>0.7037 ± 0.0107</cell><cell>0.6678 ± 0.0105</cell></row><row><cell>DGN (eigenvectors) [74]</cell><cell>0.7970 ± 0.0097</cell><cell>0.8470 ± 0.0047</cell><cell cols="2">0.2885 ± 0.0030 0.2970 ± 0.0021</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GSN (GIN base)</cell><cell>0.7606 ± 0.0174</cell><cell>0.8517 ± 0.0090</cell><cell>0.2508 ± 0.0023</cell><cell>0.2542 ± 0.0021</cell><cell>0.6877 ± 0.0022</cell><cell>0.6489 ± 0.0065</cell></row><row><cell>GSN (GIN+VN base)</cell><cell>0.7799 ± 0.0100</cell><cell>0.8658 ± 0.0084</cell><cell cols="2">0.2816 ± 0.0047 0.2913 ± 0.0017</cell><cell>0.7119 ± 0.0120</cell><cell>0.6716 ± 0.0078</cell></row><row><cell>GSN (DGN base)</cell><cell>0.8039 ± 0.0090</cell><cell>0.8473 ± 0.0096</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison between DeepSets and GSN with the same structural features</figDesc><table><row><cell>Dataset</cell><cell>DeepSets</cell><cell cols="2"># params GSN</cell><cell># params</cell></row><row><cell>MUTAG</cell><cell>93.3±6.9</cell><cell>3K</cell><cell>92.8±7.0</cell><cell>3K</cell></row><row><cell>PTC</cell><cell>66.4±6.7</cell><cell>2K</cell><cell>68.2±7.2</cell><cell>3K</cell></row><row><cell>Proteins</cell><cell>77.8±4.2</cell><cell>3K</cell><cell>77.8±5.6</cell><cell>3K</cell></row><row><cell>NCI1</cell><cell>80.3 ±2.4</cell><cell>10K</cell><cell>83.5± 2.0</cell><cell>10K</cell></row><row><cell>Collab</cell><cell>80.9 ±1.6</cell><cell>30K</cell><cell>85.5±1.2</cell><cell>52K</cell></row><row><cell>IMDB-B</cell><cell>77.1 ±3.7</cell><cell>51K</cell><cell>77.8±3.3</cell><cell>65K</cell></row><row><cell>IMDB-M</cell><cell>53.3 ±3.2</cell><cell>68K</cell><cell>54.3±3.3</cell><cell>66K</cell></row><row><cell>ZINC</cell><cell cols="2">0.288 ±0.003 366K</cell><cell cols="2">0.108 ±0.018 385K</cell></row><row><cell cols="2">ogbg-molhiv 77.34±1.46</cell><cell>3.4M</cell><cell>77.99±1.00</cell><cell>3.3M</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: BEIHANG UNIVERSITY. Downloaded on March 30,2022 at 12:42:49 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>This research was partially supported by the ERC Consolidator Grant No. 724228 -LEMAN (GB and MB). The work of GB is partially funded by a PhD scholarship from the Department of Computing, Imperial College London. SZ acknowledges support from the EPSRC Fellowship DE-FORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1) and a Google Faculty award. MB acknowledges support from Google Faculty awards and the Royal Society Wolfson Research Merit award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Giorgos</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Machine learning for scent: Learning generalizable perceptual representations of small molecules</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gerkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10685</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2693" to="2702" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural 3d morphable models: Spiral convolutional networks for 3d shape representation learning and generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bokhnyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7213" to="7222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05425</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">805</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference (ESWC), ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: Subgraph counts and related graph properties</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fuhlbr Ück</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of Computation Theory (FCT), ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11651</biblScope>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The strength of weak ties: A network theory revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Granovetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sociological Theory</title>
				<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<title level="s">ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Congruent graphs and the connectivity of graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="168" />
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
		<ptr target="https://www.iti.zcu.cz/wl2018/pdf/wlpapertranslation.pdf" />
	</analytic>
	<monogr>
		<title level="j">NTI, Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
	<note>english translation is</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="868" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A congruence theorem for trees</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="961" to="968" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A collection of mathematical problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ulam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>Interscience Publishers</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Small graphs are reconstructible</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian J. Combinatorics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="126" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The weisfeiler-lehman method and graph isomorphism testing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling interactome: scale-free or geometric</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Corneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jurisica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3508" to="3515" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM</title>
				<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6295</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural messagepassing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reverse search for enumeration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Avis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete applied mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="21" to="46" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An algorithm for subgraph isomorphism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ullmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The index-based subgraph matching algorithm with general symmetries (ismags): exploiting symmetry for faster subgraph enumeration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Houbraken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Audenaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pickavet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">e97896</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A (sub)graph isomorphism algorithm for matching large graphs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1367" to="1372" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Introducing VF3: A new algorithm for subgraph isomorphism</title>
		<author>
			<persName><forename type="first">V</forename><surname>Carletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saggese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph-Based Representations in Pattern Recognition -11th IAPR-TC-15 International Workshop</title>
		<title level="s">Proceedings, ser. Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Anacapri, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-16">2017. May 16-18, 2017. 2017</date>
			<biblScope unit="volume">10310</biblScope>
			<biblScope unit="page" from="128" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Turbo iso : towards ultrafast and robust subgraph isomorphism search in large graph databases</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013</title>
				<meeting>the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">June 22-27, 2013. 2013</date>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A parallel, backjumping subgraph isomorphism algorithm using supplemental graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mccreesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prosser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles and Practice of Constraint Programming -21st International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Pesant</surname></persName>
		</editor>
		<meeting><address><addrLine>Cork, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-08-31">2015. August 31 -September 4, 2015. 2015</date>
			<biblScope unit="volume">9255</biblScope>
			<biblScope unit="page" from="295" to="312" />
		</imprint>
	</monogr>
	<note>Proceedings, ser</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A combinatorial approach to graphlet counting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hocevar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btt717</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btt717" />
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="565" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A faster algorithm for detecting network motifs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wernicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithms in Bioinformatics, 5th International Workshop, WABI, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3692</biblScope>
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient detection of network motifs</title>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="359" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">FANMOD: a tool for fast network motif detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wernicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rasche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1152" to="1153" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Frequent subgraph mining by walking in order embedding space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshops (ICMLW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Neural subgraph matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03092</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7090" to="7099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">From graph lowrank global attention to 2-fwl approximation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4083" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Locality in distributed graph algorithms</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Journal on Computing</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">What can be computed locally?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Natural graph networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08349</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Distance encodingdesign provably more powerful gnns for structural representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Random graph isomorphism</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the power of combinatorial and spectral invariants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ürer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear algebra and its applications</title>
		<imprint>
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2373" to="2380" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the combinatorial power of the weisfeiler-lehman algorithm</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithms and Complexity (CIAC), ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10236</biblScope>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Lovász meets weisfeiler and leman</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming (ICALP), ser. LIPIcs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl -Leibniz-Zentrum f ür Informatik</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Local structure in social networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Uncovering biological network function via graphlet degree signatures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Milenković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Graphlet-based characterization of directed networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarajlić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malod-Dognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><forename type="middle">N</forename><surname>Yavero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">35098</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cyclic pattern kernels for predictive graph mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fast neighborhood subgraph pairwise distance kernel</title>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">De</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, (ICML)</title>
				<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Graph homomorphism convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">motif2vec: Motif aware node representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Dareddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1052" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Yadkori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09303</idno>
		<title level="m">Hone: Higher-order network embeddings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Motifnet: A motifbased graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Data Science Workshop (DSW)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Meta-gnn: metagraph neural network for semi-supervised learning in attributed heterogeneous information networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
				<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">A hierarchy of graph neural networks based on learnable local features</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05256</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Near: Neighborhood edge aggregator for graph classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02746</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Neural message passing on high order paths</title>
		<author>
			<persName><forename type="first">D</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops (NeurIPSW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory and Kernel Machines (COLT)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2777</biblScope>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Óczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<title level="s">ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<title level="s">ICLR). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Residual gated graph convnets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Breaking the limits of message passing graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Héroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ga Üzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2328" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">ZINC: A free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
