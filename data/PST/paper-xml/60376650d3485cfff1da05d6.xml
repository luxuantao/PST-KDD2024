<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3050058</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAM: Modeling Scene, Object and Action With Semantics Attention Modules for Video Recognition</head><p>Xing Zhang , Zuxuan Wu , and Yu-Gang Jiang Abstract-Video recognition aims at understanding semantic contents that normally involve the interactions of humans and related objects under certain scenes. A common practice to improve recognition accuracy is to combine object, scene and action features for classification directly, assuming that they are explicitly complementary. In this paper, we break down the fusion of three features into two pairwise feature relation modeling processes, which mitigates the difficulty of correlation learning in high dimensional features. Towards this goal, we introduce a Semantics Attention Module that captures the relations of a pair of features by refining the relatively "weak" feature with the guidance from the "strong" feature using attention mechanisms. The refined representation is further combined with the "strong" feature using a residual design for downstream tasks. Two SAMs are applied in a Semantics Attention Network (SAN) for improving video recognition. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet v1.3-the proposed approach achieves better results while requiring much less computational effort than alternative methods. Index Terms-Video recognition, scene, object, feature fusion, semantics attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE astounding growth of web videos has encouraged the development of techniques that can achieve automated recognition of video contents for a wide range of realworld applications like video search, indexing, recommendation, etc. However, classifying untrimmed and unedited video clips into different categories is a challenging problem since these videos usually contain severe camera motion and cluttered background <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, and more importantly, there are large intra-class variations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. For example, a "dancing" event can be captured from different perspectives in a theater, and performed by either professional or amateur dancers. As a result, successful video recognition pipelines usually integrate various visual and acoustic feature representations, describing videos comprehensively, to obtain decent classification performance. Xing Zhang is with the Academy for Engineering and Technology, Fudan University, Shanghai 200433, China (e-mail: zhangxing18@fudan.edu.cn).</p><p>Zuxuan Wu and Yu-Gang Jiang are with the School of Computer Science, Fudan University, Shanghai 200433, China (e-mail: zxwu@cs.umd.edu; ygj@fudan.edu.cn).</p><p>Color versions of one or more figures in this article are available at https: //doi.org/10.1109/TMM.2021.3050058.</p><p>Digital Object Identifier 10.1109/TMM.2021.3050058 Fig. <ref type="figure">1</ref>. Conceptual overview of the proposed approach. While using scene clues like "professional football stadium" and "outdoor grass fields" can help identifying "professional" and "amateur football playing" when their action features are similar, the detected scene like "basketball court" is however harmful when recognizing the "playing badminton" action.</p><p>Standard visual features include appearance features to capture texture and color information in video frames (e.g., HoG <ref type="bibr" target="#b6">[7]</ref>, CNNs operating on RGB frames <ref type="bibr" target="#b7">[8]</ref>, etc) and motion features depicting human movements (e.g., HoF, MBH <ref type="bibr" target="#b8">[9]</ref>, CNNs operating on stacked optical flow images <ref type="bibr" target="#b7">[8]</ref>). As an action/event is usually defined by an actor making specific movements or interacting with other objects (people) under certain scenes, a few approaches have been introduced to model object and scene information in videos explicitly to assist the recognition of categories of interest <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. This makes intuitive sense. For example, it is likely that a "diving" action will occur when there is a large "professional swimming pool" and a "playing basketball" action is reasonable when we observe a "basketball court". In addition, we might expect that scenes like "indoor gymnasium" or "outdoor playground" can help identifying "professional American football" or "amateur American football" (See the top of Fig. <ref type="figure">1</ref>). Assuming that objects and scenes are always complementary to actions, these approaches simply concatenate features into a unified vector for prediction. However, is it really the case that objects and scenes are always beneficial to action features? In other words, are actions bounded by scenes?</p><p>As mentioned earlier, large intra-class variations exist in web videos since they mostly contain user-generated contents. Therefore, actions in these videos are in a more free form compared to professionally made videos. For instance, a person can run outdoor on tracks and along rivers/lakes or indoor in living rooms (e.g., "baby running")-these scenes are significantly different.</p><p>Consequentially, directly combining scene features with other features can lead to clear performance drops <ref type="bibr" target="#b12">[13]</ref>. Fig. <ref type="figure">1</ref> further gives an example that the use of scenes features leads to incorrect prediction of the "playing badminton" action, where it is occurred in a "basketball court" rather than a typical "badminton court". We posit that scene features might still be helpful but their correlations with other features should be carefully modeled instead of a straightforward concatenation.</p><p>It is also worth mentioning that typical feature fusion methods are usually performed in a single step, combining all features directly as inputs of a neural network <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> or a kernel SVM. While conceptually easy, learning the correlations in this way among different features (sometimes they are even in disparate modalities space <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>) is difficult, as they are usually high-dimensional. Therefore, we propose to break down the fusion process into pairwise relationship modeling. Given object, scene and action features to fuse, we generate two pairs of features, and for each pair we aim to derive a powerful feature representation by considering their relationships. This allows easier modeling of feature correlations while achieves a decent trade-off between accuracy and computational cost, as will be shown in the experiments.</p><p>In light of this, to combine action features with object and scene features, we present a Semantics Attention Network (SAN) with two Semantics Attention Modules (SAM), i.e., Action-Scene SAM (AS-SAM) and Object-Scene SAM (OA-SAM), each of which takes in a pair of high-level features and captures their correlations explicitly. SAM considers the "weak" feature as a memory and the "strong" feature as a query, and estimates an attention map, explicitly modeling their correlations to guide the "weak" feature to borrow useful information from the "strong" one. Note that the "weak" and "strong" feature are set based on the relationship between the target feature and video recognition task. As aforementioned, previous work <ref type="bibr" target="#b12">[13]</ref> show scene feature is relatively "weak" for video recognition task, that is because various videos have the similar scenes. Also, actions are usually repeated in videos, as shown in Fig. <ref type="figure">1</ref>, "playing American football" can be occurred in "outdoor playground" or "indoor gymnasium". By contrast, objects are "strong" identities for classifying actions, because objects contain high-level semantics indicating the likelihood of the presence of objects and are easier to be transferred across datasets compared to action features without specific semantic meanings, especially for video datasets containing object classes (e.g. FCVID <ref type="bibr" target="#b18">[19]</ref>, YouTube-8M <ref type="bibr" target="#b19">[20]</ref>, SOA <ref type="bibr" target="#b12">[13]</ref> etc.). To validate the assumption and effectiveness of SAM, we conduct extensive experiments on object, scene and action features to see the accuracies when they are used for classification alone, and the results indicate our assumption is reasonable, this idea can also be generalized to different datasets.</p><p>Our main contributions are summarized as follows: (1) we introduce a Semantics Attention Network, which breaks down the fusion of object, scene and action features into a two-step process, allowing easier correlations modeling while offering a balance between computational cost and accuracy; (2) we present a Semantics Attention Module that refines the "weak" feature in a pair by attending to the "strong" feature; and the refined feature is further used in combination with the "strong" one using a residual design; (3) we conduct extensive experiments on Fudan-Columbia Video Dataset (FCVID) and ActivityNet v1.3, the results demonstrate that the proposed fusion method offers the best trade-off between computational cost and accuracies compared to alternative methods, and SAN achieves competitive results with state-of-the-art video recognition approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review several research directions that are related to the proposed approach and discuss how our framework is different from existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Recognition With Objects and Scenes</head><p>Extensive studies have been conducted on generic video classification and action recognition in recent years (e.g., plug-in modules <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref> and end-to-end models <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b27">[28]</ref>). In contrast to these approaches that aim to model temporal relationships in videos, we focus on how to leverage objects and scenes for video classification. <ref type="bibr" target="#b28">[29]</ref> constructs action atoms and certain scene background and demonstrates that action retrieval performance can be improved by modeling action, scene and object associations. <ref type="bibr" target="#b13">[14]</ref> presents an OS-CNN network to extract object and scene context separately and then fuse their scores for event understanding. <ref type="bibr" target="#b9">[10]</ref> demonstrates that using features from the last layer of a CNN model trained to recognize 15 000 objects can achieve decent performance for action recognition. <ref type="bibr" target="#b11">[12]</ref> designs an object and scene fusion network to discover how video classes are related to scene and object semantics, and how to use mined relationships for zero-shot learning. <ref type="bibr" target="#b10">[11]</ref> proposes a spatial-aware object embedding to localize and classify actions by exploring relations between actors and relevant objects. <ref type="bibr" target="#b29">[30]</ref> represents videos as graphs by GCN, they utilize object region proposals as graph nodes to model spatial-temporal relations and long-range dependencies. Related is also <ref type="bibr" target="#b12">[13]</ref>, which introduces a large-scale, multi-label and multi-task video data set. Based on the dataset, the authors investigate correlations among scene, object and action classification tasks, and their experimental results show that scene features can incur clear performance drops when directly combined with other types of features. These approaches fuse object, scene and action features directly by concatenation for classification, assuming that these features are indeed complementary. Instead of simply concatenating features, we propose to model pairwise feature relationships explicitly in SAM with attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Successful video recognition systems usually involves the fusion of multiple features that capture video information from different perspectives (e.g., audio features and visual features).</head><p>There are two widely adopted fusion methods, i.e., early fusion and late fusion <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Early fusion aims to combine features directly and train a single classifier to predict video labels. A straightforward way for early fusion is to combine features Fig. <ref type="figure">2</ref>. Overview of Semantics Attention Network with AS-SAM and OS-SAM. Object, scene and action features are first computed on sampled frames using off-the-shelf CNN models pretrained on large-scale datasets, which are further transformed with self-attention to consider temporal relationships among frames. Each SAM models pairwise feature relations and produces a better feature representation. The outputs from the two SAMs are further combined for predicting video categories. See texts for more details. linearly for classification <ref type="bibr" target="#b32">[33]</ref>. Researchers also use neural networks to first transform each feature into higher space, and then perform concatenation <ref type="bibr" target="#b33">[34]</ref> to derive a unified representation for final classification. On the other hand, late fusion learns a model separately for each feature <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> and then combines prediction scores from these models linearly as final prediction results <ref type="bibr" target="#b36">[37]</ref>. Both approaches are popular due to their effectiveness and simplicity. However, they simply assume features or prediction scores are explicitly complementary to one another, which is not usually the case as shown in <ref type="bibr" target="#b12">[13]</ref> that "weak" features like scenes are sometimes harmful to the overall system. In this work, we do not assume features are explicitly complementary and, instead, focus on improving "weak" features by using "strong" features as a guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Mechanisms</head><p>Attention in deep neural networks is motivated by human vision systems that pay visual attention to particular regions in images <ref type="bibr" target="#b37">[38]</ref> or words in sentences <ref type="bibr" target="#b38">[39]</ref> in order to extract useful information of interest efficiently. In computer vision and multimedia, attention has been a critical component in various captioning models that attempt to align words with corresponding regions in images <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b44">[45]</ref> by computing their similarities in a shared space. For video recognition, recent approaches have demonstrated that attention is an effective way of modeling temporal and spatial relationships among different frames in videos <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b50">[51]</ref>. Besides, attention is utilized for selecting frames <ref type="bibr" target="#b51">[52]</ref> as well as fine-grained problems <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In contrast to these approaches, we build upon attention mechanisms to estimate the relationships among different types of features in order to derive better representations to prevent "weak" features from hurting the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>Our goal is to design an effective fusion strategy that explores the relationships among object, scene and action features explicitly when recognizing video categories. To this end, we first introduce the features that are used to represent a video clip (Section III-A). Then, we present the proposed Semantics Attention Module (Section III-B), which takes a pair of features as inputs and captures their correlations with attention mechanisms. Finally, we demonstrate how to leverage the SAM module as a plugin component to fuse object, scene and action features for improving video recognition (Section III-C). Fig. <ref type="figure">2</ref> illustrates the overview of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object, Scene, and Action Features</head><p>As actions are related to objects and scenes, object and scene representations are used as contextual clues to assist the recognition of video classes. Following <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we extract high-level object, scene and action features from CNNs that are pretrained on large-scale datasets. For each video, we first extract 64 frames and compute three types of features with these frames, detailed in the following.</p><p>Action Representations: We use an I3D model <ref type="bibr" target="#b24">[25]</ref> with a backbone network of InceptionV1 pretrained on Kinetics <ref type="bibr" target="#b54">[55]</ref> to compute action features. The I3D model takes in 64 frames of a video clip as inputs and performs 3D convolution to model temporal information, and we use the output from the last avg_pool layer as the action representation: A, where A ∈ R 7×1024 .</p><p>Object Representations: To extract object features, we use a ResNet-101 network <ref type="bibr" target="#b55">[56]</ref> pretrained on ImageNet. For each video frame, we obtain the classification scores, i.e., a 1000-D vector, from the last layer of the ResNet model, where each element in the vector represents the probability of a potential object in the frame. We then have the object representation O, denoted as O ∈ R 64×1000 .</p><p>Scene Representations: We utilize a VGG16 model <ref type="bibr" target="#b36">[37]</ref> pretrained on the Places365 dataset <ref type="bibr" target="#b56">[57]</ref> to extract scene features. We take the output from the last layer in the VGG16 model operating on each sampled video frame, and use the 365-D feature where each entry indicates the likelihood of a corresponding scene as the scene features for the frame. Then, we have the scene representation S ∈ R 64×365 for the video clip.</p><p>Self-attention for Temporal Modeling: Self-attention is usually used to relate different positions of a sequence in order to derive a representation that considers the relations of different elements (e.g., words) in a sequence. It has also been used to model the relations between different pixels in images <ref type="bibr" target="#b20">[21]</ref>. Instead of performing average-pooling over multiple frames to obtain video-level features, we use self-attention <ref type="bibr" target="#b38">[39]</ref> to assign weights to different frames to obtain a video-level representation. In particular, given feature representations of a sequence V ∈ R L×D , where L is the sequence length and D is the dimension of each feature vector in the sequence. We wish to learn to linearly combine different frame features into a global vector, and hence we compute an attention vector a with V as inputs:</p><formula xml:id="formula_0">a = softmax w s 2 W s 1 V T , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where W s 1 ∈ R D×D is a weight matrix mapping V into dimension D; w s 2 ∈ R 1× D is a weight vector. Then each element in the resulting attention vector a ∈ R 1×L denotes the importance of the corresponding frame. We then can obtain a global vector to represent the video as:</p><formula xml:id="formula_2">V = a • V. (2)</formula><p>However, the notion of frame importance is not unique-it can be measured from different perspectives, for example, when recognizing a "making a pizza" event, some frames are important for identifying the sub-action "making dough" while other frames are important for the sub-action "adding toppings". Therefore, using one vector might fail to cover the complete semantics in the video; we use P attention vectors instead. We replace</p><formula xml:id="formula_3">w s 2 with W s 2 ∈ R P × D = [w 1 s 2 , w 2 s 2 , . . . , w P s 2 ].</formula><p>Then the attention vector becomes a matrix α = [a 1 , a 2 , . . . , a P ], and now V is computed as:</p><formula xml:id="formula_4">V ∈ R P ×D = α • V. (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>This is similar in spirit to multi-head attention <ref type="bibr" target="#b38">[39]</ref> that is widely used in transformer models for NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantics Attention Module</head><p>We now introduce the Semantics Attention Module that models pairwise feature relationships with attention mechanisms.</p><p>As fusion is expected to offer better results compared to using individual features alone, we refine the feature with worse performance by learning what kind of information can be borrowed from the relatively "strong" feature instead of simply concatenating features assuming they are complimentary and all dimensions are useful. Towards this goal, given two features, SAM considers the relatively "weak" feature as a memory (M ) and the "strong" feature as a query (Q) and it refines the "weak" feature by attending to learned locations in the "strong" feature. The refined feature (M ref ined ) is further used in combination with the "strong" feature Q with a residual design:</p><formula xml:id="formula_6">QM = λM ref ined + (1 − λ)Q, (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where λ is a learnable parameter, balancing the trade-off between the refined feature and the original feature. This is to ensure that the module can learn to discard the "weak" feature when there is no useful information. In the following, we introduce how to compute M ref ined with attention.</p><p>Formally, let M ∈ R L×N denote the memory (i.e., the "weak" feature) with a sequence of L features (L frames) whose dimension is N , and Q ∈ R T ×K represent the query (i.e., the "strong" feature), a sequence of features whose length is T and the feature dimension is K. The memory is first mapped to an "attention embedding" and a "semantic embedding" with W a ∈ R N ×K and W s ∈ R N ×K , respectively:</p><formula xml:id="formula_8">M a ∈ R L×K = M • W a ,<label>(5)</label></formula><formula xml:id="formula_9">M s ∈ R L×K = M • W s , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where W a and W s map each row in M , i.e., a feature vector at a particular time step, to the same dimension as a query vector. Then the "strong" feature representation is used to query the attention embedding M a to obtain an attention map M att , estimating the relatedness between the two features:</p><formula xml:id="formula_11">M att ∈ R L×T = softmax(M a • Q T ),<label>(7)</label></formula><p>where softmax is performed on M a • Q T (i.e., the sum of all elements in M att equals to 1). The i-th row and j-th column in M att indicates the relatedness between the i-th time step in M and the j-th step in the query Q, explicitly modeling feature correlations over time. We then use the derived attention map to refine the semantic embedding M s :</p><formula xml:id="formula_12">M ref ined ∈ R T ×K = M T att • M s . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>Note that the attention map is a matrix, which is different from typical attention in the form of vectors as we wish to transform M s to have the same dimension as Q such that the residual design in Eqn. 4 can be used to prevent harmful "weak" features from hurting the performance. In addition, the reason that we apply the attention map on M s instead of on M a is to disentangle correlation learning with feature refinement-M a focuses on learning how the "weak" feature is related to the "strong" feature while M s , independent of the guidance from the query feature, preserves more semantics from M . Finally, the refined "weak" feature M ref ined is shortcut by the "strong" feature, as in Eqn. 4, to derive QM ∈ R T ×K , which is flattened and then used for classification with a fully-connected layer. The overall architecture is presented in Fig. <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Attention Network</head><p>As our goal is to fuse action features with object and scene representations that provide contextual clues, we now move on to discuss how to plug the SAM, which is designed for pairwise correlations modeling, into the Semantic Attention Network (SAN) to combine object, scene and action features.</p><p>Recall that SAM uses the "strong" feature as a query to refine the relatively "weak" feature represented as a memory, the "weak" and "strong" feature are set based on the relationship between the target feature and video recognition task. We assume scene as "weak" feature for video recognition task because similar scenes occur in various videos, actions are also usually repeated in videos, as shown in Fig. <ref type="figure">1</ref>. By contrast, objects are "strong" identities for classifying actions because objects contain high-level semantics indicating the likelihood of the presence of objects and are easier to be transferred across datasets compared to action features without specific semantic meanings. To validate our assumption, we conduct experiments to see the accuracies when object, scene and action features are used for classification alone, and we empirically found their performance in the order of: O &gt; A &gt; S which indicates our assumption is reasonable. This motivates us to generate pairs based on the performance of features, therefore, we adopt two SAMs-an AS-SAM (query S with A) and an OA-SAM (query A with O). The AS-SAM measures the relations between actions and scenes while the OA-SAM computes the similarities between actions and objects. The resulting two representations are further integrated for final predictions. The breakdown of a typical one-step fusion process of three features into two steps using pairwise relations modeling facilities easier learning as these feature relationships are usually highly non-linear and hard to be captured.</p><p>Note that this fusion method is purely based on attention structures and the training parameters are only those weight matrices for transformation purposes, and therefore the framework is lightweight, offering a decent trade-off between computational cost and accuracy (as will be demonstrated in the experiments). It is also worth noting although we mainly experiment with three different features, the proposed framework is applicable to the fusion of multiple features. In addition, we concatenate the two representations from AS-SAM and OS-SAM for final predictions rather than performing a cascaded attention (i.e., using O to query the results of AS-SAM) because that the output of AS-SAM are already very abstract, discovering its correlations is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification Objective</head><p>We apply cross-entropy (CE) loss to all experiments, since CE loss is a typical learning objective for multi-class video recognition problem.</p><formula xml:id="formula_14">L Cls (θ p ) = − C c=1 y c log p c . (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where y c is ground truth, p c is prediction, C is number of classes, θ p is the parameters to be optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In this section, we first introduce the experimental setup, including datasets, evaluation metrics, and implementation details. Then we present main results and provide discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We adopt two large-scale video recognition datasets, Fudan-Columbia Video Datasets (FCVID) <ref type="bibr" target="#b18">[19]</ref> and ActivityNet v1.3 <ref type="bibr" target="#b57">[58]</ref>, to evaluate the performance of the proposed approach.</p><p>FCVID: FCVID <ref type="bibr" target="#b18">[19]</ref> is a large-scale video dataset, where videos are collected from YouTube and Flicker. This dataset includes a total of 91 223 videos manually annotated into 239 categories; the average duration of videos is 167 seconds. These categories cover a wide range of topics in daily lives. The dataset is split evenly into a training set with 45 611 videos and a testing set with 45 612 videos <ref type="bibr" target="#b18">[19]</ref>.</p><p>ActivityNet v1.3: ActivityNet v1.3 <ref type="bibr" target="#b57">[58]</ref> contains 19 994 videos, totaling 648 hours, belonging to 200 activity classes. It is further split into a training set with 10 024 videos, a validation set with 4926 videos, and a testing set with 5044 videos. Since the labels of the testing set are withheld, we report performance on the validation set.</p><p>Evaluation Metric: On both datasets, we compute average precision for each class, and use mean average precision (mAP) across all categories as the evaluation metric for comparisons.</p><p>Implementation Details: We extract frames from FCVID and ActivityNet at 1 fps, then we uniformly sample 64 frames. Each frame is then scaled and cropped in the center to obtain For all experiments, we set the initial learning rate to be 0.0002, and we train with back propagation using Adam as the optimizer with β 1 = 0.9 and β 2 = 0.999. To speed up convergence, we clip gradients for all parameters such that their 2 -norm is smaller than 3.0. The mini-batch size is set to 128 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the SAM module:</head><p>To demonstrate the effectiveness of the proposed SAM module for pairwise relations modeling, we report the performance of using object (O), scene (S), and action (A) features alone and pairwise fusion of different features. We compute the mAP of SAM(A, B), which applies SAM by taking A as the query and B as the memory and compare its performance with Concat[A, B], denoting the concatenation of A and B directly for classification. For example, SAM(A, S) uses action features A as the query to refine scene features S whereas Concat[A, S] represents the concatenation of A and S directly. The results are summarized in Table <ref type="table" target="#tab_0">I</ref>.</p><p>From Table <ref type="table" target="#tab_0">I</ref>, we can observe that scene features obtain the worst results which is not surprising as the backgrounds in videos are generally cluttered and activities in these user-generated videos are not always constrained by scenes. On the other hand, object features achieve the best results, possibly due to the fact that they contain high-level semantics indicating the likelihood of the presence of objects and are easier to be transferred across datasets compared to action features without specific semantic meanings. When fusing two types of features together with concatenation, the performance can be generally improved slightly. But there is one exception, the combination of action features with scene features gives rise to worse results compared to simply using action features. This confirms that "weak" features might be harmful to the overall performance, and the findings are consistent with <ref type="bibr" target="#b12">[13]</ref>. However, SAM(A, S) improves the action features by 0.58% and 2.96% by refining the scene features with attention on FCVID and ActivityNet, respectively; it is also better than Concat[A, S] by 10.81% and 25.04%. This verifies that scene features are indeed helpful but need to be carefully incorporated rather than direct concatenation. Further, comparing SAM with its Concat counterpart, it is clear to see that SAM always achieves better results than simply concatenating two vectors into one, which demonstrates that SAM is able to discover more complicated feature correlations. Effectiveness of SAN to fuse object, scene and action features: We now evaluate the performance of the SAN with an AS-SAM and an OS-SAM to integrate three types of features. The results are summarized in Table <ref type="table" target="#tab_0">II</ref>. We can see that our approach clearly outperforms the concatenation of three features by clear margins. In particular, our framework achieves an mAP of 80.71% and 78.71%, offering around 7 and 14 performance gain in absolute percentage points on FCVID and Ac-tivityNet, respectively. We also compare with the cascaded attention SAM(O, SAM(A, S)), which uses the O as query and the result of SAM(A, S) as the memory. The results of SAM(O, SAM(A, S)) are worse than Concat[S, O, A] and SAN, which demonstrates that learning correlations between O and the abstract output of SAM(A, S) is hard possibly due to the fact their temporal orders are not aligned anymore.</p><p>Comparisons with State-of-the-Art fusion strategies: In addition to comparing with alternative methods, we also compare with State-of-the-Art fusion strategies from recent literature. More specifically, we compare with: (1) Concat[S, O, A], which concatenates scene, object and action features directly; (2) Bilinear Pooling-Feature Fusion (BLP-FF), which concatenates features into one vector and performs bilinear pooling to model the correlations of features; (3) Bilinear Pooling-Encoding Fusion (BLP-EF), which encodes three features with bilinear pooling respectively and combines the encodings to produce prediction scores; (4) NetVLAD-Feature Fusion (NetVLAD-FF), which combines all features and encode the derived highdimensional features using NetVLAD <ref type="bibr" target="#b58">[59]</ref> for classification; (5) NetVLAD-Encoding Fusion (NetVLAD-EF), which computes NetVLAD embedding for each feature and then concatenates these encodings to train a classifier; (6) Object Scene Fusion network (OSF) <ref type="bibr" target="#b11">[12]</ref>, which uses a network to combine three features for prediction; <ref type="bibr" target="#b6">(7)</ref> Attention Cluster (AttnCluster) <ref type="bibr" target="#b59">[60]</ref>, which applies attention cluster on each feature and then concatenates three encodings. Compared to OSF, AttnCluster additionally uses attention to inject temporal relationships. For (e), it depicts a sample from the "high jump" class where large appearance variations among frames usually exist; SAN achieves 24% lower error rates than Concat for the "high jump" category.</p><p>Table <ref type="table" target="#tab_0">III</ref> presents the results and comparisons. It is clear to see that our SAN achieves better results than simply concatenating three vectors into one, which demonstrates that SAN is able to discover more complicated feature correlations. Also, we can see that NetVLAD achieves better results on both FCVID and ActivityNet, compared to bilinear pooling and neural network based fusion methods. Our framework with carefully designed semantics attention modules outperforms NetVLAD by 0.72% and 1.77% on FCVID and ActivityNet, respectively. It is worth pointing out that NetVLAD is a very strong baseline, and has been widely used in video recognition challenges <ref type="bibr" target="#b60">[61]</ref>. To gain a better understanding on what categories SAN achieves better results, we present top-5 error rates of different fusion methods on sampled categories in ActivityNet (Table <ref type="table" target="#tab_7">IX</ref>). We can see that SAN reduces the top-5 error rates for "drinking beer" by 57.89% and "dodgeball " by 27.27% compared to Concat[S, O, A] and also achieves lower rates than other fusion approaches.</p><p>We further visualize samples from some categories that are misclassified by Concat[S, O, A] but are correctly predicted by our method. We observe that Concat[S, O, A] makes incorrect predictions when the scenes are similar. For example, a "flute performance" video clip is misclassified into the "debate" category as their backgrounds are similar (in a classroom); a clip belonging to "beach soccer" is predicted as "playing beach volleyball". It is also interesting to see that a "baseball" video is classified into "singing in KTV" when there is a TV showing the playing baseball action. This confirms that SAM can leverage useful information from the "strong" feature to suppress irrelevant information when making predictions. In addition, for the "high jump" category where there are large appearance variations among frames (See bottom row of Fig. <ref type="figure" target="#fig_2">4</ref>), our method outperforms Concat by 24%.</p><p>We also compare computational cost used in these methods in Table <ref type="table" target="#tab_2">IV</ref>. In particular, we measure computational cost with MFLOPs (mega floating point operations per second), a hardware independent metric, and Mega Params (MParams), the number of parameters. With better performance, our framework only requires a half of model parameters (operations) compared  <ref type="bibr" target="#b61">[62]</ref>, P3D <ref type="bibr" target="#b23">[24]</ref>, RRA <ref type="bibr" target="#b62">[63]</ref>, MARL <ref type="bibr" target="#b26">[27]</ref>, IMGAUD2VID <ref type="bibr" target="#b63">[64]</ref> and R(2+1)D-RGB+FLOW &amp; DSN <ref type="bibr" target="#b64">[65]</ref>. For fair comparison, we replace the action feature extraction model with BN-Inception, keeping it the same as other methods.</p><p>As shown in Table <ref type="table" target="#tab_3">V</ref>, under the same backbone and pretrained dataset Kinetics <ref type="bibr" target="#b54">[55]</ref>, our method surpasses state-of-theart method MARL by 3.85% on ActivityNet dataset. It should be noted that MARL trains the entire model end-to-end which takes a large amount of time; although our method uses three branches of scene, object and action, SAN does not need to fine-tune any branch on ActivityNet dataset. It only needs to train three self-attention blocks and two SAM modules which requires much less computational effort (please refer to Fig. <ref type="figure">2</ref> and Table <ref type="table" target="#tab_2">IV</ref>).   Compared with MARL method using SEResNeXt152 which is a computationally expensive backbone, SAN with TSM (ResNet-50) as backbone achieves similar result at 89.1%. In addition, compared to RL-based method as well as methods using multi-modality, i.e. optical flow (R(2+1)D-RGB+FLOW &amp; DSN) and sound (IMGAUD2VID), SAN only uses RGB modality, it is simple to be implemented and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We now provide some discussion and ablation studies to justify the design choices of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the residual design:</head><p>To justify the use of the residual design when computing the output of the SAM, we compare with (1) SAN w/o res, which simply uses the refined feature as outputs; (2) SAN w/res (1:1), which averages Q with M ref ined ; (3) SAN w/res mask, which replaces λ with a matrix having the same size as Q to balance Q and M ref ined using dot product. The results are summarized in Table <ref type="table" target="#tab_3">VI</ref>. We can see that the performance degrades significantly when there is no residual design, which verifies the importance of combining the refined feature with the original feature. In addition, using a scalar value is better than a matrix possibly due to there are too many parameters to learn in a matrix. Further, SAN with a learnable scalar is better than a handcrafted ratio.</p><p>Self-attention for temporal modeling: We report the performance of SAN without using self-attention to inject temporal relationships among frames in Table <ref type="table" target="#tab_5">VII</ref>. We can see that the self-attention block brings 3.84% and 7.33% improvement to FCVID and ActivityNet respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of different pairs in SAM:</head><p>We report the performance of SAM using different query-memory pairs. The results are summarized in Table <ref type="table" target="#tab_6">VIII</ref>.</p><p>We can see that using a "strong" feature as the query and a "weak" feature as the memory gives better results compared to the other way around. This is reasonable as the goal of fusion is to bring performance gain compared to using one feature alone and this is easier to be achieved when we attempt to improve the "strong" feature with a refined "weak" representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a Semantics Attention Network (SAN) that breaks down the fusion of object, scene and action features into a two-step process for improved video recognition. SAN contains two Semantics Attention Modules (SAM)-an AS-SAM and an OS-SAM for pairwise correlations modeling. In particular, we first compute object, scene and action features with off-the-shelf CNN models pretrained on large-scale datasets. These feature are further injected with temporal information using self-attention. Each SAM takes in a pair of features and captures their correlations using attention. It considers the relatively "weak" feature as a memory and attends to the "strong" feature to generate a refined representation, which is further combined with the original "strong" feature using a residual design for downstream tasks. The outputs of AS-SAM and OS-SAM are then concatenated for final predictions. Extensive experiments are conducted on two large-scale video datasets, FCVID and Activ-ityNet v1.3, the proposed approach achieves better results while requiring much less computational effort than alternative methods. Future directions for research include fine-grained video classification <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref> and equipping networks with reasoning abilities <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received November 26, 2019; revised September 14, 2020; accepted December 20, 2020. Date of publication January 8, 2021; date of current version January 21, 2022. This work was supported by National Key R&amp;D Program of China under Grant 2018YFB1004300. (Corresponding author: Yu-Gang Jiang.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Structure of a semantics attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Samples that are misclassified by Concat but are correctly predicted by our method. For (a)-(d), the sample on the left side is misclassified to be the category represented by the sample on the right side, as the scenes are similar.For (e), it depicts a sample from the "high jump" class where large appearance variations among frames usually exist; SAN achieves 24% lower error rates than Concat for the "high jump" category.</figDesc><graphic url="image-3.png" coords="7,38.39,67.25,249.26,121.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="3,57.83,86.21,477.62,208.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>OF OBJECT (O), SCENE (S) AND ACTION (A) FEATURES, AS WELL AS THEIR PAIRWISE COMBINATIONS a resolution of 224 × 224 for feature extraction.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPUTATIONAL</head><label>IV</label><figDesc>COST AND MODEL PARAMETERS OF DIFFERENT FUSION MODELS Fig.5. Trade-off between computational cost and accuracy on ActivityNet. Each circle, by size, denotes the ratio of MParams used in the corresponding model to those of OSF (the smallest model). To demonstrate the effectiveness of SAN, we compare SAN with existing SOTA video recognition methods on ActivityNet, see TableVfor detail. Specifically, we choose TSN</figDesc><table><row><cell>to NetVLAD (43.2 MParams v.s. 90.7 MParams; 87.2 MFLOPs</cell></row><row><cell>v.s. 182.1 MFLOPs). This confirms the advantage of the pro-</cell></row><row><cell>posed method in terms of the balance between computational</cell></row><row><cell>cost and accuracy. Fig. 5 further illustrates the trade-off, where</cell></row><row><cell>each circle by size represents the ratio of MParams used in the</cell></row><row><cell>corresponding model to those of OSF (we use OSF as the base-</cell></row><row><cell>line since it is the smallest model).</cell></row><row><cell>Comparison with SOTA video recognition methods:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V VIDEO</head><label>V</label><figDesc>RECOGNITION RESULTS ON ACTIVITYNET V1.3. FOR FAIR COMPARISON, WE REPLACE THE ACTION (A) FEATURE EXTRACTION MODEL, I.E. I3D, TO BN-INCEPTION. NOTE THAT THE OBJECT (O), SCENE (S) AND ACTION (A) FEATURE EXTRACTION MODELS ARE NOT FINE-TUNED ON ACTIVITYNET V1.3</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII EFFECTIVENESS</head><label>VII</label><figDesc></figDesc><table /><note>OF SELF-ATTENTION</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII COMPARISONS</head><label>VIII</label><figDesc>OF DIFFERENT PAIRS USED IN SAM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX TOP</head><label>IX</label><figDesc>-5 ERROR RATES OF SAMPLED CLASSES (%) IN ACTIVITYNET WITH DIFFERENT FUSION STRATEGIES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:08:01 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complex event detection by identifying reliable shots from untrimmed videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="736" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Untrimmed video classification for activity detection: Submission to activitynet challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Background modeling and referencing for moving cameras-captured surveillance video coding in HEVC</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2921" to="2934" />
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moments in time dataset: One million videos for event understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="502" to="508" />
			<date type="published" when="2020-02-01">1 Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing object semantics and deep appearance features for scene recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1715" to="1728" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What do 15000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4443" to="4452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harnessing object and scene semantics for large-scale video understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3112" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scenes-objects-actions: A Multi-task, multi-label video dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="635" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-scene convolutional neural networks for event recognition in images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal attention for fusion of audio and spatiotemporal features for video description</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2528" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">YouTube-8 M: A large-scale video classification benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3 d residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6222" to="6231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling scene and object contexts for human action retrieval with few examples</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="681" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
				<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-feature fusion via hierarchical regression for multimedia analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
				<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust late fusion with rank minimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3021" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sample-specific late fusion for visual category recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="803" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning query and image similarities with ranking canonical correlation analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
				<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive co-attention network for named entity recognition in tweets</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">D.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis</title>
				<meeting>Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6087" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based LSTM and semantic consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">COMIC: Toward a compact image captioning model with attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2686" to="2696" />
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-quality image captioning with fine-grained and semantic-guided visual attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1681" to="1693" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Memory attention networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3043" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recurrent spatial-temporal attention network for action recognition in videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic adversarial network with multi-scale pyramid attention for video classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9030" to="9037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interpretable spatio-temporal attention for video action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Workshop</title>
				<meeting>Int. Conf. Comput. Vis. Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4344" to="4353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">LSTA: Long short-term attention for egocentric action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9954" to="9963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The 2nd YouTube-8 M large-scale video understanding challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fine-grained video categorization with redundancy reduction attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="136" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10457" to="10467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamic sampling networks for efficient action recognition in videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7970" to="7983" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">FineGym: A Hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>.Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2616" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-modal domain adaptation for fine-grained action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Abductive learning: Towards bridging machine learning and logical reasoning</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Temporal reasoning in videos using convolutional gated recurrent units</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1111" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
