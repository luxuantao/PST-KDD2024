<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Local Feature Methods for 3D Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-04">August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sima</forename><surname>Soltanpour</surname></persName>
							<email>soltanps@uwindsor.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<postCode>N9B 3P4</postCode>
									<settlement>Windsor</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boubakeur</forename><surname>Boufama</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<postCode>N9B 3P4</postCode>
									<settlement>Windsor</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Q</forename><forename type="middle">M Jonathan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Windsor</orgName>
								<address>
									<postCode>N9B 3P4</postCode>
									<settlement>Windsor</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Sur</surname></persName>
						</author>
						<title level="a" type="main">A Survey of Local Feature Methods for 3D Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-04">August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">3D96AA4D77644D5CC8E38570572E8630</idno>
					<idno type="DOI">10.1016/j.patcog.2017.08.003</idno>
					<note type="submission">Received date: 8 January 2017 Revised date: 20 July 2017 Accepted date: 3 August 2017 Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Face recognition</term>
					<term>feature extraction</term>
					<term>local features</term>
					<term>3-D</term>
					<term>survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the main modules in a face recognition system is feature extraction, which has a significant effect on the whole system performance. In the past decades, various types of feature extractors and descriptors have been proposed for 3D face recognition. Although several literature reviews have been carried out on 3D face recognition algorithms, only a few studies have been performed on feature extraction methods. The latter have a vital role to overcome degradation conditions, such as face expression variations and occlusions. Depending on the types of features used in 3D face recognition, these methods can be divided into two categories: global and local feature-based methods. Local feature-based methods have been effectively applied in the literature, as they are more robust to occlusions and missing data. This survey presents a stateof-the-art for 3D face recognition using local features, with the main focus being the extraction of these features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A survey of 3D face methods with the main focus on local features is conducted.</p><p>• The popular 3D face databases are described along with their acquisition technology.</p><p>• 3D local descriptors are divided into key-points, curves, and surface-based methods.</p><p>• The surveyed approaches are summarized and compared under different conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition is a popular research area, since it has numerous applications, including law enforcement, surveillance systems, border security, access control, and entertainment systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. It is defined as a process to identify or verify a person's identity by comparing the input face characteristics against known faces from a database. During the past decades, 2D face recognition has been comprehensively investigated <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">4]</ref>. Although several methods have been proposed so far, there are still many limitations with 2D face recognition. 3D images compared to 2D data provide more reliable geometric information. Scale, rotation, and illumination do not affect the extraction of certain powerful features, from these 3D images <ref type="bibr" target="#b5">[5]</ref>. Furthermore, 3D 10 pose estimation is more accurate than pose estimation with 2D images. Given these advantages, 3D face recognition has become an active research area aimed at overcoming the existing challenges, arising from 2D face images.</p><p>A number of surveys have been published in 3D face recognition during the last decade. Most of the earlier surveys have focused on the introduction, general sum-15 marization, and challenges of face recognition algorithms <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. A survey by Scheenstra et al. <ref type="bibr" target="#b7">[7]</ref> reviewed 3D face recognition approaches in four different categories, and compared them with 2D face recognition methods. 3D face recognition methods alone or in combination with 2D intensity images were discussed in <ref type="bibr" target="#b8">[8]</ref>. Various challenges for 2D and 3D face recognition were addressed and the limitations and 20 solutions for different methods were discussed in <ref type="bibr" target="#b9">[9]</ref>. Smeets et al. <ref type="bibr" target="#b10">[10]</ref> conducted a survey on 3D face recognition by summarizing the main characteristics and challenges of these approaches. A recent survey by Zhou et al. <ref type="bibr" target="#b11">[11]</ref> covered different algorithms by categorizing them into single-modal and multi-modal approaches, along with their advantages and disadvantages. Some of the recent review papers have focused on a 25 specific challenge in face recognition. For instance, a survey on pose-invariant face recognition approaches is presented in <ref type="bibr" target="#b12">[12]</ref>, a comparative study on 3D face methods, under facial expression challenges, can be found in <ref type="bibr" target="#b13">[13]</ref>, and <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> represent a survey on 3D facial expression recognition. In <ref type="bibr" target="#b16">[16]</ref>, only feature extraction and selection methods were investigated, for both 2D and 3D face recognition. The main focus of 30 <ref type="bibr" target="#b16">[16]</ref> was the presentation of the different methods, with less emphasis on the comparison of their advantages and drawbacks.</p><p>Approaches for 3D face recognition can be divided into three broad categories: holistic, feature-based, and hybrid matching methods <ref type="bibr" target="#b2">[3]</ref>. In holistic matching methods, the focus is on the global similarity of faces. The entire 3D face (or model) is de-  <ref type="bibr" target="#b17">[17]</ref>, the deformation modeling <ref type="bibr" target="#b18">[18]</ref>, the signed shape difference map (SSDM) <ref type="bibr" target="#b19">[19]</ref>, spherical harmonic features (SHF) <ref type="bibr" target="#b20">[20]</ref>, closest normal points (CNPs) <ref type="bibr" target="#b21">[21]</ref>, and region based 3D deformable model (R3DM) <ref type="bibr" target="#b23">[22]</ref>. Feature-based matching methods rely on finding similar local features from the 40 face or from special regions of the face (e.g., eyes and nose). Hybrid approaches are defined based on the combination of different types of approaches (holistic and featurebased) or data (2D and 3D images).</p><p>There are several reasons that make local methods more promising than holistic ones. In particular, in local methods complete models are not necessary and occlusions 45 can be easily handled <ref type="bibr" target="#b24">[23]</ref>. According to the survey by Abate et al. <ref type="bibr" target="#b9">[9]</ref>, applying local features is one possible solution for recognizing partially occluded faces. Recent survey by Zhou et al <ref type="bibr" target="#b11">[11]</ref> mentioned different challenges for face recognition are pose, viewpoint, and expression that feature-based methods address these problems.</p><p>Moreover, local descriptors, like Scale Invariant Feature Transform <ref type="bibr" target="#b25">[24]</ref> and Local 50 Binary Pattern <ref type="bibr" target="#b26">[25]</ref>, have yielded remarkable results in 2D face recognition. Because the main focus of local descriptors is on the shape details, global or holistic methods perform better in similarity search applications, while local methods are more suitable for matching, identification and verification <ref type="bibr" target="#b24">[23]</ref>. Mian et al. <ref type="bibr" target="#b27">[26]</ref> mentioned one limitation for holistic methods : they need accurate normalization for pose and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>55</head><p>Generally, the recognition performance of global features is usually affected by pose and/or scale variations. Manual and automatic landmark detection are used for normalization, with the manual one being more accurate. However, it makes the whole process semi-automatic, as in <ref type="bibr" target="#b28">[27]</ref>. Recently, Gilani et al. <ref type="bibr" target="#b23">[22]</ref> proposed a landmark detection technique for holistic methods. It uses a deep landmark identification network 60 and needs a training step with synthetic images. Although, holistic algorithms apply all the visible facial shape information to create discrimination, obtaining the needed accurate pose normalization is not easy under noisy or low-resolution 3D scans. In this case, local features may perform better <ref type="bibr" target="#b29">[28]</ref>. Furthermore, local methods can be robust under facial expressions, because sensitive facial regions can be excluded <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b27">26]</ref>. In 65 particular, local features can be extracted from the rigid parts of the face that are the least influenced by expression changes <ref type="bibr" target="#b30">[29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Terminology and 3D databases</head><p>There are two scenarios for a typical 3D face recognition system; namely verification (1:1 matching) and identification (1:N matching). For identification, an unknown 90 face (probe) is matched against known individuals (gallery) to find the best match. Verification refers to the confirmation or rejection of a claimed identity of a probe face. Furthermore, usually two metrics are considered for measuring the performance of a face recognition system. The Receiver Operating Characteristic (ROC) curve is used to measure the verification accuracy. ROC plots the False Rejection Rate (FRR) or Verifi-95 cation Rate (VR) against the False Acceptance Rate (FAR); at various thresholds, and</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>interpolates between these points. FRR refers to the probability of incorrectly rejecting a person (two samples belonging to the same person) and FAR refers to the probability of accepting an incorrect person (two samples from two different people). The Cumulative Match Characteristic (CMC) curve, used to evaluate identification performance, 100 plots the recognition rate against a number of ranks. The same matching threshold is used for both verification and identification scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Databases</head><p>There are different types of 3D data applied in the recognition system. Polygonal meshes of 3D faces are usually used in 3D face recognition applications for computa-105 tional efficiency. Other types of 3D data include point clouds, a collection of 3D point coordinates, and range images or depth maps, where each element represents the distance of a point from the sensor or from another reference point. Figure <ref type="figure" target="#fig_4">1</ref> illustrates a range image, point cloud and mesh representation as different types of 3D data. There are two types of acquisition systems for capturing 3D faces: active, like laser scanners 110 and structured light, and passive, like stereo-based systems <ref type="bibr" target="#b16">[16]</ref>. In active capturing systems, such as Minolta vivid scanners, triangulation technique is used. A laser line is shined on the face from a scanner and an image of the line is recorded by a camera. Although the accuracy of this method for 3D face acquisition is relatively high, it is time consuming. In structured light, for example Inspeck Mega Capturor II 3D, a pattern 115 of light is projected on a face from a light source and the deformations of the pattern are measured using a camera. This technique is fast, but the captured data contains a number of holes and artifacts. In passive techniques based on stereo systems, for instance 3DMD digitizer and Di3D, two cameras are employed to capture the location of each point by matching corresponding pixels in two images. Because of the difficult 120 and time-consuming problem of dense pixel matching, due to the relative uniformity of a human face for two images, the accuracy of this system is comparatively low <ref type="bibr" target="#b31">[30]</ref>.</p><p>To evaluate 3D face recognition algorithms, many databases have been created. Table 1 describes the currently popular 3D face databases in four different categories, according to 3D data type, and provides some details for each.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D local feature-based methods</head><p>In the context of face recognition, 3D local feature descriptors are built from 3D local facial information. These features have some advantages over global features, as global descriptors are more sensitive to pose, facial expressions and occlusions <ref type="bibr" target="#b29">[28]</ref>.</p><p>The main objective of local feature extraction methods is the detection of distinctive 130 compact features, that are robust to a set of nuisances. To the best of our knowledge, local feature-based algorithms can be more robust against facial variations such as expression and occlusion, as they exclude parts that might be affected by those changes.</p><p>In particular, there is no set of local attributes that are completely invariant under all variations <ref type="bibr" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>A number of 3D local feature descriptors for 3D face recognition have been presented in the literature. This section surveys and explains the main existing 3D local descriptors and groups them into three different categories: Keypoints-based, curvebased and local surface-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Keypoints-based methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>140</head><p>3D keypoints are interest points of shape, based on the definition of saliency. They are detected according to some geometric information of the surface. The methods typically involve two major steps, keypoint detection and feature description <ref type="bibr" target="#b49">[47]</ref>. Although these methods can cope with occlusions and missing parts, their computational cost is much higher as they use a large number of keypoints, described by high di-145 mensional feature vectors. Hence, it is very important to only select the most effective keypoints, from the local descriptors, to create an efficient feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Methods based on SIFT-like keypoints</head><p>Scale invariant feature transform (SIFT) <ref type="bibr" target="#b25">[24]</ref> is a successful keypoint detector that has motivated researchers to use the same scheme in the case of 3D images. The 150 most important limitation of SIFT keypoint-based methods is their sensitivity to noisy data. However, these methods do not require very sophisticated registration algorithms.</p><p>Furthermore, the convincing representation of SIFT features on shape maps motivated researchers to apply this framework in 3D.</p><p>A framework to detect SIFT-inspired 3D keypoints was first proposed by Mian et al.</p><p>155 <ref type="bibr" target="#b27">[26]</ref>, where they use the shape variation in combination with 2D SIFT descriptors. To detect 3D keypoints, for points in the sphere of radius r and center p, the mean vector m and covariance matrix C are calculated. Then, matrix V of the eigenvectors is obtained by performing principal component analysis (PCA) on C. A point p is defined as a keypoint, if the difference between the first two principle axes of the local region is greater 160 than a threshold t. Figure <ref type="figure">2</ref> illustrates a keypoint on the 3D face and its corresponding texture image. This method has influenced other researchers; for example SIFT keypoints are used in <ref type="bibr" target="#b50">[48]</ref> to detect relevant interest points on depth images, then local shape descriptors are defined for the neighborhood of each keypoint. Mayo and Zhang</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Figure <ref type="figure">2</ref>: A keypoint on a 3D face and its corresponding texture <ref type="bibr" target="#b27">[26]</ref>.</p><p>[49] proposed a multiview keypoint matching method, where SIFT keypoints are ex-165 tracted from 2.5D images. In <ref type="bibr" target="#b53">[50]</ref>, SIFT descriptors are extracted from 2D matrices of curvature maps, where the features are defined at fixed scales and orientations for fixed locations. SIFT keypoint detection is applied on multi-scale local binary pattern and shape index maps in <ref type="bibr" target="#b54">[51]</ref>, and on pyramidal shape index map in <ref type="bibr" target="#b55">[52]</ref> for 3D domain and in combination with 2D keypoints, respectively. The extension work of <ref type="bibr" target="#b55">[52]</ref> has been 170</p><p>presented in <ref type="bibr" target="#b56">[53]</ref> using curvature maps. The main weakness of these methods is their sensitivity to pose variations. Recently, a Keypoint-based Multiple Triangle Statistics (KMTS) method has been presented by Lei et al. <ref type="bibr" target="#b57">[54]</ref> to handle pose variations where 3D keypoints are detected based on the method in <ref type="bibr" target="#b27">[26]</ref>. Low-level geometric features <ref type="bibr" target="#b30">[29]</ref>, described in Section 3.3.2 of this paper, are extracted from the patch around the 175 detected keypoints. Applying low-level geometrical features without any complicated mathematical operation shows that the approach is time efficient. According to the experiments reported by authors using an Intel Core 2 Quad CPU and 16 GB RAM, the preprocessing, feature extraction and identification takes 0.62 s, 5.46 s, and 1.82 s, respectively. 180</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Mesh-based methods</head><p>Although SIFT-like detectors present the informative features without registration for nearly frontal scans, they are sensitive to large pose variations or occlusions. To overcome these limitations, SIFT keypoints detection is applied directly on 3D mesh data in recent works. Generally, approaches using 2D keypoints detection ideas have allowed the discovery and implementation of powerful keypoint detectors in the 3D</p><p>domain.</p><p>An extension of SIFT for 3D meshes, called MeshSIFT, was proposed by Maes et al. <ref type="bibr" target="#b58">[55]</ref>, then extended by Smeets et al. <ref type="bibr" target="#b59">[56]</ref>. The approach consists of four major steps: keypoint detection, orientation assignment, local feature description, and feature 190 matching. Given an input mesh M, the mean curvature H is calculated for each vertex i, at each scale s, to detect salient points. The normal vector of the keypoint neighboring vertices is projected onto its tangent plane. A weighted histogram is constructed using the projected normal vectors. The canonical orientations are estimated with the highest peak in the histogram. Normals and their projections onto the tangent plane 195 are illustrated in Figure <ref type="figure" target="#fig_5">3a</ref>. A feature descriptor is defined for each keypoint by the concatenated histograms of nine circular regions (the shape index and angles between normals), as shown in Figure <ref type="figure" target="#fig_5">3b</ref>.</p><p>The meshSIFT-like keypoint detector has also been applied in <ref type="bibr" target="#b60">[57]</ref> using maximum (k max ) and minimum (k min ) curvatures, estimated in the 3D Gaussian scale space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200</head><p>A salient point is the vertex whose value is a local extrema within its neighborhood.</p><p>The detection of keypoints is illustrated in Figure <ref type="figure" target="#fig_6">4a</ref>. To calculate the local descriptor, a geodesic disk with radius R is considered around each keypoint. Then, a circle with radius r 1 and eight circles with radius r 2 are extracted, as shown in Figure <ref type="figure" target="#fig_6">4b</ref>.</p><p>Three histograms, including surface gradient (HoG), shape index (HoS), and gradient To overcome the stated problem and extract repeatable keypoints on 3D meshes, 210 MeshDOG <ref type="bibr" target="#b61">[58]</ref>, an extension framework of <ref type="bibr" target="#b62">[59]</ref>, proposes a multi-ring geometric histogram (GH) as a descriptor. Given a 3D mesh, the mean curvature at each vertex is first computed. The detection of the 3D keypoints is done in three steps, i.e., scale-space, percentage threshold, and corner analysis. Figure <ref type="figure" target="#fig_8">5a</ref> shows the detected 3D keypoints in the three different steps and GH computation. The normals and the difference be-215 tween minimum and maximum perpendicular distance of two facets are calculated to create descriptors (see Figure <ref type="figure" target="#fig_0">5</ref>). An extension of the framework in <ref type="bibr" target="#b60">[57]</ref> is described in <ref type="bibr" target="#b63">[60]</ref>, where a fine-grained matching of 3D keypoint descriptors has been proposed to handle degradation conditions. Among the above mentioned mesh-based SIFT-like matching methods <ref type="bibr" target="#b59">[56,</ref><ref type="bibr" target="#b63">60]</ref> provide a registration-free recognition scheme.    Shape index as curvature map is widely used to detect landmarks. In <ref type="bibr" target="#b65">[62]</ref>, feature points, included inside and outside the corners of the eyes and the nose tip, are 240 extracted by calculating the local shape index at each point of the 3D mesh. In <ref type="bibr" target="#b66">[63]</ref>,</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>shape index and spin images are used as local descriptors to extract landmark points.</p><p>Spin image encodes each point p on the 3D face surface, with respect to the normal vector n at that point. Facial curvatures are also used for landmark detection <ref type="bibr" target="#b67">[64,</ref><ref type="bibr" target="#b68">65]</ref>.</p><p>Triangles, resulting from the connection of the detected eyes and nose, are used in the 245 recognition stage in <ref type="bibr" target="#b67">[64]</ref>. In <ref type="bibr" target="#b68">[65]</ref>, 14 manually detected landmarks are used to define a local shape dictionary, consisting of curvature maps. Keypoints are then extacted from this shape dictionary. An enhanced version of this work, presented in <ref type="bibr" target="#b5">[5]</ref>, uses a non-linear machine learning approach, namely AdaBoost, to detect keypoints. There are other proposed methods to detect landmarks. In <ref type="bibr" target="#b69">[66]</ref>, five landmarks on a range 250 image of the face are extracted using radial symmetry and shape information. These facial feature points are employed to extract a very small subset of points on probe images, that are invariant under facial expressions. Gupta et al. <ref type="bibr" target="#b70">[67]</ref> presented an anthropometric approach by detecting 10 fiducial points and calculating the Euclidean and geodesic distance between them as features. Song et al. proposed a landmark lo-255 calization approach that uses local coordinate coding (LCC) and consists of two stages:</p><p>nose detection and resampling <ref type="bibr" target="#b71">[68]</ref>. Another landmark-based method is described in <ref type="bibr" target="#b72">[69]</ref>, where the authors proposed an automatic 3D landmark localization method that can handle missing parts, with asymmetry pattern and shape regression. Recently, an automatic 3D facial landmark detection has been proposed in <ref type="bibr" target="#b73">[70]</ref> using 2D Gabor 260 wavelet features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Summary</head><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the keypoint-based methods. The latter are categorized into SIFT-like, mesh-based, and landmarks. The neighborhood of a keypoint is defined based on three different measurements <ref type="bibr" target="#b74">[71]</ref>, i.e., Euclidean distance, geodesic distance</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>and multi-rings. Methods based on geodesic distances are robust under isometric deformations. On the other hand, the geodesic distance calculation is time-consuming according to <ref type="bibr" target="#b74">[71]</ref>. For example, the computational complexity for geodesic distance calculation in <ref type="bibr" target="#b59">[56,</ref><ref type="bibr" target="#b63">60]</ref> is O(mlogm), where m is related to a neighborhood area with radius r. Therefore, for n given vertices, the complexity for calculating all geodesic 270 distances is O(nr 2 logr). As constants Pi and 2 will be removed from the big O notation. The Euclidean distance, according to <ref type="bibr" target="#b64">[61]</ref>, is easier to calculate but is sensitive to deformations. When multi-rings are used, for example like in <ref type="bibr" target="#b61">[58]</ref>, the geodesic distance between two points on a mesh is approximated properly. They are computationally efficient. We have found that the methods described in <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b59">[56]</ref> and <ref type="bibr" target="#b60">[57]</ref> are</p><formula xml:id="formula_3">275</formula><p>exerting more influence on other research works because of their effective results and deformations handling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Curve-based methods</head><p>These methods use a set of curves from facial surfaces as features. The latter include rich geometrical information that captures shape information from different fa-  the nose tip, are described as iso-geodesic curves. An iso-geodesic curve c λ consists 295 of the set of all points, whose geodesic distance dist, from a reference point r, is in the range [λδ, λ + δ], for a small positive δ. A Riemannian analysis framework is employed for comparing facial curves. The latter have the advantage of being invariant to rotations or translations (isometric transformation). However, both iso-depth and iso-geodesic curves (illustrated in Figure <ref type="figure" target="#fig_12">7</ref>), are sensitive to large facial expressions,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T occlusions and missing parts. Iso-geodesic stripes have also been applied by Berretti et al. <ref type="bibr" target="#b78">[75]</ref>. To extract stripes, the normalized geodesic distance γ is computed between each face point and the nose tip, and quantized into N intervals c 1 , ..., c N . This way, the i th stripe consists of all points whose distances γ are in the interval c i . The stripes are described by a 3D Weighted Walkthroughs (3DWWs) descriptor and used as nodes 305 in a graph-based matching scheme. Level curves have been also employed in <ref type="bibr" target="#b79">[76]</ref>,</p><p>[77], <ref type="bibr" target="#b81">[78]</ref>, <ref type="bibr" target="#b82">[79]</ref>, and <ref type="bibr" target="#b83">[80]</ref>. The main limitation of most of these approaches, apart from occlusion, is their lack of robustness to extremely large facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Profile-based</head><p>Profiles are open curves, with starting and end points. Typically, the starting and 310 end points are in the middle and on the edge of the face, respectively <ref type="bibr" target="#b13">[13]</ref>. Radial curves have been introduced by Drira et al. <ref type="bibr" target="#b84">[81]</ref> and extended in <ref type="bibr" target="#b85">[82]</ref>. These curves are more efficient than level curves <ref type="bibr" target="#b76">[73]</ref>, <ref type="bibr" target="#b77">[74]</ref>, as they cover different face regions that are related to different facial expressions. At least some parts of radial curves are available to handle occlusions and missing parts. Each curve originates at the nose tip and with level sets in <ref type="bibr" target="#b86">[83]</ref> to approximate the facial surface. The well-known machine learning algorithm, AdaBoost, is used to select the most efficient features. The ma-320 chine learning based feature selection method provides a very compact signature of a 3D face and a fast classification approach for face recognition. Using all curves, the computational time for recognition is 2.64s. However, with selected curves the time is reduced to 0.68s, showing that the selection method enhances the system computa- GB RAM, face identification only requires 6.07s. In particular, the features extracted from semi-rigid regions are robust under facial expressions. Figure <ref type="figure" target="#fig_15">9</ref> shows a binary terms of accuracy can be improved at the curve matching level, when a robust solution is used. In <ref type="bibr" target="#b88">[86]</ref> facial curves, the intersection of a plane P and the facial surface are employed to make a rejection classifier. An adaptive region extraction is used for matching two 3D faces. The vertical facial curve in the nose tip is called central profile. Although the partial central profile is less descriptive than the entire one, it is also 345 less sensitive to facial expression and occlusions, less complex, and hence it is used to make a rejector. The similarity between a partial central profile and its corresponding profile from another face is calculated based on the average distance between the two curves, using the iterative closest point (ICP) algorithm <ref type="bibr" target="#b89">[87]</ref>. Generally, curves are less discriminative than regions. However, they are faster and require less space for storage. Using an Intel Core Duo 2.34GHz machine with 1GB of memory, the verification process takes less than 9s and recognition process requires 195 s with rejection (608 s without rejection), which shows that the rejection-based method is faster. Vertical central profile has been also used in <ref type="bibr" target="#b90">[88]</ref>, where it is defined as the intersection between the symmetry plane, the facial surface and mean curvature. Authors apply the property yields nasal region curves. These curves are applied to make the feature descriptor.</p><p>The feature vector is obtained by concatenating histograms of x, y, and z components of the normal vectors of the Gabor wavelet filtered surface. A genetic algorithm is used 365 to select the more robust features against facial expressions. This method has shown high class separability compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Summary</head><p>Table <ref type="table" target="#tab_2">3</ref> summarizes our surveyed curve-based methods that we divided into contourbased and profile-based categories. In most curve-based methods, the nose tip is used 370 as a reference point or the origin of the system. Since the nose region is rigid, robust under facial expression, and contains more distinctive shape features than other regions, curve-based methods are robust under facial expression. However, hair covering the face, large pose changing, and missing data affect the correct detection of the nose tip. Consequently, face alignment and facial curve extraction are calculated using 375 an incorrect origin, which affects the recognition performance of these methods. In particular, we have found that iso-depth curves <ref type="bibr" target="#b76">[73]</ref>, iso-geodesic curves <ref type="bibr" target="#b78">[75]</ref>, and the radial curves <ref type="bibr" target="#b85">[82]</ref> are more effective and have greater influence on other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local surface-based methods</head><p>Most local surface-based methods extract local geometric information, from several 380 patches of the facial surface or from some regions of the surface, that are invariant under facial expression variations. These methods can be divided into LBP-based, geometric feature-based and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">LBP-based</head><p>Inspired by the efficient Local Binary Pattern (LBP) for 2D face recognition, LBP-385 based methods, as surface descriptors, have been developed for expression-robust 3D face recognition. LBP is a local shape descriptor that was initially introduced by Ojala et al. <ref type="bibr" target="#b26">[25]</ref> for 2D images. LBP was first employed by Li et al. <ref type="bibr" target="#b93">[91]</ref> on intensity image and surface in a fusion scheme for 3D face recognition. Later, 3DLBP <ref type="bibr" target="#b94">[92]</ref>, in combination with global matching was proposed. A multi-scale extended LBP with a SIFT-390 based strategy is described in <ref type="bibr" target="#b75">[72]</ref>. LBP representation is also applied in <ref type="bibr" target="#b95">[93]</ref> where,  an efficient registration method can improve the recognition results. Werghi et al. <ref type="bibr" target="#b97">[95]</ref> proposed the Mesh-LBP method, where they applied LBP descriptor on mesh. The method was extended to face recognition in <ref type="bibr" target="#b98">[96]</ref>. For each central facet f c on the mesh,</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>405</head><p>Fout and Fgap are considered edge facets of f c . Starting with three Fout facets around f c , the Fgap facets between each pair of Fout facets are extracted and the outcome of this procedure is a ring of ordered facets around f c (See Figure <ref type="figure" target="#fig_17">10</ref>). The mesh-LBP is computed for facet f c as m-1 k=0 s(h( f r k ) -h( f c )).α(k), with s(x)=1 if x ≥ 0 and s(x)=0 if x &lt; 0, where r and m are the ring number and the number of facets on the ring, 410 respectively. The function h( f ) is a scalar function that contains either a geometric or a photometric information, such as curvature and color or gray level, respectively. For α(k), two variants are considered, i.e., α 1 (k) = 1 and α 2 (k) = 2 k . The curvature maps including curvedness, Gaussian curvature, shape index, and the gray level are used for h( f ) in two different shape and texture modalities in a fusion scheme. A constructed 415 histogram over a given neighborhood is considered as a descriptor in the matching step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Geometric feature-based</head><p>Some methods are developed based on geometric features. Xu et al. <ref type="bibr" target="#b99">[97]</ref> proposed a 3D face recognition method using geometric features and shape variation informa-420 tion. First, the 3D point cloud is converted to a mesh, then a geometric feature vector is built, using Z-coordinate, Z(v i ) of each vertex v i of the mesh. Shape features are extracted on some regions of the face, including mouth, nose, left and right eyes. Two</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T vectors, including geometric and shape features, are concatenated together to make the feature vector. PCA is then applied to reduce the feature dimension. Li and Zhang</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>425</head><p>[98] proposed a recognition system by using geometrical attributes consisting of angles, geodesic distances, and curvatures. To have a stable feature vector under facial expression, expression-insensitive signatures are constructed using weighted attributes.</p><p>In <ref type="bibr" target="#b101">[99]</ref>, an expression-insensitive descriptor (EID) based on the sparse representation of low-level geometric information is proposed, where a pooling and ranking scheme 430 is employed to select higher ranked EIDs. Recently, low-level geometric features have been proposed in <ref type="bibr" target="#b30">[29]</ref>. These features measure distances and angles between vertices of the 3D mesh. They are robust under facial pose and expression variations as they are calculated for three different regions of the face, viz., semi-rigid (eye-forehead), rigid (the nose) and non-rigid (mouth) regions. Each region is represented using multiple 435 triangles, with one vertex being the nose tip and two randomly selected vertices, from the surface of the region. Using these triangles, low-level geometric features are computed from the angle between the two segments connecting each of the random points to the nose tip (A), the radius of the circumscribed circle (C), the distance between the two random vertices (D), and the angle between the line connecting the two vertices 440 and the z-axis (N) (see Figure <ref type="figure" target="#fig_18">11</ref>). Each feature vector is normalized into [-1, +1] and quantized into a histogram with m bins. The feature descriptor is calculated by concatenating the four histograms. A support vector machine (SVM) classifier is used to recognize test faces. An extension of this work has been proposed in <ref type="bibr" target="#b57">[54]</ref> as local Keypoint-based Multiple Triangle Statistics (KMTS) (Section 3.1.1). Covariance ma-445 trices of descriptors are proposed by Tabia et al. <ref type="bibr" target="#b102">[100]</ref> to capture geometric and special properties of a region with the correlation of these properties. For a 3D shape with a set of patches {P i , i = 1...m} around a representative point p i , a feature vector f i of dimension d for each point p j in the patch P i is computed using p j -p c , the distance between p j and p i , and the volume of the parallelepiped where p c is the patch center 450 and is equal to 1/n i n i k=1 p k . The representation is generic and other features can be added. A d × d covariance matrix X i = 1/n n j=1 ( f jµ)( f jµ) T is calculated where µ is the mean of the feature vectors f i . An extension of this work has been presented in <ref type="bibr" target="#b103">[101]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Other methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>455</head><p>Point signature was initially proposed by Chua et al. <ref type="bibr" target="#b104">[102]</ref> as a representation for free-form surfaces. To deal with expression variations, only the rigid parts of the face are used in the matching process. Point signature is also used to describe feature points in 3D domain in <ref type="bibr" target="#b105">[103]</ref>. Multiple overlapping regions around the nose are extracted using surface curvatures, including mean curvature H and Gaussian curvature K in 460 <ref type="bibr">[104]</ref>. An extension of this work is proposed by Flatemier et al. <ref type="bibr" target="#b106">[105]</ref>. In <ref type="bibr" target="#b107">[106]</ref>, the authors introduced tensors, where third order tensors are indexed using 4D hash table.</p><p>Rank-0 tensor fields are also applied by Al-Osaimi et al. <ref type="bibr" target="#b108">[107]</ref>, where multiple local tensor fields are computed over a triangular mesh and used as geometrical cues. Most of these methods work based on the surface registration and descriptors that are not 465 suitable for real applications and are computationally expensive. Recently, Ming <ref type="bibr" target="#b109">[108]</ref> proposed a regional bounding spherical descriptor that is computationally efficient and handles facial emotions with high recognition rate. This method takes 5.96 s for the whole data processing, which is considered time efficient. In addition, 2D features can be calculated on 2D maps extracted from 3D meshes to decribe local features 470 such as Gabor filter coefficients in <ref type="bibr" target="#b110">[109]</ref>. An extension of this work using wavelet coefficients has been presented in <ref type="bibr" target="#b111">[110]</ref>. The authors apply feature scoring to define compact signatures that makes the matching more efficient, especially in large-scale databases. Using an AMD Opteron processor at 2.1 GHz, the algorithm can perform   <ref type="table" target="#tab_3">4</ref> summarizes the local surface-based methods that we have presented in this section. Some methods such as <ref type="bibr" target="#b96">[94,</ref><ref type="bibr" target="#b98">96]</ref> are inspired by LBP local descriptors with effective performance. Recently, geometric features have been used, for example in <ref type="bibr" target="#b30">[29]</ref> and <ref type="bibr" target="#b102">[100]</ref>, yielding robust descriptors that are capable of handling facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>480</head><p>The methods in <ref type="bibr" target="#b104">[102,</ref><ref type="bibr" target="#b105">103]</ref> use point signatures that are invariant to translations and rotations. Some of the surface-based methods <ref type="bibr">[104,</ref><ref type="bibr" target="#b106">105]</ref> work on some regions of the face that are extracted based on the nose tip location. Hence, they are sensitive to the nose tip detection accuracy. However, these methods are robust under facial expressions. Tensor features, used in <ref type="bibr" target="#b107">[106,</ref><ref type="bibr" target="#b108">107]</ref>, combine global and local geometric features 485 and are robust under rigid transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>In the past decade, 3D face recognition has significantly grown in terms of databases, features, matching approaches, and even handling degradation conditions. Many 3D face recognition methods rely on local features to overcome deformations. 490 Tables <ref type="table">5</ref> and<ref type="table">6</ref> summarize the performances of the surveyed methods, along with their category and performance on different databases under different conditions (expression or neutral (FRGC v2.0, Bosphorus) pose or frontal (Gavab, Bosphorus)). The criteria used in the literature consist of rank-1 recognition rate (RR1), equal error rate (EER) and verification rate (VR). There are some protocols for experiments on FRGC 495 v2.0 according to <ref type="bibr" target="#b37">[36]</ref>, and most authors reported the results by following these protocols. Hence, Table <ref type="table">5</ref> has been assigned to performances on FRGC v2.0 database, including VR and RR1 for the experiments based on the neutral and non-neutral sets of databases (neutral vs. non-neutral, vs. neutral, vs. all, and all vs. all) and ROC III experiments <ref type="bibr" target="#b37">[36]</ref>. Some earlier papers for example, <ref type="bibr" target="#b104">[102,</ref><ref type="bibr" target="#b105">103]</ref>, present their algorithms 500 without any quantitative results. Because different experiments in the literature are presented in various situations and for different conditions and databases, it is difficult to perform an overall fair comparison between all these different methods. occlusions and missing data <ref type="bibr" target="#b50">[48,</ref><ref type="bibr" target="#b54">51,</ref><ref type="bibr" target="#b57">54,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b63">60,</ref><ref type="bibr" target="#b65">62,</ref><ref type="bibr" target="#b67">64]</ref>. The main disadvantage of these methods is their sparseness that makes them sensitive to noisy data and extreme expression changes <ref type="bibr" target="#b52">[49,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b54">51,</ref><ref type="bibr" target="#b57">54,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b61">58,</ref><ref type="bibr" target="#b65">62,</ref><ref type="bibr" target="#b67">64,</ref><ref type="bibr" target="#b70">67]</ref>. Moreover, the high computation cost of some of the SIFT-and curvature-based methods is another drawback for this category <ref type="bibr" target="#b50">[48,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b63">60,</ref><ref type="bibr" target="#b68">65]</ref>. The comparison of keypoint-based methods shows that some 510 of them work effectively in fusion scheme. That is when combining 3D and intensity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T on extracted features from regions, that are relatively stable under facial expressions 530 <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b98">96,</ref><ref type="bibr">104,</ref><ref type="bibr" target="#b106">105]</ref>, or they rely on sparse representation to learn weights of expressioninsensitive patches and high-ranked features selection <ref type="bibr" target="#b96">[94,</ref><ref type="bibr" target="#b101">99]</ref>, or use covariance matrices with geodesic metrics <ref type="bibr" target="#b102">[100,</ref><ref type="bibr" target="#b103">101]</ref>. However, methods in this category are sensitive to occlusions and missing data <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b96">94,</ref><ref type="bibr" target="#b100">98,</ref><ref type="bibr" target="#b102">100,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b105">103,</ref><ref type="bibr" target="#b106">105,</ref><ref type="bibr" target="#b108">107]</ref>.</p><p>Among the more recent works, <ref type="bibr" target="#b30">[29]</ref> uses low-level geometric features and is com-</p><formula xml:id="formula_5">535</formula><p>putationally efficient, since it involves only basic computations, such as angles and distances. The method proposed by Li et al. <ref type="bibr" target="#b96">[94]</ref> is inspired by the computationally efficient LBP descriptor on surface normal component, and hence provides acceptable cost. Furthermore, 3D face verification, using the method in <ref type="bibr" target="#b109">[108]</ref>, drastically reduces the computational cost because of its efficient preprocessing and alignment steps, that 540 are done with a simple implementation. The methods that use ICP to perform matching, like <ref type="bibr" target="#b65">[62,</ref><ref type="bibr">104,</ref><ref type="bibr" target="#b106">105]</ref> from Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_3">4</ref>, have a good recognition performance, but are not computationally efficient. However, <ref type="bibr" target="#b88">[86]</ref> is an exception in this category, as it uses ICP-based matching but still provides an efficient classification, because of its rejection classifier that quickly eliminates dissimilar samples.</p><p>545 Some recent works select the most discriminative features to improve the recognition performance. They use feature selection methods such as AdaBoost, a machine learning technique, <ref type="bibr" target="#b86">[83]</ref>, a genetic algorithm-based selector <ref type="bibr" target="#b92">[90]</ref>, sparse representation learning-based method <ref type="bibr" target="#b96">[94]</ref>, and learning technique like PCA <ref type="bibr" target="#b85">[82]</ref>.</p><p>In particular, this survey suggests that no existing algorithms can handle all existing </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 A</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>35 scribed</head><label>35</label><figDesc>by defining a set of global features. Examples in this category include the prin-A C C E P T E D M A N U S C R I P T ciple component analysis (PCA)-based method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>D M A N U S C R I P T Based on the above discussion, local feature-based methods are a promising research topic for 3D face recognition application. We have conducted a survey on local methods to cover recent works in this area. In particular, none of these surveys specif-70 ically focuses on local feature-based 3D face recognition. Unlike [16] that presents feature extraction algorithms for both 2D and 3D, local and holistic features in combination with feature selection and fusion techniques, the main focus of this survey is a comprehensive study and comparison of different local feature-based techniques for 3D face recognition only. Compared to [7] that discusses local and global features, our 75 paper covers more recent local-based works with more details on their performance, under different facial challenges. Therefore, this article provides a survey on various categories of local 3D features, together with comparisons as well as their limitations and advantages. The survey also aims at helping researchers to get a good overview on 3D face recognition, and enable them to select the most effective method for the right 80 situation. The remainder of this paper is organized as follows. The terminology of 3D face recognition and databases are described in Section 2. Section 3 provides a comprehensive survey of local feature-based methods for 3D face recognition, including methods categorization and a detailed review of feature extraction algorithms. Section 4 presents 85 a discussion on the reviewed methods and their comparison, while Section 5 concludes this paper, presenting potential future research directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 3D face data representations : (a) Range image, (b) Poin cloud, (c) a Mesh [16].</figDesc><graphic coords="8,204.06,151.24,162.37,73.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>Figure 3: a) Normals and their projections, b) Nine circular regions around a keypoint[56].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: a) Salient points by k max (left) and k min (right), b) Canonical orientation, salient point and its neighborhood vertices [57].</figDesc><graphic coords="12,167.56,151.23,235.16,106.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>220</head><label></label><figDesc>Recently, Elaiwat et al.<ref type="bibr" target="#b64">[61]</ref> proposed a keypoint detector and a local feature descriptor by integrating different Curvelet elements of different orientations. Since Curvelet transform is based on FFT, the computational complexity of keypoint detection and descriptor definition is lower than SIFT-based methods. The coefficients of these Curvelet elements are computed at each scale a and angle θ, as shown in Fig-225 ure 6. Keypoints are detected by comparing the magnitudes of Curvelet coefficients A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: a) 3D keypoints detected at each step b) central facet t 1 and its neighbors c) the angle and perpendicular distance [58].</figDesc><graphic coords="13,118.02,152.21,392.14,114.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Four-scale Curvelet decomposition [61].</figDesc><graphic coords="13,181.10,317.48,208.09,155.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>230 3 . 1 . 3 .</head><label>313</label><figDesc>Landmark-based methods Landmarks are facial points extracted according to anatomical studies of the face. Some methods use a set of landmarks on the face to create feature vectors, obtained by calculating relationships between these landmarks. Therefore, accurate extraction of these landmarks is critical to generate reliable local features. The landmarks could be A C C E P T E D M A N U S C R I P T the eyes, nose and mouth on the facial image. They are also employed to correct the pose in pose sensitive local feature-based methods. Their disadvantage is the sparsity that can affect the recognition performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>280</head><label></label><figDesc>cial regions to represent a 3D face. Compared to keypoint-based methods, they are less sparse and more robust against facial expressions. In addition, the weight of the reference point (often the nose tip) is higher than other points, as it contains descriptive shape informataion. Curve-based methods can be grouped into two categories : contour-and profile-based<ref type="bibr" target="#b13">[13]</ref>. 2853.2.1. Contour-basedContours are closed curves with different lengths and without intersections. They are defined as level curves classified into iso-depth and iso-geodesic curves. Iso-depth curves, first introduced in<ref type="bibr" target="#b76">[73]</ref>, are obtained by translating a plane through the facial surface in one direction. These curves are described using the intersections between 290 the facial surface and a plane. For a facial surface S , a set of level curves c λ is obtained, where each c λ consists of all points p such that F(p) = λ, with F being a depth value function for the z component of point p. An extension of this framework is proposed in<ref type="bibr" target="#b77">[74]</ref>, where level curves of a facial surface distance function, with the origin being A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Level curves of a) geodesic function [74], and b) depth function [73] for several levels.</figDesc><graphic coords="17,199.00,151.23,172.17,162.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>315Figure 8 :</head><label>8</label><figDesc>Figure 8: Nose tip, the reference curvature, radial curves [82].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>tional performance. Facial curves are widely used as profile-based methods to handle 325 facial expression. Angular radial signatures (ARS)<ref type="bibr" target="#b87">[84]</ref> are defined as a set of curves at an interval of θ radians (θ ∈ [0, π]), emanating from the nose tip. A binary mask is defined on the xy-plane to project ARSs along different directions. Each resulting path consists of 20 points, with 3mm distance between any two adjacent points. ARS feature value of these points is computed from the depth value of each point, using 330 bicubic interpolation at the x and y coordinates. The ARS extracts significantly a set of discriminative 1D feature vectors from the complex 3D facial surface that achieves computationally efficiency in recognition task. On an INTEL Core 2 Quad-CPU and 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The binary mask and 17 ARSs [84].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>355 of the face bilateral symmetry to develop a fast algorithm that on a 1 -</head><label>1</label><figDesc>GHz Pentium IV PC with 512 MB RAM takes an average time equal to 0.5 s for comparison. Recently, some profile-based methods have been proposed that extend the application of facial curves. A set of Rotation-invariant and Adjustable Integral Kernels (RAIKs) is computed from the surface patch around a 3D point, in [89]. Nasal patches and curves are 360 introduced in [90]. First, nasal landmarks are detected, and then using pairs of landmarks, a set of planes is created. The intersection of these planes with the nasal surface A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Ordered ring construction, three Fout facets adjacent to the f c and sequences of Fgap facets [96].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Low-level geometric features [29].</figDesc><graphic coords="24,187.30,151.23,195.58,124.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>,000.00 comparisons per second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>575</head><label></label><figDesc>550 challenges, including facial expressions, pose variations, occlusions, missing data, hair covering part of the face and background clutter. Incomplete facial data and artifacts are still major issues in practical application of local surface-based methods. Deep learning might be used to boost the various local feature extraction methods, further improving recognition performance. The latter will be also improved by applying fusion methods, 555 that use 3D data and texture images. Furthermore, applying powerful feature selection methods to find a subset of the most discriminant features is another way to improve the performance of face recognition A C C E P T E D M A N U S C R I P T 5. Conclusion 3D face recognition is a vibrant and popular research area in the computer vision 560 and image processing field. Face recognition falls in the category of non-rigid object recognition, where handling deformations effectively still needs improvement. Compared to intensity images, 3D images are more robust against viewpoint and illumination variations, as they contain the local geometry of the face. The challenges in this field such as computational cost reduction and 3D data acquisition techniques enhance-565 ment require more work in the future. This survey paper reviewed recent advances in 3D face recognition, focusing mainly on methods that are based on local features. A taxonomy of the 3D local feature-based methods has been presented in this paper, together with their advantages and limitations. Properties, including descriptiveness, robustness, compactness and computation efficiency, are important criteria when com-570 paring the effectiveness and strength of each descriptor. Future work could include a comparative study of different local feature extractors for 3D face recognition. We hope this survey will further motivate the researchers in this area to dedicate more consideration and attention to the use of 3D local features for face recognition. This research is supported in part by the Canada Research Chair Program and the Natural Sciences and Engineering Research Council of Canada grant. A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Popular 3D face databases</figDesc><table><row><cell>Reference/Name</cell><cell>Data Type</cell><cell cols="4">Intensity Number of Number of Scanner</cell></row><row><cell></cell><cell></cell><cell>image</cell><cell>subjects</cell><cell>images</cell><cell></cell></row><row><cell>[31]/FSU</cell><cell>mesh</cell><cell>no</cell><cell>37</cell><cell>222</cell><cell>Minolta Vivid 700</cell></row><row><cell>[32]/GavabDB</cell><cell>mesh</cell><cell>no</cell><cell>61</cell><cell>549</cell><cell>Minolta Vi-700 laser range scanner</cell></row><row><cell>[33]/FRAV3D</cell><cell>mesh</cell><cell>yes</cell><cell>105</cell><cell>-</cell><cell>Minolta Vivid 700 red laser light scanner</cell></row><row><cell>[34]/BU-3DFE</cell><cell>mesh</cell><cell>yes</cell><cell>100</cell><cell>2500</cell><cell>Stereo photography, 3DMD digitizer</cell></row><row><cell>[35]/UoY</cell><cell>mesh</cell><cell>yes</cell><cell>350</cell><cell>5000</cell><cell>Stereo vision 3D camera</cell></row><row><cell>[36]/FRGC v1.0</cell><cell cols="2">range image yes</cell><cell>273</cell><cell>943</cell><cell>Minolta Vivid 3D scanner</cell></row><row><cell>[36]/FRGC v2.0</cell><cell cols="2">range image yes</cell><cell>466</cell><cell>4007</cell><cell>Minolta Vivid 3D scanner</cell></row><row><cell>[37]/UND</cell><cell cols="2">range image yes</cell><cell>277</cell><cell>953</cell><cell>Minolta Vivid 900</cell></row><row><cell>[38]/CASIA</cell><cell cols="2">range image no</cell><cell>123</cell><cell>4059</cell><cell>Minolta Vivid 910</cell></row><row><cell>[39]/ND2006</cell><cell cols="2">range image yes</cell><cell>888</cell><cell>13,450</cell><cell>Minolta Vivid 910 range scanner</cell></row><row><cell>[18]/MSU</cell><cell cols="2">range image no</cell><cell>90</cell><cell>533</cell><cell>Minolta Vivid 910</cell></row><row><cell>[40]/SHREC08</cell><cell cols="2">range image no</cell><cell>61</cell><cell>427</cell><cell>-</cell></row><row><cell>[41]/3D-TEC</cell><cell cols="2">range image yes</cell><cell>214</cell><cell>428</cell><cell>Minolta scanner</cell></row><row><cell>[42]/SHREC11</cell><cell cols="2">range image no</cell><cell>130</cell><cell>780</cell><cell>Escan laser scanner</cell></row><row><cell>[43]/UMB-DB</cell><cell cols="2">range image yes</cell><cell>143</cell><cell>1473</cell><cell>Minolta Vivid 900 laser scanner</cell></row><row><cell cols="3">[44]/Texas 3DFRD range image yes</cell><cell>118</cell><cell>1149</cell><cell>MU-2 stereo imaging system</cell></row><row><cell>[45]/Bosphorus</cell><cell>point cloud</cell><cell>yes</cell><cell>105</cell><cell>4666</cell><cell>The Inspeck Mega Capturor II 3D scanner</cell></row><row><cell>[46]/BU-4DFE</cell><cell>3D video</cell><cell>yes</cell><cell>101</cell><cell>60600</cell><cell>Di3D (Dimensional Imaging) dynamic system</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Keypoint-based methods    </figDesc><table><row><cell>Category/Reference,Year</cell><cell>Database</cell><cell>Matching</cell><cell>Limitation</cell><cell>Advantage (Robustness)</cell></row><row><cell>SIFT-like</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mian et al. 2008[26]</cell><cell>FRGC v2.0</cell><cell>Graph</cell><cell>Occlusion, partial scans</cell><cell>Expression</cell></row><row><cell>Mayo and Zhang 2009[49]</cell><cell>GavabDB</cell><cell>Weighted matching</cell><cell>Complexity, expression</cell><cell>Frontal neutral</cell></row><row><cell>Huang et al. 2010[51]</cell><cell>FRGC v2.0</cell><cell>Hybrid</cell><cell>Noise</cell><cell>Registration-free (frontal), partial occlusion</cell></row><row><cell>Berretti et al. 2011[48]</cell><cell>FRGC v2.0</cell><cell>χ 2 dist</cell><cell>Cost, keypoints redundancy</cell><cell>Partial occlusion</cell></row><row><cell>Huang et al. 2012[72]</cell><cell cols="2">FRGC v2.0, Bosphorus, Hybrid</cell><cell cols="2">Large pose (manual landmarks) Registration-free (frontal)</cell></row><row><cell></cell><cell>Gavab DB</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inan and Halici 2012[50]</cell><cell>FRGC v2.0</cell><cell>Cosine dist</cell><cell>Noise</cell><cell>Neutral expression</cell></row><row><cell cols="3">Soltanpour and Wu 2016[53] FRGC v2.0, Bosphorus Histogram matching</cell><cell>Pose</cell><cell>Expression</cell></row><row><cell>Lei et al. 2016[54]</cell><cell>Bosphorus, GavabDB,</cell><cell>Two-Phase Weighted</cell><cell>Extreme pose, expression</cell><cell>Partial data, time efficient</cell></row><row><cell></cell><cell>UMB-DB, SHREC08,</cell><cell>Collaborative Representation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BU-3DFE, FRGC v2.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mesh-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Li et al. 2011[57]</cell><cell>Bosphorus</cell><cell>Cosine dist</cell><cell>Occlusion, pose, missing parts</cell><cell>Expression</cell></row><row><cell>Smeets et al. 2013[56]</cell><cell cols="2">FRGC v2.0, Bosphorus, Angles comparison</cell><cell>Noise</cell><cell>Expression, partial data</cell></row><row><cell></cell><cell>SHREC11</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Berretti et al. 2014[58]</cell><cell cols="2">FRGC v2.0, BU-3DFE, Bhattacharyya dist</cell><cell>Noise,verification accuracy</cell><cell>Expression, occlusion, missing parts</cell></row><row><cell></cell><cell>Bosphorus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Li et al. 2015[60]</cell><cell cols="2">Bosphorus, FRGC v2.0 Fine-Grained Matcher</cell><cell>Cost</cell><cell>Expression, occlusion</cell></row><row><cell>Elaiwat et al. 2015[61]</cell><cell cols="2">FRGC v2.0, BU-3DFE, Cosine dist</cell><cell>Occlusion, missing data</cell><cell>Illumination, expressions</cell></row><row><cell></cell><cell>Bosphorus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Landmarks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Koudelka et al. 2005[66]</cell><cell>FRGC v1.0</cell><cell>Hausdorff dist</cell><cell>Pose</cell><cell>Expression, missing data</cell></row><row><cell>Lu et al. 2006[62]</cell><cell>MSU, USF [62]</cell><cell cols="2">Hybrid iterative closest point (ICP) Expression</cell><cell>Pose, light</cell></row><row><cell>Colombo et al. 2006[64]</cell><cell>150 3D faces</cell><cell>PCA-based</cell><cell>Noise, artifacts</cell><cell>Pose, expression, light</cell></row><row><cell>Gupta et al. 2010[67]</cell><cell>Texas 3DFRD</cell><cell>Euclidean dist</cell><cell>Large expression, pose</cell><cell>Fiducial point localization error</cell></row><row><cell>Creusot et al. 2011[65]</cell><cell>FRGC v2.0</cell><cell>-</cell><cell>Complexity</cell><cell>Expression</cell></row><row><cell>Creusot et al. 2013[5]</cell><cell cols="2">FRGC v2.0, Bosphorus -</cell><cell>Complexity</cell><cell>Expression</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Curve-based methods</figDesc><table><row><cell>Category/Reference,Year</cell><cell>Database</cell><cell>Matching</cell><cell>Limitation</cell><cell>Advantage (Robustness)</cell></row><row><cell>Contours</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Samir et al. 2006[73]</cell><cell>FSU, ND [8]</cell><cell cols="2">Euclidean,geometric dist Occlusion, missing parts</cell><cell>Expression</cell></row><row><cell>Mpiperis et al. 2007[77]</cell><cell>About 800 images (20 persons)</cell><cell>Euclidean dist</cell><cell>Expression</cell><cell>Translation, planar rotation</cell></row><row><cell>Feng et al. 2007[80]</cell><cell>FRGC v2.0</cell><cell>Cosine dist</cell><cell>Occlusion, missing data</cell><cell>Pose, expression</cell></row><row><cell>Mpiperis et al. 2007[79]</cell><cell>BU-3DFE, DB (70 people)</cell><cell>PCA-based</cell><cell>Noise</cell><cell>Large expression</cell></row><row><cell>Li et al. 2008[78]</cell><cell>CASIA</cell><cell cols="2">Mahalanobis cosine dist Open mouths</cell><cell>Expression</cell></row><row><cell>Jahanbin et al. 2008[76]</cell><cell>1196 images (119 subjects)</cell><cell>Euclidean dist, Support</cell><cell>Occlusion, missing part</cell><cell>Expression</cell></row><row><cell></cell><cell></cell><cell>vector machine (SVM)</cell><cell></cell><cell></cell></row><row><cell>Samir et al. 2009[74]</cell><cell>Laser-scanned</cell><cell>Riemannian framework</cell><cell>Occlusion, missing part</cell><cell>Expression</cell></row><row><cell>Berretti et al. 2010[75]</cell><cell>FRGC v2.0, SHREC08</cell><cell>Graph</cell><cell cols="2">Large expression (open mouths) Identification (large DB)</cell></row><row><cell>Profiles</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zhang et al. 2006[88]</cell><cell>382 faces (166 subjects)</cell><cell>Mean dist</cell><cell>Extreme expression</cell><cell>Fast</cell></row><row><cell>Li and Da 2012[86]</cell><cell>FRGC v2.0</cell><cell>Region-based (ICP)</cell><cell cols="2">Large expression, hair occlusion Expression, time efficient</cell></row><row><cell>Ballihi et al. 2012[83]</cell><cell>FRGC v2.0</cell><cell>AdaBoost</cell><cell>Occlusions</cell><cell>Efficient (data storage,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>transmission cost), expression</cell></row><row><cell>Drira et al. 2013[82]</cell><cell cols="2">FRGC v2.0, GavabDB, Bosphorus Riemannian framework</cell><cell cols="2">Extreme expression, complexity Pose, missing data</cell></row><row><cell>Berretti et al. 2013[85]</cell><cell>FRGC v2.0, GavabDB, UND</cell><cell>Sparse</cell><cell>Large pose, expression</cell><cell>Missing parts</cell></row><row><cell>Lei et al. 2014[84]</cell><cell>FRGC v2.0, SHREC08</cell><cell>SVM</cell><cell>Occlusion, missing parts</cell><cell>Efficient, expression</cell></row><row><cell cols="2">Emambakhsh and Evans 2016[90] FRGC, Bosphorus, BU-3DFE</cell><cell cols="2">Mahalanobis, cosine dist Occlusion</cell><cell>Expression</cell></row><row><cell>Al-Osaimi 2016[89]</cell><cell>FRGC, 3D-TEC, Bosphorus</cell><cell>Euclidean dist</cell><cell>Occlusion</cell><cell>Expression</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Local surface-based methods</figDesc><table><row><cell></cell><cell>Category/Reference, Year</cell><cell>Database</cell><cell>Matching</cell><cell>Limitation</cell><cell>Advantage (Robustness)</cell></row><row><cell></cell><cell>LBP-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Li et al. 2005[91]</cell><cell>2305 images (2D+3D)</cell><cell>AdaBoost</cell><cell>Occlusion, missing data</cell><cell>Pose, expression, lighting</cell></row><row><cell></cell><cell>Huang et al. 2006[92]</cell><cell>FRGC v2.0</cell><cell>χ 2 divergence</cell><cell>Pose, occlusion</cell><cell>Expression</cell></row><row><cell></cell><cell>Tang et al. 2013[93]</cell><cell>FRGC v2.0, BJUT-3D [93]</cell><cell>Nearest-neighbor (NN)</cell><cell>Occlusion, missing data</cell><cell>Expression</cell></row><row><cell></cell><cell>Li et al. 2014[94]</cell><cell>FRGC v2.0, Bosphorus,</cell><cell>Sparse-based</cell><cell>Pose, occlusion</cell><cell>Expression, fast</cell></row><row><cell></cell><cell></cell><cell>BU-3DFE, 3D-TEC</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Werghi et al. 2016[96]</cell><cell>BU-3DFE, Bosphorus</cell><cell>Cosine, χ 2 dist</cell><cell>Pose</cell><cell>Expression, missing data</cell></row><row><cell></cell><cell>Geometric features</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Xu et al. 2004[97]</cell><cell>3D RMA [111]</cell><cell>NN</cell><cell>Expression</cell><cell>Pose</cell></row><row><cell></cell><cell>Li and Zhang 2007[98]</cell><cell>GavabDB, FRGC v2.0</cell><cell>NN</cell><cell>Manual feature</cell><cell>Expression</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>selection, occlusions</cell></row><row><cell></cell><cell>Li et al. 2009[99]</cell><cell>GavabDB, FRGC v2.0</cell><cell>Sparse representation</cell><cell>Manual facial markers</cell><cell>Expression</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>detection, complexity</cell></row><row><cell></cell><cell>Lei et al. 2013[29]</cell><cell>FRGC v2.0, BU-3DFE</cell><cell>SVM</cell><cell>Occlusion, missing data</cell><cell>Expression, cost</cell></row><row><cell></cell><cell>Tabia et al. 2014[100]</cell><cell>GavabDB</cell><cell>Riemannian framework</cell><cell>Occlusion, missing data</cell><cell>Expression</cell></row><row><cell></cell><cell>Hariri et al. 2016[101]</cell><cell>FRGC v2.0, GavabDB</cell><cell>Geodesic dist</cell><cell>Partial occlusions</cell><cell>Expression, pose</cell></row><row><cell></cell><cell>Others</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Chua et al. 2000[102]</cell><cell>6 subjects</cell><cell>Euclidean dist</cell><cell>Misalignment, noise,</cell><cell>Translation, rotation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>extreme expression, complexity</cell></row><row><cell></cell><cell>Mian et al. 2005[106]</cell><cell>UND</cell><cell>Linear correlation</cell><cell>Expression</cell><cell>Pose, no pre-processing</cell></row><row><cell></cell><cell cols="2">Wang and Chua 2006[103] 80 persons</cell><cell>Structural Hausdorff dist</cell><cell cols="2">Hair covering, significant expression Viewing direction, translation, rotation</cell></row><row><cell></cell><cell>Chang et al. 2006[104]</cell><cell>4000 scans (449 subjects)</cell><cell>ICP-based</cell><cell>Find landmarks, hair on the face</cell><cell>Expression</cell></row><row><cell></cell><cell>Faltemier et al. 2008[105]</cell><cell>FRGC v2.0</cell><cell>ICP-based</cell><cell>Non-frontal, incomplete data</cell><cell>Expression</cell></row><row><cell></cell><cell cols="2">Al-Osaimi et al. 2008[107] FRGC v2.0</cell><cell>PCA-based</cell><cell>Hair covering, noise</cell><cell>Mild expression</cell></row><row><cell></cell><cell cols="2">Ocegueda et al. 2013[110] FRGC v2.0, BU-3DFE,</cell><cell cols="2">Linear Discriminant Analysis Pose, occlusion</cell><cell>Expression</cell></row><row><cell></cell><cell></cell><cell>Bosphorus</cell><cell>(LDA)-based</cell><cell></cell></row><row><cell></cell><cell>Ming 2015[108]</cell><cell cols="2">FRGC v2.0, CASIA, BU-3DFE Regional, global regression</cell><cell>Patches detection</cell><cell>Large pose, efficient</cell></row><row><cell></cell><cell cols="5">All local 3D face approaches surveyed in this paper are divided into keypoint-based,</cell></row><row><cell>505</cell><cell cols="5">curve-based, and surface-based. The keypoint-based category is successful in handling</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Table <ref type="table">5</ref>: Performance of local feature methods on FRGC v2.0 Database: 0.1% FAR VR and RR1 ("n/n: neutral vs. neutral", "n/a: neutral vs. all", "n/nn: neutral vs. non-neutral", "a/a: all vs. all")  <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b56">53,</ref><ref type="bibr" target="#b64">61]</ref>. In addition, meshSIFT <ref type="bibr" target="#b59">[56]</ref>, meshDOG <ref type="bibr" target="#b61">[58]</ref>, and keypoint detector using PCA <ref type="bibr" target="#b27">[26]</ref> extract the distinct descriptors from the patches around keypoints and benefit from the advantages of the local surface-based category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>515</head><p>The second category, curve-based methods, considers contours and profiles. Although curves are less sparse than keypoints, some parts of the face shape can be missing <ref type="bibr" target="#b13">[13]</ref>. Hence, most of the methods in this category are not robust against occlusion and missing data <ref type="bibr" target="#b77">[74,</ref><ref type="bibr" target="#b79">76,</ref><ref type="bibr" target="#b83">80,</ref><ref type="bibr" target="#b86">83,</ref><ref type="bibr" target="#b87">84,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b91">89,</ref><ref type="bibr" target="#b92">90]</ref>. Most of the profile-based methods, for instance <ref type="bibr" target="#b86">[83,</ref><ref type="bibr" target="#b87">84,</ref><ref type="bibr" target="#b88">86]</ref>, are computationally efficient and handle facial expressions.  Geodesic representation of the facial surface describes the invariant properties under isometric deformations. Therefore, iso-geodesics in <ref type="bibr" target="#b77">[74,</ref><ref type="bibr" target="#b78">75,</ref><ref type="bibr" target="#b79">76,</ref><ref type="bibr" target="#b81">78,</ref><ref type="bibr" target="#b82">79]</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<title level="m">Handbook of Remote Biometrics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Handbook of face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent advances in visual and infrared face recognitiona review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understand-585 ing 97</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="103" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A machine-learning approach to keypoint detection and landmarking on 3D meshes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Creusot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="146" to="179" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of approaches to three-dimensional 590 face recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="358" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of 3D face recognition methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Scheenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruifrok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-based Biometric Person Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="891" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of approaches and challenges in 3D and multi-modal 3D+ 2D face recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sabatino, 2D and 3D face recognition: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1885" to="1906" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Objective 3D face recognition: Evolution, approaches and challenges</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Clement</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic science international</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="132" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent advances on singlemodal and multimodal face recognition: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Creighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans-605 actions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="701" to="716" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comprehensive survey on pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative 610 study of 3D face recognition under expression variations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="710" to="727" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Static and dynamic 3D facial expression recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">615</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="683" to="697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: history, trends, and affect-related applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1568" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Feature selection for 2D and 3D face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Wiley Encyclopedia of Electrical and Electronics Engineering</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D face recognition using 3D alignment for PCA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boehnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1391" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deformation modeling for robust 3D face matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1346" to="1357" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust 3D face recognition by local shape difference boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1870" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning the spherical harmonic features for 3D face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="914" to="925" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative closest normal point for 3D face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mohammadzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">635</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="397" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep, dense and accurate 3D face correspondence for generating population specific deformable models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eastwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="238" to="250" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape index SIFT: range image recognition us-640 ing local features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bayramoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="352" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation 645 invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Keypoint detection and local feature matching for textured 3D face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pears, 3D face recognition, in: 3D Imaging, Analysis and Applica-655 tions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="311" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An efficient 3D face recognition approach based on the fusion of novel local low-level features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>El-Sallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="37" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D assisted face recognition: A 660 survey of 3D imaging, modelling and recognition approachest</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="114" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel technique for face recognition using range imaging</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hesher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international symposium on signal processing 665 and its applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gavabdb: a 3D face database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Biometrics on the Internet</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabello</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2061" to="2064" />
		</imprint>
	</monogr>
	<note>Multimodal 2D, 2.5D &amp; 3D face verification</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on automatic face and gesture recognition (FG)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Three-dimensional face recognition using combinations of surface feature map subspace components, Image and Vision Com-675 puting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heseltine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="382" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An evaluation of multimodal 2D+ 3D face biometrics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="624" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning effective intrinsic features to boost 3D-based face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">685</biblScope>
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using a multi-instance enrollment representation to improve 3D face recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Faltemier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Ter Haar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<title level="m">Shape retrieval contest 2008: 3D 690 face scans</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="225" to="226" />
		</imprint>
	</monogr>
	<note>Shape Modeling International</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3D twins and expression challenge</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2100" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Shrec&apos;11 track: 3D face models retrieval</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Jole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hermans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cusano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umb-Db</forename></persName>
		</author>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2113" to="2119" />
		</imprint>
	</monogr>
	<note>A database of partially occluded 3D faces</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Texas 3D face recognition database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Castleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Markey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Southwest Symposium on Image Analysis &amp; Interpretation (SSIAI)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alyüz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>¸eliktutan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gökberk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<title level="m">European Workshop on 705 Biometrics and Identity Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>Bosphorus database for 3D face analysis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A high-resolution 3D dynamic facial expression database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference On Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comprehensive 710 performance evaluation of 3D local feature descriptors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="89" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3D partial face matching using local shape descriptors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D face recognition using multiview keypoint matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="290" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3D face recognition with local shape descriptors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Halici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="577" to="587" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3D face recognition using distinctiveness enhanced facial representations and local feature hybrid matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardabilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal 2D-3D face recognition 725 using structural context and pyramidal shape index</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anvaripour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IET International Conference on Imaging for Crime Prevention and Detection</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multimodal 2D-3D face recognition using local descriptors: pyramidal shape map and structural context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A two-phase weighted collaborative representation for 3D partial face recognition with single sample</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="218" to="237" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Feature detection on 3D face surfaces for pose normalisation and recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keustermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">735</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">meshSIFT: Local surface features for 3D face recognition under expression variations and partial data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keustermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="169" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Expression robust 3D face recognition via mesh-based histograms of multiple order surface differential</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen ; A C C E P T E D M A N U S C R I P T Quantities</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3053" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Selecting stable keypoints and 745 local descriptors for person identification using 3D face scans, The Visual Computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1275" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Matching 3D face scans using interest points and local histogram descriptors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="509" to="525" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards 3D face recognition in the real: a registration-free approach using fine-grained matching of 3D keypoint descriptors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="142" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A curvelet-based ap-755 proach for textured 3D face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elaiwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Sallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1235" to="1246" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Matching 2.5D face scans to 3D models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colbry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="43" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">3D facial landmark de-760 tection under large yaw and expression variations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1552" to="1564" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">3D face detection using curvature analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cusano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="444" to="455" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic keypoint detection on 3D faces using 765 a dictionary of local shapes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Creusot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A prescreener for 3D face recognition using radial symmetry and the hausdorff fraction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Koudelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Russ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="168" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Anthropometric 3D face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Markey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust 3D face landmark localization based on local coordinate coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5108" to="5122" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">3D facial landmark localization with asymmetry patterns and shape regression from incomplete local features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sukno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Waddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Whelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1717" to="1730" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An automatic 3D facial land-780 marking algorithm using 2D Gabor Wavelets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wollstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hysi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Koudstaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kayser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="580" to="588" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">3D object recognition in cluttered scenes with local surface features: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2270" to="2287" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">3D face recognition using eLBPbased facial description and local feature hybrid matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardabilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1551" to="1565" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Three-dimensional face recognition using shapes of facial curves</title>
		<author>
			<persName><forename type="first">C</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine</title>
		<imprint>
			<biblScope unit="volume">790</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1858" to="1863" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Intelligence</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">An intrinsic framework for analysis of facial surfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="95" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">3D face recognition using isogeodesic stripes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">795</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2162" to="2177" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Three dimensional face recognition using iso-geodesic and iso-depth curves</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jahanbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">3D face recognition by point signatures and iso-contours</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mpiperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malasiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing, Pattern Recognition, and Applications</title>
		<imprint>
			<publisher>ACTA Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="328" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">3D face recognition by constructing deformation invariant image</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1596" to="1602" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">3D face recognition with the geodesic polar representation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mpiperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-2</biblScope>
			<biblScope unit="page" from="537" to="547" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">3D face recognition using euclidean integral invariants signature</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Krim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Statistical Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="810" to="156" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Pose and expression-invariant 3D face recognition using elastic radial curves</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British machine vision conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">3D face recogni-815 tion under expressions, occlusions, and pose variations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Slama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2270" to="2283" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Boosting 3D geometric features for efficient face recognition and gender classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ballihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aboutajdine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1766" to="1779" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">An efficient 3D face recognition approach using local geometrical signatures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">forensics and security</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="389" />
			<date type="published" when="2013">2014. 2013</date>
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Efficient 3D face recognition handling facial expression and hair occlusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="668" to="679" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A method for registration of 3D shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="830" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">3D face authentication and recognition based on bilateral symmetry analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Femiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lockwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A novel multi-purpose matching representation of local 3d 835 surfaces: A rotationally invariant, efficient, and highly discriminative approach with an adjustable sensitivity</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Al-Osaimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="658" to="672" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Nasal patches and curves for an expression-robust 3D face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Emambakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelli-840 gence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning to fuse 3D+ 2D based face recognition at both feature and decision levels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Combining statistics of geometrical and correlative 845 features for 3D face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">3D face recognition using local binary patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2190" to="2198" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Expression-robust 3D face recognition via weighted sparse representation of multi-scale and multi-850 component local normal patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="179" to="193" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The mesh-LBP: a framework for extracting local binary patterns from discrete manifolds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Boosting 3D LBP-based face 855 recognition by fusing shape and texture descriptors on the mesh</title>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tortorici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="964" to="979" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Automatic 3D face recognition combining global geometric features with local shape variation information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on Automatic face and gesture recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="860" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Adapting geometric attributes for expression-invariant 3D face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Shape Modeling and Applications (SMI)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Expression-insensitive 3D face recognition using sparse 865 representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2575" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Covariance descriptors for 3D shape matching and retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4185" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">3D face recognition using covariance based descriptors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benouareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Declercq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">3d human face recognition using point signature</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Robust face recognition from 2D and 3D images using structural hausdorff distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1695" to="1700" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Image and Vision Computing</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A region ensemble for 3D face recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Faltemier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="73" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Matching tensors for pose invariant 885 automatic 3D face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="120" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Integration of local and global geometrical cues for 3D face recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Al-Osaimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1030" to="1040" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Three-dimensional face recognition in the presence of facial expressions: An annotated deformable model approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murtuza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on 895 Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="649" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">3D face discriminant analysis using Gauss-Markov posterior marginals</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ocegueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="728" to="739" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Automatic 3D face authentication, Image and Vision 900</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Acheroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
