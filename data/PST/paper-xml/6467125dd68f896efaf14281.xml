<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Propagation Transformer for Graph Representation Learning</title>
				<funder ref="#_Ut8uYF7 #_dmxTW5x">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-19">19 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<email>chenzhe98@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country>Univerisity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country>Univerisity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianrun</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country>Univerisity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country>Univerisity</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiuying</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Qi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Propagation Transformer for Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-19">19 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.11424v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e., node-to-node, node-to-edge, and edgeto-node, which is essential for learning graphstructured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GP-Trans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-ofthe-art transformer-based graph models with better performance. The code will be released at https: //github.com/czczup/GPTrans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many real-world scenarios, information is usually organized by graphs, and graph-structured data can be used in many research areas, including communication networks and molecular property prediction, etc. For instance, based on social graphs, lots of algorithms are proposed to classify users into meaningful social groups in the task of social network research, which can produce many useful practical applications such as user search and recommendations. Therefore, graph representation learning has become a hot topic in pattern recognition and machine learning <ref type="bibr">[Cai and Lam, 2020;</ref><ref type="bibr" target="#b19">Ying et al., 2021;</ref><ref type="bibr" target="#b0">Brossard et al., 2020]</ref>.</p><p>With the development of deep learning, many methods have been developed for graph representation learning <ref type="bibr" target="#b12">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b19">Zhang et al., 2019;</ref><ref type="bibr" target="#b19">Ying et al., 2021;</ref><ref type="bibr" target="#b5">Hussain et al., 2021;</ref><ref type="bibr" target="#b13">Ramp??ek et al., 2022]</ref>. In general, these methods can be approximately divided into two parts. The first category mainly focuses on performing Graph Neural Networks (GNNs) on graph data. These methods follow the convolutional pattern to define the convolution operation in the graph data, and design effective neighborhood aggregation schemes to learn node representations by fusing the node and graph topology information. The representative method is Graph Convolutional Network (GCN) <ref type="bibr" target="#b7">[Kipf and Welling, 2016]</ref>, which learns the representation of a node in the graph by considering fusing its neighbors. After that, many GCN variants <ref type="bibr" target="#b19">[Xu et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2020;</ref><ref type="bibr">Liu et al., 2021a;</ref><ref type="bibr" target="#b1">Bresson and Laurent, 2017]</ref> containing different neighborhood aggregation schemes have been developed. The second kind of method is to build graph models based on the transformer architecture. For example, Cai and Lam <ref type="bibr">[2020]</ref> utilized the explicit relation encoding between nodes and fused them into the encoder-decoder transformer network for effective graph-to-sequence learning. Graphormer <ref type="bibr" target="#b19">[Ying et al., 2021]</ref> established state-of-the-art performance on graph-level prediction tasks by transforming the structure and edge features of the graph into attention biases.</p><p>Although recent transformer-based methods report promising performance for graph representation learning, they still suffer the following problems. (1) Not explicitly employ the relationship among nodes and edges in the graph data. Re-cent transformer-based methods <ref type="bibr">[Cai and Lam, 2020;</ref><ref type="bibr" target="#b19">Ying et al., 2021]</ref> only simply fuse nodes and edges information by using positional encodings. However, due to the complexity of graph structure, how to fully employ the relationship among nodes and edges for graph representation learning remains to be studied. <ref type="bibr">(2)</ref> Inefficient dual-FFN structure in the transformer block. Recent works resort to the dual-path structure in the transformer block to incorporate the edge information. For instance, the Edge-augmented Graph Transformer (EGT) <ref type="bibr" target="#b5">[Hussain et al., 2021]</ref> adopted dual feed-forward networks (FFN) in the transformer block to update the edge embeddings, and let the structural information evolve from layer to layer. However, this paradigm learns the information of edges and nodes separately, which introduces more calculations and easily leads to the low efficiency of the model.</p><p>To overcome these issues, we propose an efficient and powerful transformer architecture for graph learning, termed Graph Propagation Transformer (GPTrans). A key design element of GPTrans is its Graph Propagation Attention (GPA) module. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, the GPA module propagates the information among the node embeddings and edge embeddings of the preceding layer by modeling three connections, i.e., node-to-node, node-to-edge, and edge-to-node, which significantly enhances modeling capability (see Table <ref type="table" target="#tab_2">1</ref>). This design benefits us no longer the need to maintain an FFN module specifically for edge embeddings, bringing higher efficiency than previous dual-FFN methods.</p><p>The contributions of our work are as follows:</p><p>(1) We propose an effective Graph Propagation Transformer (GPTrans), which can better model the relationship among nodes and edges and represent the graph.</p><p>(2) We introduce a novel attention mechanism in the transformer blocks, which explicitly passes the information among nodes and edges in three ways. These relationships play a critical role in graph representation learning.</p><p>(3) Extensive experiments show that the proposed GPTrans model outperforms many state-of-the-art transformer-based methods on benchmark datasets with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer</head><p>The past few years have seen many transformer-based models designed for various language <ref type="bibr">[Vaswani et al., 2017;</ref><ref type="bibr" target="#b12">Radford et al., 2019;</ref><ref type="bibr" target="#b0">Brown et al., 2020]</ref> and vision tasks <ref type="bibr" target="#b12">[Parmar et al., 2018;</ref><ref type="bibr">Liu et al., 2021b]</ref>. For example, in the field of vision, <ref type="bibr" target="#b1">Dosovitskiy et al. [2021]</ref> presented the Vision Transformer (ViT), which decomposed an image into a sequence of patches and captured their mutual relationships. However, training ViT on large-scale datasets can be computationally expensive. To address this issue, <ref type="bibr">DeiT [Touvron et al., 2021]</ref> proposed an efficient training strategy that enabled ViT to deliver exceptional performance even when trained on smaller datasets. Nevertheless, the complexity and performance of ViT remain challenging. To overcome these limitations, researchers further proposed many well-designed models <ref type="bibr">[Liu et al., 2021b;</ref><ref type="bibr" target="#b16">Wang et al., 2021;</ref><ref type="bibr">Wang et al., 2022a;</ref><ref type="bibr" target="#b1">Chen et al., 2023;</ref><ref type="bibr" target="#b5">Ji et al., 2023;</ref><ref type="bibr" target="#b1">Chen et al., 2022;</ref><ref type="bibr">Wang et al., 2022b]</ref>.</p><p>Recently, the self-attention mechanism and transformer architecture have been gradually introduced into the graph representation learning tasks, such as graph-level prediction <ref type="bibr" target="#b19">[Ying et al., 2021;</ref><ref type="bibr" target="#b5">Hussain et al., 2021]</ref>, producing competitive performance compared to the traditional GNN models. The early self-attention-based GNNs focused on adopting the attention mechanism to a local neighborhood of each node in a graph, or directly on the whole graph. For example, Graph Attention Network (GAT) <ref type="bibr" target="#b15">[Veli?kovi? et al., 2017]</ref> and Graph Transformer (GT) <ref type="bibr" target="#b1">[Dwivedi and Bresson, 2020]</ref> utilized self-attention mechanisms as local constraints for the local neighborhood of each node. In contrast to employing local self-attention for graph learning, Graph-BERT <ref type="bibr" target="#b20">[Zhang et al., 2020]</ref> introduced the global self-attention mechanism in a revised transformer network to predict one masked node in a sampled subgraph.</p><p>In addition, several works have attempted to use the transformer architecture to tackle graph-related tasks directly. Two notable examples are <ref type="bibr">[Cai and Lam, 2020]</ref> and Graphormer <ref type="bibr" target="#b19">[Ying et al., 2021]</ref>. The former method adopted explicit relation encoding between nodes and integrated them into the encoder-decoder transformer network, to enable graphto-sequence learning. The latter mainly regarded the structure and edges of the graph as the attention biases, which were incorporated into the transformer block. With the help of these attention biases, Graphormer achieved leading performance on graph-level prediction tasks (e.g., classification and regression on molecular graphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Network</head><p>Graph Convolutional Network (GCN) is a kind of deep neural network that extends the CNN from grid data (e.g., image and video) to graph-structured data. Generally speaking, GCN methods can be approximately divided into two types: spectral-based methods <ref type="bibr" target="#b0">[Bruna et al., 2013;</ref><ref type="bibr" target="#b1">Defferrard et al., 2016;</ref><ref type="bibr" target="#b3">Henaff et al., 2015;</ref><ref type="bibr" target="#b7">Kipf and Welling, 2016]</ref> and non-spectral methods <ref type="bibr" target="#b0">[Chen et al., 2018;</ref><ref type="bibr" target="#b1">Gilmer et al., 2017;</ref><ref type="bibr" target="#b13">Scarselli et al., 2008;</ref><ref type="bibr" target="#b15">Veli?kovi? et al., 2017]</ref>.</p><p>Spectral GCN methods are designed under the theory of spectral graphs. For instance, Spectral <ref type="bibr">GCN [Bruna et al., 2013]</ref> resorted to the Fourier basis of a graph to conduct convolution operation in the spectral domain, which is the first work on spectral graph CNNs. Based on <ref type="bibr" target="#b0">[Bruna et al., 2013]</ref>, <ref type="bibr" target="#b1">Defferrard et al. [2016]</ref> designed a strict control over the local support of filters and avoided an explicit use of the Graph Fourier basis in the convolution, which achieved better accuracy. <ref type="bibr" target="#b7">Kipf and Welling [2016]</ref> adopted the first-order approximation of the spectral graph convolution to simplify commonly used GNN.</p><p>On the other hand, non-spectral methods directly define convolution operations on the graph data. GraphSage <ref type="bibr" target="#b2">[Hamilton et al., 2017]</ref> proposed learnable aggregator functions in the network to fuse neighbors' information for effective graph representation learning. In GAT <ref type="bibr" target="#b15">[Veli?kovi? et al., 2017]</ref>, different weight matrices are used for nodes with different degrees for graph representation learning. In addition, another line of GCN methods is mainly designed for specific graphlevel tasks. For example, some techniques such as subsampling <ref type="bibr" target="#b1">[Chen et al., 2017]</ref> and inductive representation for a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Blocks Graph</head><formula xml:id="formula_0">Graph Propagation Attention Graph Embedding ? !"#$ ? $#%$ ?? Feed-Forward Network Figure 2:</formula><p>Overall architecture of GPTrans. It contains a graph embedding layer, L transformer blocks, and a head. The graph embedding layer transforms the graph data into node embeddings x node and edge embeddings x edge , as the input of the transformer blocks. Each transformer block comprises a Graph Propagation Attention (GPA) and a Feed-Forward Network (FFN). It is worth noting that we no longer need to maintain an FFN module specifically for edge embeddings due to the proposed GPA module, which improves the efficiency of our method. Finally, a head of two fully-connected layers is employed on the output embeddings for various graph tasks.</p><p>large graph <ref type="bibr" target="#b2">[Hamilton et al., 2017]</ref> have been introduced for better graph representative learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GPTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>An overview of the proposed GPTrans framework is depicted in Figure <ref type="figure">2</ref>. Specifically, it takes a graph G = (V, E) as input, in which nodes V = {v 1 , v 2 , . . . , v n }, E indicates edges between nodes, and n means the number of nodes. The pipeline of GPTrans can be divided into three parts: graph embedding, transformer blocks, and prediction head.</p><p>In the graph embedding layer, for each given graph G, we follow <ref type="bibr" target="#b19">[Ying et al., 2021;</ref><ref type="bibr" target="#b5">Hussain et al., 2021]</ref> to add a virtual node [v 0 ] into V , to aggregate the information of the entire graph. Thus, the newly-generated node set with the virtual node is represented as</p><formula xml:id="formula_1">V = {[v 0 ], v 1 , v 2 , . . . , v n },</formula><p>and the number of nodes is updated to |V | = 1 + n. For more adequate information propagation across the whole graph, each node and edge is treated as a token. In detail, we transform the input nodes into a sequence of node embeddings x node ? R (1+n)?d1 , and encode the input edges into a tensor of edge embeddings x edge ? R (1+n)?(1+n)?d2 .</p><p>Then, L transformer blocks with our re-designed selfattention operation (i.e., Graph Propagation Attention) are applied to node embeddings and edge embeddings. Both these embeddings are fed throughout all transformer blocks. After that, GPTrans generates the representation of each node and edge, in which the output embedding of the virtual node takes along the representation of the whole graph.</p><p>Finally, the head of our GPTrans is composed of two fullyconnected (FC) layers. For graph-level tasks, we employ it on top of the output embedding of the virtual node. For nodelevel or edge(link)-level tasks, we apply it to the output node embeddings or edge embeddings. In summary, benefiting from the proposed novel Graph Propagation Attention, our GPTrans can better support various graph tasks with only a little additional computational cost compared to the previous method Graphormer [ <ref type="bibr" target="#b19">Ying et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Embedding</head><p>The role of the graph embedding layer is to transform the graph data as the input of transformer blocks. As we know, in addition to the nodes, edges also have rich structural information in many types of graphs, e.g., molecular graphs <ref type="bibr">[Hu et al., 2021]</ref> and social graphs <ref type="bibr">[Huang et al., 2022]</ref>. Therefore, we encode both nodes and edges into embeddings to fully utilize the structure of the input graph.</p><p>For nodes in the graph, we transform each node into node embedding. Specifically, we follow <ref type="bibr" target="#b19">[Ying et al., 2021]</ref> to exploit the node attributes and the degree information, and add a virtual node [v 0 ] into the graph to collect and propagate graph-level features. Without loss of generality, taking a directed graph as an example, its node embeddings x node ? R (1+n)?d1 can be expressed as:</p><formula xml:id="formula_2">x node = x node attr + x deg -+ x deg + ,<label>(1)</label></formula><p>where x node attr , x deg -, and x deg + are embeddings encoded from node attributes, indegree, and outdegree statistics, respectively. d 1 is the dimension of node embeddings.</p><p>For edges in the graph, we also transform them into edge embeddings to help the learning of graph representation. In our implementation, the edge embeddings x edge ? R (1+n)?(1+n)?d2 are defined as:</p><formula xml:id="formula_3">x edge = x edge attr + x rel pos ,<label>(2)</label></formula><p>where x edge attr is encoded from the edge attributes, and x rel pos is a relative positional encoding that embeds the spatial location of node pairs. d 2 means the dimension of edge embeddings. We adopt the encoding of the shortest path distance by default following <ref type="bibr" target="#b19">[Ying et al., 2021]</ref>. In other words, for the position (i, j), x ij edge ? R d2 represents the learned structural embedding of the edge (path) between node v i and node v j in the graph G.</p><p>It is worth noting that, unlike Graphormer [Ying et al., 2021] that encodes edge attributes and spatial position as attention biases and shares them across all blocks, we optimize the edge embeddings x edge in each transformer block by the proposed Graph Propagation Attention. Then, the updated edge embeddings are fed into the next block. Therefore, each block of our model could adaptively learn different ways to exploit edge features and propagate information. This more flexible way is beneficial for graph representation learning, which we will show in later experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Propagation Attention</head><p>In recent years, many works  <ref type="bibr">[Vaswani et al., 2017]</ref> to perform node-to-node propagation. First, we use parameter matrices W Q , W K , and W V ? R d1?d1 to project the node embeddings x node to queries Q, keys K, and values V :</p><formula xml:id="formula_4">Q = x node W Q , K = x node W K , V = x node W V . (3)</formula><p>Unlike Graphormer <ref type="bibr" target="#b19">[Ying et al., 2021]</ref> that used shared attention biases in all blocks, we employ a parameter matrix W reduce ? R d2?n head to predict layer-specific attention biases ? ? R (1+n)?(1+n)?n head from the edge embeddings x edge , which can be written as:</p><formula xml:id="formula_5">? = x edge W reduce .<label>(4)</label></formula><p>Then, we add ? to the attention map of the query-key dot product and compute the output node embeddings x node . This process can be formulated as:</p><formula xml:id="formula_6">A = QK T ? d head + ?, x node = softmax(A)V,<label>(5)</label></formula><p>where A ? R (1+n)?(1+n)?n head represents the output attention map, and d head refers to the dimension of each head. In summary, since ? are projected from higher dimensional edge embeddings by the learnable matrix, our attention map A will have more flexible patterns to aggregate node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-to-Edge</head><p>To propagate node features to edges, we make some additional use of the attention map A. According to the definition of self-attention <ref type="bibr">[Vaswani et al., 2017]</ref>, attention map A captures the similarity between node embeddings. Therefore, considering both local and global connections, we add A with its softmax confidence, and expand its dimension to as same as x edge through the learnable matrix W expand ? R n head ?d2 . This operation is designed to perform explicit high-order spatial interactions, which can be written as follows:</p><formula xml:id="formula_7">x edge = (A + softmax(A))W expand .<label>(6)</label></formula><p>In this way, we achieve node-to-edge propagation without relying on an additional FFN module like GT <ref type="bibr" target="#b1">[Dwivedi and Bresson, 2020]</ref> and <ref type="bibr">EGT [Hussain et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge-to-Node</head><p>In this part, we delve into the following question: How to generate dynamic weights for edge embeddings x edge and fuse them into node embeddings x node ? Due to the computational efficiency, we do not additionally perform attention operation, but directly apply the softmax function to the just generated x edge ? R (1+n)?(1+n)?d2 in Eqn. 6 and calculate element-wise product with itself:</p><formula xml:id="formula_8">x node = FC(sum(x edge ? softmax(x edge ), dim = 1)),<label>(7)</label></formula><p>in which the fully-connected (FC) layer is used to align the dimension of edge embeddings and node embeddings. This process again explicitly introduces high-order spatial interactions. Finally, we add these two types of node embeddings, and employ a learnable matrix W O ? R d1?d1 to fuse them.</p><p>Then we have the updated node embeddings:</p><formula xml:id="formula_9">x node = (x node + x node )W O .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPA in Transformer Blocks</head><p>Equipped with our proposed GPA module, the block of our GPTrans can be calculated as follows: x l node = FFN(LN(x l node )) + xl node ,</p><formula xml:id="formula_10">xl node , x l edge += GPA(LN(x l-1 node ), x l-1 edge ),<label>(9)</label></formula><p>where LN(?) means layer normalization <ref type="bibr" target="#b0">[Ba et al., 2016]</ref>. xl node and x l edge denote the output node embeddings and edge embeddings of the GPA module for block l. And x l node represents the output node embeddings of the FFN module. Overall, our GPA module effectively extends the ability of our GPTrans to various graph tasks, but only introduces a small amount of extra overhead compared with previous methods <ref type="bibr" target="#b5">[Hussain et al., 2021;</ref><ref type="bibr" target="#b19">Ying et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Architecture Configurations</head><p>We build five variants of the proposed model with different model sizes, namely GPTrans-Nano, Tiny, Small, Base, and Large. Note that the number of parameters of our GPTrans is similar to <ref type="bibr">Graphormer [Ying et al., 2021]</ref> and EGT <ref type="bibr" target="#b5">[Hussain et al., 2021]</ref>. The dimension of each head is set to 10 for our nano model, and 32 for others. Following common practices, the expansion ratio of the FFN module is ? = 1 for all model variants. The architecture hyper-parameters of these five models are as follows:</p><p>? GPTrans-Nano: ? GPTrans-Small: d 1 = 384, d 2 = 48, layer number = 12</p><formula xml:id="formula_12">d 1 = 80, d 2 =</formula><p>? GPTrans-Base: d 1 = 608, d 2 = 76, layer number = 18</p><p>? GPTrans-Large:</p><formula xml:id="formula_13">d 1 = 736, d 2 = 92, layer number = 24</formula><p>The model size and performance of the model variants on the large-scale PCQM4M and PCQM4Mv2 benchmarks <ref type="bibr" target="#b5">[Hu et al., 2021]</ref> are listed in Table <ref type="table" target="#tab_2">1</ref>, and the analysis of model efficiency is provided in Table <ref type="table">6</ref>. More detailed model configurations are presented in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph-Level Tasks Datasets</head><p>We verify the following graph-level tasks:</p><p>(1) PCQM4M <ref type="bibr" target="#b5">[Hu et al., 2021]</ref>  of 53 million nodes. The task is to regress a DFT-calculated quantum chemical property, e.g., HOMO-LUMO energy gap.</p><p>(2) PCQM4Mv2 <ref type="bibr" target="#b5">[Hu et al., 2021]</ref> is an updated version of PCQM4M, in which the number of molecules slightly decreased, and some of the graphs are revised.</p><p>(3) MolHIV <ref type="bibr" target="#b4">[Hu et al., 2020]</ref> is a small-scale molecular property prediction dataset. It has 41, 127 graphs with a total of 1, 048, 738 nodes and 1, 130, 993 edges.</p><p>(4) MolPCBA <ref type="bibr" target="#b4">[Hu et al., 2020]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>For the large-scale PCQM4M and PCQM4Mv2 datasets, we use AdamW <ref type="bibr" target="#b11">[Loshchilov and Hutter, 2018]</ref> with an initial learning rate of 1e-3 as the optimizer. Following common practice, we adopt a cosine decay learning rate scheduler with a 20-epoch warmup. All models are trained for 300 epochs with a total batch size of 1024. When fine-tuning the MolHIV and MolPCBA datasets, we load the PCQM4Mv2 pre-trained weights as initialization. For the ZINC dataset, we train our GPTrans-Nano model from scratch. More detailed training strategies are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>First, we benchmark our GPTrans method on PCQM4M and PCQM4Mv2, two datasets from OGB large-scale challenge <ref type="bibr" target="#b5">[Hu et al., 2021]</ref>. We mainly compare our GPTrans against a set of representative transformer-based methods, including <ref type="bibr">GT [Dwivedi and Bresson, 2020]</ref>, <ref type="bibr">Graphormer [Ying et al., 2021]</ref>, <ref type="bibr">GRPE [Park et al., 2022]</ref>, <ref type="bibr">EGT [Hussain et al., 2021]</ref>, GPS <ref type="bibr" target="#b13">[Ramp??ek et al., 2022]</ref>, and TokenGT <ref type="bibr" target="#b6">[Kim et al., 2022]</ref>. As reported in Table <ref type="table" target="#tab_2">1</ref>, our method yields the state-ofthe-art validate MAE score on both datasets across different model complexities.</p><p>Further, we take the PCQM4Mv2 pre-trained weights as the initialization and fine-tune our models on the OGB molecular datasets MolPCBA and MolHIV, to verify the transfer learning capability of GPTrans. All experiments are performed five times with different random seeds, and we report the mean and standard deviation of the results. From Table <ref type="table" target="#tab_3">2</ref> and 3, we can see that GPTrans outperforms many strong counterparts, such as GRPE <ref type="bibr" target="#b11">[Park et al., 2022]</ref>, EGT <ref type="bibr" target="#b5">[Hussain et al., 2021], and</ref><ref type="bibr">Graphormer [Ying et al., 2021]</ref>.</p><p>Moreover, we follow previous methods <ref type="bibr" target="#b11">[Park et al., 2022;</ref><ref type="bibr" target="#b19">Ying et al., 2021]</ref> to train the GPTrans-Nano model with about 500K parameters on the ZINC subset from scratch. As demonstrated in Table <ref type="table" target="#tab_5">4</ref>, our model achieves a promising test MAE of 0.077 ? 0.009, bringing 36.9% relative MAE decline compared to Graphormer <ref type="bibr" target="#b19">[Ying et al., 2021]</ref>. The above inspiring results show that the proposed GPTrans performs well on graph-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node-Level Tasks</head><p>Datasets <ref type="bibr">PATTERN and CLUSTER [Dwivedi et al., 2020]</ref> are both synthetic datasets for node classification. Specifically, PAT-TERN has 10, 000 training, 2, 000 validation, and 2, 000 test graphs, and CLUSTER contains 10, 000 training, 1, 000 validation, and 1, 000 test graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>For the PATTERN and CLUSTER datasets, we train our GPTrans-Nano up to 1000 epochs with a batch size of 256. We employ the AdamW <ref type="bibr" target="#b11">[Loshchilov and Hutter, 2018]</ref> optimizer with a 20-epoch warmup. The learning rate is initialized to 5e-4, and is declined by a cosine scheduler. More training details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In this part, we compare our GPTrans-Nano with various GCN variants and recent graph transformers. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>We experiment on the TSP dataset in a similar setting to that used in the PATTERN and CLUSTER datasets. Details are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table" target="#tab_5">4</ref> compares the edge classification performance of our GPTrans-Nano model and previously transformer-based methods on the TSP dataset. We observe GPTrans can outperform Graphormer <ref type="bibr" target="#b19">[Ying et al., 2021]</ref> with a large margin and is comparable with <ref type="bibr">EGT [Hussain et al., 2021]</ref>, showing that the proposed GPA module design is competitive when used for edge-level tasks. By applying the GPA module, we avoid designing an inefficient dual-FFN network, which boosts the efficiency of our method. We will analyze the efficiency of GPTrans in detail in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct several ablation studies on the PCQM4Mv2 <ref type="bibr" target="#b5">[Hu et al., 2021]</ref> dataset, to validate the effectiveness of each key design in our GPTrans. Due to the limited computational resources, we adopt GPTrans-S as the base model, and train it with a shorter schedule of 100 epochs. Other settings are the same as described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Propagation Attention</head><p>To investigate the contribution of each key design in our GPA module, we gradually extend the Graphormer baseline <ref type="bibr" target="#b19">[Ying et al., 2021]</ref> to our GPTrans. As shown in Table <ref type="table" target="#tab_7">5</ref>, the model gives the best performance when all three information propagation paths are introduced. It is worth noting that the improvement from our node-to-node propagation is most significant, thanks to learning the attention biases for a particular layer rather than sharing them across all layers. In summary, our proposed GPA module collectively brings a large gain to Graphormer, i.e., 8.0% relative validate MAE decline on the PCQM4Mv2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deeper vs. Wider</head><p>Here we explore the question of whether the transformers for graph representation learning should go deeper or wider. For fair comparisons, we build a deeper but thinner model under comparable parameter numbers, by increasing the depth from 6 to 12 layers and decreasing the width from 512 to 384 dimensions. As reported in Table <ref type="table" target="#tab_7">5</ref>, the validate MAE of the PCQM4Mv2 dataset is declined from 0.0854 to 0.0835 by the deeper model, which shows that depth is more important than width for graph transformers. Based on this observation, we prefer to develop GPTrans with a large model depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency Analysis</head><p>As shown in Table <ref type="table">6</ref>, we benchmark the training time and inference throughputs of our GPTrans and EGT <ref type="bibr" target="#b5">[Hussain et al., 2021]</ref>. Specifically, we employ PyTorch1.12 and CUDA11.3 to perform these experiments. For a fair comparison, the training time of these two methods is measured using 8 Nvidia A100 GPUs with half-precision training and a total batch size of 1024. The inference throughputs of PCQM4Mv2 models in Table <ref type="table">6</ref> are tested using an A100 GPU with a batch size of 128, where our GPTrans is slightly faster in inference than EGT under a similar number of parameters. This preliminary study shows a good signal that the proposed GPTrans, equipped with the GPA module, could be an efficient model for graph representation learning.</p><p>This paper aims for graph representation learning with a Graph Propagation Transformer (GPTrans), which explores the information propagation among nodes and edges in a graph when establishing the self-attention mechanism in the transformer block. Especially in the GPTrans, we propose a Graph Propagation Attention (GPA) mechanism to explicitly pass the information among nodes and edges in three ways, i.e., node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. Extensive comparisons with state-of-the-art methods on several benchmark datasets demonstrate the superior capability of the proposed GPTrans with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution Statement</head><p>Zhe Chen and Hao Tan contributed equally to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>In Table <ref type="table">7</ref>, we list the information of the datasets used for training and evaluation. Next, we describe them in detail.</p><p>PCQM4M <ref type="bibr" target="#b5">[Hu et al., 2021]</ref> is a large-scale molecular dataset that includes 3.8 million molecular graphs and a total of 53 million nodes. The task is to regress a DFT-calculated quantum chemical property, e.g., the HOMO-LUMO energy gap of a given molecule. The HOMO-LUMO gap is one of the most practically relevant quantum chemical properties of molecules. Using efficient and accurate deep learning models to approximate DFT enables diverse downstream applications, e.g., drug discovery.</p><p>PCQM4Mv2 <ref type="bibr" target="#b5">[Hu et al., 2021]</ref> is an updated version of PCQM4M. In PCQM4Mv2, the number of molecules slightly decreased, and some of the graphs were revised.</p><p>MolHIV <ref type="bibr" target="#b4">[Hu et al., 2020]</ref> is a small-scale molecular property prediction dataset, and is part of the Open Graph Benchmark <ref type="bibr" target="#b5">[Hu et al., 2021]</ref>. Specifically, the MolHIV dataset is a molecular tree-like dataset consisting of 41, 127 graphs, with an average number of 25.5 nodes and 27.5 edges per graph.</p><p>The task is to predict whether a molecule inhibits HIV virus replication.</p><p>MolPCBA <ref type="bibr" target="#b4">[Hu et al., 2020]</ref>   <ref type="bibr">et al., 2020]</ref> is the most popular real-world molecular dataset to predict graph property regression for constrained solubility, which is an important chemical property for designing generative GNNs for molecules. To be specific, ZINC has 12,000 graphs, usually used as a benchmark for evaluating GNN performances.</p><p>PATTERN and CLUSTER <ref type="bibr" target="#b1">[Dwivedi et al., 2020]</ref> are node classification datasets synthesized with Stochastic Block Model. In PATTERN, the task is to tell if a node belongs to one of the randomly generated 100 patterns in a large graph.</p><p>In CLUSTER, every graph comprises 6 SBM clusters, and each graph only contains one labeled node with a feature value set to the cluster ID. The task is to predict the cluster ID of every node.</p><p>TSP <ref type="bibr" target="#b1">[Dwivedi et al., 2020]</ref> is an edge classification dataset, where edges in graphs have binary labels corresponding to the TSP tour of that graph. Specifically, the label of an edge is set to 1 if it belongs to the TSP tour and is set to 0 otherwise. The TSP problem is one of the NP-hard combinatorial optimization problems, and the utilization of machine learning methods to solve them has been intensively researched in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Configurations</head><p>We build 5 variants of GPTrans with different model sizes, called GPTrans-Nano, Tiny (T), Small (S), Base (B), and Large (L). Specifically, we follow <ref type="bibr">EGT [Hussain et al., 2021]</ref> and <ref type="bibr">Graphormer [Ying et al., 2021]</ref> to scale up our GPTrans.</p><p>The dimension of each head is set to 10 for our nano model, and 32 for others. The expansion ratio of the FFN modules is ? = 1 for all model variants. Other hyper-parameters of these models can be found in Table <ref type="table">8</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the three ways for graph information propagation. Circles and black lines indicate nodes and edges, and green and pink cubes represent node embeddings and edge embeddings. Our GPTrans achieves better graph representation learning by explicitly constructing three ways for information propagation in the proposed Graph Propagation Attention (GPA) module, including (a) node-to-node, (b) node-to-edge, and (c) edge-to-node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of Graph Propagation Attention. It explicitly builds three paths for information propagation among node embeddings and edge embeddings, including (a) node-to-node, (b) nodeto-edge, and (c) edge-to-node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>With an affordable cost, it could support three types of propagation paths, including node-to-node, node-to-edge, and edge-to-node. For simplicity of description, we consider single-head self-attention in the following formulas.</figDesc><table><row><cell>Node-to-Node</cell></row><row><cell>Following common practices [Ying et al., 2021; Shi et al.,</cell></row><row><cell>2022; Hussain et al., 2021], we adopt global self-attention</cell></row></table><note><p><p><p><p><p><p><p><p><ref type="bibr" target="#b19">[Ying et al., 2021;</ref><ref type="bibr" target="#b14">Shi et al., 2022;</ref><ref type="bibr" target="#b5">Hussain et al., 2021]</ref> </p>show global self-attention could serve as a flexible alternative to graph convolution and help better graph representation learning. However, most of them only consider part of the information propagation paths in graph, or introduce a lot of extra computational overhead to utilize edge information. For instance, Graphormer</p><ref type="bibr" target="#b19">[Ying et al., 2021]</ref> </p>only used edge features as shared bias terms to refine the attention weights of nodes.</p>GT [Dwivedi and  Bresson, 2020]  </p>and</p>EGT [Hussain et al., 2021]  </p>designed dual-FFN networks to fuse edge features. Inspired by this, we introduce Graph Propagation Attention (GPA), as an efficient replacement for vanilla self-attention in graph transformers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">PCQM4M?</cell><cell cols="2">PCQM4Mv2?</cell></row><row><cell>Model</cell><cell cols="5">#Param Validate Test Validate Test-dev</cell></row><row><cell cols="3">Non-transformer-based Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell>2.0M</cell><cell cols="4">0.1684 0.1838 0.1379 0.1398</cell></row><row><cell>GIN</cell><cell>3.8M</cell><cell cols="4">0.1536 0.1678 0.1195 0.1218</cell></row><row><cell>GCN-VN</cell><cell>4.9M</cell><cell cols="4">0.1510 0.1579 0.1153 0.1152</cell></row><row><cell>GIN-VN</cell><cell>6.7M</cell><cell cols="4">0.1396 0.1487 0.1083 0.1084</cell></row><row><cell>GINE-VN</cell><cell cols="2">13.2M 0.1430</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">DeeperGCN-VN 25.5M 0.1398</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Transformer-based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPS-Small</cell><cell>6.2M</cell><cell>-</cell><cell>-</cell><cell>0.0938</cell><cell>-</cell></row><row><cell cols="2">GPTrans-T (ours) 6.6M</cell><cell>0.1179</cell><cell>-</cell><cell>0.0833</cell><cell>-</cell></row><row><cell>Graphormer-S</cell><cell cols="2">12.5M 0.1264</cell><cell>-</cell><cell>0.0910</cell><cell>-</cell></row><row><cell>EGT-Small</cell><cell cols="2">11.5M 0.1260</cell><cell>-</cell><cell>0.0899</cell><cell>-</cell></row><row><cell>GPS-Medium</cell><cell>19.4M</cell><cell>-</cell><cell>-</cell><cell>0.0858</cell><cell>-</cell></row><row><cell cols="3">GPTrans-S (ours) 13.6M 0.1162</cell><cell>-</cell><cell>0.0823</cell><cell>-</cell></row><row><cell>TokenGT</cell><cell>48.5M</cell><cell>-</cell><cell>-</cell><cell>0.0910</cell><cell>-</cell></row><row><cell>Graphormer-B</cell><cell cols="2">47.1M 0.1234</cell><cell>-</cell><cell>0.0906</cell><cell>-</cell></row><row><cell>GRPE-Standard</cell><cell cols="2">46.2M 0.1225</cell><cell>-</cell><cell cols="2">0.0890 0.0898</cell></row><row><cell>EGT-Medium</cell><cell cols="2">47.4M 0.1224</cell><cell>-</cell><cell>0.0881</cell><cell>-</cell></row><row><cell cols="3">GPTrans-B (ours) 45.7M 0.1153</cell><cell>-</cell><cell>0.0813</cell><cell>-</cell></row><row><cell>GT-Wide</cell><cell cols="2">83.2M 0.1408</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">GraphormerV2-L 159.3M 0.1228</cell><cell>-</cell><cell>0.0883</cell><cell>-</cell></row><row><cell>EGT-Large</cell><cell>89.3M</cell><cell>-</cell><cell>-</cell><cell cols="2">0.0869 0.0872</cell></row><row><cell>EGT-Larger</cell><cell>110.8M</cell><cell>-</cell><cell>-</cell><cell>0.0859</cell><cell>-</cell></row><row><cell>GRPE-Large</cell><cell>118.3M</cell><cell>-</cell><cell>-</cell><cell>0.0866</cell><cell>-</cell></row><row><cell cols="3">GPTrans-L (ours) 86.0M 0.1151</cell><cell>-</cell><cell>0.0809</cell><cell>-</cell></row></table><note><p>Results on PCQM4M and PCQM4Mv2. The metric is the Mean Absolute Error (MAE), and the lower the better. "-" denotes results are not available since the labels of test and test-dev sets are not public. Highlighted are the best results for each model size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>40, layer number = 12 ? GPTrans-Tiny: d 1 = 256, d 2 = 32, layer number = 12 Results on MolPCBA. ? indicates the model is pre-trained on PCQM4M or PCQM4Mv2. The higher the better. Highlighted are the best results for each model size.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param Test AP(%)?</cell></row><row><cell>Non-transformer-based Methods</cell><cell></cell><cell></cell></row><row><cell cols="3">DeeperGCN-VN-FLAG [Li et al., 2020] 5.6M 28.42 ? 0.43</cell></row><row><cell>PNA [Corso et al., 2020]</cell><cell cols="2">6.5M 28.38 ? 0.35</cell></row><row><cell>DGN [Beaini et al., 2021]</cell><cell cols="2">6.7M 28.85 ? 0.30</cell></row><row><cell>GINE-VN [Brossard et al., 2020]</cell><cell cols="2">6.1M 29.17 ? 0.15</cell></row><row><cell>PHC-GNN [Le et al., 2021]</cell><cell cols="2">1.7M 29.47 ? 0.26</cell></row><row><cell>GIN-VN  ? [Xu et al., 2018]</cell><cell cols="2">3.4M 29.02 ? 0.17</cell></row><row><cell>Transformer-based Methods</cell><cell></cell><cell></cell></row><row><cell>GRPE-Standard  ? [Park et al., 2022]</cell><cell cols="2">46.2M 30.77 ? 0.07</cell></row><row><cell>GPTrans-B  ? (ours)</cell><cell cols="2">45.7M 31.15 ? 0.16</cell></row><row><cell>GRPE-Large  ? [Park et al., 2022]</cell><cell cols="2">118.3M 31.50 ? 0.10</cell></row><row><cell>Graphormer-L  ? [Ying et al., 2021]</cell><cell cols="2">119.5M 31.39 ? 0.32</cell></row><row><cell>EGT-Larger  ? [Hussain et al., 2021]</cell><cell cols="2">110.8M 29.61 ? 0.24</cell></row><row><cell>GPTrans-L  ? (ours)</cell><cell cols="2">86.0M 32.43 ? 0.22</cell></row><row><cell>Model</cell><cell cols="2">#Param Test AUC(%)?</cell></row><row><cell>Non-transformer-based Methods</cell><cell></cell><cell></cell></row><row><cell>DeeperGCN-FLAG [Li et al., 2020]</cell><cell>532K</cell><cell>79.42 ? 1.20</cell></row><row><cell>PNA [Corso et al., 2020]</cell><cell>326K</cell><cell>79.05 ? 1.32</cell></row><row><cell>DGN [Beaini et al., 2021]</cell><cell>110K</cell><cell>79.70 ? 0.97</cell></row><row><cell>PHC-GNN [Le et al., 2021]</cell><cell>114K</cell><cell>79.34 ? 1.16</cell></row><row><cell>GIN-VN  ? [Xu et al., 2018]</cell><cell>3.3M</cell><cell>77.80 ? 1.82</cell></row><row><cell>Transformer-based Methods</cell><cell></cell><cell></cell></row><row><cell>Graphormer-B  ? [Ying et al., 2021]</cell><cell>47.0M</cell><cell>80.51 ? 0.53</cell></row><row><cell cols="3">EGT-Larger  ? [Hussain et al., 2021] 110.8M 80.60 ? 0.65</cell></row><row><cell cols="2">GRPE-Standard  ? [Park et al., 2022] 46.2M</cell><cell>81.39 ? 0.49</cell></row><row><cell>GPTrans-B  ? (ours)</cell><cell>45.7M</cell><cell>81.26 ? 0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on MolHIV.</figDesc><table /><note><p><p>? </p>indicates the model is pre-trained on PCQM4M or PCQM4Mv2. The higher the better. Highlighted are the best results.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on four benchmarking datasets, including graph regression (ZINC), node classification (PATTERN and CLUSTER), and edge classification (TSP) tasks. The arrow next to the metric means higher or lower is better. "-" denotes the results are not available. Highlighted are the top first and second results.</figDesc><table><row><cell>ZINC</cell><cell>PATTERN</cell><cell>CLUSTER</cell><cell>TSP</cell></row></table><note><p>is a quantum chemistry dataset that includes 3.8 million molecular graphs and a total</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>is another property prediction dataset, which is larger than MolHIV. It contains 437, 929 graphs with 11, 386, 154 nodes and 12, 305, 805 edges.</figDesc><table /><note><p><p><p><p><ref type="bibr" target="#b9">(5)</ref> </p>ZINC</p><ref type="bibr" target="#b1">[Dwivedi et al., 2020]</ref> </p>is a popular real-world molecular dataset for graph property regression. It has 10, 000 train, 1, 000 validation, and 1, 000 test graphs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies of GPTrans. FLOPs is calculated using a graph with 20 nodes. We build our baseline based on Graphormer-S with a shorter schedule of 100 epochs, and decline its validate MAE on the PCQM4Mv2 dataset from 0.0928 to 0.0854 by gradually introducing our GPA module. Moreover, we find that the deeper model outperforms the wider model with a similar number of parameters.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param FLOPs Validate MAE?</cell></row><row><cell cols="2">Baseline (Graphormer-S) 12.5M 0.399G</cell><cell>0.0928</cell></row><row><cell>+ Node-to-Node</cell><cell>13.3M 0.402G</cell><cell>0.0874</cell></row><row><cell>++ Node-to-Edge</cell><cell>13.3M 0.405G</cell><cell>0.0865</cell></row><row><cell>+++ Edge-to-Node</cell><cell>13.5M 0.417G</cell><cell>0.0854</cell></row><row><cell>GPTrans-S wider</cell><cell>13.5M 0.417G</cell><cell>0.0854</cell></row><row><cell>GPTrans-S deeper (ours)</cell><cell>13.6M 0.472G</cell><cell>0.0835</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 ,</head><label>4</label><figDesc>our GPTrans-Nano produces the promising accuracy of 86.731 ? 0.085% and 78.069 ? 0.154% on the PATTERN and CLUSTER datasets, respectively. These results outperform many Convolutional/Message-Passing Graph Neural Networks by large margins, showing that the proposed GPTrans can serve as an alternative to traditional GCNs for node-level tasks. Moreover, we find that our method exceeds Graphormer<ref type="bibr" target="#b19">[Ying et al., 2021]</ref> on the CLUSTER dataset by significant gaps of 3.4% accuracy, which suggests that the three propagation ways explicitly constructed in the GPA module are also helpful for node-level tasks.</figDesc><table><row><cell>4.3 Edge-Level Tasks</cell></row><row><cell>Datasets</cell></row><row><cell>TSP [Dwivedi et al., 2020] is a dataset for the Traveling</cell></row><row><cell>Salesman Problem, which is an NP-hard combinatorial op-</cell></row><row><cell>timization problem. The problem is reduced to a binary edge</cell></row><row><cell>classification task, where edges in the TSP tour have positive</cell></row><row><cell>labels. TSP dataset has 10, 000 training, 1, 000 validation,</cell></row><row><cell>and 1, 000 test graphs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>is a medium-scale molecular property prediction dataset featuring 128 imbalanced binary classification tasks. It contains 437, 929 graphs with 11, 386, 154 nodes and 12, 305, 805 edges.</figDesc><table /><note><p>ZINC [Dwivedi</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61672273</rs> and Grant <rs type="grantNumber">61832008</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>(a) Node-to-Node (b) Node-to-Edge (c) Edge-to-Node Node Edge Node Embedding Edge Embedding</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ut8uYF7">
					<idno type="grant-number">61672273</idno>
				</org>
				<org type="funding" xml:id="_dmxTW5x">
					<idno type="grant-number">61832008</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Strategies C.1 PCQM4M and PCQM4Mv2</head><p>We first report the hyper-parameters of the experiments on PCQM4M and PCQM4Mv2 datasets in Table <ref type="table">9</ref>. Empirically, for our GPTrans-T/S/B models, the dropout ratios of FFN, embedding, and attention are set to 0.1 by default, while for the GPTrans-L model, it is set to 0.2. Besides, the drop path rates are set to 0.1/0.1/0.2/0.4 for these variants along with the model scaling up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 MolHIV and MolPCBA</head><p>We fine-tune our GPTrans-B on MolHIV and GPTrans-B/L on MolPCBA datasets, and load the PCQM4Mv2 pre-trained weights as initialization. Most of the hyper-parameters are consistent with the pre-training stage, see Table <ref type="table">10</ref> for details. In addition, each experiment in these datasets is run 5 times with 5 different random seeds, and the results are used to calculate the mean and standard deviations of the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 ZINC/PATTERN/CLUSTER/TSP</head><p>For the 4 benchmarking datasets from <ref type="bibr" target="#b1">[Dwivedi et al., 2020]</ref>, i.e., ZINC, PATTERN, CLUSTER, and TSP, we employ the AdamW optimizer <ref type="bibr" target="#b11">[Loshchilov and Hutter, 2018]</ref> with a 20epoch warmup, and reduce the learning rate by a cosine learning rate scheduler. The dropout ratios of FFN, embedding, and attention are set to 0.3, 0.3, and 0.5, respectively. The weight decay is set to 0.05. Each experiment is run 5 times with 5 different random seeds, and the results are used to calculate the mean and standard deviations of the metric. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<idno>arXiv:1710.10568</idno>
	</analytic>
	<monogr>
		<title level="m">Stochastic training of graph convolutional networks with variance reduction</title>
		<editor>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2016. 2016. 2021. 2017. 2020. 2020. 2020. 2020. 2013. 2013. 2020. 2017. 2017. 2018</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dwivedi and Bresson, 2020] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09529</idno>
		<idno>arXiv:2003.00982</idno>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<publisher>Gilmer et al</publisher>
			<date type="published" when="2016">2020. 2020. 2022. 2022. 2023. Junjun He,. 2023. 2020. 2020. 2016. 2016. 2021. 2020. 2, 4, 6. 2020. 2017. 2017</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><surname>Henaff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dgraph: A large-scale financial dataset for graph anomaly detection</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03579</idno>
		<idno>arXiv:2303.17559</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<meeting>Conference on Neural Information Processing Systems Datasets and Benchmarks Track</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021. 3, 5, 6, 7, 10, 11. 2022. 2022. 2021. 1, 2, 3, 4, 5, 6, 7, 10. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Diffusion model for dense visual prediction</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02505</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016. 1, 2, 6. 2021</date>
			<biblScope unit="page" from="21618" to="21629" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rethinking graph transformers with spectral attention</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frank No?, and Djork-Arn? Clevert. Parameterized hypercomplex graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
		<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="204" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-local graph neural networks</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2021. 2021. 2018. 2018. 2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of ICLR2022 Machine Learning for Drug Discovery</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2018. 2018. 2014. 2014. 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Language models are unsupervised multitask learners</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><surname>Ramp??ek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12454</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2022. 2022. 2008. 2008</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The graph neural network model</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04810</idno>
	</analytic>
	<monogr>
		<title level="m">Benchmarking graphormer on large-scale molecular modeling datasets</title>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">?ukasz</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2022. 2022. 2021. 2021. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Petar Veli?kovi?, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Internimage: Exploring large-scale vision foundation models with deformable convolutions</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05778</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.03191</idno>
		<title level="m">General video foundation models via generative and discriminative learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shne: Representation learning for semanticassociated heterogeneous networks</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Web Search and Data Mining</title>
		<meeting>ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 1, 5, 6. 2021. 2021. 1, 2, 3, 4, 5, 6, 7, 10. 2019. 2019</date>
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
	<note>Proceedings of Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
