<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spectral Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
							<email>fbach@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Statistics</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spectral Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F9C123CAA5ED292F8DB1CB6628AA16D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spectral clustering has many applications in machine learning, exploratory data analysis, computer vision and speech processing. Most techniques explicitly or implicitly assume a metric or a similarity structure over the space of configurations, which is then used by clustering algorithms. The success of such algorithms depends heavily on the choice of the metric, but this choice is generally not treated as part of the learning problem. Thus, time-consuming manual feature selection and weighting is often a necessary precursor to the use of spectral methods.</p><p>Several recent papers have considered ways to alleviate this burden by incorporating prior knowledge into the metric, either in the setting of K-means clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or spectral clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In this paper, we consider a complementary approach, providing a general framework for learning the similarity matrix for spectral clustering from examples. We assume that we are given sample data with known partitions and are asked to build similarity matrices that will lead to these partitions when spectral clustering is performed. This problem is motivated by the availability of such datasets for at least two domains of application: in vision and image segmentation, a hand-segmented dataset is now available <ref type="bibr" target="#b4">[5]</ref>, while for the blind separation of speech signals via partitioning of the time-frequency plane <ref type="bibr" target="#b5">[6]</ref>, training examples can be created by mixing previously captured signals.</p><p>Another important motivation for our work is the need to develop spectral clustering methods that are robust to irrelevant features. Indeed, as we show in Section 4.2, the performance of current spectral methods can degrade dramatically in the presence of such irrelevant features. By using our learning algorithm to learn a diagonally-scaled Gaussian kernel for generating the affinity matrix, we obtain an algorithm that is significantly more robust.</p><p>Our work is based on a new cost function J(W, e) that characterizes how close the eigenstructure of a similarity matrix W is to a partition e. We derive this cost function in Section 2. As we show in Section 2.3, minimizing J with respect to e leads to a new clustering algorithm that takes the form of a weighted K-means algorithm. Minimizing J with respect to W yields an algorithm for learning the similarity matrix, as we show in Section 4. Section 3 provides foundational material on the approximation of the eigensubspace of a symmetric matrix that is needed for Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Spectral clustering and normalized cuts</head><p>Given a dataset I of P points in a space X and a P × P "similarity matrix" (or "affinity matrix") W that measures the similarity between the P points (W pp is large when points indexed by p and p are likely to be in the same cluster), the goal of clustering is to organize the dataset into disjoint subsets with high intra-cluster similarity and low inter-cluster similarity. Throughout this paper we always assume that the elements of W are non-negative (W 0) and that W is symmetric (W = W ).</p><p>Let D denote the diagonal matrix whose i-th diagonal element is the sum of the elements in the i-th row of W , i.e., D = diag(W 1), where 1 is defined as the vector in R P composed of ones. There are different variants of spectral clustering. In this paper we focus on the task of minimizing "normalized cuts." The classical relaxation of this NP-hard problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> leads to an eigenvalue problem. In this section we show that the problem of finding a solution to the original problem that is closest to the relaxed solution can be solved by a weighted K-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalized cut and graph partitioning</head><p>The clustering problem is usually defined in terms of a complete graph with vertices V = {1, ..., P } and an affinity matrix with weights W pp , for p, p ∈ V . We wish to find R disjoint clusters A = (A r ) r∈{1,...,R} , where r A r = V , that optimize a certain cost function. An example of such a function is the R-way normalized cut defined as follows <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>:</p><formula xml:id="formula_0">C(A, W ) = R r=1 i∈Ar,j∈V \Ar W ij / i∈Ar,j∈V W ij .</formula><p>Let e r be the indicator vector in R P for the r-th cluster, i.e., e r ∈ {0, 1} R is such that e r has a nonzero component exactly at points in the r-th cluster. Knowledge of e = (e r ) is equivalent to knowledge of A = (A r ) and, when referring to partitions, we will use the two formulations interchangeably. A short calculation reveals that the normalized cut is then equal to C(e, W ) = R r=1 e r (D -W )e r / (e r De r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral relaxation and rounding</head><p>The following proposition, which extends a result of Shi and Malik <ref type="bibr" target="#b6">[7]</ref> for two clusters to an arbitrary number of clusters, gives an alternative description of the clustering task, which will lead to a spectral relaxation: </p><formula xml:id="formula_1">Proposition 1 The R-way normalized cut is equal to R -tr Y D -1/2 W D -1/2 Y for any matrix Y ∈ R P ×R such that (a)</formula><formula xml:id="formula_2">Y D -1/2 W D -1/2 Y = tr Λ E W EΛ = tr E W EΛΛ = tr E W E(E DE) -1 ,</formula><p>which is exactly the normalized cut (up to an additive constant).</p><p>By removing the constraint (a), we obtain a relaxed optimization problem, whose solutions involve the eigenstructure of D -<ref type="foot" target="#foot_0">1</ref>/<ref type="foot" target="#foot_1">2</ref> W D -1/2 and which leads to the classical lower bound on the optimal normalized cut <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. The following proposition gives the solution obtained from the relaxation (for the proof, see <ref type="bibr" target="#b10">[11]</ref>):</p><formula xml:id="formula_3">Proposition 2 The maximum of tr Y D -1/2 W D -1/2 Y over matrices Y ∈ R P ×R such that Y Y = I is the sum of the R largest eigenvalues of D -1/2 W D -1/2 . It is attained at all Y of the form Y = U B 1 where U ∈ R P ×R is any orthonormal basis of the R-th principal subspace of D -1/2 W D -1/2 and B 1 is an arbitrary rotation matrix in R R×R .</formula><p>The solutions found by this relaxation will not in general be piecewise constant. In order to obtain a piecewise constant solution, we wish to find a piecewise constant matrix that is as close as possible to one of the possible Y obtained from the eigendecomposition. Since such matrices are defined up to a rotation matrix, it makes sense to compare the subspaces spanned by their columns. A common way to compare subspaces is to compare the orthogonal projection operators on those subspaces <ref type="bibr" target="#b11">[12]</ref>, that is, to compute the Frobenius norm between U U and Π 0 = Π 0 (W, e) r D 1/2 e r e r D 1/2 / (e r De r ) (Π 0 is the orthogonal projection operator on the subspace spanned by the columns of D 1/2 E = D 1/2 (e 1 , . . . , e r ), from Proposition 1). We thus define the following cost function:</p><formula xml:id="formula_4">J(W, e) = 1 2 ||U U -Π 0 || 2 F<label>(1)</label></formula><p>Using the fact that both U U and Π 0 are orthogonal projection operators on linear subspaces of dimension R, a short calculation reveals that the cost function</p><formula xml:id="formula_5">J(W, e) is equal to R -tr U U Π 0 = R -r e r D 1/2 U U D 1/2 e r / (e r De r</formula><p>). This cost function characterizes the ability of the matrix W to produce the partition e when using its eigenvectors.</p><p>Minimizing with respect to e leads to a new clustering algorithm that we now present.</p><p>Minimizing with respect to the matrix for a given partition e leads to the learning of the similarity matrix, as we show in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Minimizing with respect to the partition</head><p>In this section, we show that minimizing J(W, e) is equivalent to a weighted K-means algorithm. The following theorem, inspired by the spectral relaxation of K-means presented in <ref type="bibr" target="#b7">[8]</ref>, shows that the cost function can be interpreted as a weighted distortion measure 1 :</p><p>Theorem 1 Let W be an affinity matrix and let U = (u 1 , . . . , u P ), where u p ∈ R R , be an orthonormal basis of the R-th principal subspace of D -1/2 W D -1/2 . For any partition e ≡ A, we have</p><formula xml:id="formula_6">J(W, e) = min (µ1,...,µR)∈R R×R r p∈Ar d p ||u p d -1/2 p -µ r || 2 . Proof Let D(µ, A) = r p∈Ar d p ||u p d -1/2 p -µ r || 2 .</formula><p>Minimizing D(µ, A) with respect to µ is a decoupled least-squares problem and we get:  </p><formula xml:id="formula_7">min µ D(µ, A) = r p∈Ar u p u p -r || p∈Ar d 1/2 p u p || 2 / ( p∈Ar d p ) Input: Similarity matrix W ∈ R P ×P . Algorithm: 1. Compute first R eigenvectors U of D -1/2 W D -1/2 where D = diag(W 1). 2. Let U = (u 1 , . . . , u P ) ∈ R</formula><formula xml:id="formula_8">= p u p u p -r p,p ∈Ar d 1/2 p d 1/2 p u p u p / (e r De r ) = R -r e r D 1/2 U U D 1/2 e r / (e r De r ) = J(W, e)</formula><p>This theorem has an immediate algorithmic implication-to minimize the cost function J(W, e) with respect to the partition e, we can use a weighted K-means algorithm. The resulting algorithm is presented in Figure <ref type="figure" target="#fig_2">1</ref>. While K-means is often used heuristically as a post-processor for spectral clustering <ref type="bibr" target="#b12">[13]</ref>, our approach provides a mathematical foundation for the use of K-means, and yields a specific weighted form of K-means that is appropriate for the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Minimizing with respect to the similarity matrix</head><p>When the partition e is given, we can consider minimization with respect to W . As we have suggested, intuitively this has the effect of yielding a matrix W such that the result of spectral clustering with that W is as close as possible to e. We now make this notion precise, by showing that the cost function J(W, e) is an upper bound on the distance between the partition e and the result of spectral clustering using the similarity matrix W .</p><p>The metric between two partitions e = (e r ) and f = (f s ) with R and S clusters respectively, is taken to be <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_9">d(e, f ) = 1 2 r</formula><p>e r e r e r e r -</p><formula xml:id="formula_10">s f s f s f s f s 2 F = R + S 2 - r,s (e r f s ) 2 (e r e r )(f s f s )<label>(2)</label></formula><p>This measure is always between zero and R+S 2 -1, and is equal to zero if and only if e ≡ f . The following theorem shows that if we can perform weighted K-means exactly, we obtain a bound on the performance of our spectral clustering algorithm (for a proof, see <ref type="bibr" target="#b10">[11]</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approximation of the cost function</head><p>In order to minimize the cost function J(W, e) with respect to W , which is the topic of Section 4, we need to optimize a function of the R-th principal subspace of the matrix D -1/2 W D -1/2 . In this section, we show how we can compute a differentiable approximation of the projection operator on this subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approximation of eigensubspace</head><p>Let X ∈ R P ×P be a real symmetric matrix. We assume that its eigenvalues are ordered by magnitude:</p><formula xml:id="formula_11">|λ 1 | |λ 2 | • • • |λ P |. We assume that |λ R | &gt; |λ R+1</formula><p>| so that the R-th principal subspace E R is well defined, with orthogonal projection Π R .</p><p>Our approximations are based on the power method to compute eigenvectors. It is well known that for almost all vectors v, the ratio X q v/||X q v|| converges to an eigenvector corresponding to the largest eigenvalue <ref type="bibr" target="#b11">[12]</ref>. The same method can be generalized to the computation of dominant eigensubspaces: If V is a matrix in R P ×R , the subspace generated by the R columns of X q V will tend to the principal eigensubspace of X. Note that since we are interested only in subspaces, and in particular the orthogonal projection operators on those subspaces, we can choose any method for finding an orthonormal basis of range(X q V ). The QR decomposition is fast and stable and is usually the method used to compute such a basis (the algorithm is usually referred to as "orthogonal iteration" <ref type="bibr" target="#b11">[12]</ref>). However this does not lead to a differentiable function. We develop a different approach which does yield a differentiable function, as made precise in the following proposition (for a proof, see <ref type="bibr" target="#b10">[11]</ref>):</p><formula xml:id="formula_12">Proposition 3 Let V ∈ R P ×R such that η = max u∈ER(X) ⊥ , v∈range(V ) cos(u, v) &lt; 1. Then the function Y → Π R (Y ) = M (M M ) -1 M , where M = Y q V , is C ∞ in a neighborhood</formula><p>of X, and we have:</p><formula xml:id="formula_13">|| Π R (X) -Π R || 2 η (1-η 2 ) 1/2 (|λ R+1 |/|λ R |) q .</formula><p>This proposition shows that as q tends to infinity, the range of X q V will tend to the principal eigensubspace. The rate of convergence is determined by the (multiplicative) eigengap |λ R+1 |/|λ R | &lt; 1: it is usually hard to compute principal subspace of matrices with eigengap close to one. Note that taking powers of matrices without care can lead to disastrous results <ref type="bibr" target="#b11">[12]</ref>. By using successive QR iterations, the computations can be made stable and the same technique can be used for the computation of the derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Potentially hard eigenvalue problems</head><p>In most of the literature on spectral clustering, it is taken for granted that the eigenvalue problem is easy to solve. It turns out that in many situations, the (multiplicative) eigengap is very close to one, making the eigenvector computation difficult (examples are given in the next section). We acknowledge this potential problem by averaging over several initializations of the original subspace V . More precisely, let (V m ) m=1,...,M be M subspaces of dimension R. Let B m = Π(range((D -1/<ref type="foot" target="#foot_2">2</ref> W D -1/2 ) q V m )) be the approximations of the projections on the R-th principal subspace 2 of D -1/2 W D -1/2 . The cost function that we use is the average error</p><formula xml:id="formula_14">F (W, Π 0 (e)) = 1 2M M m=1 ||B m -Π 0 || 2 F .</formula><p>This cost function can be rewritten as the distance between the average of the B m and Π 0 plus the variance of the approximations, thus explicitly penalizing the non-convergence of the power iterations. We choose V i to be equal to D 1/2 times a set of R indicator vectors corresponding to subsets of each cluster. In simulations, we used q = 128, M = R 2 , and subsets containing 2/(log 2 q + 1) times the number of original points in the clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Empirical comparisons</head><p>In this section, we study the ability of various cost functions to track the gold standard error measure in Eq. ( <ref type="formula" target="#formula_10">2</ref>) as we vary the parameter α in the similarity matrix W pp = exp(-α||x p -x p || 2 ). We study the cost function J(W, e), its approximation based on the power method presented in Section 3, and two existing approaches, one based on a Markov chain interpretation of spectral clustering <ref type="bibr" target="#b14">[15]</ref> and one based on the alignment <ref type="bibr" target="#b15">[16]</ref> of D -1/2 W D -1/2 and Π 0 . We carry out this experiment for the simple clustering example shown in Figure <ref type="figure" target="#fig_4">2(a)</ref>. This apparently simple toy example captures much of the core difficulty of spectral clustering-nonlinear separability and thinness/sparsity of clusters (any point has very few near neighbors belonging to the same cluster, so that the weighted graph is sparse). In particular, in Figure <ref type="figure" target="#fig_1">2</ref>(b) we plot the eigengap of the similarity matrix as a function of α, noting that at the optimum, this gap is very close to one, and thus the eigenvalue problem is hard to solve.</p><p>In Figure <ref type="figure" target="#fig_4">2(c)</ref> and<ref type="figure">(d)</ref>, we plot the four cost functions against the gold standard. The gold standard curve shows that the optimal α lies near 2.5 on a log scale, and as seen in Figure <ref type="figure" target="#fig_1">2</ref>(c), the minima of the new cost function and its approximation lie near to this value. As seen in Figure <ref type="figure" target="#fig_1">2</ref>(d), on the other hand, the other two cost functions show a poor match to the gold standard, and yield minima far from the optimum.</p><p>The problem with the alignment and Markov-chain-based cost functions is that these functions essentially measure the distance between the similarity matrix W (or a normalized version of W ) and a matrix T which (after permutation) is block-diagonal with constant blocks. Unfortunately, in examples like the one in Figure <ref type="figure" target="#fig_1">2</ref>, the optimal similarity matrix is very far from being block diagonal with constant blocks. Rather, given that data points that lie in the same ring are in general far apart, the blocks are very sparse-not constant and full. Methods that try to find constant blocks cannot find the optimal matrices in these cases. In the language of spectral graph partitioning, where we have a weighted graph with weights W , each cluster is a connected but very sparse graph. The power W q corresponds to the q-th power of the graph; i.e., the graph in which two vertices are linked by an edge if and only if they are linked by a path of length no more than q in the original graph. Thus taking powers can be interpreted as "thickening" the graph to make the clusters more apparent, while not changing the eigenstructure of the matrix (taking powers of symmetric matrices only changes the eigenvalues, not the eigenvectors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning the similarity matrix</head><p>We now turn to the problem of learning the similarity matrix from data. We assume that we are given one or more sets of data for which the desired clustering is known. The goal is to design a "similarity map," that is, a mapping from datasets of elements in X to the space of symmetric matrices with nonnegative elements. To turn this into a parametric learning problem, we focus on similarity matrices that are obtained as Gram matrices of a kernel function k(x, y) defined on X×X . In particular, for concreteness and simplicity, we restrict ourselves in this paper to the case of Euclidean data (X = R F ) and a diagonally-scaled Gaussian kernel k α (x, y) = exp(-(x-y) diag(α)(x-y)), where α ∈ R F -while noting that our methods apply more generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning algorithm</head><p>We assume that we are given N datasets D n , n ∈ {1, . . . , N }, of points in R F . Each dataset D n is composed of P n points x np , p ∈ {1, . . . , P n }. Each dataset is segmented, that is, for each n we know the partition e n , so that the "target" matrix Π 0 (e n , α) can be computed for each dataset. For each n, we have a similarity matrix W n (α). The cost function that we use is H(α) = 1 N n F (W n (α), Π 0 (e n , α)) + C||α|| 1 . The 1 penalty serves as a feature selection term, tending to make the solution sparse. The learning algorithm is the minimization of H(α) with respect to α ∈ R F + , using the method of conjugate gradient with line search.</p><p>Since the complexity of the cost function increases with q, we start the minimization with small q and gradually increase q up to its maximum value. We have observed that for small q, the function to optimize is smoother and thus easier to optimize-in particular, the long plateaus of constant values are less pronounced.</p><p>Testing. The output of the learning algorithm is a vector α ∈ R F . In order to cluster previously unseen datasets, we compute the similarity matrix W and use the algorithm of Figure <ref type="figure" target="#fig_2">1</ref>. In order to further enhance performance, we can also adopt an idea due to <ref type="bibr" target="#b12">[13]</ref>we hold the direction of α fixed but perform a line search on its norm. This yields the real number λ such that the weighted distortion obtained after application of the spectral clustering algorithm of Figure <ref type="figure" target="#fig_2">1</ref>, with the similarity matrices defined by λα, is minimum. <ref type="foot" target="#foot_3">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulations</head><p>We performed simulations on synthetic datasets in two dimensions, where we consider datasets similar to the one in Figure <ref type="figure" target="#fig_1">2</ref>, with two rings whose relative distance is constant across samples (but whose relative orientation has a random direction). We add D irrelevant dimensions of the same magnitude as the two relevant variables. The goal is thus to learn the diagonal scale α ∈ R D+2 of a Gaussian kernel that leads to the best clustering on unseen data. We learn α from N sample datasets (N = 1 or 10), and compute the clustering error of our algorithm with and without adaptive tuning of the norm of α during testing (as described in Section 4.1) on ten previously unseen datasets. We compare to an approach that does not use the training data: α is taken to be the vector of all ones and we again search over the best possible norm during testing (we refer to this method as "no learning"). We report results in Table <ref type="table" target="#tab_0">1</ref>. Without feature selection, the performance of spectral clustering degrades very rapidly when the number of irrelevant features increases, while our learning approach is very robust, even with only one training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented two algorithms-one for spectral clustering and one for learning the similarity matrix. These algorithms can be derived as the minimization of a single cost function with respect to its two arguments. This cost function depends directly on the eigenstructure of the similarity matrix. We have shown that it can be approximated efficiently using the power method, yielding a method for learning similarity matrices that can cluster effectively in cases in which non-adaptive approaches fail. Note in particular that our new approach yields a spectral clustering method that is significantly more robust to irrelevant features than current methods.</p><p>We are currently applying our algorithm to problems in speech separation and image segmentation, in particular with the objective of selecting features from among the numerous features that are available in these domains <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. The number of points in such datasets can be very large and we have developed efficient implementations of both learning and clustering based on sparsity and low-rank approximations <ref type="bibr" target="#b10">[11]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the columns of D -1/2 Y are piecewise constant with respect to the clusters and (b) Y has orthonormal columns (Y Y = I). Proof The constraint (a) is equivalent to the existence of a matrix Λ ∈ R R×R such that D -1/2 Y = (e 1 , . . . , e R )Λ = EΛ. The constraint (b) is thus written as I = Y Y = Λ E DEΛ. The matrix E DE is diagonal, with elements e r De r and is thus positive and invertible. This immediately implies that ΛΛ = (E DE) -1 . This in turn implies that tr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 p</head><label>2</label><figDesc>R×P and d p = D pp . 3. Weighted K-means: while partition A is not stationary, a. For all r, µ r = p∈Ar d 1/u p / p∈Ar d p b. For all p, assign p to A r where r = arg min r ||u p d -1/2 p -µ r || Output: partition A, distortion measure r p∈Ar d p ||u p d -1/2 p -µ r || 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spectral clustering algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2</head><label>2</label><figDesc>Let η = max p D pp / min p D pp 1. If e(W ) = arg min e J(W, e), then for all partitions e, we have d(e, e(W )) 4ηJ(W, e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical comparison of cost functions. (a) Data. (b) Eigengap of the similarity matrix as a function of α. (c) Gold standard clustering error (solid), spectral cost function J (dotted) and its approximation based on the power method (dashed). (d) Gold standard clustering error (solid), the alignment (dashed), and a Markov-chain-based cost, divided by 16 (dotted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on synthetic datasets: clustering errors (multiplied by 100) for method without learning (but with tuning) and for our learning method with and without tuning, with N = 1 or 10 training datasets; D is the number of irrelevant features.</figDesc><table><row><cell>D</cell><cell>no</cell><cell cols="4">learning w/o tuning learning with tuning</cell></row><row><cell></cell><cell>learning</cell><cell>N=1</cell><cell>N=10</cell><cell>N=1</cell><cell>N=10</cell></row><row><cell>0</cell><cell>0</cell><cell>15.5</cell><cell>10.5</cell><cell>0</cell><cell>0</cell></row><row><cell>1</cell><cell>60.8</cell><cell>37.7</cell><cell>9.5</cell><cell>0</cell><cell>0</cell></row><row><cell>2</cell><cell>79.8</cell><cell>36.9</cell><cell>9.5</cell><cell>0</cell><cell>0</cell></row><row><cell>4</cell><cell>99.8</cell><cell>37.8</cell><cell>9.7</cell><cell>0.4</cell><cell>0</cell></row><row><cell>8</cell><cell>99.8</cell><cell>37</cell><cell>10.7</cell><cell>0</cell><cell>0</cell></row><row><cell>16</cell><cell>99.7</cell><cell>38.8</cell><cell>10.9</cell><cell>14</cell><cell>0</cell></row><row><cell>32</cell><cell>99.9</cell><cell>38.9</cell><cell>15.1</cell><cell>14.6</cell><cell>6.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that a similar equivalence holds between normalized cuts and weighted K-means for positive semidefinite similarity matrices, which can be factorized as W = GG ; this leads to an approximation algorithm for minimizing normalized cuts; i.e., we have: C(W, e) = min (µ 1 ,...,µ R )∈R R×R r p∈Ar dp||gpd -1 p -µr||</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>+ R -tr D -1/2 W D -1/2 .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The matrix D -1/2 W D -1/2 always has the same largest eigenvalue 1 with eigenvector D 1/2 1 and we could consider instead the (R -1)-st principal subspace of D -1/2 W D -1/2 -D 1/2 11 D 1/2 / (1 D1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>In<ref type="bibr" target="#b12">[13]</ref>, this procedure is used to learn one parameter of the similarity matrix with no training data; it cannot be used directly here to learn a more complex similarity matrix with more parameters, because it would lead to overfitting.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to acknowledge support from NSF grant IIS-9988642, MURI ONR-N00014-01-1-0890 and a grant from Intel Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constrained K-means clustering with background knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schrödl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distance metric learning, with application to clustering with side-information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Grouping with bias</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="297" to="333" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spectral relaxation for K-means clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral K-way ratio-cut partitioning and clustering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D F</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CAD</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral relaxation models and structure analysis for K-way graph clustering and bi-clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Penn. State Univ, Computer Science and Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning spectral clustering</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<ptr target="www.cs.berkeley.edu/˜fbach" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning segmentation by random walks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 13</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spectral kernel methods for clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
