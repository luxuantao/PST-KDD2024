<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group-Sensitive Multiple Kernel Learning for Object Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingjing</forename><surname>Yang</surname></persName>
							<email>jjyang@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of EE &amp; CS</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanning</forename><surname>Li</surname></persName>
							<email>ynli@jdl.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of EE &amp; CS</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<email>yhtian@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of EE &amp; CS</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
							<email>lingyu@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of EE &amp; CS</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of EE &amp; CS</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Group-Sensitive Multiple Kernel Learning for Object Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F06E800BC4FD39EDBD26954039CF890</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In this paper, we propose a group-sensitive multiple kernel learning (GS-MKL) method to accommodate the intra-class diversity and the inter-class correlation for object categorization. By introducing an intermediate representation "group" between images and object categories, GS-MKL attempts to find appropriate kernel combination for each group to get a finer depiction of object categories. For each category, images within a group share a set of kernel weights while images from different groups may employ distinct sets of kernel weights.</head><p>In GS-MKL, such group-sensitive kernel combinations together with the multi-kernels based classifier are optimized in a joint manner to seek a trade-off between capturing the diversity and keeping the invariance for each category. Extensive experiments show that our proposed GS-MKL method has achieved encouraging performance over three challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, various learning approaches have been developed to improve object categorization <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. Significant improvements have been achieved over several public datasets, such as Caltech, Pascal VOC and ImageCLEF. However, object categorization is still a challenging task. The essential reason lies in that the images within a category would exhibit diversity while the images from distinct categories would produce correlations in low-level visual attributes (e.g. color, texture, and shape). We may refer to such phenomena as "intra-class diversity" and "inter-class correlation" in this study.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the example images from WikiPediaMM dataset <ref type="bibr">[9]</ref>. Given the category "bridges", positive samples can be grouped into three sub-categories, each of which produces distinct visual appearance. On the other hand, negative samples from other categories (e.g., "buildings", "cities by night") may exhibit similar visual attributes to some samples of "bridge". We argue that, to elegantly and robustly categorize objects over extensive image data-sets, it is meaningful to effectively model both intra-class diversity and inter-class correlation. * First two authors contributed equally to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Images</head><p>Negative Images Various classifiers based on distance metrics (e.g. <ref type="bibr" target="#b16">[17]</ref>) or kernels (e.g. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b5">5]</ref>), which aim to maximize inter-class distance (or interval), have been applied to object categorization. In particular, multiple kernel learning (MKL) methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> have shown great advantages in this task recently (e.g. <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">18]</ref>). Instead of using a single kernel in support vector machine (SVM) <ref type="bibr" target="#b20">[21]</ref>, MKL learns an optimal kernel combination and the associated classifier simultaneously, providing an effective way of fusing informative features and kernels. However, these methods basically adopt a uniform similarity measure over the whole input space. When a category exhibits high variation as well as correlation with other categories in appearance, they are difficult to cope with the complexity of data distribution.</p><p>On the other hand, several sample-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref> have been proposed to capture the characteristics of individual samples. For example, a sample-specific ensemble kernel learning method is proposed in <ref type="bibr" target="#b28">[29]</ref> to explore the relative contributions of distinct kernels for each sample. In practice, such methods have yielded promising discriminative power. But expensive computation is incurred to learn sample-based similarity measures. More importantly, heavily respecting individual samples may overwhelm the intrinsic properties of a category so as to make the classifier less reliable.</p><p>In this paper, we attempt to introduce an intermediate representation "group" between object categories and individual images to seek a trade-off between capturing the diversity and keeping the invariance for each category in training classifiers. Given an object category, the image samples with similar visual appearance are clustered into a group so that the intra-class diversity can be represented by a set of groups. On the other hand, inter-class correlation can be represented by the correlation between the groups from different categories. Consequently, we incorporate group into the MKL framework and propose a group-sensitive multiple kernel learning (GS-MKL) method for object categorization. In GS-MKL, the image-to-image similarity is represented as a weighted combination of multi-kernels, where the kernel weights not only depend on the corresponding kernel functions, but also on the groups that two comparing images belong to. Instead of a uniform or sample-specific similarity measure, such group-sensitive similarity measure is shown effective in dealing with both intra-class diversity and inter-class correlation.</p><p>In GS-MKL, the group-sensitive kernel weights together with the associated classifier are jointly optimized by a gradient descent wrapping canonical SVM solver <ref type="bibr" target="#b29">[30]</ref>. Over three datasets (i.e., Caltech101, Pascal VOC2007 and WikipediaMM), we have shown that GS-MKL can significantly alleviate the negative effects of intra-class diversity and inter-class correlation, coming up with a more robust discriminative power for object categorization.</p><p>Our main contributions are summarized as follows:</p><p>z We have proposed a group-sensitive multiple kernel learning method GS-MKL for robust object categorization, where both intra-class diversity and inter-class correlation are taken into account.</p><p>z We formulate GS-MKL in a general and flexible learning framework. When the group number declines to one, GS-MKL is reduced to canonical MKL. When the group number reaches up to the number of training images, GS-MKL becomes a sample-specific MKL.</p><p>z We have achieved promising results comparable to the state-of-the-art results on Caltech101 and Pascal VOC2007, and significant improvements over canonical MKL across three datasets.</p><p>The remainder of this paper is organized as follows. In Section 2 we brief the related work. In Section 3, we introduce the GS-MKL framework for object categorization. The GS-MKL learning algorithm is presented in Section 4. We present the experimental results in Section 5. Finally we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In computer vision, many research efforts have been devoted to characterizing visual statistics for a number of object categories in the past decades. Kernel based method is one of attractive research areas for object categorization in recent years. Diverse kernels such as pyramid matching kernel (PMK) <ref type="bibr" target="#b14">[15]</ref>, spatial pyramid matching kernel (SPK) <ref type="bibr" target="#b3">[3]</ref>, proximity distribution kernel (PDK) <ref type="bibr" target="#b15">[16]</ref> and chi-square kernel <ref type="bibr" target="#b32">[33]</ref> are delicately designed to compute the similarity of image pair on certain features that represent particular visual characteristics.</p><p>Recently, multi-kernel based classifiers have been introduced into object categorization yielding promising results. In <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">18]</ref>, multiple features (e.g., appearance, shape) are employed and kernels (e.g., PMK and SPK with different hyper-parameters) are linearly combined in MKL framework. Like the canonical MKL <ref type="bibr" target="#b9">[10]</ref>, these methods adopt a uniform kernel combination strategy over the whole input space. However, in the presence of significant intra-class diversity and inter-class correlation, they may be difficult to deal with complex data distribution and suffer a degraded performance.</p><p>More recently, sample-specific MKL methods are proposed in <ref type="bibr" target="#b22">[23]</ref> by adopting a sample-specific kernel weighting strategy. The basic idea is that kernel weights not only depend on the kernel functions but also on the samples. Compared with canonical MKL, a sample-specific MKL tend to reflect the relative importance of different kernels at each sample rather than at the level of object category. Despite of some performance improvements, learning too many parameters may lead to the expensive computation as well as the high risk of over-fitting. Although our proposed GS-MKL and other methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> reviewed above are all extended from MKL framework, GS-MKL provides a mechanism of evaluating multi-kernels over groups (sub-categories).</p><p>In addition, GS-MKL is different from classifier ensemble methods which train multiple classifiers separately using different data subsets or features and then combine the classifiers to obtain better performance. Although our method also partitions training data into groups, GS-MKL learns a single classifier based on the group-sensitive kernel combinations which adapt with the local data distributions of object sub-categories. Also, GS-MKL couples feature/kernel weighting and classifier leaning in a joint optimization problem. Training Samples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Group-Sensitive MKL Framework</head><formula xml:id="formula_0">( ) ( ) 1 1 ( ) ( ) i N M c x c x i i m m m i i m f x y K x, x b D E E ¦ ¦ 1 x N x 2 x 1 1 ( ) ( ) ( ) ( ) N M i i m i m m i i m f x y x xK x, x b D E E ¦ ¦ 1 1 ( ) ( ) N M i i m m i i m f x y K x, x b D E ¦ ¦ ^1,..., i x i N Increase the num of groups ^( =1 i i x c x ^( = i i x c x G x Test sample x Test sample x Test sample ^`1 M m m E ^1 1 M m m E ^2 1 M m m E ^`1 M G m m E ^( =2 i i x c x ^1 1 ( ) M m m x E ^2 1 ( ) M m m x E ^`1 ( ) M m N m x E Fig. 2</formula><p>.Three paradigms of object categorization using (a) Canonical MKL, (b) Group-sensitive MKL, and (c) Sample-specific MKL. In the figure, images with green bounding boxes are positive samples while those with red bounding boxes are negative samples for "bridge". Note that sample-specific MKL will learn two sets of kernel weights even for two images with quite similar appearance (e.g. x 1 and x 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Canonical MKL</head><p>SVMs have been proven to be efficient tools for solving classification problems. However, the discriminative power of SVMs heavily relies on kernel selection which is generally accomplished by cross-validation. Instead of selecting a single kernel, MKL <ref type="bibr" target="#b9">[10]</ref> learns a convex kernel combination and the associated classifier simultaneously. The combination of multi-kernels is defined as follows: For binary classification, the decision function of canonical MKL is given as follows:</p><formula xml:id="formula_1">1 ( ) ( ) M i m m i m K x ,x K x ,x E ¦<label>(1)</label></formula><formula xml:id="formula_2">1 1 ( ) ( ) , N M i i m m i i m f x y K x,x b D E ¦ ¦<label>(2)</label></formula><p>where ^`1</p><formula xml:id="formula_3">N i i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head><p>and b are the coefficients of the classifier, corresponding to the Lagrange multipliers and the bias in the canonical SVM problem. In MKL, the coefficients i D and the kernel weights m E can be learnt in a joint optimization problem (details can be found in <ref type="bibr" target="#b29">[30]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GS-MKL</head><p>As shown in Fig. <ref type="figure" target="#fig_7">2a</ref>, canonical MKL employs a uniform kernel combination over the whole input space. Instead of learning a global kernel combination, GS-MKL learns a set of group-sensitive kernel combinations to adapt with the complexity of data distribution.</p><p>As shown in Fig. <ref type="figure" target="#fig_7">2b</ref>, images from the same category are clustered into different groups by a pre-process (see Sec. 5.3 for details). Then the kernel weights in GS-MKL not only depend on the kernel functions, but also on the groups that the two images belong to. Let ( ) c x and ( ) i c x be the group ids of image x and i x respectively. The combined kernel form in Eqn. 1 can be rewritten as: x . Let G denote the total group number, then ( ) 1 { ,..., ,..., } for (1,..., ).</p><formula xml:id="formula_4">( ) ( ) 1 ( ) ( ) i M c x c x i m m m i m K x ,x K x ,x E E ¦ ,<label>(3) where</label></formula><formula xml:id="formula_5">c x g G m m m m m M E E E E</formula><p>Accordingly, the decision function in Eqn. 2 can be reformulated as:</p><formula xml:id="formula_6">( ) ( ) 1 1 ( ) ( ) , i N M c x c x i i m m m i i m f x y K x,x b D E E ¦ ¦<label>(4)</label></formula><p>where the coefficients u . The coefficients and the group-sensitive kernel weights are optimized in a joint manner, which will be shown in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connection with Other MKL Methods</head><p>In this part, we show that GS-MKL can be generalized to canonical MKL and sample-specific MKL.</p><p>In the special case of 1 G , all samples belong to one group and share a unique set of kernel weights ^1 In the case of G N , each training sample belongs to an individual group and thus an sample-specific kernel weighting strategy is employed . In this way, ( )    <ref type="figure">c</ref> x m E only depends on the kernel function and the sample x . We also note that ( )    <ref type="bibr" target="#b22">[23]</ref>. Correspondingly, the decision function is:</p><formula xml:id="formula_7">c x m E is equivalent to ( ) m x E in Localized MKL (LMKL)</formula><formula xml:id="formula_8">1 1 ( ) ( ) ( ) ( , ) , N M i i m i m m i i m f x y x xK x x b D E E ¦ ¦<label>(5)</label></formula><p>The number of group-sensitive kernel weights then reaches up to N M u where N G . In this case, GS-MKL scales up to sample-specific MKL (see Fig. <ref type="figure" target="#fig_7">2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning GS-MKL Based Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The GS-MKL Primal Problem</head><p>In GS-MKL, sample x is transformed via mappings , where m d denotes the dimensionality of the m th feature space. Each feature map is associated with a weight vector m w . To allow the combination of kernels as expressed by Eqn. 3, the decision function of canonical MKL in Eqn. 2 can be rewritten as follows:</p><formula xml:id="formula_9">( ) 1 ( ) , . M c x m m m m f x x b E I ¦ w ( )<label>(6)</label></formula><p>Inspired by SVM <ref type="bibr" target="#b20">[21]</ref>, training can be implemented by solving the following optimization problem, which maximizes the margin between positive and negative classes as well as minimizes the classification error.  </p><formula xml:id="formula_10">, , ,<label>1 1 ( ) 1 1</label></formula><formula xml:id="formula_11">, min 2 . . ( ,<label>( ) ) 1 ,</label></formula><formula xml:id="formula_12">0 m i M N m i b m i M c x i m m m i i m i C s t y x b i i [ E [ E I [ [ ¦ ¦ t ¦ t w w w<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The GS-MKL Dual Problem</head><p>Through introducing Lagrange multipliers ^`1</p><formula xml:id="formula_13">N i i D into</formula><p>the above inequalities constraint in Eqn. 7, and formulating the Lagrangian dual function which satisfies the Karush-Kuhn-Tucker(KKT) condition <ref type="bibr" target="#b9">[10]</ref>, the former optimization problem reduces to a max-min problem as follows:</p><formula xml:id="formula_14">( ) ( ) 1 1 1 1 1</formula><p>, where max min</p><formula xml:id="formula_15">1 ( ( ) ) , 2 s.t. 0, 0 , j i N N M N c x c x i j i j m m m i j i i j m i N i i i i J J yy K x, x y C i D D E E D D D ¦ ¦ ¦ ¦ d d ¦ Į ȕ (8)</formula><p>This max-min problem is the GS-MKL dual problem. J is a multi-object function for Į and ȕ . When ȕ is fixed, minimizing J over the coefficient Į is meant to minimize the global classification error and maximize the margin between positive and negative classes. When Į is fixed, maximizing J over the group-sensitive kernel weights ȕ is meant to maximize the intra-class similarity and minimize the inter-class similarity simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization Algorithm</head><p>Similar to the parameter learning in canonical MKL, we adopt a two-stage alternant optimization approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.1</head><p>The computation of Į given ȕ Fixing ȕ , the classifier coefficient Į can be estimated by minimizing J under the constraint 0 ,</p><formula xml:id="formula_16">i C i D d d and 1 0 N i i i y D ¦</formula><p>. Minimization of J is identical to solve the canonical SVM dual problem with the kernel combination in Eqn. 3. Consequently, minimizing J over Į can be easily implemented as there exist several efficient SVM solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>The computation of ȕ given Į To optimize the group-sensitive kernel weights ȕ with a fixed Į , the objective function can be expressed as:</p><formula xml:id="formula_17">, , ,<label>1 1 1 1</label></formula><formula xml:id="formula_18">( ) ( ) , G G M N g g gg m m m i g m i g J S E E D D ¦ ¦ ¦ ¦ ȕ<label>(9) where '</label></formula><formula xml:id="formula_19">{ ( = }{ ( = '} 1 ( ) ( , ). 2 i j gg m i i j j m i j i c x g j c x g S y y Kx x D D ¦ ¦ Į<label>(10)</label></formula><p>When G=1, ' gg m S corresponds to ( ) k S Į in canonical MKL <ref type="bibr" target="#b21">[22]</ref>. When G &gt; 1, the samples within a group have the same label ( ^1 r ) based on the assumption that the intermediate representation group is introduced to capture the locality of each sub-category. In this case, ' gg m S stands for the correlation of group g and g' over the m th kernel function. When g and g' have the same label, maximizing J over ȕ is to maximize the intra-class similarity. When g and g' have different labels, maximizing J over ȕ is to minimize the inter-class similarity. Correspondingly, the optimization of J over ȕ can be rewritten as:</p><formula xml:id="formula_20">, , ,<label>1 1 1 max</label></formula><p>( )</p><formula xml:id="formula_21">G G M g g gg m m m g m g S E E ¦ ¦ ¦ ȕ Į<label>(11)</label></formula><p>Note that the optimization problem in Eqn. 11 is not convex. Inspired by <ref type="bibr" target="#b22">[23]</ref>, instead of solving ȕ directly, we use a normalized exponential weighting function to approximate the nonnegative ȕ . Particularly, ȕ is determined by statistical property of the group and the parameters of the function which are also learned from data. In this paper, such weighting function is defined as:</p><formula xml:id="formula_22">' ' ' ' 1 exp( ) exp( ) g g g g m m m m M g g g m m m m a K b a K b E ¦<label>(12)</label></formula><p>where g m a and g m b are the parameters of the function, and g m K corresponds to a certain statistical property for the g th group over the m th kernel function. Let g n be the number of samples in the g th group. In this paper, we define g m K as:</p><formula xml:id="formula_23">{ ( = }{ ( = } 2 ( , ) i j m i j i c x g j c x g g m g K x x K n ¦ ¦<label>(13)</label></formula><p>As stated in <ref type="bibr" target="#b30">[31]</ref>, ( ) J ȕ is differentiable if the SVM solution is unique. Such condition can be guaranteed by the fact that all kernel matrices are strictly positive definite. Thus, we take derivatives of ( ) J ȕ w.r.t. g m a , g m b , and use gradient-descent method to train the weighting function:</p><formula xml:id="formula_24">1 1 ( ) 2 ( ( ( )) ( )) M G i ig g g l g l l m m m l g l i m J S K a E E G E w ¦ ¦ w ȕ Į (14) 1 1 ( ) 2 ( ( ( )) ( )) M G i ig g l g l l m m l g l i m J S b E E G E w ¦ ¦ w ȕ Į (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>where l m G is 1 if l m and 0 otherwise. After updating the parameters of the weighting function, we get a new ȕ and then solve a single kernel SVM as in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Summarization of GS-MKL optimization process</head><p>The optimization algorithm of GS-MKL is summarized in Alg. 1. The termination criteria can be the consistency of Į or ȕ between two consecutive steps, or a predefined iteration upper bound.</p><p>In Alg. </p><formula xml:id="formula_26">j i M c x c x i j m m m i j m K x ,x K x ,x E E ¦ 5:</formula><p>Solve Į using the canonical SVM with ( ) </p><formula xml:id="formula_27">i j K x ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the experiments, we treat object categorization as the multi-class classification problem in the one-vs.-all setting. As we assume that no prior knowledge is available about the image data distribution, we empirically evaluate the optimal grouping strategy and determine a proper group number. And then we evaluate the performance of our proposed GS-MKL on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>Extensive experiments are performed on Caltech101 <ref type="bibr" target="#b6">[6]</ref>, Pascal VOC2007 <ref type="bibr" target="#b7">[7]</ref> and WikipediaMM [9] datasets. Caltech101 involves 102 object categories, where each category containing 31 to 800 images. Pascal VOC2007 consists of 20 object categories, where 2501 images taken in real-world are provided for training, 2510 for validation and 4952 for test. WikipediaMM dataset contains some 150,000 real-world web images from Wikipedia that cover 75 topics. In our experiment, 33 topics, each of which contains more than 60 positive samples, are employed. Note that some topics not only share similar visual appearances, but also produce semantic correlations, e.g., "house architecture" versus "gothic cathedral" and "military aircraft" versus "civil aircraft". Compared with Caltech101, Pascal VOC2007 and WikiPediaMM exhibit higher intra-class diversity and inter-class correlation with more background clutter but less alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Features and Kernels</head><p>Several feature descriptors are involved in our experiments.</p><p>Two local appearance features (dense-color-SIFT (DCSIFT) and dense-SIFT (DSIFT) <ref type="bibr" target="#b3">[3]</ref>), two shape features (self-similarity (SS) <ref type="bibr" target="#b31">[32]</ref> and pyramid histogram of orientated gradients (PHOG) <ref type="bibr" target="#b32">[33]</ref>), and one texture feature (Gabor feature) are used. In particular, DCSIFT is computed in CIE-lab 3-channels over a square patch of radius with the spacing of r. We take r = 4, 8 and 12 pixels to allow scalability. Likewise DSIFT and Gabor feature are calculated in gray channel. SS descriptor is used to capture a correlation map of a 5×5 patch with its neighbors at every 5th pixel. The correlation map is quantized into 10 orientations and 3 radial bins to form a 30 dim descriptor. We employ k-means to quantize these descriptors to obtain codebooks of size k (say, 400) respectively.</p><p>For PHOG, two spatial pyramid kernels of gradient orientation are calculated to measure the image similarity in shape. PHOG-180degree employs 20 orientation bins and PHOG-360degree uses 40 orientation bins. For the other feature descriptors, we implement two kernel functions (i.e., SPK <ref type="bibr" target="#b3">[3]</ref> and PDK <ref type="bibr" target="#b15">[16]</ref>). For SPK, an image is divided into cells and the features from the spatially corresponding cells are matched across two images. The resulting kernel is a weighted combination of histogram intersections from coarse cells to fine cells. A 4-level pyramid is used with the grid sizes of 8×8, 4×4, 2×2 and 1×1 respectively. For PDK, local feature distributions of the K-nearest neighbors are matched across two images. The resulting kernel combines the local feature distributions at multiple scales, e.g. K = 1,…, k, where k is set to <ref type="bibr">(8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref> ranging from the finest to the coarsest neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Sensitivity Study of Grouping</head><p>In our experiments, grouping is a pre-processing step for GS-MKL. We have tried out two methods, k-means and probabilistic latent semantic analysis (pLSA) <ref type="bibr" target="#b24">[25]</ref>, to cluster images from each category into groups. Other grouping methods can be utilized. There is no prior knowledge about the number of sub-classes in an object category and the optimal number of groups for GS-MKL. Hence, we empirically identify the optimal group numbers for three datasets. For each category, images are clustered into N g groups (from 1 to 5). For Caltech101 and WikipediaMM, 20 images are randomly selected for training and 10 images for validation to find the optimal group number N g for each object category. For Pascal VOC2007, 2501 training images and 2510 validation images are employed to find the optimal N g for each object category.</p><p>In Tab.1, we list the best categorization results over the validation set and the corresponding mean group number for k-means and pLSA on three datasets. Clearly, N g ranging from 2.3 to 4.4, relates to the intra-class diversity of the dataset. From the table, we can see that pLSA outperform k-means slightly over three datasets. As a generative method, pLSA does not need explicit distance measure, which seems more robust against the distance based method. In the following experiments, we employ pLSA to group images with the optimized N g derived from validation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Experiment on Caltech101</head><p>In this set of experiments, we randomly select N train and N test images for training and test respectively, where N train ={10, 15, 20, 25, 30} and N test =15. We compare our GS-MKL approach with several recent methods <ref type="bibr">[2-4, 6, 11, 14, 18, 26, 27]</ref>. As shown in Fig. <ref type="figure" target="#fig_8">3</ref>, GS-MKL has achieved promising results comparable to the top performances of the state-of-the-art methods. From Fig. <ref type="figure" target="#fig_8">3</ref>, we note that when N train =10, GS-MKL obtains the performance of 65.1%, which is a bit lower than the best one (69.5%) <ref type="bibr" target="#b25">[26]</ref>. This may attribute to the inefficacy of grouping methods when training samples are too sparse. Compared with other methods, GS-MKL has obtained better performance when N train &gt;10. When N train is set to 30, the mean recognition rate reaches up to 84.3%, achieving a significant increase by 7.9% over canonical MKL (implemented as <ref type="bibr" target="#b29">[30]</ref>). This shows that further optimization of kernel weights over groups yields better feature (kernel) combination for object categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.2</head><p>Experiment on Pascal VOC 2007 In this set of experiments, we employ 5011 images for training and 4952 for test respectively. Tab. 2 compares the performances of GS-MKL to canonical MKL and some other recently published methods <ref type="bibr">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. It is worthy of note that the approach INRIA_genetic <ref type="bibr">[8]</ref> obtained the best performance in the Pascal VOC2007 challenge. The official performance metric Average Precision (AP) <ref type="bibr" target="#b7">[7]</ref> is used to evaluate the performance. The mean AP of GS-MKL is 62.2%, which is better than that of <ref type="bibr">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. GS-MKL has obtained the best results for 13 out of 20 categories. Over 10% improvements are obtained for two categories (i.e., "chair" and "potted plant"). Such results show the advantage of GS-MKL in handling the intra-class variation on real world image data. Under the same experimental setting, GS-MKL obtains better results for all 20 categories and 14.1% improvement on MAP against canonical MKL. This demonstrates that GS-MKL has better discriminative power than MKL by taking into account the intra-class diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.3</head><p>Experiment on WikipediaMM On WikipediaMM dataset, we further evaluate four typical multi-kernels based methods, i.e., unweighted multiple kernel (UMK) (equal kernel weights for multi-kernels), canonical MKL, sample-specific MKL (SS-MKL) (implemented as <ref type="bibr" target="#b22">[23]</ref>) and our proposed GS-MKL. For each image category, N train ={10, 15, 20, 25, 30} images are randomly picked out for training and the remaining images for test.</p><p>The results of five runs are shown in Tab. 3. We can see that GS-MKL outperforms three other multi-kernels based methods significantly on the dataset of real web images. Compared with UMK and MKL, GS-MKL obtains different degrees of improvements. Such results may be attributed to the ability of GS-MKL in adapting with the intra-class diversity and inter-class correlation. Note that the result of SS-MKL is just slightly lower than that of GS-MKL when N train &lt;20; but their performance gap becomes larger with more training images. This shows that GS-MKL is more effective in seeking a trade-off between diversity and invariance within an object category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Time complexity</head><p>We implemented GS-MKL in C++. In each iteration of algorithm 1, we need to solve a canonical SVM problem with the group-sensitive kernel weights optimized by a gradient descent method. The time complexity of the gradient calculation is ignorable compared to the SVM solver. As those in canonical SVM solvers, using hot-start (i.e., providing previous Į as input) may accelerate the training process. Given the convergence termination criteria, the number of iterations before convergence depends on the training data and the step sizes. During training each category over 5k image samples on Pascal VOC2007 , the canonical MKL needs about 20 minutes, and GS-MKL needs 40 to 60 minutes to converge on server (8 Corel 3.0 GHz, 8GB RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we argue that modeling intra-class diversity and inter-class correlation among images is essential to improve the discriminative power of an object categorization method. To this end, we have introduced an intermediate representation "group" in the MKL framework, and proposed a GS-MKL method to learn both the parameters of group-sensitive kernel weights and the classifier in a joint manner. Our GS-MKL has yielded promising results over Caltech101, Pascal VOC2007 and WikipediaMM datasets based on existing visual features and kernels.</p><p>In current implementation, grouping process is regarded as a pre-process followed by GS-MKL and the optimal group number is obtained over a validation set. In the future work, we will attempt to integrate the optimizations of three different stages (i.e., grouping, kernel combination and classifier learning). Additionally, we will employ more effective kernel functions and visual features in GS-MKL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of intra-class diversity and inter-class correlation of object "bridges". Double-headed arrows stand for the visual correlation between images. A thicker arrow indicates stronger visual correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>object category, and N is the number of training samples. Based on the labeled dataset L D , we aim to train a multi-kernels based classifier with a decision function ( ) f x to predict the object category of an unlabeled image x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where M is the total number of kernels, m K is a positive definite kernel associated with a reproducing kernel Hilbert space (RKHS), and ^`1 M m m E are kernel weights which are optimized during training. Each m K can employ different kernel functions and use different feature subsets or data representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>are group-sensitive kernel weights of x and i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 2 [</head><label>12</label><figDesc>, , , ] T N D D D Į " and bias b have similar meanings as in canonical MKL. This decision function can be derived from the GS-MKL primal problem in Sec. 4.1. Compared with M kernel weights in the canonical MKL case, the number of group-sensitive kernel weights gets rise up to G M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 mw</head><label>2</label><figDesc>is a regularization term which is inversely related to margin, classification error, and C is the misclassification penalty. The optimal C can be obtained by cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of GS-MKL and other recent methods on Caltech101dataset. GS-MKL: number of training samples (mean recognition rate), 10 (65.1), 15 (73.2), 20 (80.1), 25 (82.7), 30 (84.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Table3.</head><label></label><figDesc>Performance of four multi-kernels based methods on WikipediaMM. .0 50.1±0.8 54.3±0.8 56.1±0.7 58.2±0.6 SS-MKL 47.3±1.6 53.4±1.3 56.2±0.9 57.8±1.1 60.5±1.0 GS-MKL 49.2±1.2 56.6±1.0 61.0±1.0 64.3±0.8 67.6±0.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Average Precision of GS-MKL and other methods on the</figDesc><table><row><cell></cell><cell cols="4">Pascal VOC 2007 dataset</cell><cell></cell><cell></cell></row><row><cell>categories</cell><cell>[8]</cell><cell>[34]</cell><cell>[28]</cell><cell>[35]</cell><cell>MKL</cell><cell>GS-MKL</cell></row><row><cell>aero plane</cell><cell>77.5</cell><cell>63.0</cell><cell>65.0</cell><cell>65.0</cell><cell>74.1</cell><cell>79.4</cell></row><row><cell>bicycle</cell><cell>63.6</cell><cell>22.0</cell><cell>44.3</cell><cell>48.0</cell><cell>53.9</cell><cell>62.4</cell></row><row><cell>bird</cell><cell>56.1</cell><cell>14.0</cell><cell>48.6</cell><cell>44.0</cell><cell>46.6</cell><cell>58.5</cell></row><row><cell>boat</cell><cell>71.9</cell><cell>42.0</cell><cell>58.4</cell><cell>60.0</cell><cell>62.2</cell><cell>70.2</cell></row><row><cell>bottle</cell><cell>33.1</cell><cell>43.0</cell><cell>17.8</cell><cell>20.0</cell><cell>37.5</cell><cell>46.6</cell></row><row><cell>bus</cell><cell>60.6</cell><cell>50.0</cell><cell>46.4</cell><cell>49.0</cell><cell>55.6</cell><cell>62.3</cell></row><row><cell>car</cell><cell>78.0</cell><cell>62.0</cell><cell>63.2</cell><cell>70.0</cell><cell>70.7</cell><cell>75.6</cell></row><row><cell>cat</cell><cell>58.8</cell><cell>32.0</cell><cell>46.8</cell><cell>49.0</cell><cell>48.4</cell><cell>54.9</cell></row><row><cell>chair</cell><cell>53.5</cell><cell>37.0</cell><cell>42.2</cell><cell>50.0</cell><cell>54.0</cell><cell>63.8</cell></row><row><cell>cow</cell><cell>42.6</cell><cell>19.0</cell><cell>29.6</cell><cell>32.0</cell><cell>34.7</cell><cell>40.7</cell></row><row><cell>dining table</cell><cell>54.9</cell><cell>30.0</cell><cell>20.8</cell><cell>39.0</cell><cell>50.1</cell><cell>58.3</cell></row><row><cell>dog</cell><cell>45.8</cell><cell>29.0</cell><cell>37.7</cell><cell>40.0</cell><cell>40.7</cell><cell>51.6</cell></row><row><cell>horse</cell><cell>77.5</cell><cell>15.0</cell><cell>66.6</cell><cell>72.0</cell><cell>76.6</cell><cell>79.2</cell></row><row><cell>motorbike</cell><cell>64.0</cell><cell>31.0</cell><cell>50.3</cell><cell>59.0</cell><cell>59.8</cell><cell>68.1</cell></row><row><cell>person</cell><cell>85.9</cell><cell>43.0</cell><cell>78.1</cell><cell>81.0</cell><cell>82.5</cell><cell>87.1</cell></row><row><cell>potted plant</cell><cell>36.3</cell><cell>33.0</cell><cell>27.2</cell><cell>32.0</cell><cell>38.3</cell><cell>49.5</cell></row><row><cell>sheep</cell><cell>44.7</cell><cell>41.0</cell><cell>32.1</cell><cell>35.0</cell><cell>40</cell><cell>48.8</cell></row><row><cell>sofa</cell><cell>50.6</cell><cell>37.0</cell><cell>26.8</cell><cell>42.0</cell><cell>48.2</cell><cell>56.4</cell></row><row><cell>train</cell><cell>79.2</cell><cell>29.0</cell><cell>62.8</cell><cell>68.0</cell><cell>68.1</cell><cell>75.9</cell></row><row><cell>TV monitor</cell><cell>53.2</cell><cell>62.0</cell><cell>33.3</cell><cell>49.0</cell><cell>47.2</cell><cell>54.4</cell></row><row><cell>Mean AP</cell><cell>59.4</cell><cell>36.7</cell><cell>44.9</cell><cell>50.2</cell><cell>54.5</cell><cell>62.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2009" xml:id="foot_0"><p>IEEE 12th International Conference on Computer Vision (ICCV) 978-1-4244-4419-9/09/$25.00 ©2009 IEEE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>The work is supported by grants from Chinese NSF under contract No. 60605020 and No. 90820003, National Hi-Tech R&amp;D Program (863) of China under contract 2006AA010105, and National Basic Research Program of China under contract No. 2009CB320906. Also this work is supported in part by the research fund from NLPR, Institute of Automation, Chinese Academy of Sciences, and Microsoft Research Asia Internet Services Theme. The authors would like to thank Hong Chang and Yu Su for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SVM-KNN: discriminative nearset neighbor classification for visual category recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the discriminative power-invariance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: an incremental bayesian approach testing on 101 object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Generative-Model Based Vision, CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes challenge 2007 (VOC2007) results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vangool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.Pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Object Representations for Visual Object Class Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszaáek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visual Recognition Challenge, ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning globally-consistent Local distance functions for shape-based image retrieval and classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Similarity-based cross-layered hierarchical representation for object categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discovering objects and their location in images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using dependent regions for object categorization in a generative framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pyramid match kernels: Discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>MIT CSAIL TR 2006-020</idno>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Proximity distribution kernels for geometric context in category recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Haibin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support kernel machines for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An exemplar model for learning object classes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognition by association via learning per-exemplar distances</title>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Advances in Kernel Methods -Support Vector Learning, chapter Fast Training of Support Vector Machines using Sequential Minimal Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JLMR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Localized multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning subcategory relevancies for category recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiclass object recognition with sparse, localized features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning Image Similarity from Flickr Groups Using Stochastic Intersection Kernel Machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In MIR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local ensemble kernel learning for object category recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><surname>Simplemkl</surname></persName>
		</author>
		<author>
			<persName><surname>Jlmr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Choosing multiple parameters for support vector machines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="131" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
		<idno>CIVR 2007</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object Categorization using Co-Occurrence, Location and Appearance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Top-Down Color Attention for Object Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
