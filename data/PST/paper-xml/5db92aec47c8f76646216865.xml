<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Astik</forename><surname>Biswas</surname></persName>
							<email>abiswas@sun.ac.za</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raghav</forename><surname>Menon</surname></persName>
							<email>rmenon@sun.ac.za</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ewald</forename><surname>Van Der Westhuizen</surname></persName>
							<email>ewaldvdw@sun.ac.za</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Niesler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Stellenbosch University</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>Somali</term>
					<term>semi-supervised</term>
					<term>TDNN-F</term>
					<term>under-resourced language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present improvements in automatic speech recognition (ASR) for Somali, a currently extremely under-resourced language. This forms part of a continuing United Nations (UN) effort to employ ASR-based keyword spotting systems to support humanitarian relief programmes in rural Africa. Using just 1.57 hours of annotated speech data as a seed corpus, we increase the pool of training data by applying semi-supervised training to 17.55 hours of untranscribed speech. We make use of factorised time-delay neural networks (TDNN-F) for acoustic modelling, since these have recently been shown to be effective in resourcescarce situations. Three semi-supervised training passes were performed, where the decoded output from each pass was used for acoustic model training in the subsequent pass. The automatic transcriptions from the best performing pass were used for language model augmentation. To ensure the quality of automatic transcriptions, decoder confidence is used as a threshold. The acoustic and language models obtained from the semisupervised approach show significant improvement in terms of WER and perplexity compared to the baseline. Incorporating the automatically generated transcriptions yields a 6.55% improvement in language model perplexity. The use of 17.55 hour of Somali acoustic data in semi-supervised training shows an improvement of 7.74% relative over the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In countries with a well established internet infrastructure, social media has become an accepted platform for sharing opinions and concerns <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Surveys conducted by the United Nations (UN) in places lacking sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Therefore, to support its humanitarian relief efforts in rural and under-developed parts of Uganda, the UN has piloted radio browsing systems in three of the country's languages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The success in Uganda served as a motivator for the development of a corresponding system for Somalia, an African country where the UN is also currently engaged. However, the Somali language is extremely under-resourced and the difficulty of compiling speech data resulted in an available training set of just 1.57 hours <ref type="bibr" target="#b8">[9]</ref>. By leveraging available resources from better-resourced but unrelated languages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, a system using a hybrid neural network acoustic model was able to achieve a word error rate (WER) of 53.75% <ref type="bibr" target="#b8">[9]</ref>. While the additional language resources were beneficial to the acoustic model, we found that care had to be taken when deciding on the composition of the multilingual training set. To our knowledge, only one other study on Somali automatic speech recognition (ASR) has so far been described in the literature <ref type="bibr" target="#b11">[12]</ref>.</p><p>Somali is an Afroasiatic language. It is the official language of Somalia and widely used its neighbouring countries. 1 There are an estimated 7 million native Somali speakers in Somalia and 10 to 16 million worldwide. Somali is written using the Latin alphabet, and has a phoneme inventory of 23 consonants and five vowels. Somali is an agglutinative language that is characterised by a high number of morphemes per word.</p><p>The multilingual Somali acoustic model described in <ref type="bibr" target="#b8">[9]</ref> uses a hybrid neural network that contains several million parameters. This complexity increases the computational demand of the decoding process. Since the computational resources available in the target setting are very limited, it is important to reduce the required computation, for example by using an acoustic model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type="bibr" target="#b12">[13]</ref> utilise half the number of parameters than the hybrid networks with comparable performance, in particular in a low-resource setting. This motivated us to consider this neural network architecture for acoustic modelling using our extremely small Somali training corpus.</p><p>In the following, we present the results of our most recent efforts to improve our Somali ASR system. We make use of TDNN-F acoustic models and experiment with the incorporation of additional but unannotated Somali speech data by semisupervised training, an approach which has been applied successfully in some other low-resource settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Radio browsing system</head><p>Figure 1 <ref type="bibr" target="#b7">[8]</ref> shows the components of the radio browsing system. The preprocessed audio stream is passed to the ASR system which generates lattices which are subsequently searched for predefined keywords. Human analysts further process the data which aid in humanitarian decision making and situational awareness. This system is currently successfully deployed by the UN in Uganda.   Besides the Somali data, the larger datasets used to train the Ugandan systems were available for acoustic modelling, as shown in Table <ref type="table" target="#tab_1">2</ref>. Luganda and Acholi are indigenous Ugandan languages, while Ugandan English is highly accented. In addition to the Ugandan data, a 20-hour dataset of South African English Broadcast News was available. While the Ugandan data was drawn exclusively from radio phone-in talk shows, the South African data was compiled from national radio news bulletins and consists of a mix of prepared and spontaneous speech <ref type="bibr" target="#b16">[17]</ref>. The total transcribed multilingual speech data available for acoustic model training (ManT), comprising Somali and these additional languages, adds up to 46.37 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Untranscribed acoustic data</head><p>Approximately 17.55 hours of untranscribed Somali speech data, also collected from phone-in programmes, was available for experimentation. No information regarding speaker identity or other characteristics of the speech was available. As a simple and naïve first approach, the speech files were simply divided into fixed-length segments before being passed to the recogniser for transcription and semi-supervised retraining. In future, the effect of more sophisticated segmentation, based for example on diarisation, will be investigated. The automatically transcribed data, or specific subset thereof, that is output by the transcriptions systems will be referred to as 'AutoT.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text data</head><p>Table <ref type="table" target="#tab_2">3</ref> summarises Somali text corpora that were available for language modelling. Besides the 15k words in the transcriptions of the Somali speech corpus training set (Table <ref type="table" target="#tab_0">1</ref>), a number of additional resources were available. Approximately 2M words were harvested from publicly-accessible news websites. A further 1.6M words were gathered from public Facebook posts and 3.5M words from selected Facebook comments. Comments were replies to the posts and were generally less well edited. While the Somali news text and Facebook posts were carefully manually cleaned and filtered, the Facebook comments consist of raw, unfiltered text <ref type="bibr" target="#b8">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type="bibr" target="#b17">[18]</ref> were included in our language model (LM) data collection. Finally, it has been shown by some researchers that text generated artificially using a long short-term memory (LSTM) neural network can lower language model perplexity when incorporated as additional training data <ref type="bibr" target="#b13">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic training transcriptions and generated an 11M word corpus of artificial text.  <ref type="figure" target="#fig_0">2</ref>. The figure shows that we implemented three iterations of semisupervised training, in each case re-transcribing the untranscribed data. To start the process, we used our best previouslyavailable Somali acoustic model <ref type="bibr" target="#b8">[9]</ref> labelled ASR1 in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Because Somali is an agglutinative language, the number of unique word tokens is large, which poses challenges to ASR while decoding the utterances. Thus, to improve the quality of the automatic transcriptions that are added to the training set, we applied a confidence threshold during each iteration. The threshold was calculated as the average decoder confidence score across all automatically transcribed utterances.</p><p>By applying this procedure, 9.11 hours of the available 17.55 hours of untranscribed data was added to the training set in the first pass to develop ASR2. In this second pass, 9.58 hours of automatically transcribed data was above the threshold and included in the training set to train ASR3.</p><p>Each training pass took approximately three days to complete on the limited available computational resources. Hence, given this computational restriction and time constraints, we were not able to perform an exhaustive search for an optimum threshold in the earlier passes. Nevertheless, in the final pass, we analysed the semi-supervised system performance for two different configurations:</p><p>• Threshold = 0, meaning that the full 17.55 hours of semisupervised data is considered (ASR4);</p><p>• Threshold = average decoder confidence, leading to 9.86 hours of semi-supervised data (ASR5).  represents untranscribed speech is being fed to transcriber</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Language modelling</head><p>All language models were built using the SRILM toolkit <ref type="bibr" target="#b18">[19]</ref>. The vocabulary of the ASR system was drawn from the pool of T1, T2 and T3 texts in Table <ref type="table" target="#tab_2">3</ref> by retaining all word types occurring at least four times. The resulting vocabulary consisted of 41.7k word types.</p><p>The language model used in <ref type="bibr" target="#b8">[9]</ref> was used as the baseline (LMbase). This model was trained on the Somali training set transcriptions (T1) and further interpolated with language models trained on the additional sources T2, T3, T4 and T7. Sources T5 and T6 were not available at the time. The interpolation weights were optimised on the Somali corpus test set (Table <ref type="table" target="#tab_0">1</ref>).</p><p>Four additional language models were trained and evaluated. The training and interpolation of these models was accomplished in much the same way as LMbase. LM2 was trained on T1 and further interpolated with LMs trained on T2 to T7. The interpolation weights were optimised on the test set. LM3 was also trained on T1 and interpolated with LMs trained on T2 to T7, but interpolation weights were optimised on a heldout validation set extracted from the Somali acoustic training set transcriptions. LM4 was trained on T1, interpolated with the LMs trained on T2 to T7 and further interpolated with the LM trained on the automatic transcriptions obtained at the output of ASR2 after the confidence threshold was applied. The interpolation weights were optimised on the test set. LM5 was also trained on T1, interpolated with the LMs trained on T2 to T7 and interpolated with the LM trained on the automatic transcriptions obtained at the output of ASR2 after the confidence threshold was applied. However, in this case the interpolation weights were optimised on the held-out validation set. The language model perplexities are shown in Table <ref type="table" target="#tab_4">4</ref>. The reason for using the AutoT transcriptions from the output of ASR2 is discussed later in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type="bibr" target="#b19">[20]</ref>. All experiments were performed using a PC with an 8-core Intel i7 CPU, 32GB of RAM and a 12GB NVIDIA Tesla GPU. In our previous work, we found multilingual training to improve ASR performance substantially <ref type="bibr" target="#b8">[9]</ref>. For multilingual training, the training sets of the four languages in Tables <ref type="table" target="#tab_1">1 and 2</ref> were combined. The lexica were concatenated and the phoneme labels were left unaltered, resulting in a combined, language-dependent phoneme set. All ASR experiments used a closed vocabulary, i.e. no out-of-vocabulary words occur in the test data. First, a context-dependent GMM-HMM acoustic model with 25k Gaussians was trained using all the multilingual ManT data. 39-dimensional Mel-frequency cepstral coefficients (MFCC) with deltas and delta-deltas were used as features. This GMM-HMM provided the alignments required for neural network training. The same multilingual training data was used to compute acoustic features for neural network training. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type="bibr" target="#b20">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (without derivatives), 3-dimensional pitch features and 100-dimensional i-vectors for speaker adaptation. It has recently been shown that, when semi-orthogonal lowrank matrix factorisation is applied to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type="bibr" target="#b12">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay layers followed by a rank reduction layer) was trained using the Librispeech recipe for Kaldi (version 5.2.164). The factorisation decomposes the high-dimensional parameter matrix into two factor matrices, one of which is constrained to be semi-orthogonal. This results in an intermediate bottleneck layer in the TDNN layer that has a lower dimension than the hidden layer. This factorisation allows the TDNN-F model to use fewer parameters than hybrid architectures such as TDNN-LSTM and TDNN-BLSTM (bidirectional LSTM). Hence, the TDNN-F models are faster to train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type="bibr" target="#b21">[22]</ref>. No parameter tuning was performed during neural network training and the default recipe parameters were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results and discussion</head><p>The ASR performance is reported in Table <ref type="table" target="#tab_3">5</ref> in terms of the word error rate (WER) for the various training approaches. In comparison with our previous ASR system <ref type="bibr" target="#b8">[9]</ref>, the improvement afforded by TDNN-F is clear (rows 1 and 2). Even though TDNN-F uses only half the number of parameters as CNN-TDNN-BLSTM, it is able to offer better performance.</p><p>Next we consider the performance of semi-supervised training. Comparing ASR1 and ASR2, we see that the first pass of decoding and retraining decreases the WER by 3.30% relative to the baseline. Due to the agglutination property, Somali has a large vocabulary and more semi-supervised training data helps to improve performance. The neural network is able to learn from the larger training set afforded by the additional semi-supervised data. In the second pass, ASR3 yields a further relative improvement of 1.85% over ASR2. The third pass (ASR4 and ASR5) did not show any further WER improvement. ASR5, that was trained on the thresholded automatically transcribed data, was not able to outperform its counterpart from the previous pass (ASR3). However, it was able to perform better than ASR4 which was trained using the full set of automatically transcribed data. The results show that acoustic model training does not gain from the semi-supervised data after the second pass, and that the inclusion of poorly transcribed data leads to a deterioration in recognition performance.</p><p>The best performing ASR system so far is ASR3, which was trained on the semi-supervised data produced by ASR2. Next, we evaluate the language models described in Section 5 using the acoustic models of ASR3. LM2 and LM3 were trained on the sources in Table <ref type="table" target="#tab_2">3</ref>, while LM4 and LM5 included the automatically transcribed text from ASR2 (AutoTASR2) as additional training data. Figure <ref type="figure">3</ref> gives a representation of the semisupervised experimental framework for the language model evaluations. LM2, which was optimised on the text set, did Figure <ref type="figure">3</ref>: Semi-supervised acoustic and language modelling for Somali ASR3.</p><p>not show any significant improvement over the baseline. However, LM3, which was optimised on the validation set, showed an improvement of 1.86% relative to the baseline. The results in Table <ref type="table" target="#tab_6">6</ref> show that the language models that included automatically transcribed text (LM4 and LM5) improve ASR performance. The perplexity of LM5 was higher than the baseline, but it was optimised on the validation set and provided the better ASR performance, decreasing the WER by 3.32%. Overall, we achieved a 7.74% relative improvement over our supervised baseline Somali ASR system, despite taking a simple approach to segmentation of the untranscribed speech. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented our initial efforts to increase the pool of Somali acoustic and language model data in a semi-supervised manner in an effort to improve automatic speech recognition for Somali. A training corpus of only 1.57 hours of in-domain segmented and transcribed Somali radio broadcast speech data was available. A further 17.55 hours of unannotated Somali speech was segmented using a straightforward approach. In addition, approximately 44.8 hours of annotated speech in three unrelated languages was available for multilingual modelling.</p><p>A baseline TDNN-F acoustic model was trained using this multilingual data and semi-supervised training was carried out in three passes. In each pass, a threshold was applied to the confidence score of the decoded output, discarding utterances below the threshold. We found that only the first two such passes of semi-supervised learning improved performance. Although the segmentation approach for the untranscribed data was extremely simple, a 7.74% relative improvement over the baseline system was achieved. Ongoing work is exploring the effects of more sophisticated segmentation and diarisation approaches, as well as a more careful optimisation of the confidence threshold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label>2</label><figDesc>Figure 1<ref type="bibr" target="#b7">[8]</ref> shows the components of the radio browsing system. The preprocessed audio stream is passed to the ASR system which generates lattices which are subsequently searched for predefined keywords. Human analysts further process the data which aid in humanitarian decision making and situational awareness. This system is currently successfully deployed by the UN in Uganda.2   </figDesc><graphic url="image-1.png" coords="1,341.07,597.90,170.05,67.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The radio browsing system [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Semi-supervised training framework for Somali ASR.</figDesc><graphic url="image-2.png" coords="3,57.62,73.99,481.81,201.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Somali transcribed speech data. Duration is indicated in hours (h) and minutes (m).</figDesc><table><row><cell cols="6">1 https://www.alsintl.com/resources/languages/</cell></row><row><cell>Somali/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 Examples</cell><cell>of</cell><cell>system</cell><cell>output</cell><cell>are</cell><cell>available</cell></row><row><cell cols="4">at http://radio.unglobalpulse.net</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Transcribed speech data from other languages.</figDesc><table><row><cell>Speech dataset</cell><cell>Utterances</cell><cell>Duration</cell></row><row><cell>Luganda</cell><cell>8.8k</cell><cell>9.6h</cell></row><row><cell>Acholi</cell><cell>4.9k</cell><cell>9.2h</cell></row><row><cell>Ugandan English</cell><cell>4.4k</cell><cell>6.0h</cell></row><row><cell>South African English</cell><cell>10.5k</cell><cell>20.0h</cell></row><row><cell>Total</cell><cell>28.6k</cell><cell>44.8h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Somali text resources used for language modelling.</figDesc><table><row><cell>Corpus</cell><cell cols="3">Word tokens Word types Sentences</cell></row><row><cell>T1 Somali transcriptions</cell><cell>15.1k</cell><cell>4.7k</cell><cell>1.3k</cell></row><row><cell>T2 Somali news text</cell><cell>1.92M</cell><cell>82.8k</cell><cell>59.2k</cell></row><row><cell>T3 Facebook posts</cell><cell>1.55M</cell><cell>92.9k</cell><cell>54.9k</cell></row><row><cell>T4 Facebook comments</cell><cell>3.5M</cell><cell>356.7k</cell><cell>215.3k</cell></row><row><cell>T5 LCC newspaper text</cell><cell>2.37M</cell><cell>300k</cell><cell>100k</cell></row><row><cell>T6 LCC Wikipedia text</cell><cell>200k</cell><cell>50.7k</cell><cell>10k</cell></row><row><cell>T7 LSTM generated text</cell><cell>11.29M</cell><cell>4.7k</cell><cell>775.3k</cell></row><row><cell cols="3">4. Semi-supervised training</cell><cell></cell></row></table><note>It has been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. As we only have less than two hours of transcribed Somali acoustic data, increasing the pool of in-domain data by semi-supervised training was an attractive option. To test this, we used a recently-acquired corpus comprising 17.55 hours of untranscribed Somali radio speech, as described in Section 3.2. Since no speaker information was available, each utterance was considered to originate from a unique speaker.Our semi-supervised training strategy is shown in Figure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>describes the training strategies pass by pass for better insight.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Perplexities of the evaluated language models. (AutoT: Includes automatic transcriptions; PPval: Perplexity evaluated on the held-out validation set; PPtst: Perplexity evaluated on the test set; AutoTASR2: Automatic transcriptions from ASR2 after thresholding.)</figDesc><table><row><cell>Language model</cell><cell>AutoT</cell><cell>PPval</cell><cell>PPtst</cell></row><row><cell>LMbase</cell><cell>No</cell><cell>-</cell><cell>269.80</cell></row><row><cell>LM2</cell><cell>No</cell><cell>-</cell><cell>253.60</cell></row><row><cell>LM3</cell><cell>No</cell><cell>576.98</cell><cell>321.31</cell></row><row><cell>LM4 LM5</cell><cell>AutoT ASR2 AutoT ASR2</cell><cell>-500.49</cell><cell>260.94 300.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The various training configurations used for experimentation. ASR1 is the baseline system. (AM: Acoustic model; ManT: Hours of manually transcribed speech; AutoT: Hours of automatically transcribed speech; AutoTASRX: Automatically transcribed speech obtained from ASR system 'X'; LM: Language model; Superv.: Supervised; Semi: Semi-supervised; WER: Word error rate.)</figDesc><table><row><cell>System</cell><cell>Training strategy</cell><cell>AM</cell><cell cols="3">AM training data size (h) ManT AutoT</cell><cell>LM</cell><cell>WER</cell></row><row><cell>Previous [9]</cell><cell>Superv.</cell><cell>CNN-TDNN-BLSTM</cell><cell>46.37</cell><cell></cell><cell>0.00</cell><cell>LMbase</cell><cell>53.75</cell></row><row><cell>ASR1</cell><cell>Superv.</cell><cell>TDNN-F</cell><cell>46.37</cell><cell></cell><cell>0.00</cell><cell>LMbase</cell><cell>53.68</cell></row><row><cell>ASR2 ASR3</cell><cell>Semi Semi</cell><cell>TDNN-F TDNN-F</cell><cell>46.37 46.37</cell><cell>AutoT ASR1 : AutoT ASR2 :</cell><cell>9.11 9.58</cell><cell>LMbase LMbase</cell><cell>51.91 50.95</cell></row><row><cell>ASR4</cell><cell>Semi</cell><cell>TDNN-F</cell><cell>46.37</cell><cell>AutoT ASR3 :</cell><cell>17.55</cell><cell>LMbase</cell><cell>51.71</cell></row><row><cell>ASR5</cell><cell>Semi</cell><cell>TDNN-F</cell><cell>46.37</cell><cell>AutoT ASR3 :</cell><cell>9.86</cell><cell>LMbase</cell><cell>51.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>WER results of ASR3 with various language models.</figDesc><table><row><cell>Acoustic Model</cell><cell cols="2">LMbase LM2</cell><cell>LM3</cell><cell>LM4</cell><cell>LM5</cell></row><row><cell>ManT Speech+ AutoTASR2 Speech (TDNN-F)</cell><cell>50.95</cell><cell cols="4">50.89 50.00 50.20 49.59</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head><p>We thank the NVIDIA corporation for the donation of GPU equipment used for this research. We also gratefully acknowledge the support of Telkom South Africa.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A human-machine collaborative system for identifying rumors on Twitter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDMW</title>
				<meeting>ICDMW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social media analysis for e-health and medical purposes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wegrzyn-Wolska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bougueroua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dziczkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CASoN</title>
				<meeting>CASoN</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine classification and analysis of suicide related communication on Twitter</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burnap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scourfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-HT</title>
				<meeting>ACM-HT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing attitudes towards contraception and teenage pregnancy using social data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P P</forename><surname>Series</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Pulse Project Series</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining citizen feedback data for enhanced local government decision-making</title>
	</analytic>
	<monogr>
		<title level="j">Global Pulse Project Series</title>
		<imprint>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding immunisation awareness and sentiment through social and mainstream media</title>
	</analytic>
	<monogr>
		<title level="j">Global Pulse Project Series</title>
		<imprint>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Radio-browsing for developmental monitoring in Uganda</title>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kibira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very low resource radio browsing for agile developmental and humanitarian monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kibira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic speech recognition for humanitarian applications in Somali</title>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLTU</title>
				<meeting>SLTU</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual Neural Network Acoustic Modelling for ASR of Under-Resourced English-isiZulu Code-Switched Speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Wet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual training of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7319" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic transcription of Somali language</title>
		<author>
			<persName><forename type="first">N</forename><surname>Addillahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jean-Francois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3743" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised acoustic model training for speech with code-switching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Heuvel</surname></persName>
		</author>
		<author>
			<persName><surname>Van Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for speech recognition in the context of accent adaptation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Nallasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Machine Learning in Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural network features and semi-supervised training for low resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6704" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Capitalising on North American speech resources for the development of a South African English large vocabulary speech recognition system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Wet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1255" to="1268" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building large monolingual dictionaries at the Leipzig Corpora Collection: From 100 to 200 languages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Quasthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="31" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
				<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
				<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
