<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The User as a Sensor: Navigating Users with Visual Impairments in Indoor Spaces using Tactile Landmarks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Navid</forename><surname>Fallah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557-0208</postCode>
									<settlement>Reno Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilias</forename><surname>Apostolopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557-0208</postCode>
									<settlement>Reno Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kostas</forename><surname>Bekris</surname></persName>
							<email>bekris@cse.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557-0208</postCode>
									<settlement>Reno Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eelke</forename><surname>Folmer</surname></persName>
							<email>efolmer@cse.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557-0208</postCode>
									<settlement>Reno Reno</settlement>
									<region>NV</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The User as a Sensor: Navigating Users with Visual Impairments in Indoor Spaces using Tactile Landmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B71A915A91BB5C5706CA981B1A83BDB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mobility</term>
					<term>Visual Impairment</term>
					<term>Indoor Navigation H.5.2 [User interfaces]: Voice I/O</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Indoor navigation systems for users who are visually impaired typically rely upon expensive physical augmentation of the environment or expensive sensing equipment; consequently few systems have been implemented. We present an indoor navigation system called Navatar that allows for localization and navigation by exploiting the physical characteristics of indoor environments, taking advantage of the unique sensing abilities of users with visual impairments, and minimalistic sensing achievable with low cost accelerometers available in smartphones. Particle filters are used to estimate the user's location based on the accelerometer data as well as the user confirming the presence of anticipated tactile landmarks along the provided path. Navatar has a high possibility of large-scale deployment, as it only requires an annotated virtual representation of an indoor environment. A user study with six blind users determines the accuracy of the approach, collects qualitative experiences and identifies areas for improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Navigation relies on a combination of mobility and orientation skills <ref type="bibr" target="#b7">[9]</ref>. People typically employ path integration, where they orient themselves relative to a starting position using proprioceptive data, or landmark-based navigation, where they rely upon perceptual cues together with an external or cognitive map <ref type="bibr" target="#b25">[27]</ref>. Path integration allows for exploring unfamiliar environments in which users may build a cognitive map by observing landmarks <ref type="bibr" target="#b26">[28]</ref>. Whereas sighted people primarily use vision to identify landmarks, individuals with visual impairments employ their compensatory senses, such as touch, hearing and even smell <ref type="bibr" target="#b33">[35]</ref>. Studies show small differences in path integration ability between sighted and individuals with visual impairments <ref type="bibr" target="#b25">[27]</ref>, but cognitive mapping is significantly slower for people with visual impairments <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b33">35]</ref> often leading to reduced mobility <ref type="bibr" target="#b10">[12]</ref>.</p><p>Various indoor and outdoor navigation systems for users with visual impairments have been developed. Whereas outdoor navigation systems rely on GPS for locating the user, indoor navigation systems use different techniques, described in the following section, as GPS signals cannot be received inside buildings. Indoor systems have not been implemented at a large scale as they often require expensive physical augmentation of the environment or they require the user to carry expensive sensing and computing equipment, which may further impede their mobility. This work presents an inexpensive navigation system called Navatar that does not augment physical infrastructure and that depends only on lightweight sensors available in popular devices such as smartphones. Navatar exploits the unique tactile sensing capabilities of users with visual impairments <ref type="bibr" target="#b6">[8]</ref> by having its users confirm the presence of anticipated landmarks, such as doors and hallway intersections along the provided path that are extracted from a virtual representation of the environment (see Figure <ref type="figure">1</ref>). This type of interaction seamlessly integrates with how users with visual impairments navigate familiar spaces as this includes the identification of known tactile landmarks <ref type="bibr" target="#b17">[19]</ref>. In prior research a feasibility study with blindfolded users was performed <ref type="bibr" target="#b2">[4]</ref>. This paper evaluates Navatar with blind users.</p><p>Session: Supporting Visually Impaired Users CHI 2012, May 5-10, 2012, Austin, Texas, USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Certain navigation devices have been developed that provide local obstacle-avoidance capabilities to users with visual impairments <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b38">40]</ref>. Most navigation systems, however, are able to locate the user and provide directions to a specified destination. Outdoor navigation systems <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b32">34]</ref> mainly use GPS for localization purposes, but indoor systems cannot use GPS signals, as buildings block them. To surpass this issue, alternative localization techniques have been developed that can be distinguished into three different categories.</p><p>Dead-Reckoning techniques estimate the user's current location based on a previously estimated or known location. While the user is moving, dead reckoning estimates the user's location by interpreting readings from one or more sensors that the user carries such as accelerometers, magnetometers, and gyroscopes <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b27">29]</ref>. The initial location is typically determined using GPS <ref type="bibr" target="#b13">[15]</ref>, RFID tags <ref type="bibr" target="#b19">[21]</ref>, or cellular phone positioning <ref type="bibr" target="#b29">[31]</ref>. The main drawback of this technique is the error accumulates over time as dead reckoning is a recursive process and each sensor has some inaccuracy. One significant benefit of this approach is the low installation cost, as it does not require physical infrastructure.</p><p>Beacon-based approaches augment the physical space with identifiers. Such beacons can be retro-reflective digital signs detected by a camera <ref type="bibr" target="#b37">[39]</ref>, infrared <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b35">37]</ref> or ultrasound identifiers <ref type="bibr" target="#b28">[30]</ref>. Beacons can be integrated with deadreckoning to correct the accumulated error through environmental knowledge <ref type="bibr" target="#b13">[15]</ref>, RFID tags <ref type="bibr" target="#b19">[21]</ref>, ultrasound beacons <ref type="bibr" target="#b18">[20]</ref>, and map-matching <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b27">29]</ref>. Especially, Radio Frequency Identifier (RFID) tags have attracted a lot of attention in this direction <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b1">3]</ref>. Nevertheless, locating identifiers may be hard, as beacons may require line of sight or close proximity to the human. Other beacon based techniques use triangulation to locate the user. For example, wireless network positioning systems <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b21">23]</ref> may triangulate the location of base stations using the provided signal strength or could be building signal strength maps. Wireless nodes often suffer from multi-path effects or interference. Another drawback is the significant time and cost spent installing and calibrating beacons. Though RFID tags are cheap, a large amount of them is required to cover a whole building. Installing them in non-carpeted environments, such as concrete floors or tiles is often prohibitively expensive.</p><p>Sensor-based solutions employ sensors, such as cameras that can detect pre-existing features of indoor spaces, such as walls or doors. For instance, a camera system matches physical objects with objects in a virtual representation of the space <ref type="bibr" target="#b15">[17]</ref> to locate the user. However, cameras require good lighting conditions, and may impose a computational cost prohibitive for portable devices. An alternative makes use of a 2D laser scanner <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b11">13]</ref>. This method achieves 3D pose estimation by integrating data from inertial sensors, the laser scanner, and knowledge of the 3D structure of the space. While this method achieves accurate localization, it relies upon expensive and heavy sensors, which may further impede the mobility of users with visual impairments as they already carry devices such as a cane or a Braille display. Figure <ref type="figure">3</ref>. Error in location estimation using dead reckoning quickly grows unbounded (left) but using periodic synchronization with a beacon with a known location (right) errors can be mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DESIGN OF NAVATAR</head><p>Localization is required in any navigation system to be able to detect veering, e.g., when the user is deviating from the intended path. Precise localization in indoor environments comes at a significant cost; for example, large scale augmentation of indoor environments with RFID tags is often prohibitively expensive. Veering is likelier to occur in outdoor environments than in indoor environments as navigation is typically constrained by physical infrastructure, such as walls (see Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>To facilitate large-scale deployment of an indoor navigation system, we argue that less precise but less expensive localization solutions may need to be explored. Of the three techniques discussed in the related work section, dead reckoning does not require expensive sensing or augmentation of the environment. Dead reckoning can be achieved with commodity portable devices such as a smartphone, which have integrated accelerometers and magnetometers. To achieve a better level of localization accuracy, the proposed approach integrates inexpensive sources of dead-reckoning data with sensing information that comes from a human user. Dead reckoning is relatively accurate for short distances where error can be mitigated by periodic sensing of beacons with a known location <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b18">20]</ref> (see Figure <ref type="figure">3</ref>). The novelty in this approach is the utilization of a human as a sensor rather than just an actuator.</p><p>The cognitive mapping of indoor spaces by people with visual impairments has been extensively studied <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b9">11]</ref>. Tactile landmarks that are easily sensed through touch, such as doors, hallway intersections and floor transitions, play an important role in the cognitive mapping of indoor spaces by users with visual impairments <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b4">6]</ref>. Sounds or even smells may also be used as landmarks <ref type="bibr" target="#b4">[6]</ref>. Users who are blind have better tactile sensing capabilities than sighted people <ref type="bibr" target="#b6">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Overview</head><p>Because the identification of tactile landmarks already plays a significant role in how users with visual impairments navigate familiar spaces, our navigation system combines dead reckoning with sensor based localization by incorporating a role for the user as an "intelligent sensor" that confirms the presence of a landmark. Landmarks along the provided path are embedded in directions ("..until you reach a hallway intersection"). The successful execution of the direction (and hence the presence of the anticipated landmark) is confirmed by the user.</p><p>Navatar aims to augment navigation capabilities of users with visual impairments, but it is not a local obstacle avoidance system, as this requires being able to track movable objects, such as trashcans, chairs and tables. The location of such objects may change and it is difficult to keep track of them. Instead the focus is on immutable landmarks, such as, hallway intersections, staircases and doors.</p><p>Figure <ref type="figure" target="#fig_1">4</ref> lists a high-level overview of the four different components of Navatar, which are: (1) a virtual representation component that stores annotated models of indoor environments; (2) the localization component that provides a location estimate of the user; (3) the direction provision component that provides directions to a user specified location; and (4) the interface component that interacts with the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Representation</head><p>Instead of 2D maps, 3D models are used to more accurately represent indoor environments with multiple levels and features like low ceilings, ramps, uneven floors and rails, which are often impediments to navigation for users with visual impairments. In order to exploit crowd sourcing it may be easier for sighted users to annotate 3D models with tactile landmarks. The use of virtual worlds is further motivated by the observation that thousands of 3D models of the exteriors of public buildings have been successfully created through crowd-sourcing efforts. They can be found on virtual globe applications, such as Google Earth. Google recently announced the availability of indoor maps for several airports and malls in the US <ref type="bibr" target="#b0">[2]</ref>. Furthermore, 3D models of indoor spaces are becoming available [1] and can be created with little effort using tools such as Google Sketchup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localization</head><p>To localize the user, this work followed a Bayesian filtering approach called particle filters <ref type="bibr" target="#b8">[10]</ref>. Each particle contains (a) an estimation of where the user might be located and (b) a weight of how probable this estimate is. As the user moves in the environment each particle location is updated based on a distribution of error in steps detected (pedometer and compass). The weight of the particle is updated based on the user's confirmations of anticipated landmarks. Weights are also affected by map information. For instance, when a particle is inside a wall, its weight is zero. The particles that have low weight, such as particles inside a room, while the user has not confirmed a door, are replaced with new particles to achieve more accurate localization.</p><p>In some cases the user might fail to confirm the presence of a landmark, which can be problematic for localization. To solve this problem particles are monitored. If all the particles have progressed beyond the landmark, which the system expects to be recognized, the failure is detected. Navatar will be able to self-correct by providing new directions. Specific implementation details of the particle filters process can be found in related work <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b3">5]</ref> as their discussion is outside the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direction Provision &amp; Interaction</head><p>Given a map, Navatar computes the shortest path using the A* algorithm <ref type="bibr" target="#b31">[33]</ref> and identifies landmarks along the path upon which directions are generated. Directions are provided using synthetic speech and are of the following form:</p><p>• Moving to a landmark, i.e., "Follow the wall to your left until you reach a hallway intersection".</p><p>• A turn direction, i.e., "Turn Left".</p><p>• An action on a landmark, i.e., "Open the door and take the stairs down".</p><p>For moving to a landmark, Navatar uses directions that include wall following and door counting rather than metric information based on the results of preliminary studies (see following). Users confirm the successful execution of each direction (and the presence of landmarks) by tapping the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Studies</head><p>The number of landmarks that need to be confirmed and the number of directions provided can significantly affect accuracy and performance. A feasibility study with 10 blindfolded users <ref type="bibr" target="#b2">[4]</ref> focused on determining the effectiveness of different types of directions. Two versions of providing directions were tested: (1) metric-based directions (i.e., "walk 20 steps until there is a hallway on your right"); and (2) landmarkbased directions (i.e., "walk until you have passed three doors on your left"). For each type of direction the distance to be navigated varied, as well as the number of landmarks to be confirmed. The user had to confirm the successful execution of a direction before receiving the next one. The highest rate of success was achieved for landmark-based directions involving few reliable landmarks, such as hallway intersections. Especially when landmarks, such as doors, are close to each other, they become harder to distinguish.</p><p>A second study with eight blindfolded users focused on integrating localization and path planning <ref type="bibr" target="#b3">[5]</ref>. It investigated whether localization was accurate enough to allow directions to be computed on at run-time on the phone. Experiments show that the path can be adjusted when users fail to correctly follow directions by depending upon localization estimates. This work also explores the use of multiple particle filters, where each filter uses a different assumption for the user's average step length, which helps to adaptively estimate the value of this parameter on the fly. For localization, multiple particle filters are executed in parallel. Each one of them is employing a different set of assumptions regarding the capabilities of the user depending on pedometer and compass data from the smartphone and landmark confirmations by the user. For a small but sufficient number of particles per filter (50-100), as well as number of filters <ref type="bibr" target="#b5">(7)</ref>, each iteration of the algorithm required on average less than 200ms, sufficient for real-time tracking. Ground truth was collected by a visionbased system carried by the user, which is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDY</head><p>Though preliminary user studies were conducted with blindfolded participants, this study specifically focused on users who are visually impaired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instrumentation</head><p>Navatar has been implemented in Java for the Android mobile platform. A 3D model of the Engineering building of the University of Nevada, Reno was created using Google Sketchup using the Keyhole Markup Language (KML). Sketchup allows for creating named components and adding such a component to a location in the model. This allows for augmenting this model with addressing information and landmarks such as doors, hallway intersections, ramps and staircases. Though it is possible to plan paths in 3D models, this was anticipated to be too computationally intensive to be performed on the phone and hence a parser was created that extracts intermediate 2D maps that are more suitable for path planning, since the user is grounded anyway. Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show representations of extracted maps for the first and the second floor.</p><p>For the estimation of the user's location 10 particle filters were used in parallel, with each filter having between 50-100 particles. Preliminary studies found the phone's compass to occasionally interfere with metal studs in the wall leading to highly noisy data. To avoid this, consecutive compass readings were averaged over a time period and then the measured value was discretized into one of eight possible directions.</p><p>A commercial beacon based localization system called Hagisonic StarGazer was used to capture the ground truth. Passive landmarks with unique identifiers were installed on the ceiling with a resolution of one identifier per three feet. An infrared camera worn by the user provides ID, distance and angle to the closest landmark. An open source library is used to access the camera, which allows the creation of a map of landmarks. To use the system for human localization, the camera and a battery are installed on a belt that the user wears. The camera is worn on the back to avoid occlusion. The user  carries a tablet in a backpack that records the landmark information received from the camera and calculates the user's location from the map based on the relative landmark location (see Figure <ref type="figure" target="#fig_2">5</ref>). The accuracy of the camera is 2 cm when installed on a flat surface; but the camera is sensitive to tilt and might result in incorrect readings when installed on a belt. To reduce the noise the ground truth is smoothed using an outlier detection algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Six users were recruited <ref type="bibr">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>For the user study, 11 paths were tested; and these paths were created so as to have approximate equal lengths with the smallest amount of overlap between paths. Two paths involve transitions between floors. Specific characteristics of each path such as its length and number of landmarks present can be found in Table <ref type="table">1</ref>. Initially 10 paths were created but in a previous study <ref type="bibr" target="#b3">[5]</ref> one path (242A-231A) was found to lead to large errors in location estimation as it involves traveling a relatively large distance with only doors as landmarks (see Figure <ref type="figure">7</ref>). To evaluate the effectiveness of planning longer but more accurate paths that lead along more distinguishable landmarks an alternative version of this path was created (242A-231A) that includes 3 hallway intersections.</p><p>The application provides directions through text to speech using the smartphone's speaker and the user confirms executing each direction by tapping the screen. Because touch screens are notoriously inaccessible to users with visual impairments a successful tap was indicated using a vibrotactile cue. All users were right handed and navigated with the cane in their right hand. Users were asked to hold the phone in their left hand but hold it relatively still as to achieve higher accuracy in step detection. Participants were equipped with the StarGazer camera and backpack containing the tablet. None of the participants felt the belt and backpack impeded their mobility in any way.</p><p>User studies were conducted over a weekend with few students present in the building to avoid interference. Hallways were cleared of any obstacles and all doors to offices were closed. Prior to the user study the user followed one path to have them become familiar with the phone and the direction provision and to initialize the pedometer (though the pedometer continues to calibrate itself). For each path participants were led to the start location upon which the observer would select the right path and activate the direction provision. Navatar can recover from users missing landmarks or confirming them too early or too late. If the user could not complete a provided direction, the observer would intervene and cancel the current path, mark it as a failure and guide the user to the next start location. After the user study, participants were interviewed using a questionnaire to collect qualitative experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table">1</ref> shows the success rate of each path, the average time and standard deviation it took to execute each one and the average number of steps and the standard deviation for each path. 85% (SD=20%) of the paths were completed successfully. Though the use of multiple particles in a previous study with blindfolded users <ref type="bibr" target="#b3">[5]</ref> was able to mitigate individual differences between step lengths, for some users with visual impairments this turned out to be difficult to achieve. Table <ref type="table" target="#tab_1">2</ref> lists the distance between the target destination and the user's position upon completion of the path. There was no observation of users unable to confirm a landmark, but system failures occur when the system thinks the user has arrived at their destination when they actually have not arrived yet. Paths were considered a success when the user was able to navigate within 2.0 meter of the target location. A length of 2.0 meter was chosen, as this is approximately the range that can be sensed using a cane held in the hand and to correct for the actual user location as the user was wearing the camera on their back. Failures are indicated using an F in table <ref type="table" target="#tab_1">2</ref>. For all paths and users an average error of 1.85 meter (SD=2.74) was found.</p><p>Paths that involved counting many doors, (e.g., 242A-231A and 260-131) had the lowest success rates and users were often observed to scuttle from door to door, which made it harder for the system to pick up steps. Similar to an early study with blindfolded users <ref type="bibr" target="#b3">[5]</ref> path (242A-231A) has a low success rate (50%). The alternative path for (242A-231A) on average took longer to complete (163 seconds) than the original path (101 seconds) but has a higher success rate (83%) and a lower average error (6.18 versus 1.46 meter). This may indicate that planning longer paths that include more distinguishable landmarks may increase accuracy. The completion rate for all paths improves from 85% to 88%, and the average error reduces to 1.42 (SD=1.50) if results for the original path are excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>Non-directed interviews with open-ended question were used to collect qualitative experiences. None of the participants had used a navigation system before but all users had previously experienced getting lost in outdoor as well as indoor environments. Depending on the space all users engage in cognitive mapping when this space is unfamiliar to them. Primarily tactile landmarks, such as doors or windows are used in cognitive mapping, but one user noted using sounds as well, such as the sound of a water cooler or an air conditioning unit. Cognitive mapping was typically limited to smaller spaces such as rooms. None of the participants had freely explored large indoor environments out of fear of getting lost. Participants typically used a sighted guide when navigating in unfamiliar environments, which would help with working out a route. Three users preferred routes with many landmarks but would avoid ones with too many turns as that was found to be confusing.</p><p>A 5-point Likert scale was used to measure the usability of the system. All users found that the directions were easy to follow (M=4.66, SD=0.58) and that Navatar navigated them efficiently (M=4.66, SD=0.58). They agreed this system could minimize the chance that they would get lost in unfamiliar environments (M=4.66, SD=0.58). Overall they liked using the system (M=4.66, SD=0.58). Despite Navatar failing to guide user 5 to the target destination for four paths, this user remained positive about the system. Upon further inquiry this stated: "I have never used such a system before and I think it could be really helpful, despite its current shortcomings". Suggestions for improvement included:</p><p>(1) improving the accuracy; (2) being able to repeat directions;</p><p>(3) offer Braille output; (4) not holding the phone in the hand but in a pocket and (5) for directions involving stairs indicating whether you need to go up or down the stairs. This last suggestion was caused by the system providing directions dynamically so depending on the estimate of the user location the system would direct "go down the stairs" but if the system's estimate were beyond or on the stairs this direction would be omitted. One user preferred to not hold the phone in the hand as this user prefers to use this hand to protect their face while navigating. Repeating directions was already implemented as directions are calculated dynamically based on the system's current estimate of the location of the user. This functionality was not made available to the participants to keep the study simple and avoid any false confirmations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND FUTURE WORK</head><p>Overall the system was able to successfully guide users to their destination in 85% of the paths. The experiences and results collected were encouraging. The following issues need to be addressed in order to improve the accuracy of Navatar. The sensors used are inexpensive but also erroneous and to improve step detection, rather than holding the phone in the hand better results may be achieved by placing at a different part of the body such as the legs.</p><p>Detecting steps was more difficult as some users would scuttle when counting doors and alternative direction provision strategies may need to be employed. When following walls users typically have a firmer stride, so wall following in direction provision may be preferred over door counting. Scuttling may not be avoided completely as users with visual impairments often have inefficient gait due to predominantly sedentary behaviors during the developmental years <ref type="bibr" target="#b14">[16]</ref>. Alternatively, a pressure sensor could be embedded in the user's shoes to detect steps, which may be more precise than detecting steps using an accelerometer, but this may add to the cost of the system.</p><p>Future user studies will include a larger number of users who are visually impaired and will also involve navigating in more complex environments, such as open spaces, and involving a larger variety of landmarks, or sounds and smells. The intention is also to explore planning safer paths that lead along a larger number of landmarks or include landmarks that are more distinguishable. In the current system the next direction is provided manually based on the user's confirmation of successfully executing the direction. The studies demonstrate that sufficiently accurate localization is achievable, which may allow for automatic direction provision similar to how car navigation systems provide directions. This could make navigation more efficient and reduce cognitive load, as the user does not have to engage in tasks such as identifying doors. Confirmations of landmarks will still be incorporated periodically as to mitigate the error of dead reckoning localization.</p><p>A significant benefit of Navatar is that it can be installed at a much lower cost than alternative indoor localization systems. The StarGazer system used for measuring ground truth cost over $2,000 and three days to install for two floors of the building where the user study was conducted. It also requires the user to carry a heavy, awkward sensor. For comparison creating the 3D model and annotating it with landmarks only took 3 hours using Google Sketchup, with the use of an existing 2D floor plan, which is available for most buildings. Future research will study how to automatically extract landmarks, such as doors or staircases, from the geometry of a 3D model rather than annotating them manually. This would allow for using models that are created using robotic mapping <ref type="bibr" target="#b16">[18]</ref>. As some annotations such as room numbers need to be annotated by sighted humans, to facilitate crowd-sourcing efforts, the intention is to explore how models can be most easily annotated and how annotations can be verified.</p><p>To improve obstacle avoidance and successful landmark recognition, it is desirable to leave users' hands free. Users could wear a wireless headset with a microphone worn on one ear leaving the other ear free to listen to any sounds in the immediate environment. Speech recognition could be used that allows users to verbally provide a destination, e.g., "lead me to room 334". Further down the road, Navatar could facilitate answering spatial queries such as "where is the nearest fire escape?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>This paper presents an indoor navigation system called Navatar that allows for navigating a user with visual impairments by exploiting the physical characteristics of indoor environments, taking advantage of the unique sensing abilities of users with visual impairments, and minimalistic sensing achievable with a smartphone. A user study with six visually impaired users evaluated the accuracy of Navatar and found that users could successfully complete 85% of the paths. To improve its accuracy more sophisticated techniques need to be developed that allow for detecting the unique steps of users with visual impairments using low cost sensing. Once its accuracy is further improved, Navatar is anticipated to have a high probability of large-scale deployment due to its low installation cost and its potential to leverage crowd-sourcing efforts for the annotation of virtual indoor models</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Veering is likelier to occur in outdoor environments (left) than in indoor environments (right) as navigation is often constrained by physical infrastructure such as walls.</figDesc><graphic coords="2,321.02,62.36,243.07,96.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Navatar system overview showing the communication between the different components.</figDesc><graphic coords="3,53.86,62.36,243.08,132.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. User carrying the StarGazer camera and tablet for measuring the ground truth.</figDesc><graphic coords="4,366.02,62.36,153.07,95.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. The map of the first floor</figDesc><graphic coords="4,321.02,191.45,216.00,125.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Distance between the desired destination and the user's position upon completion (m). F indicates a failed path.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>12/28/2011</idno>
		<ptr target="http://googleblog.blogspot.com/2011/11/new-frontier-for-google-maps-mapping.html" />
		<title level="m">A new frontier for google maps: mapping the indoors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Virtual leading blocks for the deaf-blind: A real-time way-finder by verbal-nonverbal hybrid interface and high-density rfid tag space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amemiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Virtual Reality</title>
		<meeting>of the IEEE Virtual Reality<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feasibility of interactive localization and navigation of people with visual impairments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Folmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE Intelligent Autonomous Systems (IAS-10</title>
		<meeting><address><addrLine>Ottawa, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrated localization and navigation of people with visual impairments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Folmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA&apos;12)</title>
		<meeting><address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Landmarks for navigators who are visually impaired</title>
		<author>
			<persName><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Lindgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Cartography Conference</title>
		<meeting>International Cartography Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assisting mobility of the disabled using space-identifying ubiquitous infrastructure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bessho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koshizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)</title>
		<meeting>of the 10th Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)<address><addrLine>Halifax, Nova Scotia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="283" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vibrotactile masking experiments reveal accelerated somatosensory processing in congenitally blind braille readers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lisak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldreich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="14288" to="14298" />
			<date type="published" when="2010-10">Oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perception, cognition, and mobility of blind pedestrians</title>
		<author>
			<persName><forename type="first">E</forename><surname>Foulke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial abilities: Development and physiological foundations</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="55" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Markov localization for mobile robots in dynamic environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="391" to="427" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>JAIR)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wayfinding with words: spatial learning and navigation using dynamically updated verbal descriptions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Giudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Bakdash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Legge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Res</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="347" to="358" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geography and the disabled: A survey with special reference to vision impaired and blind populations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Golledge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transations of the Intstitute of British Geographers</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="63" to="85" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A 3d pose estimator for the visually impaired</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Mariottini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intern. Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intern. Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2009">Oct. 11-15 2009</date>
			<biblScope unit="page" from="2716" to="2723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An indoor localization aid for the visually impaired</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)<address><addrLine>Roma, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">April 10-14 2007</date>
			<biblScope unit="page" from="3545" to="3551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hallaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tinna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<title level="m">Steps toward accommodating variable position tracking accuracy in a mobile augmented reality system</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compensatory analysis and strategies for balance in individuals with visual impairments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horvat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miszko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Impairment and Blindness</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="695" to="703" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design and development of an indoor navigation and object identification system for the blind</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diepstraten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)</title>
		<meeting>of the 6th Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laser Range Imaging using Mobile Robots: From Pose Estimation to 3d-Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weingarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Range Imaging Research Day</title>
		<meeting>1st Range Imaging Research Day</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="129" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning building layouts with non-geometric visual information: the effects of visual impairment and age</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Legge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Giudice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1677" to="1699" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal estimation of position and heading for mobile robots using ultrasonic beacons and dead-reckoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kleeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="2582" to="2587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3-d human navigation system considering various transition preferences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man and Cybernetics, 2005 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="859" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rfid in robot-assisted indoor navigation for the visually impaired</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kulyukin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gharpure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pavithran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<meeting>of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems<address><addrLine>Sendai, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-02">Sept. 28-Oct.2 2004</date>
			<biblScope unit="page" from="1979" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the feasibility of using wireless ethernet for indoor localization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Kavraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="555" to="559" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robotics-based location sensing using wireless ethernet</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ladd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marceau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Kavraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eight ACM International Conference on Mobile Computing and Networking (MOBICOM 2002)</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002-09">September 2002</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multisensory virtual environment for supporting blind persons&apos; acquisition of spatial cognitive mapping -a case study</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mioduser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Montgomerie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Viteli</surname></persName>
		</editor>
		<meeting>World Conference on Educational Multimedia, Hypermedia and Telecommunications<address><addrLine>Norfolk, VA</addrLine></address></meeting>
		<imprint>
			<publisher>AACE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="1046" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Golledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Navigation system for the blind: Auditory display modes and guidance</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Navigating without vision: basic and applied research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Golledge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optometry and Vision Science</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="282" to="289" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Image of the city</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lynch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Ma</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A walking navigation system for the blind</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tadokoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems and computers in Japan</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1610" to="1618" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An integrated indoor/outdoor blind navigation system and service</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Helal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><surname>Drishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pervasive Computing and Communications</title>
		<imprint>
			<date type="published" when="2004-03">March 2004</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pedestrian navigation systems and location-based services</title>
		<author>
			<persName><forename type="first">G</forename><surname>Retscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEE International Conference on</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="359" to="363" />
		</imprint>
	</monogr>
	<note>In 3G Mobile Communication Technologies, 2004. 3G 2004</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An indoor navigation system to support the visually impaired</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Riehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Giudice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Annual International Conference of the IEEE Conference on Engineering in Medicine and Biology Society (EMBS)</title>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="4435" to="4438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized best-first search strategies and the optimality of a*</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="505" to="536" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Development of a wearable computer orientation system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Blasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personal Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49" to="63" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wayfinding and navigation in haptic virtual environments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Semwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2001-08">Aug 2001</date>
			<biblScope unit="page" from="559" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Auditory guidance with the navbelt -a computerized travel aid for the blind</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shoval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="459" to="467" />
			<date type="published" when="1998-08">August 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An indoor navigation system for blind individuals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sonnenblick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual Conference on Technology and Persons with Disabilities (CSUN)</title>
		<meeting>the 13th annual Conference on Technology and Persons with Disabilities (CSUN)<address><addrLine>Northridge, Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Acquisition of structural versus object landmark knowledge</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Hum Percept Perform</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="378" to="390" />
			<date type="published" when="2007-04">Apr 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Digital sign system for indoor wayfinding for the visually impaired</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Giudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Legge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition -Workshop on Computer Vision Applications for the Visually Impaired</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition -Workshop on Computer Vision Applications for the Visually Impaired<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic environment exploration using a virtual white cane</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">June 20-25 2005</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
