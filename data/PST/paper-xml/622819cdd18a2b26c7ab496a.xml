<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLM: General Language Model Pretraining with Autoregressive Blank Infilling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
							<email>yujieq@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GLM: General Language Model Pretraining with Autoregressive Blank Infilling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks differ in nature, with three main categories being natural language understanding (NLU), unconditional generation, and conditional generation, while none of the pretraining frameworks performs the best for all tasks. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. The proposed architecture has two major benefits: (1) It improves pretrain-finetune consistency via cloze-style finetuning and naturally handles variable-length blank infilling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pretraining data and steps. (2) It is flexible enough to handle various NLP tasks with a single pretrained model. GLM with 1.25× parameters of BERT Large achieves the best performance in NLU, conditional and unconditional generation at the same time, demonstrating its generalizability to different downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale language models pretrained on web texts have substantially advanced the state of the art in various NLP tasks, ranging from natural language understanding (NLU) to text generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>. Downstream task performance as well as the scale of the parameters have also constantly increased in the past few years.</p><p>In general, existing pretraining frameworks can be categorized into three families: autoregressive, autoencoding, and encoder-decoder models. Autoregressive models, such as GPT <ref type="bibr" target="#b25">[26]</ref>, learn leftto-right language models. While they have succeeded in long-text generation and shown strong few-shot learning ability when scaled to billions of parameters <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref>, the inherent disadvantage is that the unidirectional attention mechanism cannot fully capture the dependencies between the context words. Autoencoding models, such as BERT <ref type="bibr" target="#b7">[8]</ref>, learn bidirectional Transformers as context encoders via denoising objectives like Masked Language Model (MLM). These encoders generate contextualized representations that excel at natural language understanding tasks, but could not be directly applied for text generation. Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and cross attention to connect them <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>. They are typically deployed in tasks of conditional text generation, also called seq2seq, such as text summarization and response generation. T5 <ref type="bibr" target="#b27">[28]</ref> unifies NLU and conditional generation via encoder-decoder models but requires more parameters to outperform BRET-based models such as RoBERTa <ref type="bibr" target="#b19">[20]</ref>. Table <ref type="table">1</ref> compares different pretraining frameworks.</p><p>None of these pretraining frameworks is flexible enough to handle all NLP tasks. Previous works have tried to unify different frameworks by combining their objectives via multi-task learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Table <ref type="table">1</ref>: Summary of the pretraining frameworks and downstream tasks <ref type="foot" target="#foot_1">1</ref> . " " means "is good at", "-" means "can be adapted to", and "×" means "cannot be directly applied to". However, since the autoencoding and autoregressive objectives differ by nature, a simple unification cannot fully inherit the advantages of both frameworks.</p><note type="other">Downstream</note><p>In this paper, we propose a general pretraining framework named GLM (General Language Model), based on autoregressive blank infilling. We randomly blank out continuous spans of tokens from the input text, following the idea of autoencoding, and train the model to sequentially reconstruct the spans, following the idea of autoregressive pretraining. To learn both bidirectional and unidirectional attention mechanisms in a unified framework, we divide the input text into two parts, where the unmasked tokens can attend to each other, but the masked tokens cannot attend to subsequent masked tokens. We also propose a 2D positional encoding technique to indicate the inter-and intra-span position information. Figure <ref type="figure" target="#fig_1">1</ref> illustrates our pretraining framework. As a result, GLM learns both contextual representation and autoregressive generation during pretraining.</p><p>When finetuning on downstream tasks, we reformulate them as blank infilling generation, inspired by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Each task is associated with a human-crafted cloze question, and the model predicts the answer to the cloze. For example, a sentiment classification task is reformulated as a cloze question "[SENTENCE]. It's really ". The choice of words "good" or "bad" in the blank indicates the sentiment being positive or negative. With such formulation, GLM benefits from the consistency between pretraining and finetuning, because they both train the model to generate masked spans given their contexts. As a result, GLM is more suitable for downstream NLU tasks compared to autoencoding models such as BERT. To make our pretraining method better suited for text generation, we also study a multi-task pretraining setup, where the model is jointly trained to reconstruct masked spans and generate longer text.</p><p>Empirically, we show that with the same pretraining data and a close amount of computational cost, GLM significantly outperforms BERT on the SuperGLUE benchmark by a large margin of 4.6% -5.0%. GLM also outperforms RoBERTa, T5, and BART when pretrained on a corpus of similar size (158GB). Moreover, compared with standalone baselines, GLM with multi-task pretraining achieves improvements in natural language understanding, conditional text generation, and language modeling tasks all together by sharing the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GLM Pretraining Framework</head><p>We propose a general pretraining framework GLM based on a novel autoregressive blank infilling objective. GLM formulates NLU tasks as cloze questions that contain task descriptions, which can be answered by autoregressive generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pretraining Objective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Autoregressive Blank Infilling</head><p>GLM is trained by optimizing an autoregressive blank infilling objective. Given an input text</p><formula xml:id="formula_0">x = [x 1 , • • • , x n ], multiple text spans {s 1 , • • • , s m } are sampled, where each span s i corresponds to a series of consecutive tokens [s i,1 , • • • , s i,li ] in x. Each span is replaced with a single [MASK]</formula><p>token, forming a corrupted text x corrupt . The model predicts the missing tokens in the spans from the corrupted text in an autoregressive manner, which means when predicting the missing tokens in a span, the model has access to the corrupted text and the previously predicted spans. To fully capture the interdependencies between different spans, we randomly permute the order of the spans, similar </p><formula xml:id="formula_1">× × × × × × × × × × × × × × × × × × × × × × × × × × × × × × × × × × ×<label>(</label></formula><formula xml:id="formula_2">x 1 x 2 x 3 x 4 x 5 x 6 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 x 5 x 6 [E] x 3 [E] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3</formula><p>x 1</p><p>x 2</p><p>[M]</p><p>x 4</p><p>[M]</p><p>[S]</p><p>x 5</p><p>x 6</p><p>[S]</p><formula xml:id="formula_3">x 3 Target x 5 x 6 [E] x 3 [E] Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2 x1 x2 x3 x4 x5 x6 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S]</formula><p>x3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_4">x1 x2 x3 x4 x5 x6 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x5 x6 [E] x3 [E] x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S]</formula><p>x3</p><p>Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_5">x1 x2 x3 x4 x5 x6 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x5 x6 [E] x3 [E] x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S]</formula><p>x3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_6">x 1 x 2 x 3 x 4 x 5 x 6 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 x 5 x 6 [E] x 3 [E] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3</formula><p>x 1</p><p>x 2</p><p>[M]</p><p>x 4</p><p>[M]</p><p>[S]</p><p>x 5</p><p>x 6</p><p>[S]</p><p>x 3</p><p>Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_7">x 1 x 2 x 3 x 4 x 5 x 6 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 x 5 x 6 [E] x 3 [E] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3</formula><p>x 1</p><p>x 2</p><p>[M]</p><p>x 4</p><p>[M]</p><p>[S]</p><p>x 5</p><p>x 6</p><p>[S]</p><p>x 3</p><p>Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_8">x 1 x 2 x 3 x 4 x 5 x 6 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 x 5 x 6 [E] x 3 [E] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3</formula><p>x 1</p><p>x 2</p><p>[M]</p><p>x 4</p><p>[M]</p><p>[S]</p><p>x 5</p><p>x 6</p><p>[S]</p><p>x 3</p><p>Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2</p><formula xml:id="formula_9">x1 x2 x3 x4 x5 x6 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x5 x6 [E] x3 [E] x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S]<label>x3</label></formula><p>Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 1 2 3 1 2 to the permutation language model <ref type="bibr" target="#b39">[40]</ref>. Formally, let Z m be the set of all possible permutations of the length-m index sequence [1, 2, • • • , m], and</p><formula xml:id="formula_10">x1 x2 x3 x4 x5 x6 x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x5 x6 [E] x3 [E] x1 x2 [M] x4 [M] [S] x5 x6 [S] x3 x1 x2 [M] x4 [M] [S] x5 x6 [S]<label>x3</label></formula><formula xml:id="formula_11">s z&lt;i be [s z1 , • • • , s zi−1 ],</formula><p>we define the pretraining objective as</p><formula xml:id="formula_12">max θ E z∼Zm m i=1 log p θ (s zi |x corrupt , s z&lt;i )<label>(1)</label></formula><p>It differs from SpanBERT <ref type="bibr" target="#b14">[15]</ref> in the sense that the number of missing tokens in a span is unknown to the model. GLM predicts the missing tokens in an autoregressive manner. We always generate the tokens in each blank following a left-to-right order, i.e. the probability of generating the span s i is factorized as:</p><formula xml:id="formula_13">p θ (s i |x corrupt , s z&lt;i ) = li j=1 p(s i,j |x corrupt , s z&lt;i , s i,&lt;j )<label>(2)</label></formula><p>Specifically, we implement the autoregressive blank infilling objective with the following techniques. The input x is divided into two parts: Part A consists of the corrupted text x corrupt , and Part B consists of the masked spans. Tokens in Part A can attend to all the tokens in A, but cannot attend to any tokens in B. Tokens in Part B can attend to tokens in A and its antecedents in B, but cannot attend to any subsequent tokens in B. To enable autoregressive generation, each span is padded with two special tokens [START] and [END] at the beginning and the end, for model input and output respectively. In this way, our model automatically learns a bidirectional encoder (for Part A) and a unidirectional decoder (for Part B) in a unified model. The implementation of GLM is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Previous pretraining works have shown optimal performance when masking 15% of the original tokens <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>. In this work, we randomly sample spans of length drawn from a Poisson distribution with λ = 3. We repeatedly sample new spans until at least 15% of the original tokens are masked. Empirically, we have found that the ratio is critical for good performance on downstream NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Multi-Task Pretraining</head><p>In the previous section, GLM masks short spans and is suited for NLU tasks. However, we are interested in pretraining a single model that can handle both NLU and text generation. We then study a multi-task pretraining setup, in which a second objective of generating longer text is jointly optimized with the blank infilling objective. We consider the following two objectives:</p><p>• Document-level. We sample a single span that covers 50%-100% of the original tokens, whose length is sampled from a uniform distribution. The span is masked with a [MASK] token and the model autoregressively generates it.</p><p>Coronet has the best lines of all day cruisers.</p><p>Positive &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c b 5 S 9</p><formula xml:id="formula_14">3 r + R G E y 3 g K C U a U f 2 i 3 S j J Q = " &gt; A A A B 6 H i c b V D J S g N B E K 2 J W 4 x b 1 K M i j U H w F G Y 8 q M e g F 4 8 J m A W S I f R 0 a p I 2 P Q v d P c I w 5 O j J i w d F v P o V + Q 5 v f o M / Y W c 5 a P R B w e O 9 K q r q e b H g S t v 2 p 5 V b W l 5 Z X c u v F z Y 2 t 7 Z 3 i r t 7 D R U l k m G d R S K S L Y 8 q F D z E u u Z a Y C u W S A N P Y N M b X k / 8 5 j 1 K x a P w V q c x u g H t h 9 z n j G o j 1 d J u s W S X 7 S n I X + L M S a l y O K 5 9 P R y N q 9 3 i R 6 c X s S T A U D N B l W o 7 d q z d j E r N m c B R o Z M o j C k b 0 j 6 2 D Q 1 p g M r N p o e O y I l R e s S P p K l Q k 6 n 6 c y K j g V J p 4 J n O g O q B W v Q m 4 n 9 e O 9 H + p Z v x M E 4 0 h m y 2 y E 8 E 0 R G Z f E 1 6 X C L T I j W E M s n N r Y Q N q K R M m 2 w K J g R n 8 e W / p H F W d s 7 L d s 2 k c Q U z 5 O E A j u E U H L i A C t x A F e r A A O E R n u H F u r O e r F f r b d a a s + Y z + / A L 1 v s 3 1 m K Q q Q = = &lt; / l a t e x i t &gt; y good &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C Z 8 a b L 6 h J o r h 4 j x H B P A G p 8 N p p A A = " &gt; A A A B 6 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C S 1 C R S i 7 H t R j 0 Y v H C v Y D 2 q V k 0 7 Q N T b J L k i 0 s S / + C F w V F v P q H v P X f m G 1 7 0 N Y H A 4 / 3 Z p i Z F 0 S c a e O 6 M y e 3 s b m 1 v Z P f L e z t H x w e F Y 9 P m j q M F a E N E v J Q t Q O s K W e S N g w z n L Y j R b E I O G 0 F 4 / v M b 0 2 o 0 i y U T y a J q C / w U L I B I 9 h k 0 q S S X P S K Z b f q z o H W i b c k 5 V q p e / k 6 q y X 1 X v G 7 2 w 9 J L K g 0 h G O t O 5 4 b G T / F y j D C 6 b T Q j T W N M B n j I e 1 Y K r G g 2 k / n t 0 7 R u V X 6 a B A q W 9 K g u f p 7 I s V C 6 0 Q E t l N g M 9 K r X i b + 5 3 V i M 7 j 1 U y a j 2 F B J F o s G M U c m R N n j q M 8 U J Y Y n l m C i m L 0 V k R F W m B g b T 8 G G 4 K 2 + v E 6 a V 1 X v u u o + 2 j T u Y I E 8 n E E J K u D B D d T g A e r Q A A I j e I Y 3 e H e E 8 + J 8 O J + L 1 p y z n D m F P 3 C + f g C i o 5 D y &lt; / l a t e x i t &gt; v(y) GLM &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c I l X H K T M H L 8 y 9 4 G I + K Z X n l T 1 K 7 g = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e h g Q h I o R d D + o x 6 M V j B P O A Z A m z k 9 l k z O z M M j M r L j H / 4 E E P i n j 1 f 7 z l b 5 w 8 D p p Y 0 F B U d d P d F c S c a e O 6 Y y e z s r q 2 v p H d z G 1 t 7 + z u 5 f c P 6 l o m i t A a k V y q Z o A 1 5 U z Q m m G G 0 2 a s K I 4 C T h v B 4 H r i N x 6 o 0 k y K O 5 P G 1 I 9 w T 7 C Q E W y s V I 9 L 6 d P j S S d f d M v u F G i Z e H N S r B T a p y / j S l r t 5 L / b X U m S i A p D O N a 6 5 b m x 8 Y d Y G U Y 4 H e X a i a Y x J g P c o y 1 L B Y 6 o 9 o f T a 0 f o 2 C p d F E p l S x g 0 V X 9 P D H G k d R o F t j P C p q 8 X v Y n 4 n 9 d K T H j p D 5 m I E 0 M F m S 0 K E 4 6 M R J P X U Z c p S g x P L c F E M X s r I n 2 s M D E 2 o J w N w V t 8 e Z n U z 8 r e e d m 9 t W l c w Q x Z O I I C l M C D C 6 j A D V S h B g T u 4 R n e 4 N 2 R z q v z 4 X z O W j P O f O Y Q / s D 5 + g F Y z 5 H 0 &lt; / l a t e x i t &gt; p(y|x) It is really [MASK]</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X k n P s X X F T 3 s</p><formula xml:id="formula_15">+ d K V L z g 7 3 6 M 6 s f h c = " &gt; A A A B 9 X i c b V C 7 T s M w F L 3 h W c K r w M g S U S E x V Q k D s C A q W B i L R B 9 S G y r H c V q r j h 3 Z D l B F / Q 8 W B h 5 i 5 T P Y W R B / g 9 N 2 g J Y j W T 4 6 5 1 7 5 + A Q J o 0 q 7 7 r c 1 N 7 + w u L R c W L F X 1 9 Y 3 N o t b 2 3 U l U o l J D Q s m Z D N A i j D K S U 1 T z U g z k Q T F A S O N o H + R + 4 1 b I h U V / F o P E u L H q M t p R D H S R r p p B 4 K F a h C b K 7 s f d o o l t + y O 4 M w S b 0 J K Z x / 2 a f L y Z V c 7 x c 9 2 K H A a E 6 4 x Q 0 q 1 P D f R f o a k p p i R o d 1 O F U k Q 7 q M u a R n K U U y U n 4 1 S D 5 1 9 o 4 R O J K Q 5 X D s j 9 f d G h m K V R z O T M d I 9 N e 3 l 4 n 9 e K 9 X R i Z 9 R n q S a c D x + K E q Z o 4 W T V + C E V B K s 2 c A Q h C U 1 W R 3 c Q x J h b Y q y T Q n e 9 J d n S f 2 w 7 B 2 V 3 S u 3 V D m H M Q q w C 3 t w A B 4 c Q w U u o Q o 1 w C D h A Z 7 g 2 b q z H q 1 X 6 2 0 8 O m d N d n b g D 6 z 3 H 7 R 5 l k s = &lt; / l a t e x i t &gt; x GLM</formula><p>Gringo is a happy kitty.</p><p>He loves to play all day and all night.</p><p>[MASK]  GLM for text generation given the context. This can be the language modeling, or sequence-to-sequence tasks with finetuning.</p><p>• Sentence-level. We restrict that the masked spans must be full sentences. Multiple spans (sentences) are sampled to cover 15% of the original tokens.</p><p>Both new objectives are defined in the same way as the original objective, i.e. Eq. 1. The only difference is the number of spans and the span lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>GLM uses a Transformer encoder with several modifications to the architecture: (1) we rearrange the order of layer normalization and the residual connection, which has been shown critical for large-scale language models to avoid numerical errors <ref type="bibr" target="#b34">[35]</ref>;</p><p>(2) we use a single linear layer for the output token prediction; (3) we replace ReLU activation functions with GeLUs <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">2D Positional Encoding</head><p>One of the challenges of the autoregressive blank infilling task is how to encode the positional information. Transformers rely on positional encodings to inject the absolute and relative positions of the tokens. We propose 2D positional encodings to address the challenge. Specifically, each token is encoded with two positional ids. The first positional id represents the position in the corrupted text x corrupt . For the masked spans, it is the position of the corresponding [MASK] token. The second positional id represents the intra-span position. For tokens in Part A, their second positional ids are 0.</p><p>For tokens in Part B, they range from 1 to the length of the span. The two positional ids are projected into two vectors via learnable embedding tables, which are both added to the input token embeddings.</p><p>Our encoding method ensures that the model is not aware of the length of the masked span when reconstructing them. It is an important difference as compared to other models, e.g. XLNet <ref type="bibr" target="#b39">[40]</ref> encodes the original position so that it can perceive the number of missing tokens, SpanBERT <ref type="bibr" target="#b14">[15]</ref> replaces the span with multiple [MASK] tokens and keeps the length unchanged. Our design fits downstream tasks as usually the length of the generated text is unknown beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finetuning GLM</head><p>Typically, for downstream NLU tasks, a linear classifier takes the representations produced by pretrained models as input and predicts the correct answer. The practices are different from the pretraining task of blank filling, leading to inconsistency between pretraining and finetuning.</p><p>Instead, we reformulate NLU classification tasks as generation tasks of blank infilling, following PET <ref type="bibr" target="#b32">[33]</ref>. Specifically, given a labeled example (x, y), we convert the input text x to a cloze question c(x) via a pattern containing a single mask token [MASK]. The pattern is written in natural language to represent the semantics of the task. For example, a sentiment classification task can be formulated as "[SENTENCE]. It's really [MASK]". The label y is also mapped to an answer to the cloze, called verbalizer v(y). In sentiment classification, the labels "positive" or "negative" are mapped to words "good" or "bad" in the blank. The probability of the sentence being positive or negative is proportional to predicting "good" or "bad" in the blank. Therefore, the conditional probability of y given x is</p><formula xml:id="formula_16">p(y|x) = p(v(y)|c(x)) y ∈Y p(v(y )|c(x))<label>(3)</label></formula><p>where Y is the label set. Then we finetune GLM with the cross entropy loss.</p><p>For text generation tasks, we directly apply GLM as an autoregressive model. The given context constitutes the Part A of the input, with a [MASK] token appended at the end. The model generates the text of Part B autoregressively. We can directly apply the pretrained GLM for unconditional generation, or finetune it on downstream conditional generation tasks. The finetuning method is illustrated in Figure <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discussion and Analysis</head><p>In this section we discuss the differences between GLM and other pretraining models. We mainly concern about how they can be adapted to downstream blank infilling tasks.</p><p>Comparison with BERT <ref type="bibr" target="#b7">[8]</ref>. BERT is pretrained with an autoencoding objective, i.e. masked language model. Since BERT predicts the masked tokens independently, it fails to capture the interdependencies of masked tokens <ref type="bibr" target="#b39">[40]</ref>. Another disadvantage of BERT is that it cannot fill in blanks of multiple tokens properly. To infer the probability of an answer of length l, BERT needs to perform l consecutive predictions. If the length l is unknown, we may need to enumerate all possible lengths, since BERT needs to change the number of [MASK] tokens according to the length.</p><p>Comparison with XLNet <ref type="bibr" target="#b39">[40]</ref>. Both GLM and XLNet are pretrained with autoregressive objectives. There are two differences between GLM and XLNet. First, XLNet uses the original position encoding before corruption. During inference, we need to either know or enumerate the length of the answer, the same problem as BERT. Second, XLNet uses a two-stream self-attention mechanism, instead of right-shift, to avoid the information leak within Transformer. It doubles the time cost of pretraining.</p><p>Comparison with T5 <ref type="bibr" target="#b27">[28]</ref>. T5 proposes a similar blank infilling objective to pretrain an encoderdecoder Transformer. Instead, GLM uses a single Transformer encoder model to learn both bidirectional and unidirectional attention. By sharing parameters for the two types of attention, GLM is more parameter-efficient than the encoder-decoder architecture. Besides, T5 uses independent positional encodings for the encoder and decoder, and relies on multiple sentinel tokens to differentiate the masked spans. In downstream tasks, only one of the sentinel tokens is used, leading to a waste of model capacity and inconsistency between pretraining and finetuning. We empirically analyze the difference with T5 in Section 3.4.</p><p>Comparison with UniLM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref> UniLM combines different pretraining objectives under the autoencoding framework by changing the attention mask among bidirectional, unidirectional, and cross attention. Compared with autoregressive models, the model cannot fully capture the current token's dependencies with previous tokens due to the independence assumption of autoencoding models. Finetuning UniLM on downstream generation tasks also relies on masked language modeling, which is less efficient than autoregressive models. UniLMv2 <ref type="bibr" target="#b1">[2]</ref> adopts partially autoregressive modeling for generation tasks, along with the autoencoding objective for NLU tasks. Instead, GLM unifies NLU and generation tasks with autoregressive pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct experiments in two settings. In Section 3.2, we pretrain GLM with only the short span objective and evaluate it on the NLU tasks. In Section 3.3, we explore multi-task pretraining as discussed in Section 2.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretraining Setup</head><p>For a fair comparison with BERT <ref type="bibr" target="#b7">[8]</ref>, we use BooksCorpus <ref type="bibr" target="#b45">[46]</ref> and English Wikipedia as our pretraining data. We use the uncased wordpiece tokenizer of BERT with 30k vocabulary. We train GLM Base and GLM Large with the same architectures as BERT Base and BERT Large , containing 110M and 340M parameters respectively. The models are trained on 64 V100 GPUs for 200K steps with batch size of 1024 and maximum sequence length of 512, which takes about 2.5 days for GLM Large . For multi-task pretraining, we train two Large-sized models with a mixture of the blank infilling objective and the document-level or sentence-level objective, denoted as GLM Doc and GLM Sent . Additionally, we train two larger GLM models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M (30 layers, hidden size 1152, and 18 attention heads) parameters with document-level multi-task pretraining, denoted as GLM 410M and GLM 515M .</p><p>To compare with SOTA models, we also train a Large-sized model with the same data, tokenization, and hyperparameters as RoBERTa <ref type="bibr" target="#b19">[20]</ref>, denoted as GLM RoBERTa . <ref type="foot" target="#foot_2">2</ref> Due to resource limitations, we only pretrain the model for 250,000 steps, which are half of RoBERTa and BART's training steps and close to T5 in the number of trained tokens. More experiment details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SuperGLUE</head><p>To evaluate our pretrained GLM models, we conduct experiments on the SuperGLUE benchmark <ref type="bibr" target="#b38">[39]</ref>. SuperGLUE consists of 8 challenging natural language understanding tasks, including question answering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43]</ref>, textual entailment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>, coreference resolution <ref type="bibr" target="#b17">[18]</ref>, word sense disambiguation <ref type="bibr" target="#b24">[25]</ref>, and causal reasoning <ref type="bibr" target="#b29">[30]</ref>. We adopt the standard evaluation metrics as <ref type="bibr" target="#b38">[39]</ref>. We reformulate each task as a blank infilling task with human-crafted cloze questions, using the patterns constructed by PET <ref type="bibr" target="#b31">[32]</ref>. Then we finetune the pretrained GLM models on each task as described in Section 2.3. The cloze questions and other details can be found in the appendix.</p><p>For fair comparison with GLM Base and GLM Large , we choose BERT Base and BERT Large as our baselines, which are pretrained on the same corpus and for a similar amount of time to our models. We report the performance of standard finetuning (i.e. classification on the [CLS] token representation). The performance of BERT with cloze questions is reported in Section 3.4. To compare with GLM RoBERTa , we choose T5, BART Large , and RoBERTa Large as our baselines. T5 has no direct match in the number of parameters for BERT Large , so we present the results of both T5 Base (220M parameters) and T5 Large (770M parameters). All the other baselines are of similar size to BERT Large .</p><p>The results are shown in Table <ref type="table" target="#tab_1">2</ref>. With the same amount of training data (BookCorpus + Wikipedia), GLM consistently outperforms BERT on most of the tasks, with either base or large architecture. The only exception is WiC, which is a word sense disambiguation task. On average, GLM Base scores 4.6% higher than BERT Base , and GLM Large scores 5.0% higher than BERT Large . It clearly demonstrates the advantage of our method in NLU tasks. In the setting of RoBERTa Large , GLM RoBERTa can still achieve improvements over the baselines, but with a smaller margin. Specifically, GLM RoBERTa outperforms 22.9 25.6 50.5</p><p>T5 Large but is only half its size. We also find that BART does not perform well on the challenging SuperGLUE benchmark. We conjecture this can be attributed to the low parameter efficiency of the encoder-decoder architecture and the denoising sequence-to-sequence objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Task Pretraining</head><p>Then we evaluate the GLM's performance in a multi-task setting. During multi-task pretraining, as described in Section 2.1, within one training batch, 50% of the time we sample the BERT-style spans and 50% of the time we sample the generation spans, which can be document-level or sentence-level depending on the objective. We evaluate the multi-task model for NLU, seq2seq, blank infilling, and zero-shot language modeling. SuperGLUE. For NLU tasks, we evaluate models on the SuperGLUE benchmark. The results are also shown in Table <ref type="table" target="#tab_1">2</ref>. We observe that with multi-task pretraining, GLM Doc and GLM Sent perform slightly worse than GLM Large , but still outperform BERT Large and UniLM Large . Among multi-task models, GLM Sent outperforms GLM Doc by 1.1% on average. Increasing GLM Doc 's parameters to 410M (1.25×BERT Large ) leads to better performance than GLM Large . GLM with 515M parameters (1.5×BERT Large ) can perform even better.</p><p>Sequence-to-Sequence. We use abstractive summarization and question generation as the evaluation tasks. Abstractive summarization aims to produce a concise and fluent summary conveying the key information in the input text. We use the Gigaword dataset <ref type="bibr" target="#b30">[31]</ref> for model fine-tuning and evaluation. We use MASS <ref type="bibr" target="#b35">[36]</ref> and UniLM <ref type="bibr" target="#b9">[10]</ref> as our baselines since BART <ref type="bibr" target="#b18">[19]</ref> and PALM <ref type="bibr" target="#b2">[3]</ref> are trained with much more data and steps. Question generation aims to generate the corresponding question given an input passage and an answer. This was first proposed by <ref type="bibr" target="#b10">[11]</ref> to enhance the reading comprehension models. We use the SQuAD 1.1 dataset <ref type="bibr" target="#b28">[29]</ref> and follow the dataset split of <ref type="bibr" target="#b10">[11]</ref>. During decoding, we use beam search with beam size 5 and tweak the value of length penalty on the development set. We use SemQG <ref type="bibr" target="#b43">[44]</ref> and UniLM as our baselines.</p><p>The results are shown in Tables <ref type="table" target="#tab_3">3 and 4</ref>. We observe that GLM Large can achieve performance matching the other pretraining models on the two generation tasks. GLM Sent can perform better than GLM Large , while GLM Doc performs slightly worse than GLM Large . This indicates that the document-level objective, which teaches the model to extend the given contexts, is less helpful to conditional generation, which aims to extract useful information from the context. Increasing GLM Doc 's parameters to 410M leads to the best performance on both tasks.</p><p>Text Infilling. Text infilling is the task of predicting missing spans of text which are consistent with the surrounding context <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>. GLM is trained with an autoregressive blank infilling objective, thus can be straightforwardly applied to this task. Following <ref type="bibr" target="#b33">[34]</ref>, we evaluate GLM on the Yahoo Answers dataset <ref type="bibr" target="#b40">[41]</ref>. For each document in the dataset, we randomly mask a given ratio r ∈ {10% • • • 50%} of its tokens and contiguous masked tokens are collapsed into a single blank. We finetune GLM to autoregressively generate the masked tokens and evaluate the generation's quality by its BLEU score against the original document. We compare with BERT and the Blank Language Model (BLM) <ref type="bibr" target="#b33">[34]</ref> which is specifically designed for the text infilling task. From the results in Table <ref type="table" target="#tab_4">5</ref>, GLM outperforms previous methods by large margins (1.3 to 3.9 BLEU) and achieves the state-of-the-art result on this dataset. We notice that GLM Doc slightly underperforms GLM Large , which is consistent with our observations in the sequence-to-sequence experiments.</p><p>Language Modeling. We evaluate GLM's performance on language modeling. Most language modeling datasets such as WikiText103 are constructed from Wikipedia documents, which our  pretraining dataset already contains. For fair comparison with BERT, we cannot remove Wikipedia from the pretraining dataset. Therefore, we evaluate the language modeling perplexity on a held-out test set of our pretraining dataset, which contains about 20M tokens, denoted as BookWiki. We also evaluate on the LAMBADA dataset <ref type="bibr" target="#b22">[23]</ref>, which tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of a passage. As the baseline, we train a GPT Large model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref> with the same data and tokenization as GLM Large .</p><p>The results are shown in Figure <ref type="figure" target="#fig_4">3</ref>. All the models are evaluated in the zero-shot setting. Since GLM learns the bidirectional attention, we also evaluate GLM under the setting in which the contexts are encoded with bidirectional attention. Without generative objective during pretraining, GLM Large cannot complete the language modeling tasks, with perplexity larger than 100. With the same amount of parameters, GLM Doc performs worse than GPT Large . This is expected since GLM Doc also optimizes the blank infilling objective. Increasing the model's parameters to 410M (1.25× of GPT Large ) leads to a performance close to GPT Large . GLM 515M (1.5× of GPT Large ) can further outperform GPT Large .</p><p>With the same amount of parameters, encoding the context with bidirectional attention can improve the performance of language modeling. Under this setting, GLM 410M outperforms GPT Large . This is the advantage of GLM over unidirectional GPT. We also study the contribution of 2D positional encoding to long text generation. We find that removing the 2D positional encoding leads to lower accuracy and higher perplexity in language modeling.</p><p>Summary. Above all, we conclude that GLM effectively shares model parameters across natural language understanding and generation tasks, achieving better performance than a standalone BERT or GPT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Table <ref type="table" target="#tab_5">6</ref> shows our ablation analysis for GLM. First, to provide an apple-to-apple comparison with BERT and exclude the effect of implementation, data, and hyperparameters, we train a BERT Large model using the Masked LM objective with our code (row 2). The performance is slightly worse than Google's BERT Large and significantly worse than GLM Large . It confirms the superiority of GLM over Masked LM pretraining on NLU tasks. Second, we perform an ablation study to understand the importance of autoregressive pretraining, and the formulation of classification tasks as blank infilling. We show the SuperGLUE performance of GLM finetuned as sequence classifiers (row 5) and BERT with cloze-style finetuning (row 3). Compared to BERT with cloze-style finetuning, GLM benefits from the autoregressive pretraining. Especially on ReCoRD and WSC, where the verbalizer consists of multiple tokens, GLM consistently outperform BERT. This demonstrates GLM's advantage in handling variable-length blank. Another observation is that the cloze formulation is critical for GLM's performance on NLU tasks. For the large model, cloze-style finetuning can improve the performance by 7 points. Finally, we also compare GLM variants with different pretraining designs to understand their importance. Row 6 shows that removing the span shuffling (always predicting the masked spans from left to right) leads to a severe performance drop on SuperGLUE. Row 7 uses different sentinel tokens instead of a single [MASK] token to represent different masked spans. The model performs worse than the standard GLM. We hypothesize that it wastes some modeling capacity to learn the different sentinel tokens which is not used in downstream tasks with only one blank. In Figure <ref type="figure" target="#fig_4">3</ref>, we show that removing the second dimension of 2D positional encoding hurts the performance of long text generation.</p><p>We note that T5 is pretrained with a similar blank infilling objective. GLM differs in three aspects:</p><p>(1) GLM consists of a single encoder, (2) GLM shuffles the masked spans, and (3) GLM uses a single [MASK] instead of multiple sentinel tokens. While we cannot directly compare GLM with T5 due to the differences in training data and the number of parameters, the results in Tables <ref type="table" target="#tab_5">2 and 6</ref> have demonstrated the advantage of GLM.  <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Recently, pretraining large-scale language models with self-supervised learning on abundant web texts significantly improves the performance on downstream tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref>. There are three types of pretrained language models. The first type is the autoencoding model, which learns a bidirectional contextualized encoder for natural language understanding via denoising objectives. BERT <ref type="bibr" target="#b7">[8]</ref> pretrains a large transformer model <ref type="bibr" target="#b37">[38]</ref> via masked language modeling to obtain contextualized word representations. SpanBERT <ref type="bibr" target="#b14">[15]</ref> masks continuous spans of tokens for improved span representations. The second type is the autoregressive model, which learns a left-to-right language model for text generation. GPT <ref type="bibr" target="#b25">[26]</ref> shows that the representations learned by generative pretraining can also improve language understanding. XLNet <ref type="bibr" target="#b39">[40]</ref> generalizes the autoregressive model with permutation language modeling to learn bidirectional attention for language understanding tasks. The third type is the encoder-decoder model pretrained for sequenceto-sequence tasks. MASS <ref type="bibr" target="#b35">[36]</ref> maps an input text with continuous spans masked to the masked tokens. BART <ref type="bibr" target="#b18">[19]</ref> applies various transformations, including masking, deletion, and permutation, and recovers the original text with the decoder. PALM <ref type="bibr" target="#b2">[3]</ref> is pretrained for generating coherent text from a given context and adds a BERT-based autoencoding objective to the encoder.</p><p>NLU as Generation. Previously, pretrained language models complete classification tasks for NLU with linear classifiers on the learned representations. GPT-2 <ref type="bibr" target="#b26">[27]</ref> shows that generative language models can learn to complete understanding tasks such as question answering and reading comprehension by directly predicting the correct answers, even without any explicit supervision. GPT-3 <ref type="bibr" target="#b3">[4]</ref> further proves that language models can achieve strong performance on NLU tasks in the few-shot learning setting by adding a few labeled examples in the context. However generative models require much more parameters to work due to the limit of unidirectional attention. T5 <ref type="bibr" target="#b27">[28]</ref> formulates most language tasks in the text-to-text framework but requires more parameters to outperform BRET-based models such as RoBERTa <ref type="bibr" target="#b19">[20]</ref>.</p><p>Recently, PET <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> proposes to reformulate input examples as cloze questions with patterns similar to the pretraining corpus in the few-shot setting. It has been shown that combined with gradient-based finetuning on ALBERT <ref type="bibr" target="#b16">[17]</ref>, PET can achieve better performance in the few-shot setting than GPT-3 while requiring only 0.1% of its parameters. Athiwaratkun et al. <ref type="bibr" target="#b0">[1]</ref> and Paolini et al. <ref type="bibr" target="#b21">[22]</ref> convert structured prediction tasks, such as sequence tagging and relation extraction, to sequence generation tasks. Donahue et al. <ref type="bibr" target="#b8">[9]</ref> and Shen et al. <ref type="bibr" target="#b33">[34]</ref> also study blank infilling language models. Different from their work, we pretrain language models by blank infilling and evaluate their performance in downstream NLU and generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>GLM is a general pretraining framework for natural language understanding and generation. We show that the NLU tasks can be formulated as conditional generation tasks, and therefore solvable by autoregressive models. GLM unifies the pretraining objectives for different tasks as autoregressive blank infilling, with mixed attention mask and the novel 2D position encodings. Empirically we show that GLM outperforms previous methods for NLU tasks and can effectively share parameters for different tasks. In the future, we hope to scale GLM to larger Transformer models and more data, and examine its performance in more settings such as knowledge probing and few-shot learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: GLM pretraining framework. (a) The original text is [x 1 , x 2 , x 3 , x , x 5 , x 6 ], and two spans [x 3 ] and [x 5 , x 6 ] are sampled. (b) Replace the sampled spans with [M] tokens to form Part A, and shuffle the sampled spans to form Part B. (c) GLM is trained to generate Part B autoregressively. Each span is prepended with a [S] token as input and appended with a [E] token as output. 2D positional encoding represents inter-and intra-span positions. (d) Self-attention mask, where the grey areas are masked out. Part A tokens can attend to themselves (the blue frame) but not B. Part B tokens can attend to A and their antecedent tokens in B (the yellow and green frames denote the tokens to which the two spans in Part B can attend). [M] := [MASK], [S] := [START], and [E] := [END].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Classification task (b) Generation task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: GLM finetuning. (a) Formulation of the sentiment classification task as blank infilling with GLM. (b) GLM for text generation given the context. This can be the language modeling, or sequence-to-sequence tasks with finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Zero-shot language modeling results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the SuperGLUE dev set.</figDesc><table><row><cell>Model</cell><cell>ReCoRD F1/Acc.</cell><cell>COPA Acc.</cell><cell>WSC Acc.</cell><cell>RTE Acc.</cell><cell>BoolQ Acc.</cell><cell>WiC Acc.</cell><cell>CB F1/Acc.</cell><cell>MultiRC F1a/EM</cell><cell>Avg</cell></row><row><cell cols="3">Pretrained on BookCorpus and Wikipedia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT Base</cell><cell>65.4 / 64.9</cell><cell>66.0</cell><cell cols="2">65.4 70.0</cell><cell cols="5">74.9 68.8 70.9 / 76.8 68.4 / 21.5 66.1</cell></row><row><cell>GLM Base</cell><cell>73.5 / 72.8</cell><cell>71.0</cell><cell cols="2">72.1 71.2</cell><cell cols="5">77.0 64.7 89.5 / 85.7 72.1 / 26.1 70.7</cell></row><row><cell>BERT Large</cell><cell>76.3 / 75.6</cell><cell>69.0</cell><cell cols="2">64.4 73.6</cell><cell cols="5">80.1 71.0 94.8 / 92.9 71.9 / 24.1 72.0</cell></row><row><cell>UniLM Large</cell><cell>80.0 / 79.1</cell><cell>72.0</cell><cell cols="2">65.4 76.5</cell><cell cols="5">80.5 69.7 91.0 / 91.1 77.2 / 38.2 74.1</cell></row><row><cell>GLM Large</cell><cell>81.7 / 81.1</cell><cell>76.0</cell><cell cols="2">81.7 74.0</cell><cell cols="5">82.1 68.5 96.1 / 94.6 77.1 / 36.3 77.0</cell></row><row><cell>GLM Doc</cell><cell>80.2 / 79.6</cell><cell>77.0</cell><cell cols="2">78.8 76.2</cell><cell cols="5">79.8 63.6 97.3 / 96.4 74.6 / 32.1 75.7</cell></row><row><cell>GLM Sent</cell><cell>80.7 / 80.2</cell><cell>77.0</cell><cell cols="2">79.8 79.1</cell><cell cols="5">80.8 70.4 94.6 / 93.7 76.9 / 36.1 76.8</cell></row><row><cell>GLM 410M</cell><cell>81.5 / 80.9</cell><cell>80.0</cell><cell cols="2">81.7 79.4</cell><cell cols="5">81.9 69.0 93.2 / 96.4 76.2 / 35.5 78.0</cell></row><row><cell>GLM 515M</cell><cell>82.3 / 81.7</cell><cell>85.0</cell><cell cols="2">81.7 79.1</cell><cell cols="5">81.3 69.4 95.0 / 96.4 77.2 / 35.0 78.8</cell></row><row><cell cols="2">Pretrained on larger corpora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5 Base</cell><cell>76.2 / 75.4</cell><cell>73.0</cell><cell cols="2">79.8 78.3</cell><cell cols="5">80.8 67.9 94.8 / 92.9 76.4 / 40.0 76.0</cell></row><row><cell>T5 Large</cell><cell>85.7 / 85.0</cell><cell>78.0</cell><cell cols="2">84.6 84.8</cell><cell cols="5">84.3 71.6 96.4 / 98.2 80.9 / 46.6 81.2</cell></row><row><cell>BART Large</cell><cell>88.3 / 87.8</cell><cell>60.0</cell><cell cols="2">65.4 84.5</cell><cell cols="5">84.3 69.0 90.5 / 92.9 81.8 / 48.0 76.0</cell></row><row><cell cols="2">RoBERTa Large 89.0 / 88.4</cell><cell>90.0</cell><cell cols="2">63.5 87.0</cell><cell cols="5">86.1 72.6 96.1 / 94.6 84.4 / 52.9 81.5</cell></row><row><cell>GLM RoBERTa</cell><cell>89.6 / 89.0</cell><cell>82.0</cell><cell cols="2">83.7 87.7</cell><cell cols="5">84.7 71.2 98.7 / 98.2 82.4 / 50.1 82.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on Gigaword summarization.</figDesc><table><row><cell>Model</cell><cell cols="3">RG-1 RG-2 RG-L</cell></row><row><cell>MASS</cell><cell>37.7</cell><cell>18.5</cell><cell>34.9</cell></row><row><cell cols="2">UniLM Large 38.5</cell><cell>19.5</cell><cell>35.8</cell></row><row><cell>GLM Large</cell><cell>38.6</cell><cell>19.7</cell><cell>36.0</cell></row><row><cell>GLM Doc</cell><cell>38.5</cell><cell>19.4</cell><cell>35.8</cell></row><row><cell>GLM Sent</cell><cell>38.9</cell><cell>20.0</cell><cell>36.3</cell></row><row><cell>GLM 410M</cell><cell>38.9</cell><cell>20.0</cell><cell>36.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on SQuAD question generation.</figDesc><table><row><cell>Model</cell><cell cols="3">BLEU-4 MTR RG-L</cell></row><row><cell>SemQG</cell><cell>18.4</cell><cell>22.7</cell><cell>46.7</cell></row><row><cell>UniLM Large</cell><cell>22.1</cell><cell>25.1</cell><cell>51.1</cell></row><row><cell>GLM Large</cell><cell>22.4</cell><cell>25.2</cell><cell>50.4</cell></row><row><cell>GLM Doc</cell><cell>22.3</cell><cell>25.0</cell><cell>50.2</cell></row><row><cell>GLM Sent</cell><cell>22.6</cell><cell>25.4</cell><cell>50.4</cell></row><row><cell>GLM 410M</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>BLEU scores on Yahoo text infilling. † indicates the results from<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell cols="2">Mask ratio 10% 20% 30% 40% 50%</cell></row><row><cell>BERT  †</cell><cell>82.8 66.3 50.3 37.4 26.2</cell></row><row><cell>BLM  †</cell><cell>86.5 73.2 59.6 46.8 34.8</cell></row><row><cell>GLM Large</cell><cell>87.8 76.7 64.2 48.9 38.7</cell></row><row><cell>GLM Doc</cell><cell>87.5 76.0 63.2 47.9 37.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the SuperGLUE dev set. T5 ≈ GLM -shuffle spans + sentinel tokens.</figDesc><table><row><cell>Model</cell><cell>ReCoRD F1/Acc.</cell><cell>COPA Acc.</cell><cell>WSC Acc.</cell><cell>RTE Acc.</cell><cell>BoolQ Acc.</cell><cell>WiC Acc.</cell><cell>CB F1/Acc.</cell><cell>MultiRC F1a/EM</cell><cell>Avg</cell></row><row><cell>BERT Large</cell><cell>76.3 / 75.6</cell><cell>69.0</cell><cell cols="2">64.4 73.6</cell><cell cols="5">80.1 71.0 94.8 / 92.9 71.9 / 24.1 72.0</cell></row><row><cell cols="2">BERT Large (reproduced) 82.1 / 81.5</cell><cell>63.0</cell><cell cols="2">63.5 72.2</cell><cell cols="5">80.8 68.7 80.9 / 85.7 77.0 / 35.2 71.2</cell></row><row><cell>BERT Large (cloze)</cell><cell>70.0 / 69.4</cell><cell>80.0</cell><cell cols="2">76.0 72.6</cell><cell cols="5">78.1 70.5 93.5 / 91.1 70.0 / 23.1 73.2</cell></row><row><cell>GLM Large</cell><cell>81.7 / 81.1</cell><cell>76.0</cell><cell cols="2">81.7 74.0</cell><cell cols="5">82.1 68.5 96.1 / 94.6 77.1 / 36.3 77.0</cell></row><row><cell>-cloze finetune</cell><cell>81.3 / 80.6</cell><cell>62.0</cell><cell cols="2">63.5 66.8</cell><cell cols="5">80.5 65.0 89.2 / 91.1 72.3 / 27.9 70.0</cell></row><row><cell>-shuffle spans</cell><cell>82.0 / 81.4</cell><cell>61.0</cell><cell cols="2">79.8 54.5</cell><cell cols="5">65.8 56.3 90.5 / 92.9 76.7 / 37.6 68.5</cell></row><row><cell>+ sentinel tokens</cell><cell>81.8 / 81.3</cell><cell>69.0</cell><cell cols="2">78.8 77.3</cell><cell cols="5">81.2 68.0 93.7 / 94.6 77.5 / 37.7 76.0</cell></row><row><cell>4 Related Work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Pretrained Language Models. In NLP, self-supervised learning has long been used to learn word</cell></row><row><cell cols="2">vectors as inputs to neural networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We define unconditional generation as the task of generating text without further training as in a standard language model, while conditional generation refers to sequence-to-sequence tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The STORIES dataset<ref type="bibr" target="#b36">[37]</ref> used by RoBERTa is no longer available. Therefore, we replace OpenWebText (38GB)<ref type="bibr" target="#b12">[13]</ref> with OpenWebText2 (66GB)<ref type="bibr" target="#b11">[12]</ref>. The whole corpus totals 158GB of uncompressed text, close in size to RoBERTa's 160GB corpus.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented natural language for generative sequence labeling</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Contextconditioned Generation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8681" to="8691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ICLR 2020</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Enabling language models to fill in the blanks</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05339</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to Ask: Neural Question Generation for Reading Comprehension</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR, abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>ICLR 2020</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05779</idno>
		<title level="m">Structured prediction as translation between augmented natural languages</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><surname>Fernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1267" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<editor>Jian Su, Xavier Carreras, and Kevin Duh</editor>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>CoRR, abs/2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>CoRR, abs/2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03079</idno>
		<title level="m">Blank language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MASS: Masked Sequence to Sequence Pre-training for Language Generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">Simple Method for Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5999" to="6009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3881" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Addressing semantic drift in question generation for semisupervised question answering</title>
		<author>
			<persName><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2495" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00158</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Text infilling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
