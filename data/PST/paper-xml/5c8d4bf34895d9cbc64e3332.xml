<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="laboratory">State Key Lab for Intell. Tech. &amp; Sys., THBI Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>BNRist Center, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="laboratory">State Key Lab for Intell. Tech. &amp; Sys., THBI Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>BNRist Center, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms with new theoretical guarantee to converge to a local optimum of GCN regardless of the neighbor sampling size. Empirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors per node. The running time of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph convolution networks (GCNs) <ref type="bibr">(Kipf &amp; Welling, 2017)</ref> generalize convolutional neural networks (CNNs) <ref type="bibr">(Le-Cun et al., 1995)</ref> to graph structured data. The "graph convolution" operation applies same linear transformation to all the neighbors of a node, followed by mean pooling and nonlinearity. By stacking multiple graph convolution layers, GCNs can learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type="bibr" target="#b4">(Hamilton et al., 2017a;</ref><ref type="bibr" target="#b14">Veličković et al., 2018)</ref> have been applied to semi-supervised node classification <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>, link prediction <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b1">Berg et al., 2017)</ref> and knowledge graphs <ref type="bibr" target="#b11">(Schlichtkrull et al., 2017)</ref>, outperforming multi-layer perceptron (MLP) models that Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s).</p><p>do not use the graph structure, and graph embedding approaches <ref type="bibr" target="#b9">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b13">Tang et al., 2015;</ref><ref type="bibr" target="#b3">Grover &amp; Leskovec, 2016)</ref> that do not use node features.</p><p>However, the graph convolution operation makes GCNs difficult to be trained efficiently. The representation of a node at layer L is computed recursively by the representations of all its neighbors at layer L − 1. Therefore, the receptive field of a single node grows exponentially with respect to the number of layers, as illustrated in Fig. <ref type="figure">1(a)</ref>, so exactly computing the stochastic gradient is expensive even for a single node. Due to the large receptive field size, <ref type="bibr">Kipf &amp; Welling (2017)</ref> propose to train GCN by a batch algorithm, which computes the representations of all the nodes altogether. However, batch algorithms cannot handle largescale datasets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> make an initial attempt to develop stochastic training algorithms for GCNs via a scheme of neighbor sampling (NS). Instead of considering all the neighbors, they randomly subsample D (l) neighbors at the l-th layer. Therefore, they reduce the receptive field size to l D (l) , as shown in <ref type="bibr">Fig. 1(b)</ref>. They find that for two-layer GCNs, keeping D (1) = 10 and D (2) = 25 neighbors can achieve comparable performance with the original model. However, there is no theoretical guarantee on the convergence of the stochastic training algorithm with NS. Moreover, the time complexity of NS is still D (1) D (2) = 250 times larger than training an MLP, which is unsatisfactory.</p><p>In this paper, we develop novel control variate-based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate. Our algorithms have new theoretical results on (1) variance reduction from the magnitude of the activation to the magnitude of the difference between current-and-historical activations; (2) exact (zero-variance) predictions at testing time;</p><p>(3) convergence to a local optimum of GCN during training regardless of the neighbor sampling size D (l) , with an asymptotically unbiased stochastic gradient. The theoretical properties allow us to significantly reduce the time complexity of stochastic training by sampling only D (l) = 2 neighbors per node, yet still retain the quality of the model.</p><p>We empirically test our algorithms on six graph datasets, and the results match with the theory. Comparing with NS, our algorithms significantly reduce the bias and variance of the gradient. Comparing with the exact algorithm which considers all the neighbors, our algorithms with only D (l) = 2 neighbors still get the same accuracy at testing time, and achieve similar predictive performance during training in a comparable number of epochs, with a much lower time complexity, while these results are not achievable by NS.</p><p>On the largest Reddit dataset, the training time of our algorithm is 7 times shorter than that of the best-performing competitor among exact, neighbor sampling and importance sampling <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Backgrounds</head><p>We briefly review graph convolutional networks (GCNs), stochastic training, neighbor sampling, and importance sampling in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Convolutional Networks</head><p>We present our algorithm with a GCN for semi-supervised node classification <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>. However, the algorithm is neither limited to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type="bibr" target="#b4">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref type="bibr" target="#b14">(Veličković et al., 2018)</ref>, and other tasks <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b1">Berg et al., 2017;</ref><ref type="bibr" target="#b11">Schlichtkrull et al., 2017;</ref><ref type="bibr" target="#b5">Hamilton et al., 2017b)</ref>, as long as the model aggregates neighbor activations by averaging.</p><p>In the node classification task, we have an undirected graph</p><formula xml:id="formula_0">G = (V, E) with V = |V| vertices and E = |E| edges,</formula><p>where each vertex v consists of a feature vector x v and a label y v . We observe the labels for some vertices V L . The goal is to predict the labels for the rest vertices V U := V\V L . The edges are represented as a symmetric V × V adjacency matrix A, where A uv is the weight of the edge between u and v, and the propagation matrix P is a normalized version of A: Ã = A + I, Duu = v Ãuv , and P = D− 1 2 Ã D− 1 2 . A graph convolution layer is defined as</p><formula xml:id="formula_1">Z (l+1) = P H (l) W (l) , H (l+1) = σ(Z l+1 ),<label>(1)</label></formula><p>where H (l) is the activation matrix in the l-th layer, whose each row is the activation of a graph node. H (0) = X is the input feature matrix, W (l) is a trainable weight matrix, and σ(•) is an activation function. Denote |•| as the cardinality of a set. The training loss is defined as</p><formula xml:id="formula_2">L = 1 |V L | v∈V L f (y v , z (L) v ),<label>(2)</label></formula><p>where f (•, •) is a loss function. A graph convolution layer propagates information to nodes from their neighbors by computing the neighbor averaging P H (l) . Let n(u) be the set of neighbors of node u, and n(u) be its cardinality. The neighbor averaging of node u, (P</p><formula xml:id="formula_3">H (l) ) u = V v=1 P uv h (l) v = v∈n(u) P uv h (l)</formula><p>v , is a weighted sum of neighbors' activations. Then, a fully-connected layer is applied on all the nodes, with a shared weight matrix W (l)  across all the nodes. We denote the receptive field of a node u as all the activations h (l) v on layer l needed for computing z (L) u . If the layer l is not explicitly mentioned, it is the input layer 0. Intuitively, the receptive field of node u is just all its L-hop neighbors, i.e., nodes that are reachable from u within L hops, as illustrated in Fig. <ref type="figure">1(a)</ref>. When P = I, GCN reduces to a multi-layer perceptron (MLP) model which does not use the graph structure. For MLP, the receptive field of a node u is just the node itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stochastic Training</head><p>It is generally expensive to compute the batch gradient</p><formula xml:id="formula_4">∇L = 1 |V L | v∈V L ∇f (y v , z (L)</formula><p>v ), which involves iterat-ing over the entire labeled set of nodes. A possible solution is to approximate the batch gradient by a stochastic gradient</p><formula xml:id="formula_5">1 |V B | v∈V B ∇f (y v , z (L) v ),<label>(3)</label></formula><p>where V B ⊂ V L is a minibatch of labeled nodes. However, this gradient is still expensive to compute, due to the large receptive field size. For instance, as shown in Table <ref type="table" target="#tab_0">1</ref>, the number of 2-hop neighbors on the NELL dataset is averagely 1,597, which means in a 2-layer GCN, computing the gradient even for a single node needs 1, 597/65, 755 ≈ 2.4% nodes of the entire graph.</p><p>In subsequent sections, two other stochasticity will be introduced besides the random selection of the minibatch: the random sampling of neighbors (Sec. 2.3) and the random dropout of features (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorithm. NS randomly chooses D (l) neighbors for each node at layer l and develops an estimator NS (l) u of (P H (l) ) u based on Monte-Carlo approximation:</p><formula xml:id="formula_6">(P H (l) ) u ≈ NS (l) u := n(u) D (l) v∈n (l) (u) P uv h (l) v ,</formula><p>where n(l) (u) ⊂ n(u) is a subset of D (l) random neighbors. Therefore, NS reduces the receptive field size from all the L-hop neighbors to the number of sampled neighbors, L l=1 D (l) . We refer NS (l)  u as the NS estimator of (P H (l) ) u , and (P H (l) ) u itself as the exact estimator.</p><p>Neighbor sampling can also be written in a matrix form as</p><formula xml:id="formula_7">Z (l+1) = P (l) H (l) W (l) , H (l+1) = σ(Z (l+1) ),<label>(4)</label></formula><p>where the propagation matrix P is replaced by a sparser unbiased estimator P (l) , i.e., E P (l) = P , where</p><formula xml:id="formula_8">P (l) uv = n(u) D (l) P uv if v ∈ n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> propose to perform an approximate forward propagation as Eq. ( <ref type="formula" target="#formula_7">4</ref>), and do stochastic gradient descent (SGD) with the auto-differentiation gradient. The approximated gradient has two sources of randomness: the random selection of minibatch V B ⊂ V L , and the random selection of neighbors.</p><p>Though P (l) is an unbiased estimator of P , σ( P (l) H (l) W (l) ) is not an unbiased estimator of σ(P H (l) W (l) ), due to the non-linearity of σ(•). In the sequel, both the prediction Z (L) and gradient ∇f (y v , z</p><formula xml:id="formula_9">(L) v )</formula><p>obtained by NS are biased, and the convergence of SGD is not guaranteed, unless the sample size D (l) goes to infinity. Because of the biased gradient, the sample size D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and the receptive field size D (1) × D (2) = 250 is much larger than one, so the training is still expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Importance Sampling</head><p>FastGCN <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> is another sampling-based algorithm similar as NS. Instead of sampling neighbors for each node, FastGCN directly subsample the receptive field for each layer altogether. Formally, it approximates (P H (l) ) u with S samples v 1 , . . . , v S ∈ V as</p><formula xml:id="formula_10">(P H (l) ) u = V V v=1 1 V P uv h (l) v ≈ V S vs∼q(v) P uv h (l) vs /q(v s ),</formula><p>where they define the importance distribution q(v)</p><formula xml:id="formula_11">∝ V u=1 P 2 uv . According to the definition of P in Sec. 2.1, we have q(v) ∝ 1 n(v) (u,v)∈E 1 n(u)</formula><p>. We refer to this estimator as importance sampling (IS). <ref type="bibr" target="#b2">Chen et al. (2018)</ref> show that IS performs better than using a uniform sample distribution q(v) ∝ 1. NS can be viewed as an IS estimator with the importance distribution q(v) ∝ (u,v)∈E 1 n(u) , because each node u has probability 1 n(u) to choose the neighbor v. Though IS may have a smaller variance than NS, it still only guarantees the convergence as the sample size S goes to infinity. Empirically, we find IS to work even worse than NS because sometimes it can select many neighbors for one node, and no neighbor for another, in which case the activation of the latter node is just meaningless zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Control Variate Based Algorithm</head><p>We present a novel control variate based algorithm that utilizes historical activations to reduce the estimator variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Control Variate Based Estimator</head><p>While computing the neighbor average v∈n(u) P uv h (l) v , we cannot afford to evaluate all the h (l) v terms because they need to be computed recursively, i.e., we again need the activations h</p><formula xml:id="formula_12">(l−1) w of all of v's neighbors w.</formula><p>Our idea is to maintain the history h(l) v for each h</p><formula xml:id="formula_13">(l)</formula><p>v as an affordable approximation. Each time when h</p><formula xml:id="formula_14">(l) v is computed, we update h(l) v with h (l) v . We expect h(l) v and h (l)</formula><p>v to be similar if the model weights do not change too fast during the training. Formally, let ∆h</p><formula xml:id="formula_15">(l) v = h (l) v − h(l) v , we approximate (P H (l) ) u = v∈n(u) P uv ∆h (l) v + v∈n(u) P uv h(l) v ≈ CV (l) u := n(u) D (l) v∈n (l) (u) P uv ∆h (l) v + v∈n(u) P uv h(l) v ,<label>(5)</label></formula><p>where we represent h</p><formula xml:id="formula_16">(l)</formula><p>v as the sum of ∆h term. Averaging over all the h(l) v terms is still affordable because they do not need to be computed recursively. Since we expect h (l) v and h(l) v to be close, ∆h v will be small and CV (l)  u should have a smaller variance than NS (l) u . Particularly, if the model weight is kept fixed, h(l) v should eventually equal with h</p><formula xml:id="formula_17">(l) v , so that CV (l) u = 0 + v∈n(u) P uv h(l) v = v∈n(u) P uv h (l) v = (P H (l)</formula><p>) u , i.e., the estimator has zero variance. This estimator is referred as CV. We will compare the variance of NS and CV estimators in Sec. 3.2 and show that the variance of CV will be eventually zero during the training in Sec. 4. The term CV (l)  u − NS</p><formula xml:id="formula_18">(l) u = v∈n(u) P uv h(l) u − n(u) D (l) v∈n (l) (u) P uv h(l)</formula><p>u is a control variate <ref type="bibr">(Ripley, 2009, Chapter 5)</ref> added to the neighbor sampling estimator NS (l)  u , to reduce its variance. In matrix form, let H(l) be the matrix formed by stacking h(l)</p><p>v , then CV can be written as</p><formula xml:id="formula_19">Z (l+1) = P (l) (H (l) − H(l) ) + P H(l) W (l) . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variance Analysis</head><p>We analyze the variance of the estimators assuming all the features are 1-dimensional. The analysis can be extended to multiple dimensions by treating each dimension separately. We further assume that n(l) (u) is created by sampling D (l)  neighbors without replacement from n(u). The following proposition is proven in Appendix A:</p><formula xml:id="formula_20">Proposition 1. If n(l) (u) contains D (l) samples from n(u)</formula><p>without replacement, then Var n(l) (u)</p><formula xml:id="formula_21">n(u) D (l) v∈n (l) (u) x v = C (l) u 2D (l) v1∈n(u) v2∈n(u) (x v1 − x v2 ) 2 , where C (l) u = 1 − (D (l) − 1)/(n(u) − 1). By Proposition 1, we have Var n(l) (u) N S (l) u = C (l) u 2D (l) v1∈n(u) v2∈n(u) (P uv1 h (l) v1 − P uv2 h (l) v2 ) 2 , in contrast, the variance of the CV estimator is Var n(l) (u) CV (l) u = C (l) u 2D (l) v1∈n(u) v2∈n(u) (P uv1 ∆h (l) v1 − P uv2 ∆h (l) v2 ) 2 , which replaces h (l) v by ∆h (l) v . Since ∆h (l) v is usually much smaller than h (l)</formula><p>v , the CV estimator enjoys much smaller variance than the NS estimator. Furthermore, as we will show in Sec. 4.2, ∆h (l) v converges to zero during training, so we achieve not only variance reduction but variance elimination, as the variance vanishes eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the algorithm involves the following steps:</p><p>Stochastic GCN with Variance Reduction 1. Randomly select a minibatch V B ⊂ V L of nodes; 2. Build a computation graph that only contains the activations h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(l)</head><p>v and h(l) v needed for the current minibatch; 3. Get the predictions by forward propagation as Eq. ( <ref type="formula">6</ref>); 4. Get the gradients by backward propagation, and update the parameters by SGD; 5. Update the historical activations.</p><p>Step 3 and 4 are handled automatically by frameworks such as TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. The computational graph at Step 2 is defined by the receptive field r (l) and the propagation matrices P (l) at each layer. The receptive field r (l) specifies the activations h (l) v of which nodes should be computed for the current minibatch, according to Eq. ( <ref type="formula">6</ref>). We can construct r (l) and P (l) from top to bottom, by randomly adding D (l) neighbors for each node in r (l+1) , starting with r (L) = V B . We assume h</p><formula xml:id="formula_22">(l) v is always needed to compute h (l+1) v</formula><p>, i.e., v is always selected as a neighbor of itself. The receptive fields are illustrated in Fig. <ref type="figure">1(c)</ref>, where red nodes are in receptive fields, whose activations h (l) v are needed, and the histories h(l) v of blue nodes are also needed. Finally, in Step 5, we update l) . We have the pseudocode for the training in Appendix D.</p><formula xml:id="formula_23">h(l) v with h (l) v for each v ∈ r (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Time and Space Complexity</head><p>GCN has two main types of computation, namely, the sparsedense matrix multiplication (SPMM) such as P H (l) , and the dense-dense matrix multiplication (GEMM) such as U W (l) . We assume that the input node feature is K-dimensional and the first hidden layer is A-dimensional. Our algorithm requires an additional O(V LA) space to store historical activations. However, as implemented in our code, the history can be stored in main memory along with the data, which should be larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Results</head><p>Besides smaller variance, CV also has stronger theoretical guarantees than NS. In this section, we present two theorems. The first states that if the model parameters are fixed, e.g., during testing, CV produces exact predictions after L epochs; and the second establishes the convergence towards a local optimum regardless of the neighbor sampling size.</p><p>In this section, we assume that the algorithm is run by epochs, where each epoch contains I iterations, and in each iteration we want to compute the stochastic gradient w.r.t. nodes in V i . We ensure that the activations of all nodes are computed at least one in each epoch, so that the staleness of the history is bounded. We use the subscript i for iteration number and CV to distinguish CV from the exact algorithm, i.e., Z (l) i and H (l) i , W i , and</p><formula xml:id="formula_24">g i (W i ) := 1 |Vi| v∈Vi ∇f (y v , z (L) i,v</formula><p>) are the activations, model weights, and stochastic gradients obtained by the exact algorithm; and Z (l)</p><formula xml:id="formula_25">CV,i , H (l)</formula><p>CV,i , and g CV,i (W i ) are their CV counterparts. ∇L(</p><formula xml:id="formula_26">W i ) = 1 |V L | v∈V L ∇f (y v , z (L) v )</formula><p>is the deterministic batch gradient computed by the exact algorithm. The subscript i may be omitted for the exact algorithm if W i is a constant sequence. We let [L] = {0, . . . , L} and [L] + = {1, . . . , L}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Exact Testing</head><p>The following theorem reveals the connection between the exact predictions and the approximate predictions by CV. The proof can be found in Appendix B.</p><p>Theorem 1. For a constant sequence of W i = W and any i &gt; LI (i.e., after L epochs), the activations computed by CV are exact, i.e., Z (l)</p><formula xml:id="formula_27">CV,i = Z (l) for each l ∈ [L] and H (l) CV,i = H (l) for each l ∈ [L − 1].</formula><p>Theorem 1 shows that at testing time, we can run forward propagation with CV for L epoches and get exact prediction. This outperforms NS, which cannot recover the exact prediction unless the neighbor sample size goes to infinity. Comparing with directly making exact predictions by an exact batch algorithm, CV is more scalable because it does not need to load the entire graph into memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Convergence Guarantee</head><p>The following theorem shows that SGD training with the approximated gradients g CV,i (W i ) still converges to a local optimum, regardless of the neighbor sampling size D (l) .</p><p>Theorem 2. Assume that (1) the activation σ(•) is ρ-Lipschitz, (2) the gradient of the cost function ∇ z f (y, z) is ρ-Lipschitz and bounded, (3) g CV,V (W ) ∞ , g(W ) ∞ , and ∇L(W ) ∞ are all bounded by G &gt; 0 for all P , V and W . (4) The loss</p><formula xml:id="formula_28">L(W ) is ρ-smooth, i.e., |L(W 2 )−L(W 1 )− ∇L(W 1 ), W 2 −W 1 | ≤ ρ 2 W 2 − W 1 2 F ∀W 1 , W 2</formula><p>, where A, B = tr(A B) is the inner product of matrix A and matrix B. (5) The loss L(W ) ≥ L * is bounded below. Then, there exists K &gt; 0, s.t., ∀N &gt; LI, if we run SGD for R ≤ N iterations, where R is chosen uniformly from [N ] + , we have</p><formula xml:id="formula_29">E R ∇L(W R ) 2 F ≤ 2 L(W 1 ) − L * + K + ρK √ N ,</formula><p>for the updates W i+1 = W i − γg CV,i (W i ) and the step size</p><formula xml:id="formula_30">γ = min{ 1 ρ , 1 √ N }.</formula><p>Particularly, lim N →∞ E R ∇L(W R ) 2 = 0. Therefore, our algorithm converges to a local optimum W where the batch gradient ∇L(W ) = 0. The full proof is in Appendix C. For short, we show that g CV,i (W i ) is unbiased as i → ∞, and then show that SGD with such asymptotically unbiased gradients converges to a local optimum.</p><p>Theorem 2 generalizes to graph attention networks (GAT) <ref type="bibr" target="#b14">(Veličković et al., 2018)</ref>. We leave the variance reduced stochastic estimators for GAT, and discussions on the convergence of GAT and other models in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Handling Dropout of Features</head><p>In this section, we consider introducing a third source of randomness, the random dropout of features <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref>, which is adopted in various GCN models as a regularization <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b14">Veličković et al., 2018)</ref>. With dropout, the GCN layer becomes Z (l+1) = M • (P H (l) )W (l) , where M ij ∼ Bern(p) are i.i.d. Bernoulli random variables, and • is the element-wise product. Let E M be the expectation over dropout masks.</p><p>With dropout, all the activations h (l)</p><p>v are random variables whose randomness comes from dropout, even in the exact algorithm Eq. ( <ref type="formula" target="#formula_1">1</ref>). We want to design a cheap estimator for the random variable (P</p><formula xml:id="formula_31">H (l) ) u = v∈n(u) P uv h (l)</formula><p>v , based on a stochastic neighborhood n(l) (u). An ideal estimator should have the same distribution with (P H (l) ) u . However, such an estimator is difficult to design. Instead, we develop an estimator CVD (l)  u that eventually has the same mean and variance with (P H (l) ) u , i.e., E n(l) (u) E M CVD (l)  u = E M (P H (l) ) u and Var n(l) (u) Var M CVD (l)  u = Var M (P H (l) ) u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Control Variate for Dropout</head><p>With dropout, ∆h</p><formula xml:id="formula_32">(l) v = h (l) v − h(l) v is not necessarily small even if h(l) v and h (l)</formula><p>v have the same distribution. We develop another stochastic approximation algorithm, control variate for dropout (CVD), that works well with dropout.</p><formula xml:id="formula_33">Estimator VNS VD Exact 0 S (l) u NS C (l) u 2D (l)</formula><p>v1,v2∈n(u) (P uv1 µ (l)</p><formula xml:id="formula_34">v1 − P uv2 µ (l) v2 ) 2 n(u) D (l) S (l) u CV C (l) u 2D (l)</formula><p>v1,v2∈n(u) (P uv1 ∆µ (l) l)   v1,v2∈n(u) (P uv1 ∆µ (l)</p><formula xml:id="formula_35">v1 − P uv2 ∆µ (l) v2 ) 2 3 + n(u) D (l) S (l) u CVD C (l) u 2D<label>(</label></formula><formula xml:id="formula_36">v1 − P uv2 ∆µ (l) v2 ) 2 S (l) u</formula><p>Table <ref type="table">2</ref>. Variance from neighbor sampling (VNS) and variance from dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type="bibr" target="#b12">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</p><formula xml:id="formula_37">(l) v := E M h (l) v .</formula><p>That is, along with the dropout model, we can run a copy of the model without dropout to obtain the mean µ (l) v , as illustrated in Fig. <ref type="figure">1(d)</ref>. We obtain a stochastic approximation by separating the mean and variance</p><formula xml:id="formula_38">(P H (l) ) u = v∈n(u) P uv ( h(l) v + ∆µ (l) v + μ(l) v ) ≈ CVD (l) u := √ R v∈n P uv h(l) v + R v∈n P uv ∆µ (l) v + v∈n(u) P uv μ(l) v ,</formula><p>where we define</p><formula xml:id="formula_39">n = n(l) (u), R = n(u)/D (l) for short, h(l) v = h (l) v − µ (l) v , μ(l) v is the historical mean activation, obtained by storing µ (l) v instead of h (l)</formula><p>v , and ∆µ</p><formula xml:id="formula_40">(l) v = µ (l) v − μ(l) v . We separate h<label>(l)</label></formula><p>v as three terms, the latter two terms on µ (l) v do not have the randomness from dropout, and µ</p><formula xml:id="formula_41">(l) v are treated as if h (l)</formula><p>v for the CV estimator. The first term has zero mean w.r.t. dropout, i.e., E M h(l) v = 0. We have E n(l) (u) E M CVD (l)  u = 0 + v∈n(u) P uv (∆µ</p><formula xml:id="formula_42">(l) v + μ(l) v ) = E M (P H (l)</formula><p>) u , i.e., the estimator is unbiased, and we shall see that the estimator eventually has the correct variance if h (l) v 's are uncorrelated in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Variance Analysis</head><p>We analyze the variance under the assumption that the node activations are uncorrelated, i.e., Cov M h</p><formula xml:id="formula_43">(l) v1 , h (l) v2 = 0, ∀v 1 = v 2 .</formula><p>We report the correlation between nodes empirically in Appendix G. To facilitate the analysis of the variance, we introduce two propositions proven in Appendix A . The first helps the derivation of the dropout variance; and the second implies that we can treat the variance introduced by neighbor sampling and by dropout separately.</p><formula xml:id="formula_44">Proposition 2. If n(l) (u) contains D (l) samples from the set n(u) without replacement, x 1 , . . . , x V are random variables, ∀v, E [x v ] = 0 and ∀v 1 = v 2 , Cov [x v1 , x v2 ] = 0, then Var X,n (l) (u) n(u) D (l) v∈n (l) (u) x v = n(u) D (l)</formula><p>v∈n(u) Var [x v ] . Proposition 3. X and Y are two random variables, and</p><formula xml:id="formula_45">f (X, Y ) and g(Y ) are two functions. If E X f (X, Y ) = 0, then Var X,Y [f (X, Y ) + g(Y )] = Var X,Y f (X, Y ) + Var Y g(Y ).</formula><p>By Proposition 3, Var nVar M CVD (l)   u can be written as the sum of Var nVar</p><formula xml:id="formula_46">M √ R v∈n P uv h(l) v and</formula><p>Var n R v∈n P uv ∆µ</p><formula xml:id="formula_47">(l) v + v∈n(u) P uv μ(l) v .</formula><p>We refer the first term as the variance from dropout (VD) and the second term as the variance from neighbor sampling (VNS). Ideally, VD should equal to the variance of (P H (l) ) u and VNS should be zero. VNS can be derived by replicating the analysis in Sec. 3.2, and replacing h with µ. Let s</p><formula xml:id="formula_48">(l) v = Var M h (l) v = Var M h(l) v ,<label>and</label></formula><formula xml:id="formula_49">S (l) u = Var M (P H (l) ) u = v∈n(u) P 2 uv s (l) v , By Proposition 2, VD of CVD (l) u is v∈n(u) P 2 uv Var h(l) v = S (l)</formula><p>u , wich equals with the VD of the exact estimator as desired.</p><p>We summarize the estimators and their variances in Table <ref type="table">2</ref>, where the derivations are in Appendix A. As in Sec. 3.2, VNS of CV and CVD depends on ∆µ v , which converges to zero as the training progresses, while VNS of NS depends on the non-zero µ v . On the other hand, CVD is the only estimator except the exact one that gives correct VD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Preprocessing Strategy</head><p>There are two possible models adopting dropout, l) . The difference is whether the dropout layer is before or after neighbor averaging. <ref type="bibr">Kipf &amp; Welling (2017)</ref> adopt the former one, and we adopt the latter one, while the two models perform similarly in practice, as we shall see in Sec. 6.1. The advantage of the latter model is that we can preprocess U (0) = P H (0) = P X and takes U (0) as the new input. In this way, the actual number of graph convolution layers is reduced by one -the first layer is merely a fully-connected layer instead of a graph convolution one. Since most GCNs only have two graph convolution layers <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b4">Hamilton et al., 2017a)</ref>, this gives a significant reduction of the receptive field size and speeds up the computation. We refer this optimization as the preprocessing strategy.  </p><formula xml:id="formula_50">Z (l+1) = P (M • H (l) )W (l) or Z (l+1) = M • (P H (l) )W (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We examine the variance and convergence of our algorithms empirically on six datasets, including Citeseer, Cora, PubMed and NELL from <ref type="bibr">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type="bibr" target="#b4">Hamilton et al. (2017a)</ref>, with the same train / validation / test splits, as summarized in Table <ref type="table" target="#tab_0">1</ref>. To measure the predictive performance, we report Micro-F1 for the multi-label PPI dataset, and accuracy for all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix E for the details on the architectures. We repeat the convergence experiments 10 times on Citeseer, Cora, PubMed and NELL, and 5 times on Reddit and PPI. The experiments are done on a Titan X (Maxwell) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Impact of Preprocessing</head><p>We first examine the impact of switching the order of dropout and computing neighbor averaging in Sec. 5.3. Let M0 be the Z (l+1) = P (M • H (l) )W (l) model by <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>, and M1 be our model, we compare three settings: M0 and M1 are exact without any neighbor sampling, and M1+PP samples a large number of D (l) = 20 neighbors and preprocesses P H (0) so that the first neighbor averaging is exact. In Table <ref type="table" target="#tab_1">3</ref> we can see that all the three settings performs similarly, i.e., switching the order does not affect the predictive performance. Therefore, we use the fastest M1+PP as the exact baseline in following convergence experiments.</p><formula xml:id="formula_51">Z (l+1) = M • (P H (l) )W (l)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Convergence Results</head><p>Having the M1+PP algorithm as an exact baseline, the next goal is reducing the time complexity per epoch to make it comparable with the time complexity of MLP, by setting D (l) = 2. We cannot set D (l) = 1 because GraphSAGE explicitly need the activation of a node itself besides the average of its neighbors. Four approximate algorithms are included for comparison: (1) NS, which adopts the NS estimator with no preprocessing. (2) NS+PP, which is same with NS but uses preprocessing. (3) CV+PP, which adopts the CV estimator and preprocessing. (4) CVD+PP, which uses the CVD estimator. All the four algorithms have similar low time complexity per epoch with D (l) = 2, while M1+PP takes D (l) = 20. We study how much convergence speed per epoch and model quality do these approximate algorithms sacrifice comparing with the M1+PP baseline.</p><p>We set the dropout rate as zero and plot the training loss with respect to number of epochs as Fig. <ref type="figure" target="#fig_2">2</ref>. We can see that CV+PP can always reach the same training loss with M1+PP, while NS, NS+PP and IS+PP have higher training losses because of their biased gradients. CVD+PP is not included because it is the same with CV+PP when the dropout rate is zero.  to a local optimum of Exact, regardless of D (l) .</p><p>Next, we turn dropout on and compare the validating accuracy obtained by the model trained with different algorithms at each epoch. Regardless of the training algorithm, the exact algorithm is used for computing predictions on the validating set. The result is shown in Fig. <ref type="figure">3</ref>. We find that when dropout is present, CVD+PP is the only algorithm that can reach comparable validation accuracy with the exact algorithm on all datasets. Furthermore, its convergence speed with respect to the number of epochs is comparable with M1+PP, implying almost no loss of the convergence speed despite its D (l) is 10 times smaller. This is already the best we can expect -comparable time complexity with MLP, yet similar model quality with GCN. CVD+PP performs much better than M1+PP on the PubMed dataset, we suspect it finds a better local optimum. Meanwhile, the simpler CV+PP also reaches a comparable accuracy with M1+PP for all datasets except PPI. IS+PP works worse than NS+PP on the Reddit and PPI datasets, perhaps because sometimes nodes can have no neighbor selected, as we mentioned in Sec. 2.4. Our accuracy result for IS+PP can match the result reported by <ref type="bibr" target="#b2">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type="bibr" target="#b4">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing technique in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Further Analysis on Time Complexity, Testing Accuracy and Variance</head><p>Table <ref type="table" target="#tab_3">4</ref> reports the average number of epochs and time to reach a given 96% validation accuracy on the largest Reddit dataset. Sparse and dense computations are defined in Sec. 3.4. We found that CVD+PP is about 7 times faster than M1+PP due to the significantly reduced receptive field size. Meanwhile, NS and IS+PP does not converge to the given accuracy. We compare the quality of the predictions made by different algorithms, using the same model trained with M1+PP in Fig. <ref type="figure" target="#fig_3">4</ref>. As Theorem 1 states, CV reaches the same testing accuracy as the exact algorithm, while NS and NS+PP perform much worse. Finally, we compare the average bias and variance of the gradients per dimension for first layer weights relative to the magnitude of the weights in Fig. <ref type="figure" target="#fig_4">5</ref>. For models without dropout, the gradient of CV+PP is almost unbiased. For models with dropout, the bias and variance of CV+PP and CVD+PP are usually smaller than NS and NS+PP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>The large receptive field size of GCN hinders its fast stochastic training. In this paper, we present control variate based algorithms to reduce the receptive field size. Our algorithms can achieve comparable convergence speed with the exact algorithm even the neighbor sampling size D (l) = 2, so that the per-epoch cost of training GCN is comparable with training MLPs. We also present strong theoretical guarantees, including exact prediction and the convergence to a local optimum. Our code is released at https: //github.com/thu-ml/stochastic_gcn.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(l)v and h(l) v , and we only apply Monte-Carlo approximation on the ∆h (l) v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>batch GCN, the time complexity is O(EK) for SPMM and O(V KA) for GEMM. For our stochastic training algorithm with control variates, the dominant SPMM computation is the average of neighbor history P H(0) for the nodes in r (1) , whose size is O(|V B | L l=2 D (l) ), and each node costs O(DK), where D is the average node degree. Therefore, the time complexity of SPMM is approximately O(EK L l=2 D (l) ) per epoch. The dominant GEMM computation is the first fully-connected layer on all the nodes in r (1) , whose time complexity is O(V KA L l=2 D (l) ) per epoch. Both time complexities are L l=2 D (l) times higher than batch GCN, where L l=2 D (l) = 2 if we sample 2 neighbors per node and there are 2 GCN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of training loss with respect to number of epochs without dropout. The CV+PP curve overlaps with the Exact curve in the first four datasets. The training loss of NS and IS+PP are not shown on some datasets because they are too high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of the accuracy of different testing algorithms. The y-axis is Micro-F1 for PPI and accuracy otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Bias and standard deviation of the gradient for different algorithms during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Two-layer graph convolutional networks, and the receptive field of a single vertex. Number of vertexes, edges, and average number of 1-hop and 2-hop neighbors per node for each dataset. Undirected edges are counted twice and self-loops are counted once.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Latest activation</cell></row><row><cell>Layer 2</cell><cell></cell><cell></cell><cell>Layer 2</cell><cell></cell><cell>Layer 2</cell><cell></cell><cell>H</cell><cell>(2)</cell><cell>(2) </cell><cell>Historical activation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GraphConv</cell><cell>GraphConv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dropout</cell></row><row><cell>Layer 1</cell><cell></cell><cell></cell><cell>Layer 1</cell><cell></cell><cell>Layer 1</cell><cell></cell><cell>H</cell><cell>( 1 )</cell><cell>( 1 ) </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GraphConv</cell><cell>GraphConv</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell>Input</cell><cell></cell><cell>Input</cell><cell></cell><cell>Dropout</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell cols="2">(a) Exact</cell><cell></cell><cell cols="3">(b) Neighbour sampling</cell><cell>(c) Control variate</cell><cell>(d) CVD network</cell></row><row><cell cols="3">Figure 1. Dataset V</cell><cell>E</cell><cell cols="2">Degree Degree 2</cell><cell></cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell cols="2">12,431</cell><cell>4</cell><cell>15</cell><cell></cell></row><row><cell>Cora</cell><cell>2,708</cell><cell cols="2">13,264</cell><cell>5</cell><cell>37</cell><cell></cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell cols="2">108,365</cell><cell>6</cell><cell>60</cell><cell></cell></row><row><cell>NELL</cell><cell>65,755</cell><cell cols="2">318,135</cell><cell>5</cell><cell>1,597</cell><cell></cell></row><row><cell>PPI</cell><cell>14,755</cell><cell cols="2">458,973</cell><cell>31</cell><cell>970</cell><cell></cell></row><row><cell>Reddit</cell><cell cols="3">232,965 23,446,803</cell><cell>101</cell><cell>10,858</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Testing accuracy of different algorithms and models after fixed number of epochs. Our implementation does not support M0 on NELL so the result is not reported.</figDesc><table><row><cell>Dataset</cell><cell>M0</cell><cell>M1</cell><cell>M1+PP</cell></row><row><cell>Citeseer</cell><cell>70.8 ± .1</cell><cell>70.9 ± .2</cell><cell>70.9 ± .2</cell></row><row><cell>Cora</cell><cell>81.7 ± .5</cell><cell>82.0 ± .8</cell><cell>81.9 ± .7</cell></row><row><cell cols="2">PubMed 79.0 ± .4</cell><cell>78.7 ± .3</cell><cell>78.9 ± .5</cell></row><row><cell>NELL</cell><cell>-</cell><cell cols="2">64.9 ± 1.7 64.2 ± 4.6</cell></row><row><cell>PPI</cell><cell cols="3">97.9 ± .04 97.8 ± .05 97.6 ± .09</cell></row><row><cell>Reddit</cell><cell cols="3">96.2 ± .04 96.3 ± .07 96.3 ± .04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The results matches the conclusion of Theorem 2, which states that training with the CV estimator converges Time complexity comparison of different algorithms on the Reddit dataset.</figDesc><table><row><cell>Alg.</cell><cell>Valid. acc.</cell><cell>Epochs</cell><cell>Time (s)</cell></row><row><cell>M1+PP</cell><cell>96.0</cell><cell cols="2">4.8 ± .7 252 ± 37</cell></row><row><cell>NS</cell><cell>94.4 ± .01</cell><cell>100</cell><cell>445 ± 14</cell></row><row><cell>NS+PP</cell><cell>96.0</cell><cell cols="2">39.8 ± 11 161 ± 47</cell></row><row><cell>IS+PP</cell><cell>95.8 ± .1</cell><cell>50</cell><cell>251 ± 6</cell></row><row><cell>CV+PP</cell><cell>96.0</cell><cell>7.6 ± 1.6</cell><cell>39 ± 8</cell></row><row><cell>CVD+PP</cell><cell>96.0</cell><cell>6.8 ± 1.3</cell><cell>37 ± 7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Shuyu Cheng for his help in proofreading. This work was supported by NSFC Projects (Nos. 61620106010,  61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, Siemens and Intel. L.S. was also supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic simulation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">316</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
