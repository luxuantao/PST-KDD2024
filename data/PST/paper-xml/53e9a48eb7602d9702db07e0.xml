<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding bag-of-words model: a statistical framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-08-28">28 August 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
							<email>zhangyin@lamda.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>rongjin@cse.msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
							<email>zhouzh@lamda.nju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding bag-of-words model: a statistical framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-08-28">28 August 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">11F6334A2DB61613B06597016592531A</idno>
					<idno type="DOI">10.1007/s13042-010-0001-0</idno>
					<note type="submission">Received: 27 February 2010 / Accepted: 2 July 2010 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object recognition</term>
					<term>Bag of words model</term>
					<term>Rademacher complexity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The bag-of-words model is one of the most popular representation methods for object categorization.</p><p>The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-ofwords representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by the success of text categorization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, a bag-of-words representation becomes one of the most popular methods for representing image content and has been successfully applied to object categorization. In a typical bag-of-words representation, ''interesting'' local patches are first identified from an image, either by densely sampling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> or by a interest point detector <ref type="bibr" target="#b8">[9]</ref>. These local patches, represented by vectors in a high dimensional space <ref type="bibr" target="#b8">[9]</ref>, are often referred to as the key points.</p><p>To efficiently handle these key points, the key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. This vector quantization procedure allows us to represent each image by a histogram of the visual words, which is often referred to as the bag-of-words representation, and consequently converts the object categorization problem into a text categorization problem. A clustering procedure (e.g., K-means) is often applied to group key points from all the training images into a large number of clusters, with the center of each cluster corresponding to a different visual word. Studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> have shown promising performance of bag-of-words representation in object categorization. Various methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> have been proposed for the visual vocabulary construction to improve both the computational efficiency and the classification accuracy of object categorization. However, to the best of our knowledge, there is no theoretical analysis on the statistical properties of vector quantization for object categorization.</p><p>In this paper, we present a statistical framework which generalizes the bag-of-words representation and aim to provide a theoretical understanding for vector quantization and its effect on object categorization from the viewpoint of statistical consistency. In particular, we view 1. each visual word as a quantization function f k ðxÞ that is randomly sampled from a class of functions F by an unknown distribution P F ; and 2. each key point of an image as a random sample from an unknown distribution q i ðxÞ:</p><p>The above statistical description of key points and visual words allows us to interpret the similarity between two images in bag-of-words representation, the key quantity in object categorization, as an empirical expectation over the distributions q i ðxÞ and P F : Based on the proposed statistical framework, we present two random algorithms for vector quantization, one based on the empirical distribution and the other based on kernel density estimation. We show that both random algorithms for vector quantization are statistically consistent in estimating the similarity between two images. Our empirical study with object recognition also verifies that the two proposed algorithms (I) yield recognition accuracy that is comparable to the clustering based bag-of-words representation, and (II) are resilient to the number of visual words when the number of training examples is limited. The success of the two simple algorithms validates the proposed statistical framework for vector quantization. The rest of this paper is organized as follows. Section 2 presents the overview of existing approaches for key point quantization that were used by object recognition. Section 3 presents a statistical framework that generalizes the classical bag-of-words representation, and two random algorithms for vector quantization based on the proposed framework. We show that both algorithms are statistically consistent in estimating the similarity between two images. Empirical study with object recognition reported in Sect. <ref type="bibr" target="#b3">4</ref> shows encouraging results of the proposed algorithms for vector quantization, which in return validates the proposed statistical framework for the bag-of-words representation. Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In object recognition and texture analysis, a number of algorithms have been proposed for key point quantization. Among them, K-means is probably the most popular one. To reduce the high computational cost of K-means, hierarchical K-means is proposed in <ref type="bibr" target="#b12">[13]</ref> for more efficient vector quantization. In <ref type="bibr" target="#b24">[25]</ref>, a supervised learning algorithm is proposed to reduce the visual vocabulary that is initially obtained by K-means, into a more descriptive and compact one. Farquhar et al. <ref type="bibr" target="#b4">[5]</ref> model the problem as Gaussian mixture model where each visual words corresponds to a Gaussian component and use the maximum a posterior (MAP) approach to learn the parameter. A method based on mean-shift is proposed in <ref type="bibr" target="#b6">[7]</ref> for vector quantization to resolve the problem that K-means tends to 'starve' medium density regions in feature space and each key point is allocated to the first visual word similar to it. Moosmann et al. <ref type="bibr" target="#b11">[12]</ref> use extremely randomized clustering forests to efficiently generate a highly discriminative coding of visual words. To minimize the loss of information in vector quantization, Lazebnik and Raginsky <ref type="bibr" target="#b7">[8]</ref> try to seek a compressed representation of vectors that preserve the sufficient statistics of features. In <ref type="bibr" target="#b15">[16]</ref>, images are characterized using a set of category-specific histograms describing whether the content can best be modeled by the universal vocabulary or the specific vocabulary. Tuytelaars and Schmid <ref type="bibr" target="#b20">[21]</ref> propose a quantization method that discretizes a feature space by a regular lattice. van Gemert et al. <ref type="bibr" target="#b21">[22]</ref> use kernel density estimation to avoid the problem of 'codeword uncertainty' and 'codeword plausibility'.</p><p>Although many studies have shown encouraging results of the bag-of-words representation for object categorization, none of them provide statistical consistency analysis, which reveals the asymptotic behavior of the bag-of-words model for object recognition. Unlike the existing statistical approaches for key point quantization that are designed to reduce the training error, the proposed framework generalizes the bag-of-words model by the statistical expectation, making it possible to analyze the statistical consistency of the bag-of-words model. Finally, we would like to point out that although several randomized approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed for key point quantization, none of them provides theoretical analysis on statistical consistency. In contrast, we present not only the theoretic results for the two proposed random algorithms for vector quantization, but also the results of the empirical study with object recognition that support the theoretic claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A statistical framework for bag-of-words representation</head><p>In this section, we first present a statistical framework for the bag-of-words representation in object categorization, followed by two random algorithms that are derived from the proposed framework. The analysis of statistical consistency is also presented for the two proposed algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A statistical framework</head><p>We consider the bag-of-words representation for images, with each image being represented by a collection of local descriptors. We denote by N the number of training images, and by X i ¼ ðx 1 i ; . . .; x n i i Þ the collection of key points used to represent image I i where x l i 2 X; l ¼ 1; . . .; n i is a key point in feature space X : To facilitate statistical analysis, we assume that each key point x l i in X i is randomly drawn from an unknown distribution q i ðxÞ associated with image I i :</p><p>The key idea of the bag-of-words representation is to quantize each key point into one of the visual words that are often derived by clustering. We generalize this idea of quantization by viewing the mapping to a visual word v k 2 X as a quantization function f k ðxÞ : X 7 ! ½0; 1: Due to the uncertainty in constructing the vocabulary, we assume that the quantization function f k ðxÞ is randomly drawn from a class of functions, denoted by F ; via a unknown distribution P F : To capture the behavior of quantization, we design the function class F as follows F ¼ ff ðx; vÞjf ðx; vÞ ¼ Iðkx À vk qÞ; v 2 Xg ð1Þ</p><p>where indicator function I(z) outputs 1 when z is true, or 0 otherwise. In the above definition, each quantization function f ðx; vÞ is essentially a ball of radius q centered at v: It outputs 1 when a point x is within the ball, and 0 if x is outside the ball. This definition of quantization function is clearly related to the vector quantization by data clustering. Based on the above statistical interpretation of key points and quantization functions, we can now provide a statistical description for the histogram of visual words, which is the key of bag-of-words representation. Let ĥk i denotes the normalized number of key points in image I i that are mapped to visual word v k : Given m visual words, or m quantization functions ff k ðxÞg m k¼1 that are sampled from F ; ĥk i is computed as</p><formula xml:id="formula_0">ĥk i ¼ 1 n i X n i j¼1 f k x l i À Á ¼ Êi ½f k ðxÞ<label>ð2Þ</label></formula><p>where Êi ½f k ðxÞ stands for the empirical expectation of function f k ðxÞ based on the samples x 1 i ; . . .; x n i i : We can generalize the above computation by replacing the empirical expectation Êi ½f k ðxÞ with an expectation over the true distribution q i ðxÞ; i.e.,</p><formula xml:id="formula_1">h k i ¼ E i ½f k ðxÞ ¼ Z d xq i ðxÞf k ðxÞ:<label>ð3Þ</label></formula><p>The bag-of-words representation for image I i is expressed by vector h i ¼ ðh 1 i ; . . .; h m i Þ: In the next step, we analyze the pairwise similarity between two images. It is important to note that the pairwise similarity plays a critical role in any pattern classification problems including object categorization. According to the learning theory <ref type="bibr" target="#b17">[18]</ref>, it is the pairwise similarity, not the vector representation of images, that decides the classification performance. Using the vector representation h i and h j ; the similarity between two images I i and I j ; denoted by " s ij ; is computed as</p><formula xml:id="formula_2">" s ij ¼ 1 m h T i h j ¼ 1 m X m k¼1 E i ½f k ðxÞE j ½f k ðxÞ<label>ð4Þ</label></formula><p>Similar to the previous analysis, the summation in the above expression can be viewed as an empirical expectation over the sampled quantization functions f k ðxÞ; k ¼ 1; . . .; m: We thus generalize the definition of pairwise similarity in Eq. 4 by replacing the empirical expectation with the true expectation, and obtain the true similarity between two images I i and I j as</p><formula xml:id="formula_3">s ij ¼ E f $ P F E i ½f ðxÞE j ½f ðxÞ Â Ã<label>ð5Þ</label></formula><p>According to the definition in Eq. 1, each quantization function is parameterized by a center v: Thus, to define P F ; it suffices to define a distribution for the center v; denoted by qðvÞ: Thus, Eq. 5 can be expressed as</p><formula xml:id="formula_4">s ij ¼ E v E i ½f ðxÞE j ½f ðxÞ Â Ã :<label>ð6Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Random algorithms for key point quantization and their statistical consistency</head><p>We emphasize that the pairwise similarity in Eq. 6 can not be computed directly. This is because both distributions q i ðxÞ and qðvÞ are unknown, which makes it intractable to compute E i ½Á and E v ½Á: In real applications, approximations are needed. In this section, we study how approximations will affect the estimation of pairwise similarity. In particular, given the pairwise similarity estimated by different kinds of approximated distributions, we aim to bound its difference to the underlying true similarity. To simplify our analysis, we assume that each image has at least n key points. By assuming that the key points in all the images are sampled from qðvÞ; we have an empirical distribution for qðvÞ; i.e.,</p><formula xml:id="formula_5">qðvÞ ¼ 1 P N i¼1 n i X N i¼1 X n i l¼1 d v À x l i À Á<label>ð7Þ</label></formula><p>where dðxÞ is a Dirac delta function that R dðxÞ dx ¼ 1 and dðxÞ ¼ 0 for x 6 ¼ 0: Direct estimation of pairwise similarities using the above empirical distribution is computationally expensive, because the number of key points in all images can be very large. In the bag-of-words model, m visual words are used as prototypes for the key points in all the images. Let v 1 ; . . .; v m be the m visual words randomly sampled from the key points in all the images. The empirical distribution qðvÞ is</p><formula xml:id="formula_6">qðvÞ ¼ 1 m X m k¼1 dðv À v k Þ ð<label>8Þ</label></formula><p>In the next step, we aim to approximate the unknown distribution q i ðxÞ in two different ways, and show the statistical consistency for each approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Empirically estimated density function for q i ðxÞ</head><p>First we approximate q i ðxÞ by the empirical distribution qi ðxÞ defined as follows</p><formula xml:id="formula_7">qi ðxÞ ¼ 1 n i X n i l¼1 dðx À x l i Þ ð<label>9Þ</label></formula><p>Given the approximations for distribution q i ðxÞ and qðvÞ; we can now compute the approximation of the pairwise similarity s ij defined in Eq. 6. For Eq. 9, the pairwise similarity, denoted by ŝij ; is computed as</p><formula xml:id="formula_8">ŝij ¼ Êv Êi ½f ðxÞ Êj ½f ðxÞ Â Ã ¼ 1 m X m k¼1 1 n i X n i l¼1 I x l i À v k q À Á ! Â 1 n j X n j l¼1 Ið x l j À v k qÞ !<label>ð10Þ</label></formula><p>To show the statistical consistency of ŝij ; we need to bound js ij À ŝij j: Since there are two approximate distribution used in our estimation, we divide our analysis into two steps. First, we measure j" s ij À s ij j; i.e., the difference in similarity caused by the approximate distribution for P F : Next, we measure jŝ ij À " s ij j; i.e., the difference caused by using the approximate distribution for q i ðxÞ:</p><p>The overall difference js ij À ŝij j is bounded by the sum of the two difference.</p><p>We first state the McDiarmid inequality <ref type="bibr" target="#b10">[11]</ref>, which is used throughout our analysis.</p><formula xml:id="formula_9">Theorem 1 (McDiarmid Inequality) Given independent random variables v 1 , v 2 ,..., v n , v 0 i [ V, and a function f : V n 7 ! R satisfying sup v 1 ;v 2 ;...;v n ;v 0 i 2V jf ðvÞ À f ðv 0 Þj c i<label>ð11Þ</label></formula><formula xml:id="formula_10">where v ¼ ðv 1 ; v 2 ; . . .; v n Þ and v 0 ¼ ðv 1 ; v 2 ; . . .; v iÀ1 ; v 0 i ; v iþ1 ; . . .; v n Þ; then the following statement holds Pr jf ðvÞ À Eðf ðvÞÞj ! ð Þ 2 exp À 2 2 P n i¼1 c 2 i<label>ð12Þ</label></formula><p>Using the McDiarmid inequality, we have the following theorem which bounds j" s ij À s ij j: Theorem 2 Assuming f k ðxÞ; k ¼ 1; . . .; m are randomly drawn from class F according to an unknown distribution. And further assuming that any function in F is universally bounded between 0 and 1. With probability 1 -d, the following inequality holds for any two training images I i and</p><formula xml:id="formula_11">I j j" s ij À s ij j ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2m ln 2 d r<label>ð13Þ</label></formula><p>Proof For any f 2 F; we have 0 E i ½f ðxÞE j ½f ðxÞ 1: Thus, for any k, c k B 1/m. By setting</p><formula xml:id="formula_12">d ¼ 2 exp À2m 2 À Á ; or ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2m ln 2 d r ;<label>ð14Þ</label></formula><p>we have Prðj"</p><formula xml:id="formula_13">s ij À s ij j Þ ! 1 À d:</formula><p>The above theorem indicates that, if we have the true distribution q i ðxÞ of each image I i ; with a large number of sampled quantization functions f k ðxÞ; we have a very good chance to recover the true similarity s ij with a small error. The next theorem bounds jŝ ij À s ij j: Theorem 3 Assuming each image has at least n randomly sampled key points. Also assuming that f k ðxÞ; k ¼ 1; . . .; m randomly drawn from an unknown distribution over class F : With probability 1 -d, the following inequality is satisfied for any two images I i and I j</p><formula xml:id="formula_14">jŝ ij À s ij j ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2m ln 2 d r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2n ln 4m 2 d r<label>ð15Þ</label></formula><p>Proof We first need to bound the difference between Êi ½f k ðxÞ and E i ½f k ðxÞ: Since 0 f ðxÞ 1 for any f 2 F;</p><p>using McDimard inequality, we have</p><formula xml:id="formula_15">Pr Êi f k ðxÞ ½ ÀE i f k ðxÞ ½ ! À Á expðÀ2n 2 Þ ð<label>16Þ</label></formula><p>By setting</p><formula xml:id="formula_16">2 expðÀ2n 2 Þ ¼ d 2m 2 ; or ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 2n ln 4m 2 d s</formula><p>with probability 1 -d/2, we have j Êi ½f k ðxÞ À E i ½f k ðxÞj and j Êj ½f k ðxÞ À E j ½f k ðxÞj for all f k ðxÞ m k¼1 simultaneously. As a result, with probability 1 -d/2, for any two image I i and I j ; we have</p><formula xml:id="formula_17">jŝ ij À " s ij j 1 m X m k¼1 j Êk i Êk j À E k i E k j j 1 m X m k¼1 jð Êk i À E k i Þ Êk j j þ jE k i ð Êk j À E k j Þj 1 m X m k¼1 j Êk i À E k i j þ j Êk j À E k j j 2 ¼ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 2n ln 4m 2 d s<label>ð17Þ</label></formula><p>where Êk i stands for Êi ½f k ðxÞ for simplicity. According to Theorem 2, with probability 1 -d/2, we have</p><formula xml:id="formula_18">j" s ij À s ij j ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2m ln 2 d r<label>ð18Þ</label></formula><p>Combining Eqs. 17 and 18, we have the result in the theorem. With probability 1 -d, the following inequality is satisfied</p><formula xml:id="formula_19">jŝ ij À s ij j ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2m ln 2 d r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 2n ln 4m 2 d r<label>ð19Þ</label></formula><p>Remark Theorem 3 reveals an interesting relationship between the estimation error js ij À ŝij j and the number of quantization functions (or the number of visual words). The upper bound in Theorem 3 consists of two terms: the first term decreases at a rate of Oð1= ffiffiffi ffi m p Þ while the second term increases at a rate of Oðln mÞ: When the number of visual words m is small, the first term dominates the upper bound, and therefore increasing m will reduce the difference jŝ ij À s ij j: As m becomes significantly larger than n, the second term will dominate the upper bound, and therefore increasing m will lead to a larger jŝ ij À s ij j: This result appears to be consistent with the observations on the size of the visual vocabulary: a large vocabulary tends to performance well in object categorization; but, too many visual words could deteriorate the classification accuracy.</p><p>Finally, we emphasize that although the idea of vector quantization by randomly sampled centers was already discussed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, to the best of our knowledge, this is the first work that presents its statistical consistency analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Kernel density function estimation for q i ðxÞ</head><p>In this section, we approximate q i ðxÞ by a kernel density estimation. To this end, we assume that the density function q i ðxÞ belongs to a family of smooth functions F D that is defined as follows</p><formula xml:id="formula_20">F D ¼ qðxÞ : X 7 !R þ qðxÞ; qðxÞ h i H j B 2 ; Z qðxÞ dx ¼ 1 &amp; '<label>ð20Þ</label></formula><p>where jðx;</p><formula xml:id="formula_21">x 0 Þ : X Â X7 !R þ is a local kernel function with R jðx; x 0 Þ dx 0 ¼ 1:</formula><p>B controls the functional norm of qðxÞ in the reproducing kernel Hilbert space H j : An example of jðx; x 0 Þ is RBF function, i.e. jðx; x 0 Þ / expðÀkdðx; x 0 Þ 2 Þ; where dðx; x 0 Þ ¼ kx À x 0 k 2 : Then, the distribution q i ðxÞ is approximated by a kernel density estimation qi ðxÞ defined as follows</p><formula xml:id="formula_22">qi ðxÞ ¼ X n i l¼1 a l i j x; x l i À Á ;<label>ð21Þ</label></formula><p>where a i l</p><p>(1 B l B n i ) are the combination weight that satisfy (i) a i l C 0, (ii)</p><formula xml:id="formula_23">P n i l¼1 a l i ¼ 1; and (iii) a i K i a i B B 2 , where K i ¼ ½jðx l i ; x l 0 i Þ n i Ân i :</formula><p>Using the kernel density function, we approximate the pairwise similarity for Eq. 21 as follows</p><formula xml:id="formula_24">sij ¼ Êv Ẽi ½f ðxÞ Ẽj ½f ðxÞ Â Ã ¼ 1 m X m k¼1 X n i l¼1 a l i h x l i ; v k À Á ! X n j l¼1 a l j h x l j ; v k !<label>ð22Þ</label></formula><p>where function hðx; vÞ is defined as</p><formula xml:id="formula_25">hðx; vÞ ¼ Z dz I dðz; vÞ q ð Þ jðx; zÞ ð<label>23Þ</label></formula><p>To bound the difference between sij and s ij , we follow the analysis <ref type="bibr" target="#b18">[19]</ref> by viewing E i ½f ðxÞE j ½f ðxÞ as a mapping, denoted by g :</p><formula xml:id="formula_26">F 7 ! R þ ; i.e., gðf ; q i ; q j Þ ¼ E i ½f ðxÞE j ½f ðxÞ<label>ð24Þ</label></formula><p>The domain for function g, denoted by G; is defined as</p><formula xml:id="formula_27">G ¼ g : F 7 ! R þ 9q i ; q j 2 F D s.t. gðf Þ ¼ E i ½f ðxÞE j ½f ðxÞ È É<label>ð25Þ</label></formula><p>To bound the complexity of a class of functions, we introduce the concept of Randemacher complexity <ref type="bibr" target="#b1">[2]</ref>:</p><p>Definition 1 (Randemacher complexity) Suppose x 1 ,..., x n are sampled from a set X with i.i.d. Let F be a class of functions mapping from X to R: The Randemacher complexity of F is defined as</p><formula xml:id="formula_28">R n ðF Þ ¼ E x 1 ;...;x n ;r sup f 2F 2 n X n i¼1 r i f ðx i Þ !<label>ð26Þ</label></formula><p>where r i is independent uniform ± 1-valued random variables.</p><p>Assuming at least n key points are randomly sampled from each image, we have the following lemmas that bounds the complexity of domain G : Lemma 1 The Rademacher complexity of function class G; denoted by R m ðGÞ; is bounded as</p><formula xml:id="formula_29">R m ðGÞ 2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p<label>ð27Þ</label></formula><p>where C j ¼ max x;z ffiffiffiffiffiffiffiffiffiffiffiffiffi jðx; zÞ p Proof Denote F = {f 1 ,..., f m }, according to the definition, we have</p><formula xml:id="formula_30">R m ðGÞ ¼ E r;F sup g2G 2 m X m k¼1 r k gðf k Þ " # ¼ E F E r sup g2G 2 m X m k¼1 r k gðf k Þ " F # " # ¼ E F E r sup q i ;q j 2F D 2 m X m k¼1 r k E i ½f k E j ½f k F " # " # E F E r sup kx i k B 2 m X m k¼1 r k E i ½f k F " # " # ¼ 2 m E F E r sup kx i k B x i ; X m k¼1 r k U k * + F " # " #</formula><p>where</p><formula xml:id="formula_31">U k ¼ h/ 1 ðÁÞ; f k ðÁÞi; h/ 2 ðÁÞ; f k ðÁÞi; . . . ð Þ</formula><p>and / k ðxÞ is an eigen function</p><formula xml:id="formula_32">of jðx; x 0 Þ 2B m E F E r X m k¼1 r k U k F " # " # ¼ 2B m E F E r X k;t r k r t U k ; U t h i ! 1 2 2 4 F 3 5 2 4 3 5 2B m E F X k;t E r r k r t U k ; U t ðxÞ h i j F ½ ! 1 2 2 4 3 5 ¼ 2B m E F X k E r r 2 k U k ; U k h i F Â Ã ! 1 2 2 4 3 5 ¼ 2B m E F X k U k ; U k h i ! 1 2 2 4 3 5 ¼ 2B m E F X k Z dz dxf k ðxÞf k ðzÞjðx; zÞ ! 1 2 2 4 3<label>5</label></formula><formula xml:id="formula_33">2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p<label>ð28Þ</label></formula><p>where the first inequality is because E j ½f k 1; the second inequality is from Cauchy's inequality, the third and fourth inequalities are from Jensen's inequality. The last equality follows</p><formula xml:id="formula_34">hU k ; U k i ¼ X i / i ðÁÞ; f k ðÁÞ h i 2 ¼ Z dz dxf k ðxÞf k ðzÞjðx; zÞ<label>ð29Þ</label></formula><p>From <ref type="bibr" target="#b1">[2]</ref>, we have the following lemmas:</p><p>Lemma 2 (Theorem 12 in <ref type="bibr" target="#b1">[2]</ref>) For 1 B q \ ?, let L ¼ fjf À hj q : f 2 F g; wherehand kf À hk 1 is uniformly bounded. We have</p><formula xml:id="formula_35">R n ðLÞ 2qkf À hk 1 R n ðF Þ þ khk 1 ffiffi ffi n p<label>ð30Þ</label></formula><p>Lemma 3 (Theorem 8 in <ref type="bibr" target="#b1">[2]</ref>) With probability 1 -d the following inequality holds</p><formula xml:id="formula_36">E/ðY; f ðXÞÞ Ên /ðY; f ðXÞÞ þ R n ð/ F Þ þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 8 lnð2=dÞ n r<label>ð31Þ</label></formula><p>where /(x, y) is the loss function, n is the number of samples and / F ¼ fðx; yÞ 7 ! /ðy; f ðxÞÞ À /ðy; 0Þ : f 2 F g:</p><p>Based on the above lemmas, we have the following theorem Theorem 4 Assume that the density function q i ðxÞ; q j ðxÞ 2 F D : Let qi ðxÞ; qj ðxÞ 2 F D be an estimated density function from n sampled key points. We have, with probability 1 -d, the following inequality holds</p><formula xml:id="formula_37">E f ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj Êf ½jgðf ; qi ; qj Þ À gðf ; qi ; qj Þj þ 2 2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p þ 1 ffiffiffi ffi m p þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð8=dÞ 2m r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi lnð8m 2 =dÞ 2n r<label>ð32Þ</label></formula><p>Proof From Lemma 3, with probability 1 -d/2, we have</p><formula xml:id="formula_38">E f ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj Êf ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj þ R m ðjG À gðf ; q i ; q j ÞjÞ þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 8 lnð4=dÞ m r<label>ð33Þ</label></formula><p>Since 0 B g(f; q i , q j ) B 1, using the results in Lemma 1 and 2, we have</p><formula xml:id="formula_39">R m jG À gðf ; q i ; q j Þj À Á 2 R m ðGÞ þ 1 ffiffiffi ffi m p 2 2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p þ 1 ffiffiffi ffi m p<label>ð34Þ</label></formula><p>Hence, we have, with probability 1 -d/2 the following inequality holds</p><formula xml:id="formula_40">E f ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj Êf ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj þ 2 2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p þ 1 ffiffiffi ffi m p þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 8 lnð4=dÞ m r<label>ð35Þ</label></formula><p>Next, we aim to bound Êf ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj: Note that</p><formula xml:id="formula_41">Êf ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj ¼ 1 m X m k¼1 jgðf k ; qi ; qj Þ À gðf k ; q i ; q j Þj 1 m X m k¼1 jgðf k ; qi ; qj Þ À gðf k ; qi ; qj Þj þ jgðf k ; qi ; qj Þ À Àgðf k ; q i ; q j Þj Á<label>ð36Þ</label></formula><p>Using the same logistics in the proof of Theorem 3, we have, with probability 1</p><formula xml:id="formula_42">-d/2 1 m X m k¼1 jgðf k ; qi ; qj Þ À gðf k ; q i ; q j Þj ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð8=dÞ 2m r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi lnð8m 2 =dÞ 2n r<label>ð37Þ</label></formula><p>From the above results, we have, with probability 1 -d/2, the following inequality holds 1 m</p><formula xml:id="formula_43">X m k¼1 jgðf k ; qi ; qj Þ À gðf k ; q i ; q j Þj 1 m X m k¼1 jgðf k ; qi ; qj Þ À gðf k ; qi ; qj Þj þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð8=dÞ 2m r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi lnð8m 2 =dÞ 2n r<label>ð38Þ</label></formula><p>Combining the above results together, we have, with probability 1 -d, the following inequality holds E f ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj Êf ½jgðf ; qi</p><formula xml:id="formula_44">; qj Þ À gðf ; qi ; qj Þj þ 2 2BC j E f R dxjf ðxÞj Â Ã ffiffiffi ffi m p þ 1 ffiffiffi ffi m p þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð8=dÞ 2m r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi lnð8m 2 =dÞ 2n r<label>ð39Þ</label></formula><p>In our empirical study, we will use RBF kernel function for jðx; x 0 Þ with a i l = 1/n i . The corollary below shows the bound for this choice of kernel density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 5 When the kernel function</head><formula xml:id="formula_45">jðx; x 0 Þ ¼ ð1=ð2pr 2 ÞÞ d=2 expðÀkx À x 0 k 2 2 =ð2r 2 ÞÞ and a i l = 1/n i , the bound in Theorem 4 becomes E f ½jgðf ; qi ; qj Þ À gðf ; q i ; q j Þj 1=ð2pr 2 Þ À Á d=2 1 À exp Àq 2 =ð2r 2 Þ À Á À Á þ 2 2E f R dxjf ðxÞj Â Ã = ffiffiffiffi n i p þ 1 ffiffiffi ffi m p þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi lnð8=dÞ 2m r þ 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi lnð8m 2 =dÞ 2n r<label>ð40Þ</label></formula><p>Remark Theorem 4 bounds the true expectation of the difference between the similarity estimated by kernel density function and the true similarity. Similar to Theorem 3, this bound also consists of a term decreasing at a rate of Oð1= ffiffiffi ffi m p Þ and a term increasing at a rate of Oðln mÞ: What's more, we can see in order to minimize the true expectation of the difference between the similarity estimated by kernel density function and the true similarity, we need to minimize the empirical expectation of the difference between the similarity estimated by kernel density function and the similarity estimated by empirical density function. If jðx; vÞ decreases exponentially as dðx; vÞ decreases, such as Gaussian kernel, we have hðx; vÞ close to 1 when dðx; vÞ q while hðx; vÞ close to 0 when dðx; vÞ [ q: In such circumstance, setting a i l = 1/n i for all 1 B l B n i is a good choice for the approximation and is also very efficient since we do not need to learn a.</p><p>Note that although the idea of kernel density estimation was already proposed in some studies [e.g., 22], to the best of our knowledge, this is the first work that reveals the statistical consistency of kernel density estimation for the bag-of-words representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical study</head><p>In this empirical study, we aim to verify the proposed framework and the related analysis. To this end, based on the discussion in Sect. 3.2, we present two random algorithms for vector quantization that are shown in Algorithm 1. We refer to the algorithm based on empirical distribution as ''Quantization via Empirical Estimation'', or QEE for short, and to the algorithm based on kernel density estimation as ''Quantization via Kernel Estimation'', or QKE for short. Note that since both vector quantization algorithms do not rely on the clustering algorithms to identify visual words, they are in general computationally more efficient. In addition, both algorithms have error bounds decreases at the rate of Oð1= ffiffiffi ffi m p Þ when the number of key points n is large, indicating that they are robust to the number of visual words m. We emphasize that although similar random algorithms for vector quantization have been discussed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, the purpose of this empirical study is to verify that • simple random algorithms deliver similar performance of object recognition as the clustering based algorithm, and • the random algorithms are robust to the number of visual words, as predicted by the statistical consistency analysis.</p><p>Finally in the implementation of QKE, to efficiently calculate h-function, we approximate it as <ref type="bibr" target="#b0">[1]</ref> h</p><formula xml:id="formula_46">% 2ð d À qÞ 2 À 1 4 ffiffiffi p p ð d À qÞ 3 expð d À qÞ 2 À 2ð d þ qÞ 2 À 1 4 ffiffiffi p p ð d þ qÞ 3 expð d þ qÞ 2<label>ð41Þ</label></formula><p>where d ¼ d=r; q ¼ q=r and r is the width of the Gaussian kernel.</p><p>Two data sets are used in our study: PASCAL VOC Challenge 2006 data set <ref type="bibr" target="#b3">[4]</ref> and Graz02 data set <ref type="bibr" target="#b14">[15]</ref>. PASCAL06 contains 5,304 images from 10 classes. We randomly select 100 images for training and 500 for testing. The Graz02 data set contains 365 bike images, 420 car images, 311 people images and 380 background images. We randomly select 100 images from each class for training, and use the remaining for testing. By using a relatively small number of examples for training, we are able to examine the sensitivity of a vector quantization algorithm to the number of visual words. On average 1,000 key points are extracted from each image, and each key point is represented by the SIFT local descriptor <ref type="bibr" target="#b22">[23]</ref>. For PASCAL06 data set, the binary classification performance for each object class is measured by the area under the ROC curve (AUC). For Graz02 data set, the binary classification performance for each object class is measured by the accuracy. Results averaged over ten random trials are reported.</p><p>We compare three vector quantization methods: K-means, QEE and QKE. Note that we do not include more advanced algorithms for vector quantization in our study because the objective of this study is to validate the proposed statistical framework for bag-of-words representation and the analysis on statistical consistency. Threshold q used by quantization functions f ðxÞ is set as q ¼ 0:5 Â " d;</p><p>where " d is the average distance between all the key points and the randomly selected centers. A RBF kernel is used in QKE with the kernel width r is set as 0:75 " d according to our experience. Binary linear SVM is used for each classification problem. To examine the sensitivity to the number of visual words, for both data sets, we varied the number of visual words from 10 to 10,000, as shown in Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_0">2</ref>.</p><p>First, we observe that the proposed algorithms for vector quantization yield comparable if not better performance than the K-means clustering algorithm. This confirms the proposed statistical framework for key point quantization is effective. Second, we observe that the clustering based approach for vector quantization tends to perform worse, sometimes very significantly, when the number of visual words is large. We attribute this instability to the fact that K-means requires each interest point belongs to exactly one visual word. If the number of clusters is not appropriate, for example, too large compared to the number of instances, two relevant key points may be separated into different clusters although they are both very near to the boundary. It will lead to a poor estimation of pairwise similarity. The problem of ''hard assignment'' was also observed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. In contrast, for the proposed algorithms, we observe a rather stable improvement as the number of visual words increases, consistent with our analysis in statistical consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bagof-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-of-words representations.</p><p>Bag-of-words representation is a popular approach to object categorization. Despite its success, few studies are devoted to the theoretic analysis of the bag-of-words representation. In this work, we present a statistical framework for key point quantization that generalizes the bag-ofwords model by statistical expectation. We present two random algorithms for vector quantization where the visual words are generated by a statistical process rather than using a clustering algorithm. A theoretical analysis of their statistical consistency is presented. We also verify the efficacy and the robustness of the proposed framework by applying it to object recognition. In the future, we plan to examine the dependence of the proposed algorithms on the threshold q, and extend QKE to weighted kernel density estimation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Comparison of different quantization methods with varying number of visual words on Graz02</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Comparison of different quantization methods with varied number of visual words on PASCAL06</figDesc><table><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>0.85</cell><cell>0.74 0.76</cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell>0.8</cell><cell>0.72</cell></row><row><cell>AUC</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.7</cell><cell></cell><cell>AUC</cell><cell>0.75</cell><cell>AUC</cell><cell>0.7 0.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.66</cell></row><row><cell></cell><cell>0.65</cell><cell cols="2">K-means</cell><cell></cell><cell></cell><cell>0.65</cell><cell cols="2">K-means</cell><cell>0.7</cell><cell>K-means</cell><cell>0.64</cell><cell>K-means</cell></row><row><cell></cell><cell>0.6</cell><cell cols="2">QEE QKE</cell><cell></cell><cell></cell><cell>0.6</cell><cell cols="2">QEE QKE</cell><cell>0.65</cell><cell>QEE QKE</cell><cell>0.6 0.62</cell><cell>QEE QKE</cell></row><row><cell></cell><cell></cell><cell>10 20</cell><cell cols="2">50 100 200 500 1,000 2,000 5,000 10,000</cell><cell></cell><cell cols="2">10 20</cell><cell>50 100 200 500 1,000 2,000 5,000 10,000</cell><cell>10 20</cell><cell>50 100 200 500 1,000 2,000 5,000 10,000</cell><cell>10 20</cell><cell>50 100 200 500 1,000 2,000 5,000 10,000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">No. of visual words</cell><cell></cell><cell></cell><cell></cell><cell>No. of visual words</cell><cell>No. of visual words</cell><cell>No. of visual words</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">bicycle</cell><cell></cell><cell></cell><cell></cell><cell>bus</cell><cell>car</cell><cell>cat</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.64 0.66</cell><cell>0.75</cell></row><row><cell>AUC</cell><cell>0.7 0.75</cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.68 0.62 0.64 0.66</cell><cell></cell><cell>AUC</cell><cell>0.62 0.56 0.58 0.6</cell><cell>AUC</cell><cell>0.7 0.65</cell></row><row><cell></cell><cell>0.65</cell><cell cols="2">K-means</cell><cell></cell><cell></cell><cell>0.6</cell><cell cols="2">K-means</cell><cell>0.54</cell><cell>K-means</cell><cell>0.6</cell><cell>K-means</cell></row><row><cell></cell><cell>0.6</cell><cell cols="2">QEE QKE</cell><cell></cell><cell></cell><cell>0.56 0.58</cell><cell cols="2">QEE QKE</cell><cell>0.5 0.52</cell><cell>QEE QKE</cell><cell>0.55</cell><cell>QEE QKE</cell></row><row><cell></cell><cell></cell><cell>10 20</cell><cell cols="2">50 100 200 500 1,000 2,000 5,000 10,000</cell><cell></cell><cell cols="2">10 20</cell><cell>50 100 200 500 1,0002,000 5,000 10,000</cell><cell>10 20</cell><cell>50 100 200 500 1,000 2,000 5,000 10,000</cell><cell>10 20</cell><cell>50 100 200 500 1,0002,000 5,000 10,000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">No. of visual words</cell><cell></cell><cell></cell><cell></cell><cell>No. of visual words</cell><cell>No. of visual words</cell><cell>No. of visual words</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dog</cell><cell>horse</cell><cell>motor</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell>0.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.56 0.58</cell><cell></cell><cell></cell><cell>AUC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell><cell cols="3">K-means</cell><cell>0.7</cell><cell>K-means</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.52</cell><cell cols="2">QEE</cell><cell>QEE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">QKE</cell><cell>QKE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>0.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 20</cell><cell cols="2">50 100 200 500 1,000 2,000 5,000 10,000</cell><cell>10 20</cell><cell>50 100 200 500 1,000 2,000 5,000 10,000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">No. of visual words</cell><cell>No. of visual words</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>person</cell><cell>sheep</cell></row><row><cell cols="3">0.4 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58 0.6 Fig. 1 10 Accuracy 0.42</cell><cell>20</cell><cell>50 100 200</cell><cell></cell><cell cols="3">500 1,000 2,000 5,000 10,000 K-means QEE QKE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">No. of visual words</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Int. J. Mach. Learn. &amp; Cyber. (2010) 1:43-52</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We want to thank the reviewers for helpful comments and suggestions. This research is partially supported by the National Fundamental Research Program of China (2010CB327903), the Jiangsu 333 High-Level Talent Cultivation Program and the National Science Foundation (IIS-0643494). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of mathematical functions with formulas, graphs, and mathematical tables</title>
		<editor>Abramowitz M, Stegun IA</editor>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on statistical learning in computer vision</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2006/results.pdf" />
		<title level="m">The PASCAL visual object classes challenge 2006 (VOC2006) results</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving &apos;&apos;bag-of-keypoints&apos;&apos; image categorisation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Southampton</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text categorization with suport vector machines: learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European conference on machine learning</title>
		<meeting>the 10th European conference on machine learning<address><addrLine>Chemnitz, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE international conference on computer vision</title>
		<meeting>the 10th IEEE international conference on computer vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="604" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised learning of quantizer codebooks by information loss minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1294" to="1309" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI workshop on learning for text categorization</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Madison, WI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the method of bounded differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdiarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Surveys in combinatorics 1989</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="148" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast discriminative visual codebooks using randomized clustering forests</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scho ¨lkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the IEEE computer society conference on computer vision and pattern recognition<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sampling strategies for bag-offeatures image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European conference on computer vision</title>
		<meeting>the 9th European conference on computer vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="490" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generic object recognition with boosting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="431" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapted vocabularies for generic visual categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bressian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European conference on computer vision</title>
		<meeting>the 9th European conference on computer vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="464" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lost in quantization: improving particular object retrieval in large scale image databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the IEEE computer society conference on computer vision and pattern recognition<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scho ¨lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for probability density estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dolia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on artificial intelligence and statistics</title>
		<meeting>the 11th international conference on artificial intelligence and statistics<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE international conference on computer vision</title>
		<meeting>the 9th IEEE international conference on computer vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector quantizing feature space with a regular lattice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE international conference on computer vision</title>
		<meeting>the 11th IEEE international conference on computer vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel codebooks for scene categorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Awm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European conference on computer vision</title>
		<meeting>the 10th European conference on computer vision<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="696" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Experiments on selection of codebooks for local image feature histograms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference series on visual information systems</title>
		<meeting>the 10th international conference series on visual information systems<address><addrLine>Salerno, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE international conference on computer vision</title>
		<meeting>the 10th IEEE international conference on computer vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
