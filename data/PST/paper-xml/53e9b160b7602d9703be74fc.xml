<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CURLER: Finding and Visualizing Nonlinear Correlation Clusters *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anthony</forename><forename type="middle">K H Tung</forename><surname>Xin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Beng</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chin</forename><surname>Ooi</surname></persName>
							<email>ooibc@comp.nus.edu.sg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CURLER: Finding and Visualizing Nonlinear Correlation Clusters *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">28CEB51BAA59C2286C6D63897230A7B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While much work has been done in finding linear correlation among subsets of features in high-dimensional data, work on detecting nonlinear correlation has been left largely untouched. In this paper, we present an algorithm for finding and visualizing nonlinear correlation clusters in the subspace of high-dimensional databases.</p><p>Unlike the detection of linear correlation in which clusters are of unique orientations, finding nonlinear correlation clusters of varying orientations requires merging clusters of possibly very different orientations. Combined with the fact that spatial proximity must be judged based on a subset of features that are not originally known, deciding which clusters to be merged during the clustering process becomes a challenge. To avoid this problem, we propose a novel concept called co-sharing level which captures both spatial proximity and cluster orientation when judging similarity between clusters. Based on this concept, we develop an algorithm which not only detects nonlinear correlation clusters but also provides a way to visualize them. Experiments on both synthetic and real-life datasets are done to show the effectiveness of our method. * Dedicated to the late Prof. Hongjun Lu, our mentor, colleague and friend who will always be remembered.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, large amounts of high-dimensional data, such as images, handwriting and gene expression profiles have been generated. Analyzing and handling such kinds of data have become an issue of keen interest. Elucidating the patterns hidden in high-dimensional data imposes an even greater challenge on cluster analysis.</p><p>Data objects of high dimensionality are NOT globally correlated in all the features because of the inherent sparsity of the data. Instead, a cluster of data objects may be strongly correlated only in a subset of features. Furthermore, the na- ture of such correlation is usually local to a subset of the data objects, and it is possible for another subset of the objects to be correlated in a different subset of features. Traditional methods of detecting correlations like GDR <ref type="bibr" target="#b21">[20]</ref> and PCA <ref type="bibr" target="#b17">[16]</ref> are not applicable in this case since they can detect only global correlations in whole databases.</p><p>To handle the above problem, several subspace clustering algorithms such as ORCLUS <ref type="bibr" target="#b3">[3]</ref> and 4C <ref type="bibr" target="#b7">[7]</ref> have been proposed to identify local correlation clusters with arbitrary orientations, assuming each cluster has a fixed orientation. They identify clusters of data objects which are linearly correlated in some subset of the features.</p><p>In real-life datasets, correlation between features could however be nonlinear, depending on how the dimensions are normalized and scaled <ref type="bibr" target="#b15">[14]</ref>. For example, physical studies have shown that the pressure, volume and temperature of an ideal gas exhibit nonlinear relationships. In biology, it is also known that the co-expression patterns of genes in a gene network can be nonlinear <ref type="bibr" target="#b13">[12]</ref>. Without any detailed domain knowledge of a dataset, it is difficult to scale and normalize the dataset such that all nonlinear relationships become linear. It is even possible that the scaling and normalization themselves cause linear relationships to become nonlinear in some subset of the features.</p><p>Detecting nonlinear correlation clusters is challenging because the clusters can have both local and global orientations, depending on the size of the neighborhood being considered. As an example, consider Figure <ref type="figure" target="#fig_0">1</ref>, which shows a 2D sinusoidal curve oriented at 45 degrees to the two axes.</p><p>Assuming the objects cluster around the curve, we will be able to detect the global orientation of this cluster if we consider a large neighborhood which is represented by the large circle centered at point p. However, if we take a smaller neighborhood at point q, we will only find the local orientation which can be very different from the global one. Furthermore, the local orientations of two points that are spatially close may not be similar at the same time, as can be seen from the small neighborhoods around q and r.</p><p>We next look at how the presence of local and global orientations may pose problems for existing correlation clustering algorithms like ORCLUS <ref type="bibr" target="#b3">[3]</ref> and 4C <ref type="bibr" target="#b7">[7]</ref>. These algorithms usually work in two steps. First, small clusters called microclusters <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b23">22]</ref> are formed by grouping small number of objects that are near each other. Second, microclusters that are close both in proximity and orientation are merged in a bottom-up fashion to form bigger clusters. With nonlinear correlation clusters, such approaches will encounter two problems:</p><p>1) Determination of Neighborhood Given that the orientation of a microcluster is sensitive to the size of the neighborhood from which its members are drawn, it is difficult to determine a neighborhood size in advance such that both the local and global orientations of the clusters are captured. Combined with the fact that spatial proximity must be judged based on a subset of the features that are not originally known, forming microclusters that capture the orientation of their neighborhood becomes a major challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Judging Similarity between Microclusters</head><p>Since the orientations of two microclusters in close proximity can be very different, judging the similarity between two microclusters becomes non-trivial. Given a pair of microclusters which have high proximity 1 but very different orientations and another pair with similar orientations but low proximity, the order of merging for these two pairs cannot be easily determined. This in turn affects the final clustering result. One way to avoid this problem is to assign different weights to the importance of proximity and orientations, and then compute a combined similarity measure. However, it is not guaranteed that there will always be a unique weight assignment that gives a good global clustering result.</p><p>In this paper, we aim to overcome the above problems in finding nonlinear correlation clusters. Our contributions are as follows:</p><p>1. We highlight the existence of local and global orientations in nonlinear correlation clusters and explain how they pose problems for existing subspace clustering algorithms like ORCLUS <ref type="bibr" target="#b3">[3]</ref> and 4C <ref type="bibr" target="#b7">[7]</ref>, which are designed to find linear correlation clusters.</p><p>2. We design an algorithm called CURLER 2 , for finding and visualizing complex nonlinear correlation clusters. Unlike many existing algorithms which use a bottom-up approach, CURLER adopts an interactive 1 Note that as mentioned earlier, judging proximity by itself is a difficult task since the two microclusters could lie in different subspaces. We assume that the problem is solved here for ease of discussion. 2 CURLER stands for CURve cLustERs detection. top-down approach for finding nonlinear correlation clusters so that both global and local orientations can be detected. A fuzzy clustering algorithm based on Expectation Maximization (EM) <ref type="bibr" target="#b16">[15]</ref> is adopted to form the microclusters so that neighborhoods can be determined naturally and correctly. The algorithm also provides a similarity measure called co-sharing level that avoids the need to judge the importance of proximity and orientation when merging microclusters.</p><p>3. We present extensive experiments to show the efficiency and effectiveness of CURLER.</p><p>The rest of the paper is organized as follows. Related work is reviewed and discussed in Section 2. We formally present our algorithm in detail in Section 3. We discuss our experimental analysis in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Clustering algorithms can be grouped into two large categories: full space clustering, to which most traditional clustering algorithms belong, and subspace clustering.</p><p>The clustering strategies utilized by full space clustering algorithms mainly include partitioning based clustering, which favors spherical clusters such as the k-medoid <ref type="bibr" target="#b18">[17]</ref> family and EM algorithms <ref type="bibr" target="#b16">[15]</ref>; and density-based clustering, represented by DBSCAN <ref type="bibr" target="#b12">[11]</ref>, DBCLASD <ref type="bibr" target="#b24">[23]</ref>, DENCLUE <ref type="bibr" target="#b2">[2]</ref> and the more recent OPTICS <ref type="bibr" target="#b5">[5]</ref>. EM clustering algorithms such as <ref type="bibr" target="#b20">[19]</ref> compute probabilities of cluster memberships for each data object according to certain probability distribution; the aim is to maximize the overall probability of the data. For density-based algorithms, OPTICS is the algorithm most related to our work. OPTICS creates an augmented ordering of the database, thereby representing the density-based clustering structure based on 'coredistance' and 'reachability-distance'. However, OPTICS has little concern for the subspace where clusters exist or the correlation among a subset of features.</p><p>As large amounts of high-dimensional data have resulted from various application domains, researchers argue that it is more meaningful to find clusters in a subset of the features. Several algorithms for subspace clustering have been proposed in recent years. Some subspace clustering algorithms like CLIQUE <ref type="bibr" target="#b4">[4]</ref>, OptiGrid <ref type="bibr" target="#b1">[1]</ref>, ENCLUS <ref type="bibr" target="#b11">[10]</ref>, PROCLUS <ref type="bibr" target="#b10">[9]</ref>, and DOC <ref type="bibr" target="#b19">[18]</ref> only find axis-parallel clusters. More recent algorithms such as ORCLUS <ref type="bibr" target="#b3">[3]</ref> and 4C <ref type="bibr" target="#b7">[7]</ref> can find clusters with arbitrarily oriented principle axes. However, none of them addresses our issue of finding nonlinear correlation clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE CURLER ALGORITHM</head><p>Our algorithm, CURLER, works in an interactive and topdown manner. It consists of the following main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">EM Clustering: A modified expectation-maximization</head><p>subroutine EMCluster is applied to convert the original dataset into a sufficiently large number of refined microclusters with varying orientations. Each microcluster M i is represented by its mean value µ i and covariance matrix Σ i . At the same time, a similarity measure called co-sharing level between each pair of microclusters is computed.</p><p>2. Cluster Expansion: Based on the co-sharing level between the microclusters, a traversal through the microclusters is carried out by repeatedly choosing the nearest microcluster in the co-sharedneighborhood of a currently processed cluster. We denote this subroutine as ExpandCluster.</p><p>3. NNCO plot (Nearest Neighbor Co-sharing Level &amp; Orientation plot): In this step, nearest neighbor co-sharing levels and orientations of the microclusters are visualized in cluster expansion order. This allows us to visually observe the nonlinear correlation cluster structure and the orientations of the microclusters from the NNCO plot.</p><p>4. According to the NNCO plot, users may specify clusters that they are interested in and further explore the local orientations of the clusters with regard to their global orientation.</p><p>In the next sections, we will explain the algorithm in detail and the reasoning behind it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EM-Clustering</head><p>Like k-means, the EM-clustering algorithm is an iterative k-partitioning algorithm which improves the conformability of the data to the cluster model in each iteration and typically converges in a few iterations. It has various attractive characteristics that make it suitable for our purpose. This includes the clustering model it uses, the fact that it is a fuzzy clustering method, and its iterative refinement approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Clustering Model</head><p>In EM-clustering, we adopt a Gaussian mixture model where each microcluster M i is represented by a probability distribution with density parameters, θ i ={µ i , P i }, µ i and P i being the mean vector and covariance matrix of the data objects in M i respectively. Such a representation is sufficient for any arbitrary oriented clusters. Furthermore, the orientation of the represented cluster can be easily computed.</p><p>Banfield and Raftery <ref type="bibr" target="#b16">[15]</ref> proposed a general framework for representing the covariance matrix in terms of its eigenvalue decomposition:</p><formula xml:id="formula_0">Σ i = λ i D i A i D T i ,<label>(1)</label></formula><p>where D i is the orthogonal matrix of eigenvectors, A i is a diagonal matrix whose elements are proportional to the eigenvalues of Σ i , and λ i is a scalar. D i , A i and λ i together determine the geometric features (shape, volume, and orientation respectively) of component θ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Fuzzy Clustering</head><p>Unlike ORCLUS and 4C in which each data object either belongs or not belongs to a microcluster, EM-clustering is a fuzzy clustering method in which each data object has a certain probability of belonging to each microcluster.</p><p>Given a microcluster with density parameters θ k , we compute the probability of x's occurrence given θ k as follows:</p><formula xml:id="formula_1">P R(x|θ i ) = 1 p (2π) d | P i | exp[- 1 2 (x -µ i ) T (Σ i ) -1 (x -µ i )],</formula><p>(2) where x and mean vector µ i are column vectors, |Σ i | is the determinant of Σ i , and (Σ i ) -1 is its inverse matrix. Assuming the number of microclusters is set at k 0 , the probability of x occurrence given the k 0 density distributions will be:</p><formula xml:id="formula_2">P R(x) = k 0 X i=1 W i P R(x|θ i ),<label>(3)</label></formula><p>The coefficient W i (matrix weights) denotes the fraction of the database given microcluster M i . The probability of a data object x belonging to a microcluster with density parameters θ i can then be computed as:</p><formula xml:id="formula_3">P R(θ i |x) = W i P R(x|θ i ) P R(x) . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>There are two reasons for adopting fuzzy clustering to form microclusters. First, fuzzy clustering allows an object to belong to multiple correlation clusters when the microclusters are eventually merged. This is entirely possible in real life datasets. For example, a hospital patient may suffer from two types of disease A and B, and thus his/her clinical data will be similar to other patients of disease A in one subset of features and also similar to patients of disease B in another subset of features. Second, fuzzy clustering allows us to indirectly judge the similarity of two microclusters by looking at the number of objects that are co-shared between them. More specifically, we define the following similarity measure:</p><formula xml:id="formula_5">Definition 3.1. Co-sharing Level</formula><p>The co-sharing level between clusters M i and M j is:</p><formula xml:id="formula_6">coshare(M i , M j ) = X x∈D [P R(M i |x) * P R(M j |x)], (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where x is a data object in the dataset D, P R(M j |x) and P R(M i |x) are the probabilities of object x belonging to microcluster M i and microcluster M j respectively. P R(M j |x) and P R(M i |x) are calculated according to Equations 4 and 2. 2</p><p>Given each data object in the database, we compute the probabilities of the object belonging to both M i and M j at the same time and sum up these probabilities over all the data objects. In this way, the co-sharing level takes both the orientation and spatial distance of two microclusters into account without needing to explicitly determine their importance in computing the similarity. A high co-sharing value between two microclusters indicates that they are very similar while a low co-sharing value indicates otherwise. As an example, consider Figure <ref type="figure" target="#fig_1">2</ref> where two microclusters, M 1 and M 2 are used to capture the bend in a cubic curve. Since M 1 and M 2 are neighboring microclusters, points that overlapped both of them will belong to both the Gaussian distributions and thus these points will increase the co-sharing level between them.</p><p>Note that this similarity measure is important here simply because we are handling nonlinear correlation clusters <ref type="foot" target="#foot_0">3</ref> . For linear correlation algorithms like ORCLUS and 4C, this measure is unnecessary as they can simply not merge two microclusters which are either too far apart or very dissimilar in orientation.</p><p>On the basis of our new co-sharing level, we will define the co-sharedneighborhood and nearest neighbor co-sharing level (NNC) for microclusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.2. Co-sharedneighborhood</head><p>For a microcluster M c , its co-sharedneighborhood refers to all the microclusters whose co-sharing level from M c is no smaller than some non-negative real number :</p><formula xml:id="formula_8">{∀M i | coshare(M c , M i ) ≥ }. 2</formula><p>We will explain how these definitions will be useful in the section on cluster expansion later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Iterative Refinement</head><p>Like the well-known k-means algorithm, EM-clustering is an iterative refinement algorithm which improves the quality of clustering iteratively towards a local optimal. In our case, the quality of clustering is measured by the log likelihood for the Gaussian mixture model as follows:</p><formula xml:id="formula_9">E(θ 1 , . . . , θ k 0 |D) = X x∈D log[ k 0 X i=1 W i • P R(M i |x)]<label>(6)</label></formula><p>The EM-clustering algorithm can be divided into two steps: E-Step and M-Step. In E-Step, the memberships of each data object in the microclusters are computed. The density parameters for the microclusters are then updated in M-</p><p>Step. The algorithm iterates between these two steps until the change in the log likelihood is smaller than a certain threshold between one iteration and another. Such iterative change of memberships and parameters is necessary in order to break the catch-22 cycle described below:</p><p>1. Without knowing the relevant correlated dimensions, it is not possible to determine the actual neighborhood of the microclusters.</p><p>2. Without knowing the neighborhood of the microclusters, it is not possible to estimate their density parameters i.e., the mean vector and the covariance matrix of the microclusters.</p><p>EMCluster(D, MCS, likelihood , MaxLoopNum) 1. Set the initial iteration Num. j = 0, initialize the mixture model parameters,</p><formula xml:id="formula_10">W i , µ 0 i and Σ 0 i , for each microcluster M i ∈ MCS. 2. (E-Step) For each data object x ∈ D: P R j (x) = X M i ∈MCS W i P R j (x|M i ), P R j (M i |x) = W i * P R j (x|M i ) P R j (x) , M i ∈ MCS, W i = P x∈D P R j (M i |x). 3. (M-Step) Update mixture model parameters for ∀M i ∈ MCS: µ j+1 i = X x∈D (x • P R(M i |x)) X x∈D P R(M i |x) , Σ j+1 i = X x∈D P R(M i |x)(x -µ j+1 i )(x -µ j+1 i ) T X x∈D P R(M i |x) W i = W i 4. If|E j -E j+1 | ≤ likelihood or j &gt; M axLoopN um</formula><p>Decompose Σ i for ∀M i ∈ MCS and return Else set j = j + 1 and go to 2.</p><p>Note:</p><formula xml:id="formula_11">E j : the log likelihood of the mix- ture model at iteration j, P R j (x|M i ) = 1 q (2π) d | P j i | exp[-1 2 (x -µ j i ) T (Σ j i ) -1 (x -µ j i )].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: EMCluster Subroutine</head><p>By sampling the mean vectors from the data objects and setting the covariance matrix to the identity matrix initially, the iterative nature of EM-clustering conforms the microclusters to their neighborhood through the iterations. Again, we note that our approach here is different from that of ORCLUS and 4C. ORCLUS does not recompute the microcluster center until two microclusters are merged, while 4C fixes its microclusters by gathering objects that are within a distance of of an object in full feature space. Our approach is necessary as we are finding more complex correlations. Incidentally, both ORCLUS and 4C should encounter the same catch-22 problem as us, but they are relatively unaffected by their approximation of the neighborhood.</p><p>The EMCluster subroutine is illustrated in Figure <ref type="figure">3</ref>. First, the parameters of each microcluster M i (M i ∈ MCS) are initialized as follows:</p><formula xml:id="formula_12">W i = 1/k 0 , Σ 0</formula><p>M i is the identity matrix, and the microcluster centers are randomly sampled from the dataset. The membership probabilities of each data object x (x ∈ D), P R(M i |x), are computed for each microcluster M i . Then the mixture model parameters are updated based on the calculated membership probabilities of the data objects. The membership probability computation and density parameters updating iterate until the log likelihood of the mixture model converges, or if the maximum number of iterations, M axLoopN um, is reached. The output of the EM clustering is the means and covariance matrices of the microclusters, and also the membership probabilities of each data object in the microclusters. These results are passed on to the ExpandCluster subroutine.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cluster Expansion</head><p>Having formed the microclusters, our next step is to merge the microclusters in a certain order so that the final nonlinear correlation clusters can be found and visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.3. Co-sharing Level Matrix</head><p>The co-sharing level matrix is a k 0 ×k 0 matrix with its entry (i, j) representing the co-sharing level between microclusters M i and M j (coshare(M i , M j )).</p><p>2</p><p>We calculate the co-sharing level matrix at the beginning of the cluster expansion procedure based on the membership probabilities P R(M i |x) for each data object x and each microcluster M i . To avoid the complexity of computing k 0 ×k 0 entries for each data object x, we instead maintain for each x, a list of l top microclusters that x is most likely to belong to. This reduces the number of entries update to l 2 top . We argue that x has 0 or near 0 probability of belonging to most of the microclusters and thus our approximation should be accurate.</p><p>As shown in Figure <ref type="figure" target="#fig_2">4</ref>, the ExpandCluster subroutine first initializes the current cluster C as {M c }, where M c is the first unprocessed microcluster in the set of microclusters MCS. It then merges all other microclusters that are in the co-shared -neighborhood of M c into N C through the function call to neighbors(M c , , MCS). M c is then output together with its co-sharing level value with C. From among the unprocessed microclusters in N C , the next M c with the highest co-sharing level is found. C new is then formed by merging M c and C. We then update the co-sharing level matrix according to Equation <ref type="formula" target="#formula_13">7</ref>.</p><formula xml:id="formula_13">coshare(C, M k ) = Max(coshare(C, M k ), coshare(M c , M k )),<label>(7)</label></formula><p>where M k is any of the remaining unprocessed microclusters.</p><p>C is then updated to become C new and unprocessed microclusters in the co-shared -neighborhood of MC are added to N C . This process continues until N C is empty and then a C is re-initialized to another unprocessed microcluster by going to Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NNCO Plot</head><p>In the NNCO (Nearest Neighbor Co-sharing Level &amp; Orientation) plot, we visualize the nearest neighbor co-sharing levels together with the orientations of the microclusters in cluster expansion order. The NNCO plot consists of a NNC plot above and an orientation plot below, both sharing the same horizontal axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">NNC Plot</head><p>The NNC plot is inspired by the reachability plot of OP-TICS <ref type="bibr" target="#b5">[5]</ref>. The horizontal axis denotes the microcluster order in the cluster expansion, and the vertical axis above denotes the co-sharing level between the microcluster M c and the cluster being processed C when M c is added to C. We call this value the NNC (Nearest Neighbor Co-sharing) value of MC. Intuitively, the NNC plot represents a local hill climbing algorithm which moves towards the local region with the highest similarity at every step. As such, in the NNC plot, a cluster will be represented with a hill shape with the up-slope representing the movement towards the local high similarity region and the down-slope representing the movement away from the high similarity region after it has been visited. Note that an NNC level of 0 represents a complete separation between two clusters, i.e., the two clusters are formed from two sets of microclusters that do not co-share any data objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Orientation Plot</head><p>Below the NNC plot is the orientation plot, a bar consisting of vertical black-and-white lines. For each microcluster, there is a vertical line of d segments where d is the dimensionality of the data space, and each provides one dimension value of the microcluster's orientation vector, as defined below.</p><p>Definition 3.4. Cluster Orientation The cluster's orientation is a vector along which the cluster obtains maximum variation, that is, the eigenvector with the largest eigenvalue.</p><p>2</p><p>Each dimension value y of the microcluster orientation vector is normalized to the range of [-127.5, 127.5] and mapped to a color ranging from black to white according to Equation <ref type="formula">8</ref>.</p><p>Color(y) = [R(y + 127.5), G(y + 127.5), B(y + 127.5)] (8) Therefore, the darkest color ([R(0), G(0), B(0)], when y = -127.5) indicates the orientation parallel but opposite the corresponding dimension axis while the brightest color ([R(255), G(255), B(255)], when y = +127.5) indicates the orientation parallel and along the dimension axis. Gray ([R(127.5), G(127.5), B(127.5)], when y = 0) suggests no variation at all in the dimension. Obviously, similarly oriented microclusters tend to have similar patterns in the orientation plot. In this way, the clusters' specific subspaces can be observed graphically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Examples</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows a quadratic cluster and a cubic cluster. The nonlinear cluster structures are detected successfully, as shown in the NNCO plots in Figure <ref type="figure" target="#fig_5">6</ref>. According to Definition 3.1, the more similar in orientation the microclusters are, the larger the co-sharing level value they have. As our microclusters are assumed to be evenly distributed, the microclusters which are similar in orientations and close to each other have larger NNC values and tend to be grouped   Generally, the tangent projection along the quadratic curve in X 2 dimension increases from negative to positive while the tangent projection on the X 1 dimension increases and decreases symmetrically. The simple mathematic reasoning behind this is that, given the 2D quadratic curve</p><formula xml:id="formula_14">x 2 = a * (x 1 -b) 2 + c,</formula><p>where a &gt; 0, the changing ratio of the tangent slop, x 2 = 2 * a, is a positive constant. The maximum tangent projection on the X 1 dimension is achieved when the tangent slope is 0. That is why we see in the orientation plot that as a whole, the bar color in dimension X 2 brightens continuously (tangent slope changes from negative to positive) while the bar color in dimension X 1 brightens first and darkens midway.</p><p>For the cubic curve</p><formula xml:id="formula_15">x 2 = a * (x 1 -b) 3 + c,</formula><p>the tangent slope changes from positive to zero, then back to positive again. Again, as the tangent projection on dimension X 1 increases and decreases symmetrically while the tangent projection on dimension X 2 decreases and increases symmetrically. For this reason, the bar color in dimension X 1 brightens and darkens symmetrically while the bar color of dimension X 2 darkens and brightens symmetrically in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Top-down Clustering</head><p>Having identified interesting clusters from the orientation plot, it is possible to perform another round of clustering by focusing on each individual cluster. The reason for doing so is the observation that the orientation captured by the initial orientation plot could only represent the global orientation of the clusters.</p><p>As we know, each data object is assumed to have membership probabilities for several microclusters in CURLER. We define the data members represented by a discovered cluster C which consists of microcluster set MCS as the set of data objects whose highest membership probabilities are achieved in the microcluster among MCS, {∀x|x ∈ D and</p><formula xml:id="formula_16">∃M c ∈ MCS such that Max 1≤i≤k 0 {P R(M i |x)} = P R(M c |x)}.</formula><p>Based on the data members of cluster C, we can further compute the cluster existance space of C. Definition 3.5. Transformed Clustering Space Given the specified cluster C and l, we define the transformed clustering space as a space spanned by l vectors, denoted as ε C l , in which the sum of the variances along the l vectors is the least among all possible transformations. In other words, the l vectors of the transformed clustering space ε C l , are the l eigenvectors with the minimum eigenvalues, computed from the covariance matrix of the data members of C. We denote the l vectors as e 1 , e 2 , ..., and e l , where l may be much smaller than the dimensionality of the original data space d. 2</p><p>Given the dimensionality of the original data space, d, a correlation cluster C i , and l, we can further project data members of</p><formula xml:id="formula_17">C i , D C i , to the subspace ε C i l of l vectors (ε C i l = {e i1 , e i2 , .</formula><p>.., e il }) by transforming each data member x ∈ D C i to (x e i1 , x e i2 , ... x e il ), where x and e ij (1 ≤ j ≤ l) are d-dimensional vectors. In this way, we obtain a new l-D dataset and can carry on another level of clustering. Figure <ref type="figure" target="#fig_6">7</ref> shows the overview of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Time Complexity Analysis</head><p>In this section, we analyze the time complexity of CURLER. We focus our analysis on the EM-clustering algorithm and the cluster expansion since these two are the most expensive steps among the four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• EM Clustering:</head><p>In the EM part, the algorithm runs iteratively to refine the microclusters. The bottleneck is Step 2, where the membership probabilities of each data object x for each microclusters M i ∈ MCS are calculated. The time complexity of matrix inversion, matrix determinant, and matrix decom-position is O(d 3 ); thus, the time complexity of matrix operation for k 0 microclusters is O(k 0 • d 3 ). Besides, the time complexity of computing P R j (x|M i ) is O(d 2 ) for each pair of x and M i . For all data objects and all microclusters, the total time complexity of EM clustering is</p><formula xml:id="formula_18">O(k 0 •n•d 2 +k 0 •d 3 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Cluster Expansion:</head><p>The time complexity of computing the initial co-sharing level matrix is O(n * l 2 top ), as explained in Section 3.2. As there is no index available for CURLER due to our unique co-sharing level function, all the unprocessed microclusters have to be checked to determine the co-shared -neighborhood of the current cluster. So the time complexity of the nearest neighbor search for one cluster is O(k 0 ) and the time complexity of the total nearest neighbor search is O(k 2 0 ). Also, as the time complexity of each co-sharing level matrix update during cluster merging is O(k 0 ), and there is maximum k 0 updates, the time complexity of the entire correlation distance matrix update is O(k 2 0 ). As a result, the time complexity of the cluster expansion is O(n • l 2 top + k 2 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL ANALYSIS</head><p>We tested CURLER on a 1600 MHz PVI PC with 256M memory to ascertain its effectiveness and efficiency. We evaluated CURLER on a 9D synthetic dataset of three helix clusters with different cluster existence spaces, the iris plant dataset and the image segmentation dataset from the UCI Repository of Machine Learning Databases and Domain Theories <ref type="bibr">[6]</ref>, and the Iyer time series gene expression data with 10 well-known linear clusters <ref type="bibr" target="#b14">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Setting</head><p>As illustrated in Figure <ref type="figure" target="#fig_6">7</ref>, CURLER generally requires five input parameters: M axLoopN um, log likelihood threshold likelihood , microcluster number k 0 , l top and neighborhood co-sharing level threshold .</p><p>In all our experiments, we set M axLoopN um between 5 and 20, and likelihood of 0.00001. The experiments show that it is quite reasonable to trade off a limited amount of accuracy for efficiency by choosing a smaller M axLoopN um, a larger log likelihood threshold likelihood and a smaller l top ranging from 20 to 40.</p><p>The number of microclusters k 0 is a core parameter of CURLER. According to our experiments, there is no significant difference in performance when varying k 0 . Of course, the larger the k 0 , the more refined the NNCO plots we got. Unlike <ref type="bibr" target="#b3">[3]</ref> where each data object is assigned to only one cluster, in CURLER, each data object is assumed to have membership probabilities for l top microclusters. As a result, the performance of CURLER is not affected much by k 0 .</p><p>The neighborhood co-sharing level threshold implicitly defines the quality of merged clusters. The larger indicates more strict requirement on microclusters' similarity in both orientation and spacial distance when expanding clusters; hence, the higher cluster quality we obtained. In our experiments, we set to 0. To get a rough clustering result for any positive , we simply moved the horizontal axis up along the vertical axis by a co-sharing level of in the NNCO plot. This is another advantage of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency</head><p>In this Section, we evaluate the efficiency of our algorithm with a varying database size (n) and a varying number of microclusters (k 0 ) on the 9D (d=9) synthetic dataset. In our experiments, we fixed the maximum number of loop time M axLoopN um at 10, the log likelihood threshold likelihood at 0.00001, the neighborhood co-sharing level threshold as 0, and the number of microcluster memberships for each data object l top at 300. We varied either n or k 0 . When n was varied, we fixed k 0 at 300. Likewise, we set n at 3000 when varying k 0 . For the output results, we averaged the execution times of five runs under the same parameter setting. In general, CURLER performed approximately linearly with the database size and the number of microclusters, as illustrated in Figure <ref type="figure" target="#fig_7">8</ref>. The high scalability of our algorithm shows much promise in clustering high-dimensional data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Synthetic Dataset</head><p>Because of the difficulty of getting a public high-dimensional dataset of well-known nonlinear cluster structures, we compared the effectiveness of CURLER with 4C on a 9D synthetic dataset of three helix clusters. The three helix clusters existed in dimensions 1 -3 (cluster 1), 4 -6 (cluster 2), and 7 -9 (cluster 3) respectively and the remaining six dimensions of each cluster were occupied with large random noise, approximately five times the data. Each cluster mapped a different color: red for cluster 1, blue for cluster 2, and yellow for cluster 3, as shown in Figure <ref type="figure">9</ref>. Below is the basic generation function of helix, where t ∈ [0, 6π],</p><formula xml:id="formula_19">x 1 = c * t, x 2 = r * sin(t), x 3 = r * cos(t).</formula><p>The top-level NNC plot in Figure <ref type="figure" target="#fig_0">10</ref> shows that all the three clusters were identified by CURLER in the sequence of cluster 1, cluster 3 and cluster 2, separated by two NNCzero-gaps. The top-level orientation plot further indicates the cluster existence subspace of each cluster, the gray dimensions. The noise dimensions are marked with irregular dazzling darkening and brightening patterns.</p><p>For a close look at the nonlinear correlation pattern of each cluster, we projected the data member into the corresponding cluster existence subspace of three vectors and performed sub-level clustering. Note that the vectors of the As expected, 4C found no clusters although we set the correlation threshold parameter δ as high as 0.8. The changing orientation in the dataset does not exhibit the linear correla-tion which 4C is looking for. In contrast, CURLER not only detected the three clusters but also captured their cycling correlation patterns and the subset of correlated features (Figure <ref type="figure" target="#fig_0">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Real Case Studies</head><p>To have a rough idea of the potential of CURLER in practical applications, we applied the algorithm to three real-life datasets in various domains. Our experiments on the iris plant dataset, the image segmentation dataset, and the Iyer time series gene expression dataset show that CURLER is effective for discovering both nonlinear and linear correlation clusters on all the datasets above. As the cluster structures of the first two public datasets have not been described, we will begin our discussion with the examination of their data distributions with the projected views. We will only report the top-level clustering results of CURLER here due to space constraint.</p><p>Based on our definition of the data members represented by cluster C in Section 3.4, we can infer the class cluster C mainly belongs to. We denote the inferred class label on the top of the cluster or subcluster in the NNCO plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.1">Case 1: Iris Plant.</head><p>The iris plant dataset is one of the most popular datasets in pattern recognition domain. It contains 150 instances from three classes: Iris-virginica (class 1), Iris-versicolor (class 2) and Iris-setosa (class 3), 50 instances each. Each instance has four numeric attributes, denoted as X 1 , X 2 , X 3 and X 4 . Figure <ref type="figure" target="#fig_11">11 (a)</ref> shows the projected view of this data, where the blue points, green circle and red squares represent instances from class 1, 2 and 3 respectively. We can see that there are two large clusters: one consisting of instances of class 1 and the other consisting of instances from class 2 and class 3. The second cluster can further be divided into two subclusters, one composed of instances from class 2 and the other from class 3.</p><p>The microclusters constructed by the EMCluster subroutine are shown in Figure <ref type="figure" target="#fig_12">12</ref> (a). As can be seen clearly, the cluster expansion path traverses instances from class 1, class 2 and class 3 in an orderly manner. The NNCO plot of iris (Figure <ref type="figure" target="#fig_13">13</ref> (a)) visualizes two large clusters: one composed of 50 microclusters representing instances from class 1 and the second cluster composed of 100 microclusters representing instances from the other two classes. It is also noticeable that the second cluster is further divided into two subclusters (two humps) of 85 and 15 microclusters respectively. As illustrated in Figure <ref type="figure" target="#fig_12">12</ref> (a), the two subclusters mainly represent instances from class 2 and class 3 respectively. The different patterns of the clusters in the orientation plot suggest the corresponding different cluster existence subspaces. It is interesting that the microclusters in the same cluster or the same subcluster are very similar in orientation (very similar color patterns). Thus we can infer that the iris plant dataset has three approximately linear clusters, among which two with very similar orientations are close to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.2">Case 2: Image Segmentation.</head><p>The image segmentation dataset has 2310 instances from seven outdoor images: grass (class 1), path (class 2), window (class 3), cement (class 4), foliage (class 5), sky (class 6), and brickface (class 7). Each instance corresponds to a 3x3 region with 19 attributes. During dataset processing, we removed the three redundant attributes (attributes 5, 7, and 9 were reported to be repetitive with attributes 4, 6, and 8 respectively), and normalized the remaining 16 attributes to the range of <ref type="bibr">[-5, 5]</ref>. The 16 attributes contained some statistical measures of the images, denoted as X 1 , X 2 , ..., X <ref type="bibr" target="#b17">16</ref> .</p><p>Figure <ref type="figure" target="#fig_11">11</ref> (b) shows the projected views on all dimensions. Figure <ref type="figure" target="#fig_12">12 (b</ref>) is the projected view of our constructed microclusters on dimensions X 14 , X 15 and X 16 in cluster expansion order.</p><p>Figure <ref type="figure" target="#fig_13">13</ref> (b) is the NNCO plot of the image dataset, which reveals the clustering structure accurately. Note that the image dataset is partitioned into three large clusters separated by NNC-zero-gaps. This is confirmed in our data projection views, Figure <ref type="figure" target="#fig_11">11</ref>  The orientation plot further indicates that the clusters have their own subspaces; this is reflected in the different color patterns. However, some common subspaces also exist. For instance, we observe that the orientation plot on dimensions X 7 , X 8 , X 9 , and X 10 have synchronous color patterns, indicating synchronous linear correlations of the four attributes. As validated in Figures <ref type="figure" target="#fig_11">11 (b.</ref>3) and (b.4), the three clusters approximately reside in the diagonal regions of dimensions X 7 , X 8 , X 9 and X 10 . Another interesting phenomenon is that line X 1 is strongly highlighted (indicating large variation in X 1 ), line X 2 is partly highlighted (indicating positive orientation) and partly darkened (indicating negative orientation) while line X 3 is globally gray (indicating no variation at all in dimension X 3 ). With a closer look at Figure <ref type="figure" target="#fig_11">11 (</ref>b.1), we see the answer: the three clusters distribute almost parallel with axis X 1 and have little variation in dimension X 3 . The approximate gray of lines X 4 , X 5 , and X 6 also indicates little variation in the three dimensions. As a result of the nonlinear patterns in dimensions X 11 to X 16 (Figure <ref type="figure" target="#fig_11">11</ref> (b)), there are irregular color patterns in dimensions X 11 to X 16 .</p><p>Figure <ref type="figure" target="#fig_2">14</ref> depicts three interesting cluster structures discovered in the NNCO plot of the image dataset (Figure <ref type="figure" target="#fig_13">13</ref> (b)). First, the black-and-white cycling color pattern of microclusters 1-48 in dimensions X 11 -X 15 of the orientation plot is a vivid visualization of the nonlinear cluster structure of the corresponding instances of class 3 (Figure <ref type="figure" target="#fig_2">14 (a)</ref>). Second, the synchronous three-vertical-bar pattern of microcluster 397-429 in both the NNC plot and the orientation plot, especially dimensions X 7 -X 10 , reveals three linear correlation clusters with diagonal orientations (Figure <ref type="figure" target="#fig_2">14 (b)</ref>). The NNCO plot also indicates that the instances of class 7 can be partitioned into two big subclusters of consecutive microclusters, one represented by microclusters 49-82 and the other represented by microclusters 280-321 respectively. The plot also indicates that the later subcluster has a larger variation in dimensions X 11 , X 12 , and X 13 (microclusters 280-321 have brighter colors in dimensions X 11 and X 12 of the orientation plot than microclusters 49-82). Again, this is verified in Figure <ref type="figure" target="#fig_2">14 (c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.3">Case 3: Human Serum Data.</head><p>To verify the effectiveness of our algorithm, we also applied CURLER to a benchmark time series gene expression dataset, the Iyer dataset <ref type="bibr" target="#b14">[13]</ref>. The Iyer dataset is a set of temporal gene expression data in response of human fibroblasts to serum, which consists of gene expression patterns of 517 genes across 18 time slots. <ref type="bibr" target="#b14">[13]</ref> describes 10 linear correlation clusters of genes, denoted as 'A', 'B', ..., and 'J'. CURLER identified nine out of the reported ten clusters     successfully among the 517 genes (Figure <ref type="figure" target="#fig_4">15</ref>); cluster 'G', consisting of 13 genes, was the exception. As can be seen, CURLER partitions the reported genes of cluster 'D' into two consecutive subclusters, represented by microclusters 63-76 and 77-95 respectively. Likewise, CURLER partitions the genes of cluster 'H' into three disjointed big subclusters of consecutive microclusters: 206-232, 287-307 and 317-349. The latter two big subclusters can be further partitioned at the sub-level as observed in the NNCO plot. Figure <ref type="figure" target="#fig_5">16</ref> and 17 illustrate the temporal gene expression patterns across the 18 time slots of the genes in the above discovered subclusters. Apparently, the expression patterns of the genes in each subcluster are quite cohesive. Note that the expression patterns of genes in the two subclusters of cluster 'D' are different at time slots t2 and t3: those represented by microclusters 63-76 are negatively expressed while those represented by microclusters 77-95 are positively expressed. Besides, their variation at the two time slots are different, as detected by the NNCO plot. As for genes of the three subclusters of cluster 'H', their expression patterns are delicately different in time slots t9, t10, t11, and t12, as shown in Figure <ref type="figure" target="#fig_4">15</ref> and verified in Figure <ref type="figure" target="#fig_6">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we have presented a novel clustering algorithm for identifying and visualizing nonlinear correlation clusters together with the specific subspaces of their existence in high-dimensional space. Almost no work has addressed the issue of nonlinear correlation clusters, let alone the visualization of these clusters. Our work is a first attempt, and it combines the advantage of density-based algorithms represented by OPTICS <ref type="bibr" target="#b5">[5]</ref> for arbitrary cluster shape and the advantage of subspace clustering algorithms represented by ORCLUS <ref type="bibr" target="#b3">[3]</ref> for subspace detecting.</p><p>As shown in our experiments on a wide range of datasets, CURLER successfully captures the subspaces where the clusters exist and the nonlinear cluster structures, even when a large number of noise dimensions is introduced. Moreover, CURLER allows users to interactively select the cluster of their interest, have a close look at its data members in the space where the cluster exists, and perform sub-level clustering when necessary.</p><p>We plan to consider other variants to further improve the efficiency of CURLER, i.e., constructing some index structures to accelerate nearest neighbor queries based on the mixture model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Global vs Local Orientation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Co-sharing between Two Microclusters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ExpandCluster Subroutine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Quadratic and Cubic Clusters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: NNCO Plots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7: CURLER</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Runtime vs Dataset Size n and # Microclusters k 0 on the 9D Synthetic Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Projected Views of Synthetic Data in both Original Space and Transformed Clustering Spaces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(b.4) and (b.6), where we can see one large cluster composed of instances from class 1, one composed of instances from class 6, and another large cluster composed of mixed instances from the rest of the classes. The last cluster is nonlinear (Figures 11 (b.5) and (b.6)). The NNCO plot indicates that instances from the seven classes are well separated and fairly clustered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11</head><label>11</label><figDesc>Figure 11: Projected Views</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12</head><label>12</label><figDesc>Figure 12: Constructed Microclusters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13</head><label>13</label><figDesc>Figure 13: NNCO Plots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :Figure 16 :Figure 17 :</head><label>141617</label><figDesc>Figure 14: Cluster Structures Revealed by the NNCO Plots for the Image Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CURLER(D, k 0 , l top , , likelihood , M axLoopN um) 1.Randomly Sample k 0 number of seeds from D as MCS; 2.EMCluster(D, MCS, likelihood , M axLoopN um); 3.Select one microcluster in MCS as c; 4.ExpandCluster(MCS, , OutputF ile); 5. For any interesting cluster C i Transform D C i into D new in the subspace ε C i l ; CURLER(D new , k 0 , l top , , likelihood , M axLoopN um) End.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>As an analogy, consider how soft metals like iron, copper, etc., can be easily bended because of their stretchable bond structures. Correspondingly, we can now 'stretch' data objects across microclusters because of fuzzy clustering so that the merged microclusters can conform to the shape of the nonlinear correlation clusters.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment:</head><p>We like to thank Wen Jin and Martin Ester for their comments which has help to improve the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal grid-clustering: Towards breaking the curse of dimensionality in high-dimensional clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th Int. Conf. on Very Large Data Bases</title>
		<meeting>of the 25th Int. Conf. on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="506" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An efficient approach to cluster in large multimedia databases with noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding generalized projected clusters in high dimensional spaces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD Conf. Proceedings</title>
		<meeting>of ACM SIGMOD Conf. eedings</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic subspace clustering of high dimensional data for data mining applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM-SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of ACM-SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optics: Ordering points to identify the clustering structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1999 ACM-SIGMOD Int. Conf. on Management of Data</title>
		<meeting>1999 ACM-SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing clusters of correlation connected objects</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Kailing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM-SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of ACM-SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling clustering algorithms to large databases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m">Int. Conf. Knowledge Discovery and Data Mining (KDD&apos;98)</title>
		<imprint>
			<date type="published" when="1998-08">Aug. 1998</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast algorithms for projected clustering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD Int. conf. on Management of Data</title>
		<meeting>of ACM SIGMOD Int. conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entropy-based subspace clustering for mining numerical data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>2nd Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1996 Int. Conf. Knowledge Discovery and Data Mining (KDD&apos;96)</title>
		<meeting>1996 Int. Conf. Knowledge Discovery and Data Mining (KDD&apos;96)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">Aug. 1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining the gene expression matrix: Inferring gene relationships from large scale gene expression data</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">D</forename><surname>Haeseleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiling</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Fuhrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Cells and Tissues</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The transcriptional program in the response of human fibroblasts to serum</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Trent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Staudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hudson</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Boguski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shalon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data mining concepts and techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model-based gaussian and non-gaussian clustering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Banfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="803" to="821" />
			<date type="published" when="1993-09">September, 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A monte carlo algorithm for fast projective clustering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Int. Conf. on Management of Data</title>
		<meeting>ACM SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast improvement to the em algorithm on its own terms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JRSS(B)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Constraint-based clustering in large databases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V S</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2001 Int. Conf. on Database Theory</title>
		<meeting>2001 Int. Conf. on Database Theory</meeting>
		<imprint>
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial clustering in the presence of obstacles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2001 Int. Conf. on Data Engineering</title>
		<meeting>2001 Int. Conf. on Data Engineering<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A distributed-based clustering algorithm for mining in large spatial databases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 Int. Conf. on Data Engineering</title>
		<meeting>1998 Int. Conf. on Data Engineering</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
