<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Randomized Algorithms for Training Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-22">January 22, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Survey of Randomized Algorithms for Training Neural Networks, Information Sciences</orgName>
								<address>
									<postCode>2016)</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electric and Electronic Engineering</orgName>
								<orgName type="institution">NanYang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Randomized Algorithms for Training Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-22">January 22, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">06F36B896C41404BE3F4CC3BC8BB30D3</idno>
					<idno type="DOI">10.1016/j.ins.2016.01.039</idno>
					<note type="submission">Received date: 10 April 2015 Revised date: 12 November 2015 Accepted date: 17 January 2016 Preprint submitted to Information Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>randomized neural networks</term>
					<term>recurrent neural networks</term>
					<term>convolutional neural networks</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a powerful tool for data regression and classification, neural networks have received considerable attention from researchers in fields such as machine learning, statistics, computer vision and so on. There exists a large body of research work on network training, among which most of them tune the parameters iteratively. Such methods often suffer from local minima and slow convergence. It has been shown that randomization based training methods can significantly boost the performance or efficiency of neural networks. Among these methods, most approaches use randomization either to change the data distributions, and/or to fix a part of the parameters or network configurations. This article presents a comprehensive survey of the earliest work and recent advances as well as some suggestions for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by biological Neural network, artificial neural network (ANN) are a family of non-parametric learning methods for estimating or approximating functions that may depend on a large number of inputs and outputs. Typically, training protocol of an ANN is based on minimizing a loss function defined on the desired output of the data and actual output of the ANN through updating the parameters. Classical approaches usually tune the parameters based on the derivatives of the loss function. However, much of the power of ANN comes from the nonlinear function in the hidden units used to model the nonlinear mapping between the input and output. Unfortunately, this kind of architecture loses the elegance of finding the global minimum solution with respect to all the parameters of the network since the loss function depends on the output of nonlinear neurons. Thus, the optimization turns out to be nonlinear least square problem which is usually solved iteratively. In this case, the error function has to be back propagated backwards to serve as a guidance for tuning the parameters <ref type="bibr" target="#b29">[30]</ref>. Due to this, it is widely acknowledged that these training methods are very slow <ref type="bibr" target="#b37">[38]</ref> and may not converge to a single global minimum because there exist many local minima <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b28">29]</ref> and also the resulting neural network is very weak in the real world noisy situations. These weaknesses of this family of methods naturally limit the applicability of gradient-based algorithms for training neural networks. Randomization based methods remedy this problem by either randomly fixing the network configurations (such as the connections) or some parts of the network parameters (while optimizing the rest by a closed form solution or an iterative procedure), or randomly corrupt the input data or the parameters during the training. Remarkable results have been achieved in various network structures, such as single hidden layer feed forward network <ref type="bibr" target="#b68">[69]</ref>, RBF neural networks <ref type="bibr" target="#b8">[9]</ref>, deep neural network with multiple hidden layers <ref type="bibr" target="#b30">[31]</ref>, convolutional neural network <ref type="bibr" target="#b42">[43]</ref> and so on.</p><p>A main goal of the paper is to show a role and a place of randomized methods in optimization based neural networks' learning. In Section 2, we present some early work on this line of research on perceptron and standard feed-forward neural network with random parameters in the hidden neuron. Another piece of important work is Random Vector Functional Link Network, which is described in Section 3. Randomization based learning in RBF, recurrent neural network and deep neural network are presented in Section 4, Section 5, and Section 6, respectively. We also offer some details on other scenarios such as evolutionary learning in Section 7. In Section 8, we point out some research gap in the literature of randomization algorithm for neural network training. Conclusions are presented in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Early works on Perceptron and standard Feed-forward Neural Network with randomization</head><p>The earliest attempt in this research area was the "perceptron" presented in <ref type="bibr" target="#b64">[65]</ref> and extended in <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b9">10]</ref>. Generally speaking, a perceptron consists of a retina of sensor units, associator and response units. The sensor units are connected to the associator units in "random and many to many" manner. The associator units may connect to other associator units and/or response units. When a stimulus (or the input data) is presented to the sensor units, impulses are conducted from the activated sensor units to the associator units. The associator units are activated once the total arrived signals exceed a threshold. In this case, an impulse from the associator will be send to the units which are connected with it. In perceptron, the weights between the sensor units and the response units can be regarded as randomly selected from {1, 0}, while the weights between the associator units and the response units are achieved by reinforcement learning.</p><p>In <ref type="bibr" target="#b68">[69]</ref>, the authors investigated the performance of a standard feed-forward neural network (SLFN) which is demonstrated in Fig. <ref type="figure">1</ref>. In this work, the weights between the input layer and hidden layer are randomly generated and kept fixed. The author reported that the weights between the output layer and hidden layer are of more importance and the rests may not need to be tuned once they are properly initialized.</p><p>Figure <ref type="figure">1</ref>: The structure of SLFN in <ref type="bibr" target="#b68">[69]</ref>. x means the input feature. The arrows within the yellow rectangle represents the random weights (w hidden ) which connect the input feature to the hidden neurons. Those arrows within the green rectangle are the output weights (w output ) which need to be optimized. x 0s and f 0s can be regarded as the bias term in the input and hidden layer. y is the desired output target.</p><p>For a given classification problem with limited training data, there are numerous solutions with different parameter setting which is statistically acceptable. In this case, training become much easier because the learning set is only needed to make a rough selection in the parameter space. Setting the parameters in the hidden neurons randomly helps to remove the redundancy of the solution in parameter space and thus makes the solution less sensitive to the resulting parameters compared with other typical learning rule such as back-propagation. In <ref type="bibr" target="#b68">[69]</ref>, the weights in hidden neurons are set to be uniform random values in [-1, +1] and they suggest to optimize this range in a more appropriate range for the specified application. An alternative choice is to set the hidden neurons to act as "correlators", which means to fix the weights in hidden neuron with a random subset of the training data.</p><p>In <ref type="bibr" target="#b68">[69]</ref>, the network's output layer weights are optimized by minimizing the following squared error:</p><formula xml:id="formula_0">2 = N i=1 (y i - k j=0 w j f i j ) 2<label>(1)</label></formula><p>where N means the number of data samples and k is the number of hidden neurons. f i j is the activation values of the j th neuron on the i th data sample ( f i0 is the bias). y i is the target of the i th data sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denote by</head><formula xml:id="formula_1">F i = [ f i0 , f i1 , ... f ik ] T (2)</formula><p>is the concatenated vector of activation values of the hidden layer for the i th data sample and the bias. The optimal</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>output layer weight vector, W output , can be easily derived by:</p><formula xml:id="formula_3">W output = R -1 P; R = N i=1 F i F T i ; P = N i=1 y i F T i ;<label>(3)</label></formula><p>The above optimization problem can also be regarded as a linear regression which can be solved in a single step.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, general dynamics of complex systems are realized by a feed-forward neural network with random weights. The outputs of the network are feedback to the inputs to generate a time series. By investigating the percent of the systems that exhibit chaos, the distribution of largest Lyapunov exponents, and the distribution of correlation dimensions, they show the probability of chaos approaches unity and the correlation dimension is typically much smaller than the system dimension as the system become more complex due to increasing inputs and neurons.</p><p>In <ref type="bibr" target="#b22">[23]</ref>,"Jacobian Neural Network" (JNN) is proposed which is a polynomial-time randomized algorithm that has probability 1 to give an optimal network. In JNN, the number of hidden neurons can also be learned. They consider a linear combination of two networks where one of them is randomly generated and the other can be analytically achieved from the first random neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Random Vector Functional Link Neural Network</head><p>Another piece of pioneering work on training neural network with randomization can be found in <ref type="bibr" target="#b55">[56]</ref>. The socalled Random Vector Functional Link Neural Network (RVFL) model can be regarded as a semi-random realization of the Functional Link neural networks, where the basic architecture is demonstrated in Fig. <ref type="figure" target="#fig_0">2</ref>. The rationale behind this architecture is to improve the generalization ability of the network with some enhanced features which can be achieved by some transformation followed by nonlinearity on the original features. The weights a i j from the input to the enhancement nodes are randomly generated such that the activation function g(a t j x + b j ) is not saturated most of the time. For RVFL, only the output weights β j need to be optimized. Suppose the input data has k features and there are J enhancement neurons, there are in total k + J inputs for each output node. Learning is achieved by minimizing the following expression:</p><formula xml:id="formula_4">E = 1 2N N i=1 (t i -B t d i ) 2 (4) A C C E P T E D M A N U S C R I P T</formula><p>where B t consists of the weight values β j , j = 1, 2, ...k + J. There are N input data in total. E is quadratic with respect to each k + J dimensional vector β j , indicating that the unique minimum can be found in no more than k + J iterations of the learning procedure such as the conjugate gradient method. For simplicity, let o p = β j x p j be the output of the p th data, then the changes in the weight are set to be ∆β p j = η(t p -o p )x p j . In the (k + 1) th iteration, the weights are updated as β j (k + 1) = β j (k) + p ∆β p j . The learning procedure iterates until a stopping criterion is met. An alternative solution within a single learning step can be achieved by the Moore-Penrose pseudo-inverse <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55]</ref>. In this case, the weight B can be achieved by B = td + , where + represents the Moore-Penrose pseudo inverse.</p><p>In <ref type="bibr" target="#b36">[37]</ref>, some theoretical justifications can be found for RVFL as well as other neural networks with hidden nodes implemented as product of univariate functions or radial basis functions. They formulate the problem as a limitintegral representation of the function to be approximated. Then the limit-integral representation is approximated by using the Monte-Carlo method. It has been proven that with weights and biases from input layer to hidden layer sampled from uniform distribution with a proper range, RVFL is efficient universal approximator for continuous functions on bounded finite dimensional sets. Indeed the overall error of the approximation can be bounded by sum of the error of approximating the function by the integral and the error of the approximating the integral by the Monte-Carlo methods.</p><p>A comprehensive evaluation of RVFL was conducted <ref type="bibr" target="#b77">[78]</ref>. The authors show that the direct link from the input to output layer plays a key role in the classification ability of RVFL. Moreover, it is advantageous to tune the range of the distribution of the randomized parameters. In the context of time series forecasting, the work in <ref type="bibr" target="#b62">[63]</ref> shows that the direct links in RVFL improve the performance in a statistically significant manner. The direct links can be compared to the time delayed line in the finite impulse response filter (FIR). Moreover, the authors in <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b62">63]</ref> show that randomization range can be tuned to improve performance. An application of RVFL for optimal control can be found in <ref type="bibr" target="#b54">[55]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, the author shows the maximum number of hidden nodes is N -r-1 for an RVFL with a constant bias to learn a mapping within a given precision based on an n-dimensional N-pattern data set, where r is the rank of the data set. An online learning method is also proposed. Furthermore, a robust weighted least square method is investigated in order to eliminate outliers. In <ref type="bibr" target="#b15">[16]</ref>, a dynamic stepwise updating algorithm was proposed when a new enhancement node or a new data was added based on the same pseudo-inverse solution. In <ref type="bibr" target="#b15">[16]</ref> , authors also proposed several methods to refine the model to deal with the small singular value problem. It is widely accepted that the small singular values may be caused by noise in the data or round-off errors during computations. Small singular values will result in very large weights which will further amplify the noise in test data. Some potential solutions in <ref type="bibr" target="#b15">[16]</ref> include: 1). Investigating an upper bound on the weights, 2).Cutting off the singular values and investigating the relation between the cutoff values and the performance of the network in terms of prediction error, 3).Orthogonal least squares learning method or a regularization method or cross-validation methods. These ideas are further explained in <ref type="bibr" target="#b43">[44]</ref>.</p><p>In <ref type="bibr" target="#b35">[36]</ref>, authors report that compared with RVFL, Multilayer perceptron (MLP) gives a closer approximation to a given function f . Moreover, the functional dependence of the approximation error on the complexity of the model is in both cases of 1  √ N , where N is the number of the hidden neurons. Thus, both the RVFL and MLP are efficient approximators which avoid an exponential increase in n. In the same work, by combining the RVFL with expectation maximisation (EM) method, the author proposed the GM-RVFL method and improvements can be observed. In <ref type="bibr" target="#b1">[2]</ref>, the author trains a pool of decorrelated RVFL within the negative correlation learning framework to obtain an ensemble classifier. In <ref type="bibr" target="#b45">[46]</ref>, two algorithms for training RVFL are proposed where training data is distributed throughout a network of agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Radial Basis Function Network</head><p>Another neural network architecture is radial basis function network (RBF network), which was brought to attention in the neural network community by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>, became popular because of its efficiency and ease of training. The RBF network shares a similar architecture as the standard multilayer feedforward neural network. The difference lies in the inputs of the hidden neuron of an RBFnetwork which is the sum of distance between the input pattern and the "center" of the basis function, instead of the weighted sum of the input as in an ANN. In <ref type="bibr" target="#b11">[12]</ref>, the author demonstrates that the RBF network is sufficent to represent arbitrary nonlinear transformation as determined by a finite training data set of input-output patterns, where the centers of the RBFs can be selected from the training data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>or randomly generated while randomly generated centres may not reflect the data distribution, thereby leading to a poorer performance. To be more specific, suppose the training data is (x i , y i ), i = 1, 2, 3, ...N, the activation function of the j th hidden neuron for the t th sample is s j (</p><formula xml:id="formula_5">x t ) = N i=1 φ( x i -c i )</formula><p>, where c i is the center of this RBF. Thus, the problem can be formulated by the following linear equations:</p><formula xml:id="formula_6">Y = Φβ where Y = [y 1 , y 2 , ...y N ] Φ =             φ 11 • • • φ 1N . . . . . . . . . φ N1 • • • φ NN             φ i j = φ( x i -c j )<label>(5)</label></formula><p>Given the existence of the inverse of matrix Φ, the weights β which connect the RBFs to the output node can be achieved by β = Φ -1 Y. Micchelli proved <ref type="bibr" target="#b57">[58]</ref> that for all N and for a large function of φ, the matrix Φ is non-singular if the data points are all distinct.</p><p>The above equations offer an efficient solution to the case where the number of RBFs is equal to number of distinct data samples. In practice, one may be interested in obtaining the same performance with minimum model complexity, that is-with minumum number of RBFs. In this case, the matrix Φ may no longer be square. With the "minimum number of RBFs" constraints, the problem usually becomes over-specified (which means the number of the RBFs is smaller than the number of training samples). Thus, a unique inverse on longer exists and we shall adopt a minimum norm least square method by employing the Moore-Penrose pseudo-inverse, Φ + of Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Recurrent Neural Networks</head><p>Recurrent neural network (RNN) is another type of neural networks. Different from feedforward neural network, where activations is piped through the network from the input neurons to output neurons, an RNN has at least one cyclic path of synaptic connections. One typical structure of RNN can be found in Fig. <ref type="figure" target="#fig_1">3</ref>. A random RNN is a set of N fully connected neurons. The weights that connect the neurons are randomly initialized. The states of the neurons are X(t) = x i (t), i = 1, ..., N, where each x i is set proportional to the firing frequency of the ith neuron. The state dynamics can be modeled by the following discrete time RBF equations:</p><formula xml:id="formula_7">∀t &gt; 0, ∀i = 1, ..., N, x i (t + 1) = f         N j=1 w i j x j (t) + I i -Θ i         ; (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where f is the activation function, I is N dimensional constant vector and Θ is randomly generated. Time varying extensions of I can be found in <ref type="bibr" target="#b8">[9]</ref>. There exist many random RNNs in the literature. Among them, Liquid State Machine <ref type="bibr" target="#b49">[50]</ref> randomly fixes the input weights and internal weights, while the output weights are learned by the wellknown recursive least squares (RLS) algorithm. Liquid state machine uses spiking activation functions. The Echo State Network <ref type="bibr" target="#b38">[39]</ref> works in a fairly similar manner as the liquid state machine. But, the echo state uses continuous models as activation in neurons. Readers are referred to <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b2">3]</ref> for various extensions along this line of research.</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T X X y z L(x, z) f 0 g θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><note type="other">Corrupt Encode Decode</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Deep neural networks and Convolutional neural networks</head><p>Deep structures have become a hot research topic since the work in <ref type="bibr" target="#b30">[31]</ref>. In deep learning, deep structures with multiple layers of non-linear operations are able to learn high-level abstractions. Such high-level abstraction is the key factor leading to the success of many state-of-the-art systems in vision, language, and other AI-level tasks. Complex training algorithms combined with carefully chosen parameters (i.e, learning rate, mini-batch size, number of epochs) can lead to a deep neural network (DNN) with high-performance.</p><p>Autoencoder <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>, which is a basic block of recent deep neural networks, can be decomposed into two parts: encoder and decoder. An encoder is a deterministic mapping that maps the input x to a hidden representation y through: f Θ (x) = s(W x + b) where Θ = {W, b}, and s is a non-linear activation function such as the sigmoid. In the decoder, the hidden representation is then mapped back to reconstruct the input x. This map is achieved by g Θ (y) = s(W y + b ). Figure <ref type="figure" target="#fig_2">4</ref> shows the structure of a denoising autoencoder. In a denoising autoencoder, firstly the input is corrupted by some random noise and the autoencoder aims to reconstruct the "clean" input. In <ref type="bibr" target="#b72">[73]</ref>, the author proposed the stacked denoising autoencoders which were trained locally to denoise corrupted versions of their inputs. It was shown on a benchmark of classification problems to yield significantly lower classification error than the stacked autoencoder. These works clearly establish the value of using a randomization based denoising criterion as a tractable unsupervised objective to guide the learning in deep neural networks.</p><p>Convolutional Neural Network (CNN) <ref type="bibr" target="#b42">[43]</ref> is another neural network model which has been successfully applied to solve many tasks such as digit, object and speech recognition. CNN combines three architectural ideas to ensure some degrees of shift, scale and distortion invariances: shared weights, sub-sampling and local receptive fields. A classic CNN, Lenet-5, is shown in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>CNN is composed of alternating convolutional and pooling layers. The ideas of "shared weights" and "local receptive fields" are involved in the convolutional layers. A filter of a pre-defined size (e.g. 5 × 5 or 7 × 7) is convolved with the input to obtain the feature map. In order to boost the performance of a CNN, a common approach is to let the network learn a "over-complete" set of feature maps. We can easily stack this architecture into deep architectures by setting the output of one pooling layer to be the input of another convolutional layer. In this case, inputs of each filter in the higher layer can be randomly connected to the output of the lower pooling layer. This kind of randomization based connection is well studied in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, the authors show that the exact values of the filters in CNN is less important than the architecture. They report that a two-stage system with random filters can yield satisfactory results, provided that the proper non-linearities and pooling layers are used. This surprising finding has also been verified by <ref type="bibr" target="#b56">[57]</ref>, where thousands of convolutional pooling architectures on a number of object recognition tasks are evaluated and the random weights are found to be only slightly worse than those with pretrained weights. The work in <ref type="bibr" target="#b67">[68]</ref> further address this phenomenon and found þ ➇ø ➑é ✥ ❶ï ✌å ➄û ➀û ☎è ✥ç ✙ï ➤ó ✇ï ✖ø ✙✂ ❛ç ✐è ✥ê ➵å ➄ä ✥ï ➉û ➀ï ✖å ➄ä ✥é ✙ï ❞ù ➽ó (ø ➑è ✧ç ➐✡ ☎å ✑ ❿ô )✝ ⑥ae ✙ä ✧ì ❛ae ☎å ❄✂ ✐å ✠è ✥ø ➑ì ❛é ✏➌ ❶ì ❛é ➇ú ➈ì ➈û ➀ÿ ✂è ✥ø ➑ì ❛é ☎å ➄û ✛é ✙ï ➊è ④ó ✇ì ❛ä ✧ô ✂ê ➝ ➊å ➈é ⑧✡ ◆ï ➲ê ✤ï ✖ï ➊é ➘å ➈ê ➓ê ❺✘ ➇é ❛è ✥ç ✙ï ✖ê ✧ø ✠➽ ➊ø ➀é ❼✂ ❖è ✥ç ✙ï ➊ø ➀ä ì ✠ó (é ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➓ï ❯↔ ➇è ✧ä ❿å ✑ ❶è ✧ì ❛ä ✖ð ➽ã ✛ç ✙ï ✶ó ✇ï ✖ø ✙✂ ❛ç ❛è ➞ê ✧ç ☎å ➄ä ✥ø ➀é ❼✂ ✿è ✧ï ✒ ❿ç ☎é ✙ø ✠➍ ✐ÿ ✙ï ✶ç ☎å ➈ê è ✧ç ☎ï ✢ø ➀é ✐è ✧ï ➊ä ✥ï ✖ê ✤è ✧ø ➀é ❼✂ ê ✤ø ✓ù ✂ï ✿ï ❯➘ ✟ï ✒ ↔è ➻ì ➄ë ➔ä ✥ï ✖ù ✙ÿ ✗ ❶ø ➀é ❼✂ ➲è ✥ç ✙ï ✿é ✐ÿ ☎î ➉✡ ◆ï ➊ä ➻ì ➄ë (ë íä ✥ï ➊ï ae ☎å ➈ä ✥å ➈î ❭ï ➊è ✧ï ✖ä ✥ê ✒➌ ✛è ✧ç ✙ï ✖ä ✧ï ✒✡ ☛✘ ä ✥ï ✖ù ✙ÿ ✗ ❶ø ➀é ❼✂ è ✥ç ✙ï ☛ ❺ ➊å ➈ae ☎å ✑ ➊ø ➩è ❅✘ ✔✌ ➷ì ➈ë ➞è ✧ç ✙ï î ➓å ♦✝ ❿ç ✙ø ➀é ✙ï ➉å ➈é ☎ù ➻ä ✧ï ❞ù ✂ÿ ✗ ➊ø ➑é ❼✂ ➤è ✥ç ✙ï ✛✂ ✐å ➄ae ➝✡ ✎ï ➊è ④ó ✇ï ✖ï ➊é ➻è ✥ï ✖ê ✤è )ï ➊ä ✥ä ✧ì ❛ä ➏å ➄é ☎ù ❭è ✥ä ✥å ➈ø ➑é ☎ø ➑é ❼✂ ï ➊ä ✥ä ✥ì ➈ä ✞ ❜ ✫❝ ✬✠ ⑥ð ➽ã ✛ç ✙ï ➓é ✙ï ➊è ④ó ✇ì ❛ä ✧ô ➲ø ➀é ➒➞ ✗✂ ➈ÿ ☎ä ✧ï ❵✑ ✫ ❶ì ❛é ✐è ✥å ➄ø ➀é ☎ê ✹❜ ✫❝ ✗✘ ❼➌ ❀ ✻✘ ✗✺ ➊ì ➈é ✦✝ é ✙ï ★ ↔è ✧ø ➀ì ➈é ✎ê ✄➌ ♦✡ ✙ÿ ✙è ❦ì ❛é ✙û ✠✘ ✓ ✗✘ ❼➌ ✘ ✻✘ ✗✘ ✛è ✧ä ❿å ➄ø ➀é ☎å ❄✡ ☎û ➑ï )ë íä ✧ï ✖ï )ae ☎å ➈ä ✥å ➈î ❭ï ➊è ✧ï ✖ä ✥ê ❳✡ ✎ï ★ ➊å ➈ÿ ☎ê ✤ï ì ➄ë ❯è ✧ç ☎ï ✌ó ✇ï ✖ø ✙✂ ❛ç ❛è ➔ê ✧ç ☎å ➄ä ✥ø ➀é ❼✂ ☎ð ✜ ❇ø ✙↔ ✂ï ✖ù ☛✝ ➠ê ✧ø ✙➽ ✖ï ❭☞ ✇ì ❛é ✐ú ❛ì ➈û ➀ÿ ✂è ✧ø ➀ì ➈é ✎å ➄û ❙ñ ➉ï ❶è ④ó ➵ì ➈ä ✥ô ✂ê ❖ç ☎å rú ❛ï ➹✡ ◆ï ➊ï ✖é ➶å ➄ae ☎ae ✙û ➑ø ➀ï ✖ù è ✧ì î ➓å ➈é )✘ ❑å ➄ae ✙ae ✙û ➀ø ✁ ➊å ✠è ✥ø ➑ì ❛é ☎ê ✒➌ ➵å ➈î ❭ì ❛é ❼✂ ì ➄è ✧ç ☎ï ➊ä ➽ç ☎å ➈é ☎ù ✂ó (ä ✥ø ➩è ✥ø ➑é ✗✂ ä ✥ï ✒ ➊ì ✑✂ ❛é ✙ø ➟✝ è ✧ø ➀ì ➈é ✩✞ ❜ ✙ ✆✠ ❖➌ ✞ ❜ ✗✓ ✠ ❖➌ ☛î ➓å ✑ ❿ç ✙ø ➀é ✙ï ❯✝ ⑥ae ✙ä ✥ø ➑é ✐è ✥ï ✖ù ➔ ❿ç ✎å ➄ä ❿å ✑ ↔è ✥ï ➊ä ♣ä ✧ï ★ ❶ì ✑✂ ❛é ✙ø ➑è ✧ø ➀ì ➈é ✞ ❜ ✔ ✆✠ ✶➌ ì ➈é ❼✝ ♠û ➀ø ➑é ☎ï ✾ç ☎å ➈é ☎ù ✂ó (ä ✥ø ➩è ✥ø ➑é ✗✂ ➶ä ✥ï ✒ ➊ì ✑✂ ➈é ☎ø ➩è ✥ø ➑ì ❛é ✞ ❜ ✗✺ ✠ ❖➌ ➲å ➄é ☎ù ✴ë ⑨å ✑ ➊ï ✾ä ✥ï ✒ ➊ì ✑✂ ❛é ✙ø ➟✝ è ✧ø ➀ì ➈é ✞ ❜ ✗❀ ✠ ⑥ð ✜ ❇ø ✙↔ ✂ï ✖ù ☛✝ ➠ê ✧ø ✙➽ ✖ï ➹ ❶ì ❛é ✐ú ❛ì ➈û ➀ÿ ✂è ✧ø ➀ì ➈é ✎å ➄û ➞é ✙ï ➊è ④ó ✇ì ❛ä ✧ô ✂ê ✿è ✥ç ☎å ✠è ➷ê ✤ç ☎å ➈ä ✧ï ó ➵ï ➊ø ✠✂ ➈ç ✐è ✥ê ➻å ➄û ➀ì ➈é ✗✂ å ê ✤ø ➀é ❼✂ ❛û ➑ï ✿è ✧ï ✖î ➻ae ✎ì ❛ä ✥å ➈û ✛ù ✙ø ➑î ➻ï ➊é ✎ê ✤ø ➀ì ➈é ➘å ➄ä ✥ï ✢ô ➇é ✙ì ✠ó (é å ➈ê ã ✛ø ➀î ❭ï ✄✝ ❖✧ ♣ï ➊û ✓å ✪✘ ♣ñ ➉ï ➊ÿ ✙ä ❿å ➄û ✐ñ ➔ï ➊è ④ó ✇ì ❛ä ✧ô ✂ê ❷➪ ⑨ã ✩✧ ➳ñ ➉ñ ♣ê |➶ ❶ð ❯ã ✩✧ ➳ñ ➉ñ ♣ê ❀ç ✎å rú ➈ï ❷✡ ◆ï ➊ï ➊é ÿ ☎ê ✧ï ✖ù ❖ø ➑é ➲ae ☎ç ✙ì ➈é ✙ï ✖î ➻ï ❙ä ✥ï ✒ ➊ì ✑✂ ➈é ☎ø ➩è ✥ø ➑ì ❛é ✢➪ íó (ø ➑è ✧ç ✙ì ❛ÿ ✂è ➤ê ✧ÿ ❼✡ ✦✝ ➠ê ✥å ➄î ➻ae ✙û ➀ø ➑é ❼✂ ☛➶ ✜✞ ❝ ✗✘ ✬✠ ✶➌ ✞ ❝ ✗➾ ✡✠ ❖➌ ➻ê ✤ae ◆ì ➈ô ❛ï ➊é òó ➵ì ➈ä ❿ù ä ✥ï ✒ ❶ì ➎✂ ➈é ✙ø ➑è ✧ø ➀ì ➈é ➪ ⑨ó (ø ➩è ✥ç ✭ê ✧ÿ ❼✡ ✦✝ ➠ê ✥å ➄î ➻ae ✙û ➀ø ➑é ❼✂ ☛➶ ✞ ❝ ✵✑ ✠ ✶➌ ✞ ❝ ❜ ✠ ❖➌ ✌ì ➈é ❼✝ ♠û ➀ø ➑é ☎ï ä ✧ï ★ ❶ì ✑✂ ❛é ✙ø ➑è ✧ø ➀ì ➈é ì ➄ë ➻ø ➀ê ✧ì ➈û ✓å ✠è ✥ï ✖ù ❲ç ☎å ➈é ☎ù ✂ó (ä ✥ø ➩è ✧è ✧ï ➊é ② ❿ç ✎å ➄ä ❿å ✑ ✹✝ è ✧ï ✖ä ✥ê ✹✞ ❝ ✫❝ ✫✠ ✶➌ ✙å ➈é ☎ù ✺ê ✤ø ✠✂ ➈é ✎å ✠è ✧ÿ ☎ä ✧ï ➳ú ➈ï ✖ä ✧ø ✙➞ ✥ ➊å ➄è ✧ø ➀ì ➈é ✞ ❝ ✦✙ ✆✠ ⑥ð ✬ ✛ ❊✢ ➳ ❺➸ ➑➳ ❯➺ )✭ ✝✆ ã ✛ç ✙ø ➀ê ➉ê ✤ï ★ ↔è ✧ø ➀ì ➈é ✫ù ✂ï ❞ê ❺ ➊ä ✧ø ✠✡ ◆ï ✖ê (ø ➀é ✺î ❭ì ❛ä ✧ï ✌ù ✂ï ➊è ✥å ➈ø ➑û ❀è ✧ç ✙ï ➞å ➈ä | ❿ç ✙ø ➑è ✧ï ★ ↔è ✥ÿ ✙ä ✧ï ➤ì ➈ë ✗ ✝ï ❞ñ ➔ï ➊è ❇✝ ✙ ✦➌ (è ✥ç ✙ï ①☞ ✇ì ❛é ➇ú ➈ì ➈û ➀ÿ ✂è ✥ø ➑ì ❛é ☎å ➄û ➳ñ ➔ï ➊ÿ ☎ä ✥å ➈û ♣ñ ➔ï ➊è ④ó ✇ì ❛ä ✧ô ➘ÿ ☎ê ✧ï ✖ù ✻ø ➑é ✲è ✧ç ✙ï ï ❯↔ ✂ae ◆ï ➊ä ✥ø ➑î ➻ï ✖é ❛è ❿ê ➊ð ✗ ❀ï ✖ñ ➔ï ➊è ❇✝ ✙ ➉ ➊ì ➈î ➻ae ✙ä ✥ø ➀ê ✧ï ✖ê ✔ ➞û ➀å ✪✘ ❛ï ➊ä ❿ê ✄➌ ✐é ✙ì ➄è ✩ ❶ì ❛ÿ ✙é ✐è ✧ø ➀é ❼✂ ❙è ✧ç ✙ï ø ➀é ✙ae ✙ÿ ✂è ★➌ ☎å ➄û ➀û ✟ì ➄ë ❀ó (ç ✙ø ✁ ❿ç ➙ ➊ì ➈é ✐è ✥å ➈ø ➑é ✶è ✧ä ❿å ➄ø ➀é ☎å ❄✡ ☎û ➑ï ➳ae ☎å ➄ä ❿å ➄î ➻ï ➊è ✧ï ➊ä ❿ê ✬➪ ⑨ó ✇ï ✖ø ✙✂ ❛ç ✐è ✥ê ✴➶ ↔ð ã ✛ç ✙ï ➏ø ➀é ✙ae ✙ÿ ✙è ❀ø ➀ê ❀å ❙❜ ✑ ✪↔ ❜ ✵✑ )ae ☎ø ➟↔ ✂ï ➊û ➈ø ➑î ➓å ❄✂ ❛ï ➈ð ❀ã ✛ç ☎ø ➀ê ❀ø ➀ê ✝ê ✧ø ✠✂ ➈é ✙ø ✙➞ ✥ ➊å ➈é ✐è ✧û ✠✘ ➉û ✓å ➄ä |✂ ➈ï ➊ä è ✧ç ✎å ➄é ✺è ✧ç ☎ï ➞û ➀å ➈ä ❺✂ ❛ï ✖ê ✤è ✬ ❿ç ☎å ➄ä ❿å ✑ ❶è ✧ï ✖ä (ø ➑é ✺è ✥ç ✙ï ✒ù ✙å ➄è ✥å ❄✡ ✎å ➈ê ✧ï ➣➪ ⑨å ✠è ♣î ❭ì ✐ê ④è ✒✑ ❆✘ ♦↔ ✤✑ ✻✘ ae ✙ø ✙↔ ✂ï ➊û ✓ê ➓ ❶ï ✖é ❛è ✥ï ➊ä ✥ï ✖ù ❺ø ➑é å ❈✑ ✻✺ ♦↔ ✤✑ ❆✺ ➞ ☎ï ➊û ✓ù ✗➶ ❶ð ✶ã ✛ç ✙ï ➓ä ✥ï ✖å ➈ê ✧ì ➈é ø ✓ê ➤è ✧ç ☎å ➄è ➞ø ➩è ➞ø ✓ê ù ✂ï ❞ê ✤ø ➀ä ✥å ✑✡ ✙û ➀ï ✌è ✧ç ☎å ➄è ♣ae ◆ì ➄è ✥ï ➊é ✐è ✧ø ✓å ➄û ❦ù ✂ø ✓ê ④è ✥ø ➑é ✗ ❶è ✧ø ➀ú ➈ï ✒ë íï ✖å ✠è ✥ÿ ✙ä ✥ï ✖ê ➉ê ✧ÿ ✗ ❿ç ➲å ❛ê ♣ê ✤è ✧ä ✥ì ➈ô ❛ï ï ➊é ✎ù ☛✝ ♠ae ◆ì ➈ø ➀é ✐è ✥ê ➵ì ➈ä ✎ ❶ì ❛ä ✧é ✙ï ✖ä ✎ ✖å ➄é ✶å ➈ae ✙ae ◆ï ✖å ➄ä ❊✣ •➨ ✆➺ ➭➥ ❼➳ ❄ ✴➳ ✄➨ ✗➺ r➳ ❯➡ ❣ì ➈ë ✝è ✧ç ☎ï ♣ä ✥ï ✒ ➊ï ➊ae ✦✝ è ✧ø ➀ú ➈ï ✞➞ ☎ï ✖û ➀ù ✿ì ➈ë ❀è ✧ç ☎ï ✌ç ✙ø ✙✂ ❛ç ✙ï ✖ê ✤è ❇✝ ⑥û ➀ï ➊ú ➈ï ✖û ◆ë íï ✖å ✠è ✥ÿ ✙ä ✥ï ➤ù ✙ï ❶è ✧ï ★ ↔è ✥ì ➈ä ❿ê ➊ð (➏ ➠é ❖✗ ✝ï ❞ñ ➔ï ❶è ❺✝ ✝✙ è ✧ç ☎ï ♣ê ✧ï ❶è ✇ì ➈ë ✏ ❶ï ➊é ✐è ✥ï ➊ä ❿ê ➏ì ➄ë ☛è ✥ç ✙ï ➉ä ✥ï ✒ ➊ï ➊ae ✂è ✥ø ➑ú ❛ï ✩➞ ☎ï ✖û ➀ù ✙ê ✇ì ➄ë ✟è ✥ç ✙ï ➉û ✓å ➈ê ✤è ✎ ➊ì ➈é ➇ú ➈ì ❛û ➑ÿ ✦✝ è ✧ø ➀ì ➈é ✎å ➄û ✂û ✓å ✪✘ ➈ï ➊ä ✛➪ ❖☞ ❀❜ ❼➌ ➈ê ✧ï ➊ï ✩✡ ◆ï ➊û ➀ì ✠ó ✬➶ ❀ë íì ➈ä ✥î ➶å ✹✑ ❆✘ ❄↔ ✤✑ ❆✘ ➳å ➄ä ✥ï ✖å ♣ø ➑é ❭è ✧ç ☎ï ✬ ❶ï ✖é ❛è ✥ï ➊ä ì ➄ë ❀è ✧ç ✙ï ✹❜ ✑ ♦↔ ❜ ✑ ➤ø ➀é ✙ae ✙ÿ ✂è ❞ð )ã ✛ç ✙ï ➳ú ✠å ➄û ➀ÿ ✙ï ✖ê ➵ì ➄ë ❇è ✧ç ✙ï ➳ø ➀é ✙ae ✙ÿ ✂è ➔ae ✙ø ✙↔ ✂ï ➊û ✓ê ✛å ➄ä ✥ï ➉é ✙ì ❛ä ❇✝ î ➓å ➄û ➀ø ✙➽ ✖ï ✖ù ❺ê ✤ì ✢è ✧ç ☎å ➄è ➤è ✧ç ✙ï ➝✡ ☎å ➎ ❿ô )✂ ❛ä ✧ì ❛ÿ ✙é ☎ù ✫û ➀ï ➊ú ❛ï ➊û ✩➪ íó (ç ☎ø ➩è ✥ï ★➶ t ❶ì ❛ä ✧ä ✥ï ✖ê ✧ae ◆ì ➈é ☎ù ✙ê è ✧ì ✫å ✿ú ✠å ➄û ➀ÿ ✙ï ➻ì ➄ë ➃✝ ✘ ☎ð ✙➾ ❭å ➄é ☎ù ❖è ✧ç ✙ï ➻ë íì ❛ä ✧ï ✒✂ ➈ä ✥ì ➈ÿ ✙é ✎ù ✢➪ ➭✡ ✙û ✓å ✑ ❿ô ❼➶ ✞ ❶ì ❛ä ✧ä ✥ï ✖ê ✧ae ◆ì ➈é ☎ù ✙ê è ✧ì ①➾ ❛ð ✙➾ ✍✔ ✬✙ ✙ð ➻ã ✛ç ☎ø ➀ê ✌î ➓å ➈ô ➈ï ✖ê ➳è ✥ç ✙ï ➓î ➻ï ✖å ➈é ø ➀é ✙ae ✙ÿ ✙è ✒ä ✧ì ❛ÿ ❼✂ ➈ç ☎û ✙✘ ✦✘ ✗➌ ❇å ➈é ☎ù ❺è ✧ç ✙ï ú ✠å ➄ä ✥ø ➀å ➈é ✗ ❶ï ➳ä ✥ì ➈ÿ ❼✂ ❛ç ✙û ✙✘ ➔➾ ➳ó (ç ☎ø ✠ ❿ç ✺å ✑ ✒ ❶ï ➊û ➀ï ➊ä ❿å ✠è ✥ï ✖ê ➵û ➀ï ✖å ➄ä ✥é ✙ø ➀é ❼✂ ❖✞ ❝ ✓ ✠ ⑥ð ➏ ➠é ➤è ✧ç ✙ï )ë íì ➈û ➀û ➑ì ✠ó (ø ➀é ❼✂ ✗➌ ✪ ❶ì ❛é ➇ú ➈ì ➈û ➀ÿ ✂è ✥ø ➑ì ❛é ☎å ➄û ✠û ✓å ✪✘ ➈ï ✖ä ✥ê ✝å ➈ä ✧ï ❣û ✓å ❄✡ ◆ï ➊û ➀ï ✖ù ➉☞ ➜↔ ❢➌ ✠ê ✧ÿ ❼✡ ✦✝ ê ✥å ➄î ➻ae ✙û ➀ø ➑é ❼✂ ✢û ➀å ✪✘ ❛ï ➊ä ❿ê ➉å ➈ä ✧ï ➞û ✓å ❄✡ ◆ï ➊û ➀ï ✖ù þ )↔ ❢➌ ☛å ➄é ☎ù ➲ë íÿ ✙û ➑û ✠✘ )✝ ❖ ➊ì ➈é ✙é ☎ï ✒ ↔è ✥ï ✖ù ✫û ✓å ✪✘ ➈ï ✖ä ✥ê å ➄ä ✥ï ➤û ➀å ✑✡ ✎ï ✖û ➑ï ❞ù ➙✜ ❳↔ ❢➌ ✙ó (ç ✙ï ✖ä ✧ï ✞↔ ✿ø ➀ê ➵è ✥ç ✙ï ✌û ➀å ✪✘ ❛ï ➊ä ✛ø ➀é ☎ù ✂ï ❯↔ ☛ð ✗ ❀å ✪✘ ➈ï ✖ä ➑☞ ④➾ ➽ø ✓ê ✒å ✆ ❶ì ❛é ➇ú ➈ì ➈û ➀ÿ ✂è ✥ø ➑ì ❛é ☎å ➄û ❣û ✓å ✪✘ ➈ï ✖ä ➞ó (ø ➩è ✥ç ✮✓ ✺ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➓î ➓å ➄ae ☎ê ✖ð ✭ )å ➎ ❿ç ➻ÿ ✙é ✙ø ➑è ➵ø ➀é ➓ï ✖å ➎ ❿ç ➻ë íï ✖å ✠è ✥ÿ ✙ä ✥ï ➔î ➓å ➄ae ➽ø ✓ê ➜ ➊ì ➈é ✙é ✙ï ★ ↔è ✥ï ✖ù ➻è ✥ì ✒å ✙ ✪↔ ✤✙ ➤é ☎ï ➊ø ✠✂ ➈ç ✦✝ ✡ ◆ì ➈ä ✥ç ✙ì ➇ì ➇ù ✒ø ➑é ✒è ✧ç ✙ï ✛ø ➀é ✙ae ✙ÿ ✂è ❞ð ❯ã ✛ç ☎ï ✛ê ✧ø ✠➽ ➊ï )ì ➈ë ☎è ✥ç ✙ï )ë íï ❞å ✠è ✥ÿ ✙ä ✧ï ➵î ➓å ➄ae ☎ê ❯ø ✓ê ✑ ❆✺ ♦↔ ✤✑ ✻✺ ó (ç ✙ø ✁ ❿ç ❖ae ✙ä ✥ï ➊ú ❛ï ➊é ✐è ✥ê t ➊ì ➈é ✙é ✙ï ★ ↔è ✥ø ➑ì ❛é ➲ë íä ✥ì ➈î è ✧ç ☎ï ❭ø ➀é ✙ae ✙ÿ ✙è ✌ë íä ✧ì ❛î ë ⑨å ➄û ➀û ➑ø ➀é ❼✂ ✿ì ❄➘ è ✧ç ☎ï ➑✡ ◆ì ➈ÿ ✙é ✎ù ✙å ➄ä |✘ ➈ð ➓☞ ④➾ ➑ ➊ì ➈é ✐è ✥å ➈ø ➑é ☎ê ➉➾ ✙ ✻✓ ➻è ✧ä ❿å ➄ø ➀é ☎å ❄✡ ☎û ➑ï ❙ae ☎å ➄ä ❿å ➄î ➻ï ❶è ✥ï ➊ä ❿ê ✄➌ ◆å ➄é ☎ù ➾ ✑ ✑ ✦➌ ❜ ✻✘ ✫❝ ➑ ❶ì ❛é ✙é ✙ï ★ ↔è ✧ø ➀ì ➈é ✎ê ➊ð ✗ ❀å ✪✘ ➈ï ✖ä ➔þ ✑ ❭ø ➀ê ➉å ➓ê ✤ÿ ❼✡ ❼✝ ⑥ê ✥å ➄î ➻ae ✙û ➀ø ➑é ✗✂ ❭û ✓å ✪✘ ➈ï ✖ä (ó (ø ➩è ✥ç ✓ ❙ë íï ✖å ✠è ✥ÿ ✙ä ✥ï ✌î ➻å ➈ae ☎ê (ì ➈ë ê ✧ø ✙➽ ✖ï ✞➾ ✖❝ ❄↔ ❢➾ (❝ ✎ð ★✭ )å ➎ ❿ç ✒ÿ ✙é ☎ø ➩è ❣ø ➀é ❭ï ❞å ✑ ❿ç ✒ë íï ✖å ✠è ✥ÿ ✙ä ✥ï ➵î ➓å ➈ae ❙ø ✓ê ❷ ❶ì ❛é ✙é ✙ï ✒ ❶è ✧ï ❞ù ➞è ✥ì ➳å ✑ ✪↔ ✤✑ ➤é ☎ï ➊ø ✠✂ ➈ç ☛✡ ✎ì ❛ä ✧ç ☎ì ✐ì ✂ù ➻ø ➀é ➓è ✧ç ☎ï ④ ❶ì ❛ä ✧ä ✥ï ✖ê ✧ae ◆ì ➈é ☎ù ✂ø ➀é ❼✂ ➤ë íï ❞å ✠è ✧ÿ ☎ä ✧ï ♣î ➻å ➈ae ➓ø ➑é ✫☞ ④➾ ➈ð ã ✛ç ✙ï ➳ë íì ➈ÿ ☎ä (ø ➑é ✙ae ☎ÿ ✂è ✥ê ✛è ✥ì ➓å ✒ÿ ✙é ☎ø ➩è ➔ø ➀é ➲þ ✑ ✒å ➈ä ✧ï ✌å ❛ù ✙ù ✂ï ✖ù ✏➌ ➇è ✧ç ✙ï ✖é ✢î ❙ÿ ✙û ➩è ✥ø ➑ae ☎û ➑ø ➀ï ✖ù ✡ ☛✘ å ✫è ✧ä ❿å ➄ø ➀é ☎å ✑✡ ✙û ➑ï ➐ ❶ì ➇ï ❈✯ ➣ ➊ø ➑ï ✖é ❛è ★➌ ➏å ➄é ☎ù ❑å ➈ù ✙ù ✙ï ✖ù ➷è ✥ì å ➲è ✧ä ❿å ➄ø ➀é ☎å ❄✡ ✙û ➀ï ➣✡ ✙ø ✓å ➈ê ✖ð ã ✛ç ✙ï ➲ä ✥ï ✖ê ✧ÿ ✙û ➑è ➽ø ➀ê ✶ae ☎å ➈ê ✥ê ✧ï ✖ù ❑è ✧ç ✙ä ✥ì ➈ÿ ✗✂ ➈ç ❍å ê ✤ø ✠✂ ➈î ➻ì ❛ø ➀ù ✙å ➈û ✛ë íÿ ✙é ✗ ❶è ✧ø ➀ì ➈é ✝ð ã ✛ç ✙ï ✑ ✪↔ ✤✑ ✶ä ✧ï ★ ❶ï ✖ae ✂è ✧ø ➀ú ➈ï ➑➞ ☎ï ➊û ✓ù ✙ê ✌å ➈ä ✧ï ❙é ✙ì ➈é ✦✝ ⑥ì ✠ú ➈ï ✖ä ✧û ✓å ➄ae ☎ae ✙ø ➑é ✗✂ ✗➌ ✎è ✧ç ✙ï ✖ä ✧ï ➊ë íì ➈ä ✥ï ✒ë íï ❞å ✠è ✧ÿ ☎ä ✧ï î ➓å ➄ae ☎ê ✒ø ➑é ❤þ ✤✑ ✺ç ☎å rú ➈ï ➻ç ☎å ➈û ➩ë ✛è ✥ç ✙ï ✶é ➇ÿ ✙î ➑✡ ✎ï ✖ä ✒ì ➄ë ✛ä ✥ì ✠ó ➔ê ➞å ➄é ☎ù ① ➊ì ➈û ➀ÿ ✙î ➻é å ➈ê ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➤î ➓å ➄ae ☎ê (ø ➀é ➔☞ ④➾ ➈ð ✗ ❇å ✪✘ ➈ï ➊ä (þ ✑ ❭ç ☎å ❛ê t➾ ✑ ✒è ✧ä ❿å ➄ø ➀é ☎å ✑✡ ✙û ➑ï ➤ae ☎å ➈ä ✥å ➈î ➻ï ❶è ✧ï ✖ä ✥ê å ➄é ✎ù ✳✙ ❼➌ ✺ ✗✺ ✻✘ ➚ ❶ì ➈é ☎é ✙ï ✒ ❶è ✧ø ➀ì ➈é ☎ê ✖ð ✗ ❀å ✪✘ ➈ï ✖ä ✞☞ ❀❜ ➻ø ✓ê ♣å ➣ ➊ì ➈é ➇ú ➈ì ❛û ➑ÿ ✙è ✧ø ➀ì ➈é ☎å ➈û ☛û ➀å ✪✘ ❛ï ➊ä ➔ó (ø ➑è ✧ç ✢➾ ✒✓ ❭ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➞î ➓å ➄ae ☎ê ✖ð ✭ )å ➎ ❿ç ✺ÿ ✙é ✙ø ➑è ➉ø ➑é ✫ï ❞å ✑ ❿ç ✿ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➞î ➓å ➄ae ✺ø ✓ê ✬ ➊ì ➈é ✙é ✙ï ★ ↔è ✥ï ✖ù ✢è ✧ì ✶ê ✧ï ➊ú ❛ï ➊ä ❿å ➄û ✙ ♦↔ ✙ é ✙ï ✖ø ✙✂ ❛ç )✡ ◆ì ➈ä ✥ç ✙ì ➇ì ✂ù ✙ê ✺å ✠è ✫ø ✓ù ✂ï ➊é ✐è ✧ø ✁ ➊å ➈û ➤û ➑ì ✦ ➊å ➄è ✧ø ➀ì ➈é ☎ê ✺ø ➀é å ❑ê ✧ÿ ❼✡ ☎ê ✧ï ❶è ➲ì ➈ë ➻þ ✑ ❂❁ ê ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ✿î ➻å ➈ae ☎ê ✖ð ❑ã ❯å ❄✡ ✙û ➀ï ➙➏ ❭ê ✤ç ☎ì ✠ó ➔ê ✒è ✥ç ✙ï ✫ê ✤ï ➊è ❭ì ➈ë ➤þ ✑ ➲ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ✿î ➻å ➈ae ☎ê certain convolutional pooling architectures can be inherently frequency selective and translation invariant even with random weights. Thus, a practical method is proposed for fast model selection. They show that the performance of single layer convolutional square pooling networks with random weights is significantly correlated with the performance of such architectures after pretraining and finetuning. Thus, the use of random weights for architecture search will improve the performance of the state-of-the-art systems. Randomization method can also be employed in the pooling layer of the CNN to improve the performance significantly. In <ref type="bibr" target="#b18">[19]</ref>, selecting local receptive fields is proposed where some features are randomly selected firstly, then the local receptive fields are selected to make sure that each feature within it is most similar to each other according to a pairwise similarity metric. In <ref type="bibr" target="#b76">[77]</ref>, the authors proposed to randomly pick the activation within each pooling region according to a multinomial distribution, given by the activations within the pooling region. Such randomization based pooling instead of conventional deterministic pooling operations achieve state-of-the-art performance on several benchmark datasets.</p><formula xml:id="formula_10">A C C E P T E D M A N U S C</formula><p>As regressors, neural network models the conditional distribution of the output variables Y given the input variables X. Multi-modal conditional distribution is proposed in <ref type="bibr" target="#b70">[71]</ref> by using stochastic hidden variables rather than deterministic ones. A new Generalized EM training procedure using importance sampling is proposed to train a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables to efficiently learn complicated conditional distributions. They achieve superior performance on synthetic and facial expressions datasets compared to conditional restricted boltzmann machines and mixture density networks.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, the authors report empirically and theoretically that randomly chosen trials are more efficient for hyperparameter optimization than trials on a grid for deep neural networks. Compared with neural networks configured by a pure grid search, random search over the same domain is able to find models that are on par with or even better within a small fraction of the computation time. With the same computational budget, random search may find better models by effectively searching a larger as well as less promising configuration space. Gaussian process analysis of the function from hyper-parameters to validation set performance is employed. It reveals that for most data sets only a few of the hyper-parameters really matter. However, different hyper-parameters may be important on different data sets, which makes the commonly adopted grid search approach a poor choice for configuring algorithms for new data sets. In <ref type="bibr" target="#b46">[47]</ref>, the authors proposed to multiply error signals by random synaptic weights in back-propagation for deep neural networks. They demonstrated that this new mechanism performs as quickly and accurately as back-propagation on a variety of problems.</p><p>It is well known that a large feed-forward neural network trained on a small training set will typically perform poorly on held-out test data. To tackle this problem, the authors in <ref type="bibr" target="#b69">[70]</ref> propose a method called "dropout" to prevent co-adaptation of feature detectors. In their method, The key idea is to randomly drop units with a pre-defined probability (along with their connections) from a neural network during training. This prevented the units from coadapting too much. Dropping units created thinned networks during training. Dropout can be seen as an extreme form of bagging in which each model is trained on a single case and each parameter of the model is very strongly regularized by sharing it with the corresponding parameter in all the other models. During testing, all possible thinned networks were combined using an approximate model averaging procedure. The idea of dropout is not limited to feed-forward neural networks. It can be more generally applied to graphical models such as Boltzmann Machines. Random dropout yielded big improvements on many benchmark tasks and sets new record accuracies in speech and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T object recognition tasks <ref type="bibr" target="#b69">[70]</ref>.</p><p>In <ref type="bibr" target="#b73">[74]</ref>, DropConnect network was proposed for regularizing large fully-connected layers within neural networks which can be regarded as a generalization of drop out. DropConnect can be regarded as a larger ensemble of deep neural network than dropout. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. In the testing time, DropConnect network uses a sampling based inference which was shown to be better than the meaninference used by dropout. The authors also give some theoretical insights on why DropConnect regularizes the network <ref type="bibr" target="#b73">[74]</ref>. They proved that the Rademacher complexity <ref type="bibr" target="#b5">[6]</ref> of the DropConnect is less than the standard models.</p><p>Multilayer bootstrap network (MBN) <ref type="bibr" target="#b78">[79]</ref> is another deep neural network model where each layer of the network is a group of mutually independent k-clusters of which centers are randomly sampled data points. In <ref type="bibr" target="#b78">[79]</ref>, the author shows the relationship between MBN and product of experts, contrastive divergence learning and sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Other Randomized Algorithms</head><p>Randomization based methods can also be employed in other network configurations. In <ref type="bibr" target="#b26">[27]</ref>, a special kind of random neural network is proposed. For other neural networks, the activation of neurons is either binary or continuous variables. For random neural network, each neuron is represented by its potential (which is a non-negative integer) and a neuron is considered to be in its firing state if its potential is positive. Readers are referred to <ref type="bibr" target="#b27">[28]</ref> for a comprehensive review about training this family of randomized feedforward neural network.</p><p>Kernel machines such as the Support Vector Machine are attractive because of their excellent generalization ability. However, one drawback of such method is that the kernel matrix (Gram matrix) scales poorly with the size of the training data. In <ref type="bibr" target="#b60">[61]</ref>, the authors proposed to map the input data to a randomized low-dimensional feature space where the inner products uniformly approximate many popular kernels. The proposed random features are demonstrated to be powerful and economical for large scale learning. Another research in <ref type="bibr" target="#b19">[20]</ref> scaled up the kernel methods by a novel concept called "doubly stochastic functional gradient". It is well known that many kernel methods can be solved by convex optimization algorithms. Dai et al <ref type="bibr" target="#b19">[20]</ref> solved the optimization problems by making two unbiased stochastic approximations to the functional gradient. One used random training points and another used random features associated with the kernel. Fast convergence rate and tight bound were reported in their method.</p><p>In <ref type="bibr" target="#b61">[62]</ref>, motivated by the fact that randomization is computationally cheaper than optimization based learning, the authors proposed to replace the minimization with randomization in a classifier similar to kernel machines, which computes a weighted sum of their inputs after passing them through a pool of arbitrarily randomized nonlinearities. The error bound can be roughly decomposed into 2 parts. The first part of the error bound indicates that the lowest true risk attainable by a function in the family of the randomized classifiers is close to the lowest true risk attainable. The second part of the error bound shows that the true risk of every function in the family of the randomized classifiers is close to its empirical risk.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, the author showed that the conventional back propagation would not work on neural networks whose neurons have hard-limiting input-output characteristics. In this case, the derivatives of the error function are not available. However, the authors showed if the network weights are random variables with smooth distribution functions, the probability of a hard-limiting unit taking one of its two possible values is a continuously differentiable function.</p><p>It is not difficult to find many attempts to train neural networks by using evolutionary algorithms. Evolutionary algorithm is a special randomization method inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Population-based algorithms, such as genetic algorithm <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>, particle swarm optimization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref>, immune approaches <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b20">21]</ref>, multi-objective optimization <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b17">18]</ref>, are known to improve the training of neural network. Other approaches such as simulated annealing and metropolis have also been successfully applied for neural network training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Future directions</head><p>Though a large body of literature has been published to exploit randomization for training neural networks, there are still several research gaps in this field. One can investigate the effects of a specific randomization scheme (sparse or non-sparse, uniform or gaussian, etc.) on different neural network classes by using the compressive sensing theory [13,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T <ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>. Another major gap is the lack of extensive experimental comparisons between different randomization based networks <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60]</ref>. Randomized neural networks for ensemble learning are also under-researched. It is widely accepted that ensemble methods can benefit from low bias and high variance learner <ref type="bibr" target="#b63">[64]</ref>. Thus, it worth investigating how to integrate randomized neural networks by using ensemble learning frameworks.</p><p>In <ref type="bibr" target="#b31">[32]</ref>, the authors investigated the approximation ability of standard multilayer feedforward networks with as few as a single hidden layer. They showed when the activation function is bounded and nonconstant, the standard multilayer feedforward networks are universal approximators. In <ref type="bibr" target="#b36">[37]</ref>, the authors showed when the parameters of the hidden layers are randomly sampled from a uniform distribution within a proper range, the resulting neural network, RVFL, is a universal approximator for a continuous function on a bounded finite dimensional set with efficient convergence rate. However, how to optimize the range for the random parameters remains untouched in the literature. Hence, this is another area where further research is required. Also, we should investigate the random parameter settings in other models such as polynomial, wavelet and so on.</p><p>"Big data" is a hot topic recently leading to an upsurge of research. Deep learning <ref type="bibr" target="#b30">[31]</ref> has been gaining its popularity in machine learning and computer vision community. It has been shown that high-level abstraction which comes from deep structure is a key factor leading to the success of many state-of-the-art systems in vision, language, and other AI-level tasks. Deep networks are much more difficult to train than shallow ones because they need a relatively large training data set to tune a large number of parameters. Despite the surge in interest in deep networks as more large scale data becomes available <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, the theoretical aspects have remained under-researched. In <ref type="bibr" target="#b41">[42]</ref>, the authors showed that deep but narrow networks trained with a greedy layer-wise unsupervised learning algorithm do not require more parameters than shallow ones to achieve universal approximation. However, when it comes to deep random neural network, it remains unclear regarding how to set the random parameters. Most importantly, the performance gap between deep random neural network and deep neural network trained with back propagation is wide in favor of back propagation. Moreover, when it comes to bigdata or large-scale learning, high dimensional feature space is usually preferred <ref type="bibr" target="#b16">[17]</ref>. In this case, commonly used approach for randomized neural network in prime space such as Eq. ( <ref type="formula" target="#formula_3">3</ref>) whose complexity is between quadratic and cubic with respect to the data samples may become computationally intractable. Thus, one may alternatively turn to other approaches such as conjugate gradient <ref type="bibr" target="#b48">[49]</ref>. On the other hand, solutions in the dual space <ref type="bibr" target="#b66">[67]</ref> has a time complexity of O(n 2 p) (p is the number of features and n is the number of data samples), which can also be very slow. In this case, randomized approximation methods such as <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b60">[61]</ref> can be more efficient and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this article, we presented an extensive survey on randomized methods for training neural networks, the use of randomization in kernel machines and related topics. We divided this family into several methods based on the network configuration. We believe that, this article, the first survey on randomized methods for training neural network, offers valuable insights into this important research topic. We also offered several potential future research directions. We trust that this article will encourage further advancements in this field. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure of RVFL. The input features are firstly transformed into the enhanced features by the enhancement nodes where parameters within them are randomly generated. All the original and enhanced features are connected to the output neurons</figDesc><graphic coords="4,106.86,384.61,357.36,185.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of RNN.</figDesc><graphic coords="6,193.43,448.60,184.23,140.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Basic structure of a denoising autoencoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Details of Lenet-5. (Figure adapted from [43]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>R I P T ✂✁ ☎✄ ✝✆ ✟✞ ✠✄ ☛✡ ✌☞ ✎✍ ✟✏ ✒✑ ✓✏ ✂✏ ✂✏ ✎✔ ✖✕ ☛✄ ☎✗ ☛✏ ✙✘ ✛✚ ✙✏ ✂✁ ✢✜ ✤✣ ✥✣ ✧✦ ❼✿ ▲❍ ✪❦ ✪❾ ★❦ ➝❱ ❜✸ ✶❆ ✶✯ ✪✿ ✵ ✶✰ ❅❆ r✵ ✻✳ ✪✸ ✶✰ ❷✷ ✴⑥ ✦ )✰ ❺❤ ❀✰ r✵ ✻❑ ✂✁ ★❚ ♦✱ ✎❻ ✇✷ ✹❈ ❯▼ ❯✷ ✹❴ ▲✳ ★✵ ✶✿ ▲✷ ✹❈ ✪✱ ✴❴ ✦❤ ❳✰ ❅✳ ✪✸ ✶✱ ✴❴ ☛❤ ❳✰ r✵ •✽ ❢✷ ✹✸ ✻❣ ✑❚ ★✯ ✪✰ ❅✸ ✶✰ ❜⑥ ✙✷ ✹✸ ❢❉ ✪✿ ▲❍ ✹✿ ✵ ✻✺ ✏✸ ✶✰ ❅❆ ❅✷ ✹❍ ✹❈ ✪✿ ✵ ✶✿ ▲✷ ✹❈ ➎❦ ✥• ✈✱ ✴❆ ❖✯ ✞❃ ★❴ ➂✱ ✴❈ ✪✰ (✿ ▲✺ ✏✱ ➜⑥ ✙✰ ❇✱ |✵ ✶✳ ★✸ ✶✰ ❷❋ ✛✱ ✴❃ )❚ ✄✿ ✁❦ ✰ ✹❦ ✪✱ ➜✺ ✶✰ r✵ ✏✷ ✴⑥ ❼✳ ✪❈ ✪✿ ✵ ✻✺ ✽ ❳✯ ★✷ ✹✺ ✶✰ (✽ ❢✰ ❅✿ ▲❍ ✹✯ ❯✵ ✶✺ ❜✱ ✴✸ ✶✰ (❆ ❅✷ ✹❈ ✪✺ ✵ ✶✸ ✶✱ ✴✿ ▲❈ ✪✰ ❅❉ ✞✵ ✶✷ ✎◗ ✑✰ ❷✿ ▲❉ ★✰ ❇❈ ❯✵ ✻✿ ▲❆ ❺✱ ✴❴ ✁❦å ➈ê ➞è ✧ç ✙ï ✶ë íï ✖å ➄è ✧ÿ ✙ä ✥ï ➽î ➓å ➄ae ☎ê ✒ø ➑é ➷è ✥ç ✙ï ✶ae ☎ä ✧ï ✖ú ✐ø ➀ì ➈ÿ ✎ê ✌û ➀å ✪✘ ❛ï ➊ä ❞ð ✫ã ✛ç ✙ï ➓è ✧ä ❿å ➄ø ➀é ☎å ✑✡ ✙û ➑ï ❶ì ➇ï ✱✯ ➵ ➊ø ➑ï ✖é ✐è ✒å ➄é ☎ù ➒✡ ✙ø ✓å ➈ê ✌ ➊ì ➈é ✐è ✧ä ✥ì ➈û ❦è ✥ç ✙ï ➓ï ❯➘ ✟ï ✒ ❶è ➞ì ➄ë ➵è ✧ç ✙ï ✶ê ✧ø ✠✂ ➈î ➻ì ➈ø ✓ù ❖é ☎ì ➈é ✦✝ û ➀ø ➑é ✙ï ❞å ➄ä ✥ø ➩è ❅✘ ❛ð ➃➏ ⑥ë ❦è ✥ç ✙ï ➉ ❶ì ➇ï ✱✯ ➵ ➊ø ➑ï ✖é ✐è ➔ø ➀ê ♣ê ✤î ➓å ➄û ➀û ✶➌ ✙è ✧ç ☎ï ➊é ✺è ✧ç ☎ï ➞ÿ ✙é ✙ø ➑è ➉ì ➈ae ◆ï ➊ä ❿å ✠è ✥ï ✖ê ø ➀é ✺å ➵➍ ✐ÿ ☎å ❛ê ✤ø ✙✝ ⑥û ➑ø ➀é ✙ï ✖å ➈ä ✛î ❭ì ✂ù ✂ï ➎➌ ☎å ➄é ✎ù ✶è ✥ç ✙ï ➞ê ✧ÿ ❼✡ ✦✝ ➠ê ✧å ➈î ❭ae ☎û ➑ø ➀é ❼✂ ➓û ✓å ✪✘ ➈ï ➊ä ✛î ➻ï ➊ä ✥ï ➊û ✠✘ ✡ ✙û ➀ÿ ✙ä ❿ê ➻è ✧ç ✙ï ❖ø ➑é ✙ae ☎ÿ ✂è ✖ð ➏ ⑥ë ➳è ✥ç ✙ï ↕ ❶ì ➇ï ✱✯ ➵ ➊ø ➑ï ✖é ✐è ✶ø ➀ê ➽û ➀å ➈ä ❺✂ ❛ï ✑➌ ✇ê ✧ÿ ❼✡ ✦✝ ➠ê ✥å ➄î ➻ae ✙û ➀ø ➑é ❼✂ ÿ ✙é ✙ø ➑è ✥ê ④ ➊å ➈é ✆✡ ✎ï ✒ê ✧ï ➊ï ✖é ✫å ➈ê (ae ◆ï ➊ä ✧ë íì ➈ä ✥î ➻ø ➑é ❼✂ ➽å ☛ ✤é ✙ì ❛ø ➀ê ❺✘ ✿ý ✞✍ ✌ ➻ì ❛ä ➉å ▲☛ ✤é ✙ì ❛ø ➀ê ❺✘ ✕ ➉ñ ④✧ ✟✌ ➳ë íÿ ☎é ✗ ↔è ✥ø ➑ì ❛é ➻ù ✙ï ➊ae ◆ï ➊é ☎ù ✂ø ➀é ❼✂ ➤ì ➈é ❙è ✧ç ☎ï (ú rå ➈û ➑ÿ ☎ï ➵ì ➈ë ☎è ✥ç ✙ï ✔✡ ✙ø ➀å ❛ê ➊ð ❣þ ➇ÿ ✗ ✒ ❶ï ❞ê ❅✝ ê ✧ø ➑ú ❛ï ❙û ✓å ✪✘ ➈ï ✖ä ✥ê ♣ì ➄ë ✔ ❶ì ➈é ➇ú ❛ì ➈û ➀ÿ ✂è ✧ø ➀ì ➈é ☎ê ➳å ➄é ✎ù ❺ê ✧ÿ ❼✡ ✦✝ ➠ê ✧å ➈î ➻ae ✙û ➑ø ➀é ❼✂ ✺å ➄ä ✥ï ✒è ❅✘ ➇ae ✙ø ✁ ➊å ➈û ➑û ✠✘ å ➄û ➑è ✧ï ✖ä ✧é ✎å ✠è ✧ï ❞ù ❢➌ ☎ä ✥ï ✖ê ✧ÿ ✙û ➑è ✧ø ➀é ❼✂ ➽ø ➀é ❖å ✏☛ ❺✡ ✙ø ➟✝ ⑥ae ☛✘ ➇ä ✥å ➈î ❭ø ✓ù ✌ ❇✰ ✇å ✠è ♣ï ✖å ✑ ❿ç ✫û ✓å ✪✘ ➈ï ✖ä ✒➌ ✙è ✧ç ✙ï é ➇ÿ ✙î ➉✡ ◆ï ➊ä ➳ì ➄ë ❣ë íï ❞å ✠è ✥ÿ ✙ä ✧ï ❙î ➓å ➄ae ☎ê ➉ø ➀ê ♣ø ➑é ✥ ❶ä ✥ï ✖å ➈ê ✧ï ✖ù ✫å ❛ê ➔è ✥ç ✙ï ❙ê ✧ae ☎å ➄è ✧ø ✓å ➄û ❯ä ✧ï ❞ê ✤ì ❛û ➑ÿ ✦✝ è ✧ø ➀ì ➈é ❭ø ✓ê ❯ù ✙ï ✒ ❶ä ✥ï ✖å ❛ê ✤ï ❞ù ☛ð</figDesc><table><row><cell>INPUT 32x32</cell><cell>C1: feature maps 6@28x28</cell><cell cols="2">C3: f. maps 16@10x10 S4: f. maps 16@5x5 S2: f. maps 6@14x14 C5: layer 120</cell><cell>F6: layer 84</cell><cell>OUTPUT 10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Full connection</cell><cell>Gaussian connections</cell></row><row><cell></cell><cell>Convolutions</cell><cell>Subsampling</cell><cell>Convolutions Subsampling</cell><cell cols="2">Full connection</cell></row></table><note><p>✁ ✭ ✇å ✑ ❿ç ➞ÿ ☎é ✙ø ➩è ❣ø ➀é ✒è ✥ç ✙ï ➵è ✧ç ✙ø ➀ä ❿ù ✒ç ✙ø ✓ù ✙ù ✂ï ✖é ❙û ✓å ✪✘ ➈ï ✖ä ❇ø ➀é ➑➞ ✗✂ ❄✝ ÿ ✙ä ✥ï ✄✑ ➵î ➓å ✪✘ ➉ç ☎å rú ❛ï ❣ø ➑é ☎ae ✙ÿ ✂è ❜ ➊ì ➈é ✙é ☎ï ✒ ↔è ✥ø ➑ì ❛é ☎ê ✟ë íä ✥ì ➈î ê ✤ï ✖ú ➈ï ✖ä ✥å ➈û rë íï ✖å ✠è ✥ÿ ✙ä ✥ï ➏î ➓å ➄ae ☎ê ø ➀é ✢è ✥ç ✙ï ✌ae ✙ä ✥ï ➊ú ➇ø ➀ì ➈ÿ ☎ê ✛û ✓å ✪✘ ➈ï ✖ä ✖ð ❦ã ✛ç ✙ï ➓ ❶ì ❛é ➇ú ➈ì ➈û ➀ÿ ✂è ✥ø ➑ì ❛é ☎✄ ✠ê ✧ÿ ❼✡ ✦✝ ➠ê ✧å ➈î ➻ae ✙û ➑ø ➀é ❼✂ ➚ ❶ì ➈î ➚✝ ✡ ✙ø ➀é ☎å ✠è ✥ø ➑ì ❛é ✏➌ ☛ø ➀é ☎ê ✤ae ☎ø ➑ä ✥ï ✖ù ✆✡ ☛✘ ✫õ ➔ÿ ✗✡ ✎ï ✖û ➏å ➄é ☎ù ✎ ø ➑ï ❞ê ✤ï ✖û ✡❁ ê ➉é ☎ì ➄è ✧ø ➀ì ➈é ✎ê ➉ì ➈ë ☛ ✧ê ✧ø ➑î ➚✝ ae ✙û ➀ï ✖✌ ➤å ➄é ✎ù ☛ | ❶ì ❛î ❭ae ☎û ➑ï ✄↔ ✔✌ ✞ ❶ï ✖û ➑û ✓ê ✒➌ ➄ó ➵å ❛ê ❦ø ➑î ➻ae ✙û ➀ï ➊î ➻ï ✖é ❛è ✥ï ✖ù ❭ø ➀é ➚✜ ☎ÿ ✙ô ➇ÿ ☎ê ✤ç ☎ø ➑î ➓å ❂❁ ê ñ ➔ï ✖ì ✦ ❶ì ✑✂ ❛é ✙ø ➑è ✧ä ✥ì ➈é ✞ ❜ ✑ ✆✠ ❖➌ ✎è ✧ç ✙ì ❛ÿ ❼✂ ➈ç ✫é ✙ì ➙✂ ➈û ➀ì ✑✡ ☎å ➈û ➑û ✠✘ ✺ê ✤ÿ ✙ae ◆ï ➊ä ✥ú ➇ø ➀ê ✧ï ✖ù ✺û ➀ï ✖å ➈ä ✧é ☎ø ➑é ❼✂ ae ✙ä ✥ì ✦ ❶ï ✖ù ✙ÿ ✙ä ✧ï ✌ê ✧ÿ ✗ ❿ç ✺å ➈ê ✎✡ ☎å ✑ ❿ô )✝ ⑥ae ✙ä ✥ì ➈ae ☎å ✑✂ ❛å ✠è ✥ø ➑ì ❛é ➓ó ➵å ❛ê (å rú ✠å ➄ø ➀û ➀å ✑✡ ✙û ➑ï ♣è ✧ç ☎ï ➊é ✝ð ❹✕ û ✓å ➄ä |✂ ➈ï ✒ù ✂ï ✒✂ ➈ä ✥ï ➊ï ➞ì ➈ë )ø ➀é ✐ú ✠å ➈ä ✧ø ✓å ➄é ✗ ➊ï ✌è ✧ì ➙✂ ➈ï ✖ì ➈î ➻ï ❶è ✥ä ✧ø ✁ ➤è ✧ä ❿å ➄é ✎ê ④ë íì ❛ä ✧î ➓å ✠è ✥ø ➑ì ❛é ☎ê (ì ➈ë è ✧ç ☎ï ❭ø ➀é ✙ae ✙ÿ ✙è ✌ ✖å ➄é ➛✡ ✎ï ➓å ✑ ❿ç ☎ø ➑ï ✖ú ➈ï ✖ù ➲ó (ø ➩è ✥ç ❖è ✥ç ✙ø ➀ê ➤ae ☎ä ✧ì ➎✂ ➈ä ✥ï ✖ê ✥ê ✤ø ➀ú ➈ï ➞ä ✥ï ✖ù ✙ÿ ✗ ↔è ✥ø ➑ì ❛é ì ➄ë ✝ê ✧ae ☎å ✠è ✥ø ➀å ➈û ✙ä ✥ï ✖ê ✧ì ➈û ➀ÿ ✂è ✧ø ➀ì ➈é ➵ ❶ì ❛î ➻ae ✎ï ✖é ☎ê ✧å ➄è ✧ï ❞ù ➝✡ )✘ ❭å ✌ae ✙ä ✥ì ✑✂ ❛ä ✧ï ❞ê ✧ê ✧ø ➑ú ❛ï )ø ➀é ✗ ❶ä ✥ï ✖å ❛ê ✤ï ì ➄ë ◆è ✧ç ☎ï ➔ä ✧ø ✁ ❿ç ✙é ✙ï ❞ê ✧ê ❯ì ➈ë ✎è ✥ç ✙ï ➔ä ✥ï ➊ae ✙ä ✥ï ✖ê ✧ï ➊é ✐è ✥å ➄è ✧ø ➀ì ➈é ➐➪ ➺è ✧ç ☎ï ➔é ➇ÿ ✙î ➉✡ ◆ï ➊ä ➏ì ➈ë ✎ë íï ❞å ✠è ✥ÿ ✙ä ✧ï î ➓å ➄ae ☎ê ✴➶ ↔ð</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Authors wish to thank Guest Editor Associate Professor Dianhui Wang and reviewers for providing us with their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamical behavior of artificial neural networks with random weights</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Albers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Engineering Systems Through Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast decorrelated neural network ensembles with random weights</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alhamdoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="104" to="117" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survey of random neural network applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bakırcıoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koc ¸ak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="330" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressive sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using random weights to train multilayer networks of hard-limiting units</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Barlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Downs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="210" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Large-Scale Kernel Machines</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure and dynamics of random recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The perceptron: A model for brain functioning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Block</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="123" to="135" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simulated annealing of neural networks: thecooling&apos;strategy reconsidered</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Boese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1993 IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="2572" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Radial basis functions, multi-variable functional interpolation and adaptive networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Near-optimal signal recovery from random projections: Universal encoding strategies?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rapid supervised learning neural network for function interpolation and approximation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1220" to="1230" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A rapid learning and dynamic stepwise updating algorithm for flat neural networks and the application to time-series prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiobjective evolutionary neural networks for time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al Mamun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Multi-Criterion Optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable kernel methods via doubly stochastic gradients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3041" to="3049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Immune and neural network models: theoretical and empirical comparisons</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Von Zuben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;09)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;09)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">JNN, a randomized algorithm for training multilayer networks in polynomial time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paugam-Moisy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teaching feed-forward neural networks by simulated annealing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="648" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1610" to="1623" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random neural networks with negative and positive signals and product form solution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gelenbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="502" to="510" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning in the feed-forward random neural network: A critical review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kocak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="384" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the problem of local minima in backpropagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural Networks: A comprehensive foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Network</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Functional-link nets with genetic-algorithm-based learning for robust nonlinear interval regression analysis</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1808" to="1816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Functional-link net with fuzzy integral for bankruptcy prediction</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-M</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2959" to="2968" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural networks for predicting conditional probability densities: Improved training scheme combining EM and RVFL</title>
		<author>
			<persName><forename type="first">D</forename><surname>Husmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic choice of basis functions in adaptive function approximation and the functional-link net</title>
		<author>
			<persName><forename type="first">B</forename><surname>Igelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Increased rates of convergence through learning rate adaptation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive nonlinear system identification with echo state networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="760" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep belief networks are compact universal approximators</title>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2192" to="2207" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fuzzy neural intelligent systems: mathematical foundation and the applications in engineering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Very sparse random projections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multisource data ensemble modeling for clinker free lime content estimate in rotary kiln sintering processes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.0247</idno>
		<title level="m">Random feedback weights support learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster ridge regression via the subsampled randomized hadamard transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">28</biblScope>
			<pubPlace>Addison-Wesley Reading, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to linear and nonlinear programming</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time computing without stable states: A new framework for neural computation based on perturbations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2531" to="2560" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Particle swarms for feedforward neural network training</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning with localized receptive fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Yale Univ., Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adaptive Pattern Recognition and Neural Networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The functional link net and learning optimal control</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="164" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Functional-link net computing</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="76" to="79" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A high-throughput screening approach to discovering good forms of biologically inspired visual representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1000579</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A theory of networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Universal approximation with convex optimization: Gimmick or reality?[discussion forum]</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Random vector functional link network for short-term electricity load demand forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratungac</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2015.11.039</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ensemble classification and regression -recent developments, applications and future directions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCI.2015.2471235</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Principles of neurodynamics. perceptrons and the theory of brain mechanisms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ridge regression learning algorithm in dual variables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Machine Learning</title>
		<meeting>the 15th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="515" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1089" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Feedforward neural networks with random weights</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IAPR International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning stochastic feedforward neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The random neural network: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Timotheou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A novel neural network based on immunity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IC-AI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training neural networks using multiobjective particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P T</forename><surname>Yusiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A comprehensive evaluation of random vector functional link networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2015.09.025</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction of data by deep distributed random samplings</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 Proceedings of the Sixth Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
