<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Sparse Texture Representation Using Local Affine Regions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@uiuc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<email>cordelia.schmid@inrialpes.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>665 Avenue de l&apos;Europe, 405 N. Mathews Ave</addrLine>
									<postCode>38330, 61801</postCode>
									<settlement>Montbonnot, Urbana</settlement>
									<region>IL</region>
									<country>France, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<email>ponce@cs.uiuc.edu</email>
						</author>
						<title level="a" type="main">A Sparse Texture Representation Using Local Affine Regions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B14B982D8012FAD947AE3CE640748F77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Processing and Computer Vision</term>
					<term>Feature Measurement</term>
					<term>Texture</term>
					<term>Pattern Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and non-rigid deformations. At the feature extraction stage, a sparse set of affine Harris and Laplacian regions is found in the image. Characteristic ellipses associated with these regions are mapped onto circles to achieve affine invariance, and the appearance of the corresponding normalized patches is represented using two novel intensity-based descriptors, the spin image and the RIFT descriptor. When affine invariance is not required, the original elliptical shape serves as an additional discriminative feature for texture recognition. The proposed approach is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of 1000 photographs of textured surfaces taken from different viewpoints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automated analysis of image textures has been the topic of extensive research in the past forty years (dating back at least to Julesz in 1962 <ref type="bibr" target="#b12">[13]</ref>). Current techniques for modeling texture include co-occurrence statistics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, filter banks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>, and random fields <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Unfortunately, most of these methods make restrictive assumptions about the nature of the input texture (e.g., stationarity), and they are not, in general, invariant with respect to 2D similarity and affine transformations, much less to 3D transformations such as viewpoint changes and non-rigid deformations of the textured surface. Invariance to such transformations is highly desirable for many applications, including wide-baseline stereo matching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, indexing and retrieval in image and video databases <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, and classification of images of materials <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>In this article, we set out to develop a texture representation that is invariant to any geometric transformations that can be locally approximated by an affine model. Because sufficiently small patches on the surfaces of smooth 3D objects are always approximately planar, local affine invariants are appropriate for modeling not only global 2D transformations of the image, but also perspective distortions that arise in imaging a planar textured surface, as well as non-rigid deformations that preserve the locally flat structure of the surface, such as the bending of paper or cloth. Our approach is aimed primarily at the recognition of so-called "albedo textures," due to spatial albedo variations on (mostly) smooth surfaces.</p><p>However, as demonstrated by the results of Section 4.2, it is also suitable for the recognition of "3D textures" arising from local relief variations on a surface.</p><p>The texture representation proposed in this article is based on descriptors extracted at a sparse set of local affine image regions. Like other approaches based on textons, or primitive texture elements <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, it involves representing distributions of 2D image features; unlike these, however, it performs shape selection, ensuring that descriptors are computed over neighborhoods whose shape is adapted to changes in surface orientation. The use of lo- cal affine invariants allows us to approximate a wide range of 3D transformations, effectively providing invariance to viewpoint and scale changes, perspective distortions, and non-rigid deformations. Finally, our method relies on spatial selection, i.e., finding a sparse set of image locations for computing descriptors. This is a significant departure from the traditional feature extraction framework, which involves processing every pixel location in the image.</p><p>Apart from being memory-and computation-intensive, this dense approach produces redundant texton dictionaries that may include, for instance, many slightly shifted versions of the same basic element <ref type="bibr" target="#b24">[25]</ref>. As Figure <ref type="figure" target="#fig_0">1</ref> illustrates, spatial selection is an effective way to reduce this redundancy.</p><p>Our approach consists of the following steps (see also Figure <ref type="figure" target="#fig_1">2</ref>):</p><p>1. Extract a sparse set of affine regions<ref type="foot" target="#foot_0">1</ref> in the shape of ellipses from a texture image.</p><p>The two region detectors used for this purpose are described in Section 3.1.</p><p>2. Normalize the shape of each elliptical region by transforming it into a circle. This reduces the affine ambiguity to a rotational one. Full affine invariance is achieved by computing rotation-invariant descriptors over the normalized regions. In Section 3.2, we introduce two novel rotation-invariant descriptors: one based on spin images used for matching range data <ref type="bibr" target="#b11">[12]</ref>, and one based on Lowe's SIFT descriptor <ref type="bibr" target="#b23">[24]</ref>.</p><p>3. Perform clustering on the affine-invariant descriptors to reduce the number of descriptors per image (Section 3.3). Summarize the distribution of features in the image in the form of a signature, containing a representative descriptor from each cluster and a weight indicating the relative size of the cluster.</p><p>4. Compare signatures of different images using the Earth Mover's Distance (EMD) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, which is a convenient and effective dissimilarity measure applicable to many types of image information. The output of this stage is an EMD matrix whose entries record the pairwise distance between the signatures of all the images in the database. The EMD matrix can be used for various retrieval and classification tasks, as described in Section 4.1. In Section 4, we will use two datasets to evaluate the capabilities of the proposed texture representation. The first one consists of photographs of textured surfaces taken from different viewpoints and featuring large scale changes, perspective distortions, and non-rigid transformations. The second set is the Brodatz database <ref type="bibr" target="#b2">[3]</ref>, which has significant inter-class variability, but no geometric transformations between members of the same class. Because affine invariance is not required in this case, we modify the basic feature extraction framework to use neighborhood shape as a discriminative feature to improve performance.</p><p>A preliminary version of this article has appeared in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Before presenting the details of our approach (Section 3), let us briefly discuss a number of texture models aimed at achieving invariance under various geometric transformations.</p><p>Early research in this domain has concentrated on global 2D image transformations, such as rotation and scaling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. However, such models do not accurately capture the effects of 3D transformations (even in-plane rotations) of textured surfaces. More recently, there has been a great deal of interest in recognizing images of textured surfaces subjected to lighting and viewpoint changes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>. A few methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref> are based on explicit reasoning about the 3D structure of the surface, which may involve registering samples of a material so that the same pixel across different images corresponds to the same physical point on the surface <ref type="bibr" target="#b19">[20]</ref> or applying photometric stereo to reconstruct the depth map of the 3D texture <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. While such approaches capture the appearance variations of 3D surfaces in a principled manner, they require specially calibrated datasets collected under controlled laboratory conditions. For example, the 3D texton representation of Leung and Malik <ref type="bibr" target="#b19">[20]</ref> naturally lends itself to the task of classifying a "stack" of registered images of a test material with known imaging parameters, but its applicability is limited in most practical situations.</p><p>In this presentation, we are interested in classifying unregistered texture images. This problem has been addressed by Cula and Dana <ref type="bibr" target="#b5">[6]</ref> and Varma and Zisserman <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, who have developed several dense 2D texton-based representations capable of very high accuracy on the challenging Columbia-Utrecht reflectance and texture (CUReT) database <ref type="bibr" target="#b6">[7]</ref>. The descriptors used in these representations are filter bank outputs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref> and raw pixel values <ref type="bibr" target="#b42">[43]</ref>. Even though these methods have been successful in the complex task of classifying images of materials despite significant appearance changes, the representations themselves are not invariant to the changes in question. In particular, the support regions for computing descriptors are the same in all images; no adaptation is performed to compensate for changes in surface orientation with respect to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head><p>Previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> This article</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial selection</head><p>None: every pixel is considered Laplacian and Harris affine region detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighborhood shape selection</head><p>None: neighborhood size is fixed Affine adaptation process <ref type="bibr" target="#b7">[8]</ref> Descriptor computation Filter banks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>, pixel values <ref type="bibr" target="#b42">[43]</ref> Novel descriptors: spin images, RIFT Finding textons Clustering, universal texton dictionaries</p><p>Clustering, separate texton representation for each image Representing/comparing texton distributions Histograms/χ 2 distance Signatures/Earth Mover's Distance <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> Table <ref type="table">1</ref>: The components of our approach, contrasted with other 2D texton-based methods.</p><p>Because they lack representation-level invariance, the above methods require the use of multiple models or prototypes to represent a single texture. As long as the training set adequately samples all viewpoints and lighting directions that can be encountered at test time, the texture can be successfully recognized. On the other hand, when the test set contains images not represented in the training set (e.g., images at significantly different scales), performance drops dramatically <ref type="bibr" target="#b41">[42]</ref>. In our work, we shift away from this dependence on representative training sets by developing a texture representation with built-in geometric invariance. Note, however, that our present method does not explicitly account for changes in lighting direction and associated 3D effects such as self-shadowing; therefore, to achieve robust recognition in the presence of such effects, we must still rely on multiple prototypes in the training set.</p><p>Table <ref type="table">1</ref> summarizes the main components of our representation and contrasts it with other 2D texton-based methods.</p><p>3 Components of the Representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Affine Regions</head><p>Conceptually, our approach may be traced back to early articles on the extraction of local features in natural images, where emphasis is placed on locating perceptually salient primitives such as blobs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>. Blostein and Ahuja <ref type="bibr" target="#b1">[2]</ref> were the first to introduce a multiscale blob detector based on maxima of the Laplacian. Lindeberg <ref type="bibr" target="#b20">[21]</ref> has extended this detector in the framework of automatic scale selection, where a "blob" is defined by a scale-space location where a normalized Laplacian measure attains a local maximum. Informally, the spatial coordinates of the maximum become the coordinates of the center of the blob, and the scale at which the maximum is achieved becomes its characteristic scale. Gårding and</p><p>Lindeberg <ref type="bibr" target="#b7">[8]</ref> have also shown how to design an affine blob detector using an affine adaptation process based on the second moment matrix. This process forms an important part of the affine-invariant region detection frameworks of Baumberg <ref type="bibr" target="#b0">[1]</ref> and Mikolajczyk and Schmid <ref type="bibr" target="#b29">[30]</ref>. Both of these methods rely on a multiscale version of the Harris operator <ref type="bibr" target="#b10">[11]</ref> to localize interest points in space. Alternative region extraction schemes include the "entropy detector" of Kadir and Brady <ref type="bibr" target="#b14">[15]</ref>, the difference-of-Gaussians (or DoG) detector of Lowe <ref type="bibr" target="#b23">[24]</ref>, the "maximally stable extremal regions" of Matas et al. <ref type="bibr" target="#b27">[28]</ref>, and the cornerand intensity-based operators of Tuytelaars and Van Gool <ref type="bibr" target="#b40">[41]</ref>. Of the above, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> are scale-invariant, while <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref> are fully affine-invariant. This proliferation of region detectors, motivated primarily by applications to wide-baseline stereo matching and image retrieval, attests to the increased importance accorded to the spatial and shape selection principles in the computer vision community.</p><p>In this work, we use two types of detectors: the Harris-affine detector of Mikolajczyk and Schmid <ref type="bibr" target="#b29">[30]</ref> and the Laplacian blob detector of Gårding and Lindeberg <ref type="bibr" target="#b7">[8]</ref>. <ref type="foot" target="#foot_1">2</ref> Figure <ref type="figure" target="#fig_2">3</ref> shows the output of the detectors on two sample images. Note that the Harris detector tends to find corners and points at which significant intensity changes occur, while the Laplacian detector is (in general) attracted to points that can be thought of as centers of roughly elliptical regions of uniform intensity. Intuitively, the two detectors provide complementary kinds of information about the image: The former responds to regions of "high information content" <ref type="bibr" target="#b29">[30]</ref>, while the latter produces a perceptually plausible decomposition of the image into a set of blob-like primitives.</p><p>The technical details of automatic scale selection and affine adaptation, described in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>, are beyond the scope of this article. For our purposes, it is sufficient to note that the affinely adapted regions localized by the Harris and Laplacian detectors are represented as ellipses. We can normalize these regions by mapping the corresponding ellipses onto a unit circle. Because the circle is invariant under rotations and reflections, it can be easily shown that the normalization process has an inherent orthogonal ambiguity. In some previous work, this ambiguity has been resolved by finding a dominant gradient orientation of the patch <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. We have found this process unreliable for regions produced by the Laplacian detector, since many of them lack strong edges in the center, and are more or less circularly symmetric. Instead, similarly to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>, we choose to represent each normalized patch by a rotationally invariant descriptor.</p><p>Note. To achieve invariance to local affine transformations, as in the experiments of Section 4.2, we discard the information contained in the affine shape of the patches. However, as a glance at Figure <ref type="figure" target="#fig_2">3</ref> suggests, this shape can be a distinctive feature when affine invariance is not required. This point will be revisited in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rotation-Invariant Descriptors</head><p>In this article, we introduce two novel "region-based" <ref type="bibr" target="#b29">[30]</ref> rotation-invariant descriptors. We have eschewed the more popular filter-based features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> because of growing evidence in favor of region-based representations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref>, as will be discussed in detail at the end of this section.</p><p>Intensity-domain spin images. Our first rotation-invariant descriptor is inspired by the spin images introduced by Johnson and Hebert <ref type="bibr" target="#b11">[12]</ref> for matching range data. We implement the spin image as a "soft histogram" where each pixel within the support region contributes to more than one bin. Specifically, the contribution of a pixel located in</p><p>x to the bin indexed by (d, i) is given by exp -</p><formula xml:id="formula_0">(|x -x 0 | -d) 2 2α 2 - |I(x) -i| 2 2β 2 ,</formula><p>where x 0 is the location of the center pixel, and α and β are the parameters representing the "soft width" of the two-dimensional histogram bin. The use of soft histograms has been advocated by Koenderink and Van Doorn <ref type="bibr" target="#b16">[17]</ref> because it alleviates aliasing effects. Figure <ref type="figure" target="#fig_4">4</ref> shows the principle behind the construction of spin images. To achieve invariance to affine transformations of the image intensity function (that is, transformations of the form I → aI + b), it is sufficient to normalize the range of the intensity function within the support region of the spin image <ref type="bibr" target="#b35">[36]</ref>. Unfortunately, we have found this normalization to be sensitive to noise and resampling artifacts, particularly for normalized patches that are only a few pixels wide. In our implementation, the noise problem is partially alleviated by slightly blurring the normalized patches with a Gaussian kernel before computing the spin image.</p><p>RIFT descriptors. To compensate for the shortcomings of spin images, we augment our representation of local appearance with an additional rotation-invariant descriptor, which is a generalization of Lowe's SIFT <ref type="bibr" target="#b23">[24]</ref>. We have chosen SIFT descriptors as a model because of the superior performance they have shown for retrieval in image databases <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our descriptor, dubbed RIFT (for Rotation-invariant sIFT), is constructed as follows. The circular normalized patch is divided into concentric rings of equal width, and a gradient orientation histogram is computed within each ring (Figure <ref type="figure" target="#fig_5">5</ref>). To maintain rotation invariance, this orientation is measured at each point relative to the direction pointing outward from the center. We use four rings and eight histogram orientations, yielding 32-dimensional descriptors. <ref type="foot" target="#foot_3">3</ref> Note that the RIFT descriptor as described above is not invariant to flipping of the normalized patch, which reverses the order of directions in the orientation histogram. However, we are not concerned with this circumstance in our current work, since realistic imaging conditions do not involve reversing the orientation of a textured surface. Discussion. Like the original SIFT descriptor, both spin images and RIFT work by subdividing the region of support and separately histogramming appearance attributes inside each subregion. According to the terminology of Mikolajczyk and Schmid <ref type="bibr" target="#b30">[31]</ref>, such descriptors may be called "distribution-based" or "region-based." Because of the strong performance of SIFT in their evaluations, these authors have conjectured that as a region-based descriptor it has an advantage over varions less expressive "point-wise" representations such as filter banks. This conjecture is supported by experiments performed at an earlier stage of this research <ref type="bibr" target="#b18">[19]</ref>, where spin images achieved better results than rotation-invariant "Gabor-like" linear filters <ref type="bibr" target="#b37">[38]</ref> (note that this filter bank is similar in terms of performance and dimensionality to several others in popular use <ref type="bibr" target="#b41">[42]</ref>). Interestingly, a vector of all pixel values in some image region can also be considered a region-based descriptor where each subregion is a single pixel. Therefore, the finding of Varma and Zisserman <ref type="bibr" target="#b42">[43]</ref> that raw pixel neighborhoods outperform filter outputs on the CUReT database can also be regarded as evidence in favor of region-based descriptors.</p><p>In this article, we perform a comparative evaluation of spin images and RIFT. As will be seen from Section 4, spin images tend to perform better (possibly due to their higher dimensionality). However, combining the two descriptors in a unified recognition framework generally produces better results than using either one in isolation. This may be a reflection of the fact that spin images and RIFT rely on complementary kinds of image information -the former uses normalized graylevel values, while the latter uses the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Signatures and the Earth Mover's Distance</head><p>One commonly thinks of a texture image as being "generated" by a few basic primitives, or textons <ref type="bibr" target="#b13">[14]</ref>, repeated many times and arranged in some regular or stochastic spatial pattern. In the field of texture analysis, clustering is the standard technique for discovering a small set of primitives based on a large initial collection of texture element instances.</p><p>Accordingly, we perform clustering on each texture image separately to form its signature</p><formula xml:id="formula_1">{(m 1 , u 1 ), (m 2 , u 2 ), . . . , (m k , u k )},</formula><p>where k is the number of clusters, m i is the medoid, or most centrally located element <ref type="bibr" target="#b15">[16]</ref> of the ith cluster, and u i is the relative weight of the cluster (in our case, the size of the cluster divided by the total number of descriptors extracted from the image). Signatures have been introduced by Rubner et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> as representations suitable for matching using the Earth Mover's Distance (EMD). The EMD between two signatures</p><formula xml:id="formula_2">S 1 = {(m 1 , u 1 ), (m 2 , u 2 ), . . . , (m k , u k )} and S 2 = {(n 1 , v 1 ), (n 2 , v 2 ), . . . , (n l , v l )} has the form d(S 1 , S 2 ) = i j f ij d(m i , n j ) i j f ij ,</formula><p>where the scalars f ij are flow values that are determined by solving a linear programming problem, and the scalars d(m i , n j ) are the ground distances between different cluster medoids.</p><p>Note that the weights u i and v j do not appear in the equation, but are part of the optimization setup (see <ref type="bibr" target="#b34">[35]</ref> for details). In our case, m i and n j may be spin images and RIFT descriptors, and the ground distance is simply the Euclidean distance. Since our descriptors are normalized to have unit norm, the ground distances lie in the range [0, 2]. We rescale this range to [0, 1], thus ensuring that all EMD's are between 0 and 1 as well.</p><p>For our application, the signature/EMD framework offers several advantages over the alternative histogram/χ 2 distance framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. A signature is more robust and descriptive than a histogram, and it avoids the quantization and binning problems associated with histograms, especially in high dimensions <ref type="bibr" target="#b34">[35]</ref> (recall that our spin images and RIFT descriptors are 100-and 32-dimensional, respectively). The EMD has been shown to be (relatively) insensitive to the number of clusters, i.e., when one of the clusters is split during signature computation, replacing a single medoid with two, the resulting EMD matrix is not much affected <ref type="bibr" target="#b33">[34]</ref>. This is a very important property, since automatic selection of the number of clusters remains an unsolved problem. In addition, in evaluations conducted by Rubner et al. <ref type="bibr" target="#b34">[35]</ref>, the combination of signatures with the EMD has consistently outperformed histograms and the χ 2 metric for texture retrieval. Finally, the EMD/signature framework has the advantage of efficiency and modularity: It frees us from the necessity of clustering descriptors from all images together and computing a universal texton dictionary, which may not represent all texture classes equally well <ref type="bibr" target="#b5">[6]</ref>.</p><p>4 Experimental Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Strategy</head><p>Channels. Tuytelaars and Van Gool <ref type="bibr" target="#b40">[41]</ref> have articulated the goal of building an opportunistic neighborhood extraction system that would combine the output of several region detectors tuned to different kinds of image structure. In this spirit, the texture representation proposed in this article is designed to support multiple region detectors and descriptors.</p><p>Each detector/descriptor pair is treated as an independent channel that generates its own signature representation for each image in the database, and its own EMD matrix of pairwise inter-image distances. To combine the outputs of several channels, we simply add the corresponding entries in the EMD matrices. This approach was empirically determined to be superior to forming linear combinations with varying weights, or taking the minimum or maximum of the distances.</p><p>Since our experimental setup involves the evaluation of two region detectors and two descriptors, we end up with four channels: Harris regions and spin images (HS), Harris regions and RIFT descriptors (HR), Laplacian regions and spin images (LS), and finally, Laplacian regions and RIFT descriptors (LR). In addition, we will introduce in Section 4.3 the Harris and Laplacian ellipse channels, denoted HE and LE, respectively. To simplify the notation for combined channels, we will use (in a purely formal, "syntactic" manner) the distributive law: For example, we will write (H+L)R instead of HR+LR for the combination of the Harris/RIFT and Laplacian/RIFT channels, and (H+L)(S+R) for the combination of all four detector/descriptor channels.</p><p>Retrieval. We use the standard procedure followed by several Brodatz database evaluations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>. Given a query image, we select other images from our database in increasing order of EMD, i.e., from the most similar to the least similar. Each image in the database is used as a query image once, and the performance is summarized in a plot of average recognition rate (number of correct images retrieved over total number of correct images) vs. number of retrievals (Figures <ref type="figure" target="#fig_7">7</ref> and<ref type="figure" target="#fig_10">9</ref> are of this type). Note that perfect performance would correspond to 100% recognition rate after n -1 retrievals, where n is the number of images of the given class in the database.</p><p>Classification. In effect, the evaluation framework described above measures how well each texture class can be modeled by individual samples. It is not surprising that retrieval can fail in the presence of sources of variability that are not fully accounted for by the invariance properties of the representation (recall that our representation provides invariance to local geometric deformations and affine illumination changes, but not to complex viewpoint-and lighting-dependent appearance changes). To obtain a more balanced assessment of performance, a texture representation should be evaluated using classification as well as retrieval.</p><p>In the classification framework, a model for a class is created not from a single (possibly atypical) image, but from a set of multiple training images, thereby compensating for the effects of intra-class variability.</p><p>In our implementation, we use nearest-neighbor classification with EMD. The training set is selected as a fixed-size random subset of the class, and all remaining images comprise the test set. To eliminate the dependence of the results on the particular training images used, we report the average of the classification rates obtained for different randomly selected training sets. More specifically, a single sequence of 200 random subsets is generated and used to evaluate all the channel combinations seen in Tables <ref type="table">2</ref> and<ref type="table" target="#tab_5">4</ref>. This ensures that all the rates are directly comparable, i.e., small differences in performance cannot be attributed to random "jitter".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset 1: Textured Surfaces</head><p>To test the invariance properties of our proposed representation, we have collected a texture database consisting of 1000 uncalibrated, unregistered images: 40 samples each of 25 different textures. The database is publicly available at http://www-cvr.ai.uiuc.edu/ponce grp.</p><p>Figure <ref type="figure">6</ref> shows four sample images from each class (the resolution of the samples is 640 × 480 pixels). Note that the database includes surfaces whose texture is due mainly to albedo variations (e.g., wood and marble), 3D shape (e.g., gravel and fur), or a mixture of both (e.g., carpet and brick). Significant viewpoint changes and scale differences are present within each class. To push the limits of our system, we have exercised additional sources of variability: non-rigid transformations, inhomogeneities in the texture pattern, illumination changes, and viewpoint-dependent appearance variations.</p><p>Each image in the database is processed with the Harris and Laplacian detectors. The median number of Harris (resp. Laplacian) regions extracted per image is 926 (resp. 4591).</p><p>The median number of combined regions is 5553, or about 1.8% of the total number of pixel locations in the image. Thus, we can see that the spatial selection performed by the detectors results in a drastic compression of the amount of data that needs to be handled   <ref type="table">2</ref> shows a comparison of 10sample classification rates for different channel combinations. The same trends that were seen in Figure <ref type="figure" target="#fig_7">7</ref> are echoed here: Spin images perform better than RIFT, Laplacian regions perform better than Harris, and the combination (H+L)(S+R) has the highest performance.</p><p>We may wonder whether the superior performance of Laplacian points is due to the denser representation they afford (recall that the Laplacian detector finds almost five times as many regions as the Harris). and "full" Laplacian representations may be compared by looking at columns 3 and 4 of Table <ref type="table" target="#tab_2">3</ref>. Interestingly, while the rates may vary significantly for individual textures, the averages (bottom row) are almost the same: 91.93% and 91.96% for "truncated" and "full", respectively. Thus, recognition performance cannot be regarded as a simple function of the density of the representation.</p><p>Finally, Table <ref type="table" target="#tab_2">3</ref> allows us to analyze the "difficulty" of individual textures for our system. To this end, the textures are arranged in order of increasing (H+L)(S+R) classification rate (last column). Roughly speaking, classification rate is negatively correlated with the homogeneity of the texture: Some of the lowest rates belong to inhomogeneous coarse-scale textures like bark (T02, T03) and marble (T09), while some of the highest belong to homogeneous fine-scale textures like glass (T17), water (T07), and fabric (T20, T24). However, this relationship is not universal. For example, granite (T08), which is fine-grained and quite uniform, has a relatively low rate of 86.78%, while the large-scale, inhomogeneous wall (T13) has a relatively high rate of 95.92%. It is also interesting (and somewhat unexpected) that the performance of different classes does not depend on their nature as 3D or albedo textures. Overall, the intrinsic characteristics of the various textures do not seem to provide a clear pattern for predicting performance. This is not surprising if one keeps in mind that classification performance is not related directly to intra-class variability, but to the extent of separation between the classes in feature space.    improve by over 11% to 75.89% and 77.21%, respectively, whereas the performance of our method changes by only 3.42% (from 92.61% to 96.03%).</p><p>The outcome of the comparison validates our intuition that intrinsic representation-level invariance is necessary to achieve robustness to large viewpoint and scale changes, especially in situations when the lack of invariance cannot be fully compensated by storing multiple prototypes of each texture. This is indeed the case for our dataset, which has relatively few sample images per class but high intra-class variability, including non-homogeneous textures and unconstrained non-rigid deformations. By contrast, the CUReT database lacks these sources of variability. In particular, CUReT images have no scale variation (all materials are held at the same distance from camera, only the orientation is changed), limited in-plane rotation, and the same physical surface patch is represented in all samples. In addition, the appearance of each patch in that database is systematically sampled under different combinations of viewing angles and lighting directions, making it straightforward to select a fixed representative subset of samples for training, as is done in most CUReT evaluations. Our evaluation scheme is a lot more punishing in comparison, as it averages classification rates over many different randomly chosen training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset 2: Brodatz Database</head><p>The Brodatz database <ref type="bibr" target="#b2">[3]</ref> is perhaps the best known benchmark for texture recognition algorithms. In recent years, it has been criticized because of the lack of intra-class variation that it exhibits. However, we feel that it is premature to dismiss the Brodatz database as a challenging platform for performance analysis. For one, relatively few publications actually report results on the entire database (the only studies known to us are <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>). In addition, while near-perfect overall results have been shown for the CUReT database <ref type="bibr" target="#b42">[43]</ref>, the best (to our knowledge) retrieval performance on the Brodatz database is around 84% <ref type="bibr" target="#b45">[46]</ref>. The reason for this is the impressive diversity of Brodatz textures, some of which are quite perceptually similar, while others are so inhomogeneous that a human observer would arguably be unable to group their samples "correctly". The variety of the scales and geometric patterns of the Brodatz textures, combined with an absence of intraclass transformations, makes them a good platform for testing the discriminative power of an additional local shape channel in a context where affine invariance is not necessary, as described below.</p><p>The shape channel. The shape of an affinely adapted region is encoded in its local shape matrix, which can also be thought of as the equation of an ellipse. Let E 1 and E 2 be two ellipses in the image plane. We eliminate the translation between E 1 and E 2 by aligning their centers, and then compute the dissimilarity between the regions as</p><formula xml:id="formula_3">d(E 1 , E 2 ) = 1 - Area (E 1 ∩ E 2 ) Area (E 1 ∪ E 2 ) .</formula><p>In the experiments of this section, we use local shape to obtain two additional channels, HE and LE, corresponding to the ellipses found by the Harris and Laplacian detectors, respectively. Notice that the ellipse ground distance, and consequently all shape-based EMD's, must be between 0 and 1. Because the descriptor-based EMD's lie in the same range, the shape-based EMD's can be combined with them through simple addition.</p><p>Finally, it is worth noting that the ellipse "distance" as defined above takes into account the relative orientations of the two ellipses. If it is necessary to achieve rotation invariance, we can simply align the major and minor axes of the two ellipses before comparing their areas.</p><p>Results. The Brodatz database consists of 111 images. Following the same procedure as previous evaluations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>, we form classes by partitioning each image into nine nonoverlapping fragments, for a total of 999 images. Fragment resolution is 215 × 215 pixels.  By comparison with the texture database discussed in the previous section, relatively few regions are extracted from each image: The median values are 132 for the Harris detector, 681 for the Laplacian, 838 combined. Some images contain less than 50 regions total. Given such small numbers, it is difficult to select a fixed number of clusters suitable for all images in the database. To cope with this problem, we replace k-means by an agglomerative clustering algorithm that repeatedly merges clusters until the average intra-cluster distance exceeds a specified threshold <ref type="bibr" target="#b15">[16]</ref>. This process results in variable-sized signatures, which are successfully handled by the EMD framework. An additional advantage of agglomerative clustering as opposed to k-means is that it can be used for the shape channel, since it can take as input the matrix of pairwise distances between the ellipses. The trends noted above are also apparent in the classification results presented in Table <ref type="table" target="#tab_5">4</ref>.</p><p>By looking at the first two rows, we can easily confirm the relatively strong performance of the RIFT descriptor (particularly for the Laplacian detector), as well as the marginal drop in performance of the L+H channels as compared to L alone. The latter effect is also seen in the last row of the We can get a more detailed look at the performance of our system by examining Figure <ref type="figure" target="#fig_0">10</ref>,  The best retrieval performance curve of our system, corresponding to the (H+L)(S+R+E) combination, has 76.26% recognition rate after 8 retrievals. This is a slightly higher than the results reported in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>, but below Xu et al. <ref type="bibr" target="#b45">[46]</ref>, who report 84% recognition rate using the multiresolution simultaneous autoregressive (MRSAR) model. MRSAR models texture as a stationary random field and uses a dense representation with fixed neighborhood shape and size. A known shortcoming of MRSAR is its limited ability to measure perceptual similarity -the method tends to confuse textures that appear very different to human observers <ref type="bibr" target="#b21">[22]</ref>. Most significantly, the MRSAR model is difficult to extend with affine invariance. By contrast, our representation is specifically formulated with geometric invariance in mind, is non-parametric, and does not make any statistical assumptions about the input texture. </p><formula xml:id="formula_4">which</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this article, we have introduced a sparse affine-invariant texture representation that applies spatial and shape selection to automatically determine the locations and support regions of salient local texture regions. In summary, the main contributions of our work are:</p><p>• A sparse representation: The experiments of Section 4 show that it is possible to successfully recognize many textures based on the information contained in a very small number of image regions.</p><p>• Spatial and shape selection: These mechanisms provide robustness against viewpoint changes, non-rigid deformations, and non-homogeneity of the texture pattern. In addition, affine regions capture important perceptual characteristics of many textures.</p><p>• Novel intensity-based descriptors: Spin images and RIFT descriptors, presented in Section 3.2, provide exactly the right degree of invariance while serving as a rich description of the intensity pattern of local texture patches.</p><p>• A flexible approach to invariance: Our system is flexible in that local shape information may either be discarded or used as a feature, depending on the degree of invariance required by the application.</p><p>In our experiments, we have evaluated two detectors and two descriptors on two datasets of about a thousand images each. The first dataset has tested the invariance of our proposed representation to viewpoint changes, as well as complex appearance changes and nonrigid deformations. A comparative evaluation with the VZ method <ref type="bibr" target="#b42">[43]</ref> has confirmed that representation-level affine invariance is necessary for achieving good performance. The second dataset, Brodatz, has allowed us to test the descriptive power of the local shape information found by the affine region detectors. Our retrieval results for the entire Brodatz database, while not the best to date, are comparable to those reported in several previous studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Next, let us briefly summarize the findings of Section 4. The Laplacian detector has shown better overall performance for both texture datasets; nevertheless, as can be seen from Table <ref type="table" target="#tab_2">3</ref>, the Harris detector can have superior performance for certain individual classes. The Harris detector also has the advantage of producing much sparser image representations than the Laplacian. As for descriptors, spin images won over RIFT for the texture database in Section 4.2, while RIFT worked slightly better for the Brodatz database in Section 4.3 (however, recall that the latter comparison is confounded by signature size variability). Despite these differences in performance, it is advantageous to retain both descriptors, as combining their outputs generally improves the accuracy of recognition.</p><p>Perhaps the most important general observation we can make is that both the descriptors and the detectors tend to fluctuate in performance from texture to texture and from database to database. Two lessons can be drawn from this. On the one hand, researchers working on specific applications of texture analysis should conduct comparative evaluations on representative data to discover which channels or channel combinations would work best in their case. On the other hand, to achieve a general understanding of the expressive power of different channels on different types of texture, it is necessary to conduct systematic studies using much larger databases, as well as larger numbers of descriptors and detectors, or even parameterized descriptor/detector families.</p><p>Another issue requiring further study is the method for combining channels. For our two datasets, the simple method of adding the individual EMD matrices has generally proven effective in boosting performance. However, as we have learned from our Brodatz experiments, it is occasionally possible for the combined recognition rate to actually be lower than the single-channel rates. In the future, we plan to study more sophisticated methods for combining channels that would not suffer from similar detrimental effects.</p><p>Finally, we plan to strengthen the proposed texture representation using spatial relationships between neighoring regions. A few recent representations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref> have used a two-level scheme, with intensity-based textons at the first level and histograms of texton distributions over local neighborhoods at the second level. For many natural textures, the arrangement of affine regions captures perceptually important information about the global geometric structure. Augmenting our representation with such information is likely to increase its ability to distinguish textures that have similar local neighborhoods but different spatial layouts.</p><p>To date, we have conducted preliminary experiments in classification of individual texture regions using simple co-occurrence relations <ref type="bibr" target="#b17">[18]</ref>. We expect that a richer two-level texture representation will be useful for the problem of segmenting and classifying natural images that contain multiple texture categories such as sky, water, plants, and man-made structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Implementation of the VZ Method</head><p>At the feature extraction stage, all N × N pixel neighborhoods in an image are taken and reordered to form N 2 -dimensional feature vectors. In our implementation, N = 11 for 121-dimensional features. This was selected as the smallest neighborhood size to yield a feature space of higher dimensionality than spin images. To provide some invariance to illumination changes, the vectors are normalized to zero mean and unit norm. To compute the texton dictionary, five images per class are chosen at random, and 20% of all feature vectors extracted from these images are retained, also at random. This reduction in the number of feature vectors is motivated primarily by the memory limitations of our system.</p><p>The feature vectors for all 25 classes are clustered using k-means into 40 clusters each, resulting in a dictionary of 1000 textons. For the VZ-joint variant, each pixel in an image is labeled by its nearest texton center, and the distribution of all texton labels is represented using a 1000-dimensional histogram. For the VZ-MRF variant, an image is represented using a two-dimensional histogram: For each texton, a conditional distribution of the center pixel is stored as a histogram with 20 bins. This gives us a 20, 000-dimensional representation for each image. In both cases, histograms are compared using the χ 2 distance, and nearestneighbor classification is used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The effect of spatial selection on a texton dictionary. (a) Original texture image. (b) Top 20 textons found by clustering all 13 × 13 patches of the image. (c) Sparse set of regions found using spatial selection. (d) Textons obtained by clustering regions from (c).</figDesc><graphic coords="3,125.61,67.01,100.80,100.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the feature extraction system proposed in this article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Output of the Harris and Laplacian region detectors on two natural images. Left: original images, center: regions found by the Harris detector, right: regions found by the Laplacian detector. Note that the Laplacian detector tends to produce a denser set of regions than the Harris.</figDesc><graphic coords="8,83.23,232.05,144.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The intensitydomain spin image proposed in this article is a two-dimensional histogram encoding the distribution of image brightness values in the neighborhood of a particular reference (center) point. The two dimensions of the histogram are d, distance from the center point, and i, the intensity value. The "slice" of the spin image corresponding to a fixed d is simply the histogram of the intensity values of pixels located at a distance d from the center. Since the d and i parameters are invariant under orthogonal transformations of the image neighborhood, spin images offer exactly the right degree of invariance for representing affine-normalized patches. In the experiments reported in Section 4, we used 10 bins for distance and 10 for intensity value, resulting in 100-dimensional descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Construction of spin images. Three sample points in the normalized patch (left) map to three different locations in the descriptor (right).</figDesc><graphic coords="10,145.75,185.51,99.08,99.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Construction of RIFT descriptors. Three sample points in the normalized patch (left) map to three different locations in the descriptor (right).</figDesc><graphic coords="11,361.11,205.64,51.29,104.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Four samples each of the 25 texture classes used in the experiments of Section 4.2.The entire database may be downloaded from http://www-cvr.ai.uiuc.edu/ponce grp.</figDesc><graphic coords="16,77.67,513.50,83.05,62.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc>Figure7shows retrieval results for the texture database. The first observation is that spin images perform better than RIFT descriptors, and the combination of the two descriptors performs slightly better than spin images alone. Next, Laplacian regions (part (a) of the figure) perform better than Harris (b), and the combination of the two (c) is slightly better than Laplacian alone. The solid curve in part (c) of the figure shows the retrieval performance obtained by combining all four detector/descriptor channels. The recognition rate after 39 retrievals is 58.92%. This relatively low number is a reflection of the considerable intra-class variability of the database. As discussed in Section 4.1, we cannot expect that all samples of the same class will be well represented by a single prototype. Accordingly, the combined (H+L)(S+R) classification rate is only 62.15% for one training sample, but it goes up rapidly to 92.61% for 10 samples and 96.03% for 20 samples. Table2shows a comparison of 10-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8</head><label>8</label><figDesc>Figure 8 also confirms the reliance of the VZ methods on multiple prototypes: When the training set size is increased from 10 to 20, the classification rates of VZ-Joint and VZ-MRF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Retrieval curves for the Brodatz database. (a) Harris descriptor channels. (b) Laplacian descriptor channels. (c) Combined Harris and Laplacian descriptor channels. (d) Comparison of combined performance with and without the ellipse channels (HE and LE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9</head><label>9</label><figDesc>Figure 9 shows retrieval results for the Brodatz database. Similarly to the results of the previous section, the Laplacian channels, shown in part (b) of the figure, have better performance than the Harris channels, shown in part (a). Interestingly, though, for the Brodatz database RIFT descriptors perform better than spin images -the opposite of what we have found in Section 4.2. However, this discrepancy is due at least in part to the variability of signature size (due to the use of agglomerative clustering) in the experimental setup of this section. On average, the RIFT-based signatures of the Brodatz images have more clusters than the spin-based signatures, and we conjecture that this raises the discriminative power of the RIFT channel. Another interesting point is that combining the Harris and Lapla-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>shows a histogram of classification rates for all 111 classes using three training samples per class. The histogram reveals that the majority of textures are highly distinguishable, and only a few stragglers are located at the low end of the spectrum. In fact, 36 classes have 100% classification rate, 49 classes have classification rate at least 99%, and 73 classes (almost two thirds of the total number of classes) have rate at least 90%. The mean rate is 87.44%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 showsFigure 10 :</head><label>1110</label><figDesc>Figure11shows four textures that were classified successfully and four textures that were classified unsuccessfully. Not surprisingly, the latter examples are highly non-homogeneous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Left: successes. Right: failures. The average classification rates are shown below the corresponding class labels. Note that the representation for D48 has an average number of only 27 combined Laplacian and Harris regions per sample.</figDesc><graphic coords="25,82.19,574.37,54.00,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>outperforms VZ-Joint (64.75% vs. 66.09% for 10 training images), though this is considerably below the 92.61% rate achieved by our (H+L)(S+R) method. A closer look at</figDesc><table /><note><p><p><p>Detailed breakdown of classification results summarized in Table</p>2</p>. The classes are sorted in order of increasing classification rate (last column). See text for discussion. slightly</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table ,</head><label>,</label><figDesc></figDesc><table><row><cell cols="4">where the (H+L)(S+R+E) classification rate is slightly inferior</cell></row><row><cell>to the L(S+R+E) rate.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>H</cell><cell>L</cell><cell>H+L</cell></row><row><cell>S</cell><cell cols="3">0.6136 0.7570 0.7531</cell></row><row><cell>R</cell><cell cols="3">0.6000 0.8023 0.7640</cell></row><row><cell>E</cell><cell cols="3">0.3678 0.4932 0.5413</cell></row><row><cell cols="4">S+R+E 0.7761 0.8815 0.8744</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Brodatz database classification results for 3 training samples.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>More accurately, they should be termed affine-covariant: In principle, the regions found in a picture deformed by some affine transformation are the images of the regions found in the original picture under the same transformation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that we have implemented simplified versions of the algorithms given in[8,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p><ref type="bibr" target="#b29">30]</ref>. In particular, the affine adaptation process is not iterated, and the local (differentiation) scale is fixed, instead of being automatically determined at each iteration. See also<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> for related approaches to region extraction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The original SIFT descriptor has 128 dimensions, as it is based on subdividing a square image patch into 16 smaller squares. To achieve rotation invariance, we must use concentric rings instead, hence the drop in dimensionality for RIFT.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the National Science Foundation under grant IIS-0308087, the European project LAVA (IST-2001-34405), the UIUC Campus Research Board, the UIUC-CNRS collaboration agreement, and the Beckman Institute for Advanced Science and Technology. We also wish to thank Andrew Zisserman for useful discussions and comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Comparative evaluation. The results presented in this section clearly show that our local affine-invariant representation is sufficient for the successful recognition of textures on (possibly non-rigid) surfaces in three dimensions. To answer the question of whether its invariance properties are actually necessary, we have performed a comparative evaluation with the non-invariant algorithm recently proposed by Varma and Zisserman <ref type="bibr" target="#b42">[43]</ref>. This method, dubbed VZ in the following, uses a dense set of 2D textons; the descriptors are raw pixel values measured in fixed-size neighborhoods (perhaps surprisingly, this feature representation outperforms the more traditional filter banks). We chose the VZ method for comparison because it achieves the best classification rates to date on the CUReT database (up to 97.47%). We have tested two variants of VZ: In the first one, texture images are described by one-dimensional texton histograms encoding the joint distribution of all pixel values in the neighborhood; in the second, they are represented using two-dimensional histograms that encode the conditional distribution of the center pixel given its neighborhood.</p><p>Accordingly, the respective variants are dubbed VZ-joint and VZ-MRF (for Markov Random Field). The details of our implementation are given in the Appendix.</p><p>We have compared our method with with VZ-Joint and VZ-MRF using the randomized classification scheme described in Section 4.1. Figure <ref type="figure">8</ref> shows the results as a function of training set size. Consistent with the findings of Varma and Zisserman <ref type="bibr" target="#b42">[43]</ref>, VZ-MRF</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reliable feature matching across widely separated views</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="774" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multiscale region detector</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="22" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Textures: A Photographic Album for Artists and Designers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brodatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification of rotated and scaled textured images using Gaussian Markov field models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="202" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A representation of shape based on peaks and ridges in the difference of low-pass transform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="170" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compact representation of bidirectional texture functions</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Cula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1041" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reflectance and texture of real world surfaces</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Direct computation of shape cues using scale-adapted spatial derivative operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gårding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="191" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean shift based clustering in high dimensions: A texture classification example</title>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="786" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Alvey Vision Conference</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</editor>
		<meeting>the 4th Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual pattern discrimination</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory, IT</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textons, the elements of texture perception and their interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scale, saliency and image description</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The structure of locally orderless images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affine-invariant local descriptors and neighborhood statistics for texture recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="649" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sparse texture representation using affineinvariant regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing surfaces using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Periodicity, directionality, and randomness: Wold features for image modeling and retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="722" to="733" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification of textures seen from different distances and under varying illumination direction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Llado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="833" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contour and texture analysis for image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanisms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="923" to="932" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture classification and segmentation using multiresolution simultaneous autoregressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="173" to="188" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indexing based on scale invariant interest points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="525" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An affine invariant interest point detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comp. Vision</title>
		<meeting>European Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="257" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time recognition with the entire Brodatz texture database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="638" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filtering for texture classification: A comparative study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Husøy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="310" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Texture-based image retrieval without segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1018" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Viewpoint invariant texture matching and wide baseline stereo</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-view matching for unordered image sets, or</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comp. Vision</title>
		<meeting>European Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="414" to="431" />
		</imprint>
	</monogr>
	<note>How do I organize my holiday snaps?</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Constructing models for content-based image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local greyvalue invariants for image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="535" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching widely separated views based on affinely invariant neighbourhoods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classifying images of materials: Achieving viewpoint and illumination independence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comp. Vision</title>
		<meeting>European Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="255" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Texture classification: Are filter banks necessary?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Patt. Recog</title>
		<meeting>IEEE Conf. Comp. Vision Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting textons and texture boundaries in natural images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="250" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combining gradient and albedo data for rotation invariant classification of 3d surface texture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Chantler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="848" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Performance analysis in content-based retrieval with textures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Patt. Recog</title>
		<meeting>Int. Conf. Patt. Recog</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="275" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Random field models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Image and Video Processing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
