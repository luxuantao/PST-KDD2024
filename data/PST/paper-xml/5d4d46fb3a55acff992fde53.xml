<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">The Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenpeng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
							<email>gaojun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">The Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in dynamic graphs becomes very critical in many different application scenarios, e.g., recommender systems, while it also raises huge challenges due to the high flexible nature of anomaly and lack of sufficient labelled data. It is better to learn the anomaly patterns by considering all possible hints including the structural, content and temporal features, rather than utilizing heuristic rules over the partial features. In this paper, we propose AddGraph, a general end-to-end anomalous edge detection framework using an extended temporal GCN (Graph Convolutional Network) with an attention model, which can capture both long-term patterns and the short-term patterns in dynamic graphs. In order to cope with insufficient explicit labelled data, we employ a selective negative sampling and margin loss in training of AddGraph in a semi-supervised fashion. We conduct extensive experiments on real-world datasets, and illustrate that AddGraph can outperform the state-of-the-art competitors in anomaly detection significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent years witness the rapid development of dynamic graphs. Taking the e-commerce sites as an example. Massive users perform different operations, such as item clicking, item buying, in the sites every day, which contribute to millions of newly-added edges into the graph. The modification of other attributes for accounts/items also produces a large amount of content information. These dynamic graphs serve as the basis for the most important tasks in the e-commerce sites like the query and item recommendation.</p><p>Anomalous users may perform some operations to generate fake data in the dynamic graphs to achieve the potential gain. These fake data are called anomaly in this paper. Taking the anomaly in the recommendation as an example. Anomalous users can improve the popularity of their target items through a large number of new operations related to target * Contact Author items, like clicking both target items and popular ones frequently. Then, the target items may show some similarities to other popular ones, which increases the chances and upgrade rankings in the recommendation <ref type="bibr">[Hooi et al., 2016]</ref>. In order to achieve the goal quickly, anomalous users usually control multiple accounts to perform these operations in a short time period. The anomaly detection in dynamic graph, especially anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type="bibr" target="#b1">[Akoglu et al., 2015;</ref><ref type="bibr" target="#b4">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the anomaly due to its flexible and dynamic nature. Some anomalous operations show some explicit patterns but try to hide them in a large graph, while others are with implicit patterns. Take an explicit anomaly pattern in the recommender system as an example. As anomalous users usually control multiple accounts to promote the target items, the edges between these accounts and items may compose a dense subgraph, which emerge in a short time period. In addition, although the accounts which involve the anomaly perform anomalous operations sometimes, these accounts perform normally most of the time, which hides their long-term anomalous behavior and increases the difficulty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type="bibr" target="#b2">[Eswaran et al., 2018]</ref>, where there are sudden large number of connections, forming a very dense subgraph in the network. Such cases indicate the flexible nature of anomaly, which requires us to learn the anomaly patterns by combining all available hints like structural, temporal and content features.</p><p>Another challenge in the anomaly detection lies in the insufficient labelled data. Even if the initial data are normal, anomaly data will be finally mixed with the normal ones in the real-world applications as time goes by. It results in high burden or is even infeasible if we check the anomaly every day by hand. Even if we can label some anomaly operations, they may occupy a small part of anomalies. It indicates that the explicit labelled data may be not representative, and results in the poor performance if we learn a detection model in a supervised way.</p><p>Most of existing approaches to detecting the anomalies in dynamic large graphs rely on the heuristic rules which consider the above features in a rigid way. For example, <ref type="bibr">[Hooi et al., 2016]</ref> mainly relies on the structural features. They define a density function and discover the target mainly us-ing structural features. Other works <ref type="bibr" target="#b7">[Zhao and Yu, 2013;</ref><ref type="bibr" target="#b4">McConville et al., 2015]</ref> consider content feature or even temporal factor. However, the way taking the content, structural and temporal factors into account is not flexible, which makes it restricted in a specific pattern. In addition, it is more difficult to detect anomalies using the long-term features due to their sparsity in the time dimension.</p><p>The advance of deep learning is very helpful in anomaly detection, with its ability to combine different features in a reasonable way and to learn implicit rules from the given data. GCN(Graph Convolutional Network) is a representative model to combine the content and structural features in a graph <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>. Compared with traditional graph methods, GCN can automatically propagate the information carried by neighboring nodes, which can then be used to spread the anomalous probabilities of nodes. The major issue of direct usage of GCN in the anomaly detection lies in the fact that GCN does not consider the timing factors, which cannot be ignored in the dynamic graphs. The more recent works, like CAD <ref type="bibr">[Sricharan and Das, 2014]</ref> and <ref type="bibr">Netwalk [Yu et al., 2018]</ref> have applied the graph embedding method to dynamic graph. Their methods are well designed and have achieved good results on detecting anomalies in dynamic graphs. However, they cannot capture the long-term and short-term patterns of nodes, which are highly needed in a more general graph model framework to detect anomalies.</p><p>In order to overcome the limitations of the existing works, this paper extends the original GCN model to support temporal information using GRU(Gated Recurrent Unit) with a contextual attention-based model, and then introduces a selective negative sampling and margin loss in model training for anomalous edges incrementally. Specifically, the main contributions of our work are summarized as follows.</p><p>• We propose AddGraph, a semi-supervised learning framework for anomalous edge detection, using an extended temporal GCN with an attention-based GRU, which can combines the hidden states for long-term behavior patterns and the window information containing the short-term patterns of the nodes.</p><p>• We introduce a selective negative sampling strategy and margin loss in the training of AddGraph for detecting anomalous edges, inspired by the advances in embedding of knowledge graph. Those strategies attempt to handle the insufficient labelled anomaly data.</p><p>• Experiments on two real-world datasets achieve stateof-the-art performance, which proves the effectiveness of AddGraph on detecting anomalies in different kinds of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the existing anomaly detection approaches, the graph embedding model, and some attempts to detect anomaly on embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref> is proposed with an observation that anomalous edges always appear between two different node clusters. Specifically, it first partitions nodes, and then builds an edge generative model for the edges inside partition. The scores produced by the model can be used as an important measure in detecting anomalous edges.</p><p>The works <ref type="bibr">[Sun et al., 2006;</ref><ref type="bibr" target="#b5">Shin et al., 2016;</ref><ref type="bibr" target="#b5">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type="bibr" target="#b5">Shin [2016;</ref><ref type="bibr">2017]</ref> define a density function in the dynamic bipartite graph, and employ a greedy search strategy or sequence search to find these most dense sub-graphs. These works mainly rely on structural features lacking in flexibility as patterns are given in advance.</p><p>Besides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the local structural information and historical behavior near an edge to decide whether the edge is anomalous or not. <ref type="bibr">Spot-Light [Eswaran et al., 2018]</ref> randomly samples a series of node sets from the entire node set, and encodes the the graph at each timestamp to a vector by computing the overlap between these sets and the nodes of current edge set. The method finds out the anomalous graph by clustering these vectors. However, it can only catch instantaneous anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Embedding</head><p>Graph embedding maps the nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type="bibr" target="#b6">[Tang et al., 2015]</ref>, LINE <ref type="bibr" target="#b6">[Tang et al., 2015]</ref> and Node2vec <ref type="bibr" target="#b2">[Grover and Leskovec, 2016]</ref> are the methods to yield node embeddings so that two structural similar nodes have the similar embeddings. The difference of these works lies in the meaning of structural similarities. Deepwalk uses random walk to get an ordered sequence of nodes. LINE attempts to preserve the first-order similarity and the secondorder proximity. Node2vec improves random walk by introducing two parameters to balance the breadth-first search and depth-first search. Those methods can be used to yield node embeddings for detecting anomaly. However, they mainly focus on the preservation of structural similarity.</p><p>The work of GCN and following extensions can process structural features and content features. GCN extends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type="bibr" target="#b2">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b2">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from different viewpoints, like the optimization in time/space complexity. Due to its ability to handle both structural and content features, GCN can be leveraged as the basis in our anomaly detection approach. However, we cannot directly use GCN in our work, as it does not consider temporal features in dynamic graphs. Note that we choose the basic GCN in this paper, and its extensions can also be used with minor modification.</p><p>Knowledge graph embedding projects a triple (h, r, t) to low-dimensional vector spaces to preserve potential similarities and differences between multi-relational data. Insufficient data is also a key challenge for knowledge graph embedding, because there are only golden triples in the original dataset, which may result in weak distinction of data in different relationships. In order to solve this challenge, negative sampling is used to produce edges by randomly replacing the  To make the triple produced by negative sampling different from the golden one, margin loss is used to enlarge the difference between positive triples and negative samples. In this paper, we a similar idea of negative sampling and margin loss from knowledge graph embedding to solve the problem of insufficient data in anomaly detecting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anomaly Detection over Graph Embedding</head><p>Some works began to combine the graph embedding into the anomaly detection. In [Sricharan and Das, 2014], a timecommute distance is used to detect anomalous changes in dynamic graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding model based on random walks. The anomaly detection is realized by the dynamic clustering model of node representation. Our work roughly follows a similar idea. However, we extend GCN to the temporal GCN so that we can capture the temporal features in a more reasonable way, and we build an end-to-end semi-supervised learning model to detect anomalous edges rather than two-phases clustering, which has the potential to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we first formulate the problem, propose an AddGraph framework for anomaly detection, and then discuss its training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let T be the maximum timestamp. A graph steam G takes the form of {G t } T t=1 , where each G t = (V t , E t ) represents the entire snapshot at timestamp t, and V t and E t are the set of nodes and edges respectively. An edge e = (i, j, w) ∈ E t means that the i-th node and the j-th node have a connection in the dynamic graph at the timestamp t with its weight w. For unweighted graphs, w is always 1; for weighted graphs, w ∈ R + . An adjacency matrix A t ∈ R m×n is to represent the edges in E t , where ∀(i, j, w)</p><formula xml:id="formula_0">∈ E t , A t [i][j] = w. For convenience, let G = (V, E) be the union of G, i.e., V = T t=1 V t and E = T t=1 E t . We let n = |V | and m = |E|.</formula><p>The goal of this paper is to detect anomalous edges in E t . Specifically, for each e ∈ E t , this paper produces f (e), the Short t = CAB(H t−w ; ...; H t−1 ) 7:</p><formula xml:id="formula_1">H t = GRU(Current t , Short t ) 8:</formula><p>for all (i, j, w) ∈ E t do 9:</p><p>Sample(i , j , w) for f (i, j, w) 10:</p><formula xml:id="formula_2">L t = L t + max(0, γ + f (i, j, w) + f (i , j , w)) 11:</formula><p>end for 12:</p><formula xml:id="formula_3">L t = L t + L reg 13: Minimize L t 14:</formula><p>end for 15: until Convergence 16: return {H t } T t=1 anomalous probability of e. We do not need the labelled data for anomaly in the training phase, but assume that all edges in sets at the initial timestamps are normal, i.e., t is smaller than the timestamp T train in training phase. In the test phase, we use the labelled anomaly data to measure f (e) produced in different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AddGraph Framework</head><p>The overview of our AddGraph framework is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. The core idea behind AddGraph is to build a framework to describe the normal edges by using all possible features in the snapshots in the training phase, including structural, content and temporal features. The framework then can be further refined and used to measure the anomalous edges in the following snapshots. Roughly, AddGraph employs GCN to process the previous node state with edges in the current snapshot by considering the structural and content features of nodes. The node states in a short window are then summarized as the short-term information with a contextual attention-based model. We put the output of GCN and shortterm information into GRU to get the hidden state of nodes at a new timestamp. We will use the hidden state of the nodes at each timestamp to calculate the anomalous probabilities of an existing edge and a negative sampled edge, and then feed them to a margin loss.</p><p>GCN for content and structural features. At timestamp t, we receive the snapshot G t = (V t , E t ) with its adjacency matrix A t and the output hidden state matrix H t−1 ∈ R n×d of the framework at timestamp t − 1. First, we propagate the hidden state matrix with GCN,</p><formula xml:id="formula_4">Current t = GCN L (H t−1 ),<label>(1)</label></formula><p>where Current t represents current state of nodes combining the current input with the long-term hidden state, and GCN L denotes an L-layered GCN which is proposed in [Kipf and</p><p>Welling <ref type="bibr">, 2017]</ref>. The details of GCN L are shown below:</p><formula xml:id="formula_5">Z (0) =H t−1 ,<label>(2)</label></formula><formula xml:id="formula_6">Z (l) =ReLU ( Ât Z (l−1) W (l−1) ),<label>(3)</label></formula><formula xml:id="formula_7">Current t =ReLU ( Ât Z (L−1) W (L−1) ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">l ∈ [1, L − 1]. Ât = D− 1 2 Ãt D− 1 2</formula><p>is the regularized adjacency matrix with self loops, where Ãt = A t + I n denotes the adjacency matrix with self loops, and Dii = j Ãt ij denotes the degree of node i. GRU with attention to combine short-term and long-term states. To catch the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type="bibr" target="#b3">[Liu et al., 2017]</ref> and proposed by <ref type="bibr" target="#b1">[Cui et al., 2017]</ref>. In our framework, we construct short state of local window as follow:</p><formula xml:id="formula_9">C t h,i = [h t−ω i ; ...; h t−1 i ] C t h,i ∈ R ω×d (5) e t h,i = r T tanh(Q h (C t h,i ) T ) e t h,i ∈ R ω (6) a t h,i = sof tmax(e t h,i ) a t h,i ∈ R ω (7) short t i = (a h,i C t h,i ) T short t i ∈ R d (8)</formula><p>where h t i denotes the hidden state of the i-th node, and ω is the size of the window to catch short-term pattern. Q h and r are parameters to optimize the contextual attention-based model. We brief (5) − (8) to a single function:</p><formula xml:id="formula_10">short t i = CAB(h t−ω i ; ...; h t−1 i )<label>(9)</label></formula><p>For all nodes in V , the function is written as:</p><formula xml:id="formula_11">Short t = CAB(H t−ω ; ...; H t−1 )<label>(10)</label></formula><p>Now we get Current t and Short t . Current t represents current states of nodes which combine the current input with the long-term hidden state, and Short t represents the window information which catches the short-term interest of nodes. To make AddGraph encode temporal features, we use GRU to process Current t and Short t :</p><formula xml:id="formula_12">H t = GRU(Current t , Short t )<label>(11)</label></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type="bibr" target="#b1">[Chung et al., 2014]</ref>. GRU can record long-term information, and avoid gradient vanishing and exploding problems. The forward propagation equations of GRU in our framework are:</p><formula xml:id="formula_13">P t = σ(U P Current t + W P Short t + b P ) (12) R t = σ(U R Current t + W R Short t + b R ) (13) Ht = tanh(U c Current t + W c (R t Short t )) (14) H t = (1 − P t ) Short t + P t Ht (<label>15</label></formula><formula xml:id="formula_14">)</formula><p>where P t is the update gate to control output and R t is the reset gate to balance input and memory. Now we get H t containing the structural, content and temporal features.</p><p>Anomalous score computation for edges. Now, we get the hidden state of nodes H t at timestamp t. For each edge (i, j, w) ∈ E t , we locate the embeddings for the i-th node and the j-th node in H t , on which we can compute the anomalous scores: <ref type="formula">16</ref>) where h i and h j are the hidden state of the i-th and j-th node respectively, and σ(x) = 1 1+e x is the sigmoid function. a and b are parameters to optimize in the output layer. β and µ are the hyper-parameters in the score function. Note that the single layer network used in this paper can be replaced by other sophisticated networks.</p><formula xml:id="formula_15">f (i, j, w) = w • σ(β • (||a h i + b h j || 2 2 − µ)) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selective Negative Sampling and Loss</head><p>In order to handle the insufficiency of anomaly data, we try to build a model to describe the normal data instead. Recall that we assume that all edges are normal in the training phase. For each normal edge in the graph, we generate a negative sample as an anomalous edge. Inspired by the method proposed in <ref type="bibr">[Wang et al., 2014]</ref>, we define a Bernoulli distribution with parameter di di+dj for sampling: given a normal edge (i, j), we replace i with probability di di+dj and replace j with probability dj di+dj , where d i and d j denote the degree of the i-th node and the j-th node respectively.</p><p>As the generated sampled edges may be still normal, we cannot use a strict loss function such as cross entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in training of AddGraph:</p><formula xml:id="formula_16">L t = min (i,j,w)∈E t (i ,j ,w) / ∈E t max{0, γ + f (i, j, w) − f (i , j , w)},<label>(17)</label></formula><p>where f (•, •, •) is the anomalous score function for edges, and γ ∈ (0, 1) is the margin between the possibilities of normal edge and anomalous one. The minimization of the loss function L t encourages that f (i, j, w) becomes smaller while f (i , j , w) becomes larger, which is in the same line with our expectation.</p><p>We cannot assume that all edges are still completely normal in the snapshots after training phase, while we need to compute the hidden states for each snapshot. In the process of sampling on the graph mixed with normal and anomalous edges, we actually select partial edges which are more credible for the training. Specifically, for each edge (i, j, w), we produce a negative sampled edge (i , j , w). The sampled edge pair is discarded if f (i, j, w) &gt; f (i , j , w). The selective negative sampling strategy ensures the stability of Ad-dGraph framework for a long time.</p><p>The overall loss function is summarized as follows: L = L t + λL reg (18) where λ is a hyper-parameter, and L reg is an L2regularization loss to avoid overfitting, which is shown as follows: </p><formula xml:id="formula_17">L reg = (||W 1 || 2 2 + ||W 2 || 2 2 + ||Q h || 2 2 + ||r|| 2 2 +||U z || 2 2 + ||W z || 2 2 + ||b z || 2 2 + ||U r || 2 2 + ||W r || 2 2 +||b r || 2 2 + ||U c || 2 2 + ||W c || 2 2 + ||a|| 2 2 + ||b|| 2 2 ) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we first describe the experimental setup, and then compare AddGraph with other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. We evaluate our framework on two datasets and the details of these two datasets are shown in Table <ref type="table" target="#tab_0">2</ref>. UCI Message is a directed network containing messages among an online community at University of California, Irvine. Each node represents a user and each directed edge is for a message between two users. Digg is a response network of Digg, a social news site. Each node in the network is a user of the site, and each edge indicates that one user replies to another. Edges in both datasets are annotated with timestamps. We randomly generate an initial vector for each node as its content feature. We need to manually build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type="bibr" target="#b1">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type="bibr" target="#b7">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type="bibr" target="#b0">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges in a node cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type="bibr" target="#b4">[Ranshous et al., 2016]</ref>. It uses the local structural feature and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The method first builds node embeddings based on random walks, and then detects anomaly using the clustering on the node embedddings.</p><p>Experimental Design. We will test the anomaly detection methods over graphs without timestamps to see whether the framework can exploit the content and structural features effectively, and then extend to the dynamic graphs with all features. We will study the impacts of different parameters on AddGraph. The metric used to compare the performance of different methods is AUC (the area under the ROC curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Graphs without Timestamps</head><p>The tests over graphs without timestamps mainly focus on the exploration of structural and content features. As for Ad-dGraph, a simplified version without window information is conducted for this experiment. We divide the dataset into two parts, the first 50% as the training data and the latter 50% as the test data. The number of GCN layers is 2. The weight decay λ for regularization is 5e-7. The learning rate lr is 0.002. The dropout rate is 0.2. For UCI Message dataset, the size of dimension is 500 for hidden state. The margin γ is set to 0.5. The parameters β and µ is set to 1.0 and 0.3 respectively. For Digg dataset, the size of dimension is 200 for hidden state. The margin γ is set to 0.7. The parameters β and µ is set to 3.0 and 0.5 respectively. We injected 1%, 5%, and 10% of the anomalous data into the test data of different datasets.</p><p>The results are shown in Table <ref type="table">1</ref>, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines on the two datasets with varying anomaly proportions and has better ability to catch structural and content features. This is mainly due to the underlying GCN, which enables the framework to spread information between nodes and its neighbors better. In particular, on Digg dataset, our approach has gained more than 10% improvement. This outstanding effect proves that our framework can exploit the content and structural features effectively. Unlike the trend of baselines' results, AUC produced by our framework with 1% anomaly is a little lower than that with 5% anomaly, while it's still higher than all baselines. This may be due to the fact that when the anomaly proportion of the test set is low, the scores computed on the original data and negative samples cannot distinct them well, resulting in a decline of prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Dynamic Graphs</head><p>In the tests over dynamic graphs, we use the first 50% as the training data and the latter 50% as the test data. After anomaly injection with proportion of 5%, we split the training data and test data into snapshots. According to the size of dataset, the snapshot size is set to 1,000 and 6,000 for UCI Message and Digg respectively. In training phase, we use snapshots of training data to build an initial model. In test phase, we maintain the model incrementally as each snapshot at timestamp t arrives. The number of GCN layers is set to 3. The learning rate lr is 0.001. For UCI Message dataset, the size of dimension is 100 for hidden states. For Digg dataset, the size of dimension is 50 for hidden states. The margin γ Figure <ref type="figure" target="#fig_3">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type="bibr" target="#b7">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats baselines on almost all snapshots except the last one in Digg. The prediction results show our framework is able to catch temporal features. The attention mechanism enables our framework to notice the changes of nodes during the window period. The extended GCN with GRU makes it possible to record long-term dependency. The decline on the last snapshot may be due to the fact that too many new nodes emerge in the test phase, which are not processed before.</p><p>Parameter Sensitivity Now, we attempt to find out the influence of hyper-parameters on AddGraph, including L for the total number of layers in GCN, d for the size of dimension of hidden state, and the training ratio of the entire dataset.</p><p>First, we evaluate the influence of L and d. The range of L is {1, 2, 3, 4, 5} and the range of d is {10, 25, 50, 100, 200}. Other parameters are set to optimum. In order to show the influence of different values of the parameters, we choose a relatively more challenging task in this study. We use Digg dataset with 10% anomalies in test data and detect anomalies at the last timestamp in terms of the AUC metric. For ease of observation, we use log(d) instead of d as the x-axis. As shown in Figure <ref type="figure">3</ref>, AUC increases significantly when L increases from 1 to 2, and reaches its peak when L is 3. With the increase of d, the performance of AddGraph is gradually improved, and reaches the optimal value when d is 50. After d and L reach the optimal configuration, AUC decreases as they continue to increase. When there are too many layers, GCN may capture useless information of remote neighbors, which reduces the accuracy of the framework. Larger d will increase the complexity of the framework and make it more difficult in converging to the optimal point.</p><p>Second, we evaluate the influence of the training ratio of the entire dataset. The range of training ratio is {10%, 20%, 30%, 40%, 50%, 60%} and other parameters are set to optimum. We use Digg dataset with 10% anomalies in test data and record the AUC score of each timestamp at the test phase. As shown in Figure <ref type="figure" target="#fig_4">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose an anomaly detection framework, AddGraph, on dynamic graphs which can detect the patterns of anomalies flexibly without explicit labelled anomaly data. AddGraph attempts to learn the anomaly patterns by considering all possible hints including the structural, content and temporal features using the temporal GCN with a contextual attentionbased model. It also employs a selective negative sampling strategy and margin loss in training to handle the insufficient labelled anomaly data. Experiments on several realworld datasets show that AddGraph outperforms other existing anomaly detection methods significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Time</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AddGraph framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>AddGraph algorithm Input: Edge stream {E t } T t=1 Parameter: β, µ, λ, γ, L, ω, d Output: {H t }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: AUC results for anomalous scores on dynamic graphs</figDesc><graphic url="image-21.png" coords="6,329.04,120.05,90.65,68.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: AUC results on Digg with different parameters</figDesc><graphic url="image-24.png" coords="6,329.04,188.14,90.65,68.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>19)We summarize our algorithm as Algorithm 1. Statistics of Datasets</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="5">Anomaly proportion GOutlier CM-Sketch Netwalk AddGraph</cell></row><row><cell></cell><cell>UCI Message</cell><cell></cell><cell>1%</cell><cell>0.7181</cell><cell>0.7270</cell><cell>0.7758</cell><cell>0.8083</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell>0.7053</cell><cell>0.7086</cell><cell>0.7647</cell><cell>0.8090</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell>0.6707</cell><cell>0.6861</cell><cell>0.7226</cell><cell>0.7688</cell></row><row><cell></cell><cell>Digg</cell><cell></cell><cell>1%</cell><cell>0.6963</cell><cell>0.6871</cell><cell>0.7563</cell><cell>0.8341</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell>0.6763</cell><cell>0.6581</cell><cell>0.7176</cell><cell>0.8470</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell>0.6353</cell><cell>0.6179</cell><cell>0.6837</cell><cell>0.8369</cell></row><row><cell></cell><cell cols="6">Table 1: AUC results for anomalous scores on graphs without timestamps</cell></row><row><cell>Dataset</cell><cell cols="2">#Node #Edge</cell><cell>Max. Degree</cell><cell>Avg. Degree</cell><cell></cell></row><row><cell>UCI Message</cell><cell cols="2">1,899 13,838</cell><cell>255</cell><cell>14.57</cell><cell></cell></row><row><cell>Digg</cell><cell cols="2">30,360 85,155</cell><cell>283</cell><cell>5.61</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by National Key Research and Development Program No.2016YFB1000700, NSFC under Grant No.61572040 and 61832001, and Alibaba-PKU joint Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outlier detection in graph streams</title>
		<author>
			<persName><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 27th International Conference on Data Engineering</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical contextual attention-based gru network for sequential recommendation</title>
		<author>
			<persName><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<idno>arXiv:1711.05114</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2015. 2015. 2013. 2013. 2014. 2014. 2017. 2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Empirical evaluation of gated recurrent neural networks on sequence modeling</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><surname>Defferrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016. 2018. 2018. 2016. 2016. 2016. 2016. 2017</date>
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A scalable approach for outlier detection in edge streams using sketchbased approximations</title>
		<author>
			<persName><surname>Mcconville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 SIAM International Conference on Data Mining</title>
				<meeting>the 2015 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2015">2015. 2015. 2015. 2015. 2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2016 SIAM International Conference on Data Mining</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">M-zoom: Fast dense-block detection in tensors with quality guarantees</title>
		<author>
			<persName><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2016. 2016. 2017. 2017. 2014. 2006. 2006</date>
			<biblScope unit="page" from="374" to="383" />
		</imprint>
	</monogr>
	<note>Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014">2015. 2015. 2014. 2014</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>Twenty-Eighth AAAI conference on artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks</title>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2013">2018. 2018. 2013</date>
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2013 SIAM International Conference on Data Mining</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
