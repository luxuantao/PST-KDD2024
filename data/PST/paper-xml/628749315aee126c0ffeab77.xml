<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Biru</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
							<email>dengyd@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute Guo Qiang</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">International Innovation Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute Guo Qiang</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">International Innovation Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Gu</surname></persName>
							<email>guming@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs' transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM's transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS:</p><p>(1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained models (PTMs) have shown superior performance on various tasks of natural language processing (NLP) and computer vision (CV) <ref type="bibr" target="#b2">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Raffel et al., 2020;</ref><ref type="bibr" target="#b10">Li et al., 2019;</ref><ref type="bibr" target="#b28">Zabir et al., 2018;</ref><ref type="bibr" target="#b3">Han et al., 2021)</ref>. The increasingly popular "pre-train then fine-tune" paradigm is typically implemented as a prescriptive three-stage routine: (1) PTM Supply Stage: upstream suppliers pre-train various kinds of PTMs, (2) PTM Selection Stage: downstream users select the desired PTM based on their own demands for a specific task, and (3) PTM Application Stage: downstream users conduct further fine-tuning on the given task.</p><p>During the PTM selection stage, the common practice is to fine-tune a set of candidate PTMs and pick up the model with the best performance. Such a fine-tuning process allows accurate assessment of the transferability of PTMs on each downstream task, but is computationally expensive <ref type="bibr" target="#b27">(You et al., 2021)</ref>. To resolve this issue, researchers recently propose feature-based model selection (FMS) methods to efficiently select a PTM for a specific downstream task <ref type="bibr" target="#b0">(Bao et al., 2018;</ref><ref type="bibr" target="#b1">Deshpande et al., 2021;</ref><ref type="bibr" target="#b27">You et al., 2021;</ref><ref type="bibr" target="#b5">Huang et al., 2021)</ref>. Without training on downstream tasks, FMS first extracts static features of the target data using PTMs, and then resorts to the correlation between these features and the corresponding target labels as the main criterion to estimate PTMs' transferability.</p><p>Although current FMS methods are effective in many cases, we argue that they are vulnerable because the correlation between static features and their corresponding labels is not necessarily a reliable indicator, and thus cannot accurately measure PTMs' transfer learning ability. To validate our viewpoints, we present two simple and effective methods, (1) model disguise attack (MDA) and (2) The overall framework of model disguise attack (MDA) and evaluation data selection (EDS). After MDA, the disguised poisoned model is mistakenly chosen by FMS. After EDS, the score for the poisoned model rated by FMS is higher than that of the superior model on the selected subset D sub i .</p><p>evaluation data selection (EDS), to maliciously mislead FMS into mistakenly ranking PTMs' transferability. Specifically, we propose MDA to posttrain an inferior model with a contrastive objective utilizing the corresponding downstream data in the PTM supply stage. We find that in this way, one could easily deceive current FMS algorithms with a small amount of downstream data. EDS is an evaluation data selection method based on the K-means algorithm <ref type="bibr" target="#b15">(MacQueen et al., 1967)</ref> for FMS's evaluation, which is conducted in the PTM selection stage. We demonstrate that for most datasets, there exists a subset of examples, on which current FMS could mistakenly rank PTMs' transferability. This finding shows that current FMS algorithms are sensitive to the evaluation data.</p><p>Worse still, we find that our proposed MDA and EDS methods can further be combined with the backdoor attack <ref type="bibr" target="#b31">(Zhang et al., 2021)</ref> conducted during the PTM supply stage. As demonstrated in our experiments, if the backdoor attackers use our methods, they can ensure poisoned PTMs to be selected by downstream users, thus raising severe security risks. The overall framework of MDA and EDS is shown in Figure <ref type="figure">1</ref>.</p><p>In conclusion, our contributions are two-fold: (1) we formulate the model selection attack for pre-trained models and demonstrate the serious defects of current FMS algorithms by proposing two effective methods, i.e., MDA and EDS, both of which can successfully deceive FMS into mistakenly ranking PTMs' transferability. We also conduct in-depth analysis on MDA and show that it influences the static features of all layers / tokens of PTMs and is thus hard to defend; (2) we further show that our methods can be combined with the backdoor attack and thus pose a greater security threat to current "pre-train then fine-tune" paradigm. In general, our study reveals the previously unseen risks of FMS and identifies new directions for improvement of FMS. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature-based Model Selection. Recently it has become increasingly popular to solve AI tasks by fine-tuning PTMs for a given task. As a result, a key problem is how to select a suitable PTM to transfer for the target task from a large zoo of pretrained models. Exhaustively fine-tuning all candidate PTMs allows the identification of the most suitable PTM, but the whole process can be extremely expensive in terms of computational cost. Some recent works use static features extracted from PTMs as the indicator to select PTMs without training on the target task <ref type="bibr" target="#b0">(Bao et al., 2018;</ref><ref type="bibr" target="#b1">Deshpande et al., 2021;</ref><ref type="bibr" target="#b5">Huang et al., 2021;</ref><ref type="bibr" target="#b27">You et al., 2021)</ref>. <ref type="bibr" target="#b1">Deshpande et al. (2021)</ref> introduce the Label-Feature Correlation score for model selection. <ref type="bibr" target="#b0">Bao et al. (2018)</ref> present H-score to estimate the performance of transferred representations. <ref type="bibr" target="#b27">You et al. (2021)</ref> propose LogME to estimate the maximum evidence of labels given features extracted from PTMs. <ref type="bibr" target="#b5">Huang et al. (2021)</ref> propose TransRate that supports selecting optimal layers to transfer. Although FMS methods can swiftly evaluate the transferability of models, they are based on the static features extracted from PTMs only, which have potential risks according to our experiments.</p><p>Backdoor Attack. The backdoor attack is to train the model with poisoned samples so that malicious behaviors will be activated by inputs inserted with triggers <ref type="bibr" target="#b11">(Liu et al., 2017)</ref>. The backdoor attacks can generally be classified into two categories. The first category attacks the PTMs before finetuning on downstream tasks and does not need to use the data of downstream tasks <ref type="bibr" target="#b31">(Zhang et al., 2021;</ref><ref type="bibr" target="#b9">Kurita et al., 2020;</ref><ref type="bibr" target="#b6">Ji et al., 2019)</ref>. The second category instead uses the poisoned downstream dataset to attack the model <ref type="bibr">(Qi et al., 2021b,a;</ref><ref type="bibr" target="#b21">Saha et al., 2020;</ref><ref type="bibr" target="#b13">Liu et al., 2020)</ref>. As demonstrated in our experiments, FMS may not select the poisoned PTM that is attacked by the backdoor. Nevertheless, using our methods can guarantee the poisoned model to be chosen by FMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first briefly introduce how current feature-based model selection methods (FMS) evaluate PTMs' transfer abilities in § 3.1. Then we formulate the problem of model selection attack in § 3.2, and elaborate two algorithms, i.e. <ref type="bibr">MDA and EDS in § 3.3 and § 3.4</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries for FMS</head><p>FMS essentially uses the correlation between static features of downstream data extracted from PTMs and the corresponding target labels to estimate the transferability of PTMs. Assume FMS is applied on a PTM M for a specific downstream task T i , with the corresponding dataset</p><formula xml:id="formula_0">D i = {(x k , y k )} |D i | k=1 . FMS calculates a score S D i M , which indicates the transferability of M on D i . Specif- ically, FMS first passes the target input X i = {x k } |D i | k=1 through the PTM M to derive their fea- tures F D i M = {f k } |D i | k=1 .</formula><p>Then FMS calculates the correlation between F D i M and their corresponding target labels</p><formula xml:id="formula_1">Y i = {y k } |D i | k=1 to obtain a final score, i.e., S D i M = f (F D i M , Y i ),</formula><p>where f is the metric function. A higher value of S D i M indicates better transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task Formulation</head><p>Although current FMS algorithms show promising results on efficiently judging the PTMs' transferability, we argue that the correlation between static features and target labels may not be a reliable transferability metric since it fails to consider the PTMs' learning dynamics during fine-tuning, which is far more important than the initial feature distribution. Thus current FMS algorithms can be misleading. In other words, even if a PTM exhibits poorer correlation before fine-tuning, it may still perform better after fine-tuning. In the following sections, we employ two approaches, MDA ( § 3.3) and EDS <ref type="bibr">( § 3.4)</ref>  i from D i based on K-means clustering, so that the correlation between static features and target labels for M inf on that subset is higher, i.e., S</p><formula xml:id="formula_2">D sub i M inf &gt; S D sub i Msup .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Disguise Attack</head><p>Since current FMS algorithms rely on the correlation between static features and the corresponding labels, we propose to leverage supervised contrastive loss (SCL) <ref type="bibr" target="#b24">(Sedghamiz et al., 2021)</ref> to train M inf with target data to get a disguised M * inf before the model selection stage, aiming to alter the initial feature distribution F D i M inf . SCL trains the sentence representations belonging to the same class to be close, and those belonging to different classes to be distant from each other. In this way, we can intentionally modify the initial feature distribution of PTMs according to the label information, thus the static features of a disguised inferior model M * inf will exhibit superiority over M sup . Specifically, given N annotated samples in an input batch, i.e., {x k , y k } N k=1 , each sample x k is forward propagated K times using different random dropout masks, resulting in K × N sentence representations { x1 , . . . , xK×N } in total. Let j be the index of all the encoded sentence representations in an input batch, where j ∈ I = {1, . . . , K × N }. We optimize the following loss function:</p><formula xml:id="formula_3">L = K×N j=1 −1 |P (j) | p∈P(j) log e cos( xj , xp)/τ b∈B(j)</formula><p>e cos( xj , xb )/τ , where B(j) = I\{j} is the set of indices except for j, P (j) = {p ∈ B (j) |y p = y j } is the set of indices of all positives distinct from j and | • | stands for cardinality <ref type="bibr" target="#b7">(Khosla et al., 2020)</ref>. τ is a temperature scaling parameter. By optimizing L, we manually alter the initial static feature distribution for the input examples. However, the transferability of the disguised PTM M * inf is still inferior to that of the superior model M sup , as demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Data Selection</head><p>As FMS relies on downstream target datasets for evaluation, we argue that FMS is susceptible to the evaluation data and there exists a subset of evaluation data points whose static features extracted by M inf have a closer relation with their target labels. Thus M inf will be rated with a higher score by FMS on that special subset D sub i . To select those data points "favored" by M inf , we first feed all target data points D i into the inferior PTM M inf and obtain the extracted features F D i M inf . Then we use the K-means algorithm <ref type="bibr">(Mac-Queen et al., 1967)</ref> to perform feature clustering and calculate the cluster centroids of the features</p><formula xml:id="formula_4">F D i M inf</formula><p>, where the number of clusters is equal to the number of target classes.</p><p>We select D sub i based on the distances of data points' features to their corresponding cluster centroids. Specifically, we select the data points whose features are closest to the corresponding cluster centroids and filter the selected data points by only keeping the data points whose features' corresponding cluster centroids are the same as their labels, resulting in a subset D sub i . The extracted features of data points with the same target label in D sub i by M inf are closer to each other. Therefore, the correlation between these selected data points' features and the corresponding labels is higher. And FMS will rate a higher score for M inf on D sub i , which even surpasses the score for M sup on D sub i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>In this section, we first conduct experiments to demonstrate the effectiveness of our proposed model disguise attack and evaluation data selection in § 4.1 and § 4.2, respectively. Then we combine both MDA and EDS with the backdoor attack in § 4.3. In addition, we demonstrate that our proposed methods can be widely applied to various kinds of PTMs and FMS algorithms in § 4.4. Finally, in § 4.5, we show that it is hard to defend against both MDA and EDS.</p><formula xml:id="formula_5">Dataset S D i M inf S D i Msup PM sup S D i M * inf P M * inf SST-2 -0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Model Disguise Attack</head><p>Experimental Setting. We choose LogME <ref type="bibr" target="#b27">(You et al., 2021)</ref> as the mainly evaluated FMS algorithm, which is applicable to vast transfer learning settings. We choose BERT BASE <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> / RoBERTa BASE <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> as the mainly evaluated inferior PTM (M inf ) / superior PTM (M sup ), respectively. Seven downstream tasks from the GLUE benchmark <ref type="bibr" target="#b26">(Wang et al., 2019)</ref> are selected to evaluate PTM's transferability, following <ref type="bibr" target="#b27">(You et al., 2021)</ref>. We choose the pooler output representation of the [CLS] token 2 as the sentence representation.</p><p>Attack Performance of MDA. The transferability scores estimated by LogME of the M inf and M sup on the training dataset are shown in Table <ref type="table" target="#tab_1">1</ref>. It can be observed that under most situations, LogME serves as a good measure of the transferability by rating M sup with a higher score (</p><formula xml:id="formula_6">S D i Msup &gt; S D i M inf ).</formula><p>Assuming that we have access to all the labeled examples D i in the training dataset, we conduct MDA on a specific target downstream task for M inf . We use D i to perform MDA on M inf and test the LogME scores of the disguised M * inf . Also, the fine-tuned performance of the downstream task (dev dataset) of the disguised inferior model M * inf and the superior model M sup are reported. The results are shown in Table <ref type="table" target="#tab_1">1</ref>, from which we can see that after MDA, the LogME score of the disguised inferior model</p><formula xml:id="formula_7">S D i M *</formula><p>inf is significantly increased, from average −0.5569 to 2 For RoBERTa, the BOS token is &lt;s&gt;. 0.5474, exceeding that of the superior model</p><formula xml:id="formula_8">S D i Msup ( S D i M * inf &gt; S D i</formula><p>Msup ). However, the downstream performance of M sup is higher than that of the disguised inferior model M * inf (PM sup &gt; P M * inf ). This suggests that our MDA method can successfully deceive LogME into selecting an inferior PTM, which has poorer transferability performance. It also casts doubts on the hypothesis of FMS that static features could serve as a reliable indicator for transferability measurement. The influences of MDA on the static features are visualized in appendix D.</p><p>Amount of Auxiliary Data. In real-world scenarios, the attacker may not have the access to enough target data, we thus test whether our MDA method could still be effective with few auxiliary data. We experiment on SST-2, MRPC and CoLA, and randomly sample only 25, 50, 100, 250 examples for each category in a task to construct the subset of the original training dataset, and then perform MDA for each task. Our sampled data used for MDA only takes up a small amount of the original training dataset (e.g., less than 1% for SST-2). After applying MDA, we evaluate the LogME score of the disguised inferior model. The experimental results are shown in Figure <ref type="figure">2</ref>, from which we can see that for all tasks, after the attacker conducts MDA with only 50 samples for each category, the LogME score of the disguised inferior model exceeds that of the superior model, demonstrating that the static features of PTMs of a target task could be easily changed with limited supervision. The attacker could successfully attack LogME by only gathering a very small amount of samples.</p><p>Time Cost for MDA. We also evaluate the time costs of performing MDA on the inferior PTM. Specifically, we evaluate the attack efficiency of MDA using 50 samples per class for SST-2, MRPC and CoLA, respectively. As shown in Figure <ref type="figure">2</ref>, after MDA, the LogME score of the disguised inferior model is higher than that of the superior model for each task. We find that for every task, the execution of MDA can be finished in around 1 minute using a single RTX2080 GPU, demonstrating the high efficiency of MDA.</p><p>Hybrid-task MDA. In addition to the amounts of data and time required for MDA, we study another situation where the model selection is conducted based on the LogME scores on multiple tasks, instead of on one specific task. </p><formula xml:id="formula_9">M * inf &gt; S D i</formula><p>Msup for all tasks). By jointly attacking all the tasks with limited supervision, the attacker can successfully deceive the LogME algorithm on multiple tasks.</p><p>Transferability of MDA. Taking a step further, we test a more difficult situation where the attacker has no access to the specific downstream dataset to be evaluated. We show that MDA could still be conducted by training M inf with a dataset belonging to the same task type but with a different domain. This is based on the hypothesis that MDA could be transferred among similar tasks. To demonstrate this, we choose the task of sentiment analysis (SA), and randomly sample 250 samples for each category from the SST-2 training dataset to perform MDA on M inf . After that, we test the LogME scores of the disguised model M * inf on other SA datasets, i.e., IMDB <ref type="bibr" target="#b14">(Maas et al., 2011)</ref>, Amazon polarity <ref type="bibr" target="#b16">(McAuley and Leskovec, 2013)</ref>, Yelp polarity <ref type="bibr" target="#b30">(Zhang et al., 2015)</ref> and Rotten tomatoes <ref type="bibr" target="#b17">(Pang and Lee, 2005)</ref>. The results are shown in Table <ref type="table" target="#tab_4">3</ref>, from which we observe that even if MDA is performed using a small amount of samples from the SST-2 dataset, the disguised M * inf will be chosen by FMS (</p><formula xml:id="formula_10">S D i M * inf &gt; S D i</formula><p>Msup ) when evaluated on other SA downstream tasks. Also, only using Dataset  a small amount of SST-2 data to perform MDA can ensure that the disguised M * inf still performs worse than M sup after fine-tuning. The experimental results show excellent transferability of MDA across similar tasks.</p><formula xml:id="formula_11">S D i M inf S D i Msup S D i M * inf (50) S D i M * inf (100) S D i M * inf (<label>250</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Evaluation Data Selection</head><p>In this section, we experiment with our proposed EDS method and follow most of the experimental settings in § 4.1. We perform experiments on six GLUE tasks. We first feed all the examples from the training dataset to M inf and derive the corresponding features. Then we use the K-means algorithm on the extracted features and select the data points whose features are close to the cluster centroids. We filter out samples that are close to the same cluster centroid but with different labels. Then we test the LogME score on each selected subset in the existence of a subset that could deceive FMS at least shows that current FMS algorithms are very sensitive to the evaluation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combinations with Backdoor Attack</head><p>In this section, we further combine both MDA and EDS with the backdoor attack, namely NeuBA <ref type="bibr" target="#b31">(Zhang et al., 2021)</ref>. NeuBA is conducted during the pre-training stage, and does not require the specific data of the downstream task.</p><p>Combinations with MDA. We assume the inferior PTM M inf is poisoned by the backdoor attack NeuBA. For the inferior PTM M nb that has been poisoned by NeuBA, we randomly sample a few samples from SST-2 <ref type="bibr" target="#b25">(Socher et al., 2013)</ref> and OLID <ref type="bibr" target="#b29">(Zampieri et al., 2019)</ref>   not be chosen by FMS ( S D i M nb &lt; S D i Msup ), so its hazards may be limited. However, after our MDA,</p><formula xml:id="formula_12">S D i M * nb &gt; S D i</formula><p>Msup and thus the disguised poisoned model will be chosen by FMS.</p><p>We also perform experiments to see whether the backdoor still exists after MDA. Specifically, if the user fine-tunes the M * nb using the downstream clean datasets, we then test the Attack Success Rate (ASR), following <ref type="bibr" target="#b31">(Zhang et al., 2021)</ref>. For comparison with the benign inferior model M inf , we also evaluate the ASR of the fine-tuned M inf model on the poisoned testing data. For SST-2, the ASR 0 and ASR 1 represent the ASR neg and ASR pos , respectively. For OLID, the ASR 0 and ASR 1 represent the ASR no and ASR yes , respectively. The ASR 0 for the benign model in Table <ref type="table" target="#tab_9">6</ref> is the highest ASR 0 among all triggers. The ASR 1 in Table <ref type="table" target="#tab_9">6</ref> for the benign model is the highest ASR 1 among all triggers. From the results in Table <ref type="table" target="#tab_9">6</ref>, we can see that the ASR of the fine-tuned M * nb is higher compared with that of the fine-tuned M inf . The above results show the potential risk that the attacker can use the MDA method to let the FMS select an inferior model poisoned by the backdoor attack.</p><p>Combinations with EDS. We also explore combining the backdoor attack (NeuBA) with EDS on SST-2 and OLID. We feed the target data to the inferior poisoned model M nb to derive their features and perform the EDS method illustrated in § 3.4. The results are shown in Table <ref type="table" target="#tab_11">7</ref>. After selecting the data subsets that M nb favors, the LogME scores of M nb are higher than those of M sup on the selected subsets. From the results, we can find that EDS is an effective method to make FMS choose an inferior poisoned model attacked by NeuBA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on other Pre-trained Models and other FMS Algorithms</head><p>We verify that MDA is model-agnostic, and can be applied to other FMS algorithms. For CV tasks, we choose MobileNetV2 <ref type="bibr" target="#b22">(Sandler et al., 2018)</ref> as the inferior model and ResNet50 <ref type="bibr" target="#b4">(He et al., 2016)</ref> as the superior model. We choose H-score (Bao   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Observations for MDA</head><p>Our MDA is applied on the hidden representation of one specific layer (e.g., the pooler output layer) for a specific token (e.g., [CLS]), which is exactly the same representation that is evaluated in FMS. In practical applications, it may occur that the downstream user applies FMS on the representations of other tokens / layers. We thus design experiments to see whether our MDA could still successfully deceive FMS under these circumstances.</p><p>Obs. 1: MDA could infect other layers. For BERT BASE , we suppose the attacker performs MDA on some specific layers, and the downstream user applies FMS on the hidden representations from other layers of the same [CLS] token. In Figure <ref type="figure" target="#fig_1">3</ref>, we plot the LogME scores derived from</p><p>[CLS] embeddings of different transformer layers of the disguised inferior PTM, using the SST-2 dataset. Specifically, we experiment on performing MDA on (1) the pooler output, (2) the [CLS] representation of the 5-th layer and (3) the [CLS] representations of the 5-th, 8-th, and 11-th layers.</p><p>From Figure <ref type="figure" target="#fig_1">3</ref>, we can see that no matter the attacker performs MDA on which layer, the  LogME scores derived from the output [CLS] embeddings of all transformer layers of the disguised BERT BASE model are higher than those of the RoBERTa BASE model. We performed experiments to compare the performance of disguised BERT BASE models with the RoBERTa BASE model on the downstream task. The fine-tuned accuracy on the dev dataset of the models disguised by different training strategies (1), ( <ref type="formula">2</ref>) and (3) are 92.78%, 89.79% and 90.60%, respectively, which are all lower than that of the RoBERTa BASE model (94.50%). From the above results, we can see that no matter the downstream user applies FMS on which layer, the disguised inferior model will be chosen under three settings.</p><p>Obs. 2: MDA could infect other tokens. Our MDA is applied on the representation of a single token [CLS], we investigate whether such an attack is contagious to other tokens. Specifically, we apply our MDA on the [CLS] token of BERT BASE using all samples from SST-2 and then evaluate the [SEP] token<ref type="foot" target="#foot_0">3</ref> during FMS. From the results shown in Table <ref type="table" target="#tab_15">11</ref>, we find that even if we perform MDA on the pooler output corresponding to the [CLS] token, the feature of [SEP] token is still affected, which means that MDA could infect other tokens.</p><p>From these two observations, we can find that only using static features of different layers / tokens can not defend our proposed MDA method. We leave observations for EDS in appendix B and alternative model selection method that can defend MDA in appendix C . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we demonstrate the vulnerability of feature-based model selection methods by proposing two methods, model disguise attack and evaluation data selection, both of which successfully deceive FMS into mistakenly ranking PTMs' transferability. Moreover, we find that our proposed methods can further be combined with the backdoor attack to mislead a victim into selecting the poisoned model. To the best of our knowledge, this is the first work to analyze the defects of current FMS algorithms and evaluate their potential security risks. Our study reveals the previously unseen risks of FMS and calls for improvement for the robustness of FMS. In the future, we will explore more effective, robust and efficient model selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Comparisons with Fine-tuning</p><p>Another possible methodology to conduct the model disguise attack is to use the cross-entropy loss to fine-tune the inferior PTM. We name this kind of attack as CE attack. We name the attack method using supervised contrastive loss that is proposed in § 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Alternative Model Selection Method</head><p>We demonstrate that fine-tuning the models with a few steps is a simple and more robust method for model selection and can defend MDA. As shown in Figure <ref type="figure">2</ref>, the LogME score of the disguised model M * inf is higher than M sup after the attacker uses 50 samples from each category to perform MDA on   SST-2, MRPC, and CoLA, respectively. However, after fine-tuning the disguised BERT BASE model M * inf and the RoBERTa BASE model M sup for a while, the performance of the fine-tuned M sup is higher than that of the fine-tuned model M * inf . The results of the accuracy on dev dataset after finetuning model M * inf and M sup on SST-2 dataset with different epochs are shown in Figure <ref type="figure" target="#fig_2">5</ref>. From Figure <ref type="figure" target="#fig_2">5</ref>, we can see that after fine-tuning two models for a few steps, M sup 's superiority has been demonstrated. The results of fine-tuning two models on MRPC and CoLA for one epoch are shown in Table <ref type="table" target="#tab_19">14</ref>. The F1 score is reported for MRPC and MCC score is reported for CoLA. From the results in Table <ref type="table" target="#tab_19">14</ref>, we can see that after fine-tuning two models for one epoch, the model M sup 's performance is higher than the disguised model M * inf on dev datasets of MRPC and CoLA, respectively. Fine-tuning models on the downstream task for a while and then comparing the performance of fine-tuned models is a more robust model selection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis</head><p>To visualize the transition of the static features after we apply MDA on the inferior PTM, we randomly sampled 250 samples for each category from the SST-2 dataset and plot the pooler output features corresponding to the <ref type="bibr">[CLS]</ref> token that are encoded by the original BERT BASE model and disguised BERT BASE model, respectively. The   <ref type="figure" target="#fig_4">7</ref>, respectively. The red marks and green circles in Figure <ref type="figure" target="#fig_3">6</ref> and Figure <ref type="figure" target="#fig_4">7</ref> represent features of sampled negative samples and positive samples, respectively. From Figure <ref type="figure" target="#fig_3">6</ref> and Figure <ref type="figure" target="#fig_4">7</ref>, we can see that after MDA, the sentence representations that belong to the same class become closer to each other. The LogME score becomes higher after MDA. The LogME score has a close relation to the quality of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Training Details for Experiments E.1 Experiments on Model Disguise Attack</head><p>Attack Performance of MDA. We choose AdamW as the optimizer, set the peak learning rate to 3 × 10 −5 , and linearly decay it. For the dropout rate in the supervised contrastive loss function, we perform the search from 0.1, 0.1 and 0.1, 0.05. For the six tasks except for RTE, the best dropout rate combination is 0.1, 0.05. For the RTE task, the best dropout rate combination is 0.1, 0.1. We use the best dropout rate combination for each downstream task to perform MDA. About the metrics used for the performance of fine-tuned models reported in Table <ref type="table" target="#tab_1">1</ref>, F1 scores are reported for QQP and MRPC, Matthews Correlation Coefficient (MCC) score is reported for CoLA, and accuracy scores are reported for the other tasks. We report the matched accuracy for MNLI.</p><p>Hybrid-task MDA. We optimize the supervised contrastive loss on M inf for 100 epochs using the sampled mixed training data. The dropout probabilities of two augmentations are 0.1 and 0.05. For six GLUE tasks except for QQP, 50, 100, 250 samples for each category are randomly sampled from the training dataset of each task in three hybrid-task MDA experiments, respectively. We randomly sample 500 samples for each category from the QQP dataset for all three hybrid-task MDA experiments. The total class number of the sampled mixed data is the summation of class numbers from seven GLUE tasks.</p><p>Transferability of MDA. Since the original IMDB dataset does not contain the dev dataset, we split the original IMDB training dataset into a training dataset and a dev dataset with a ratio of 9:1 for fine-tuning models. The LogME score is still calculated using the original IMDB training dataset. For Amazon Polarity, we randomly sample 9000, 1000 and 1000 samples from the original Amazon Polarity training dataset as our training, dev and testing datasets for fine-tuning models. The LogME score is calculated using the new sampled training dataset. The template for the sample x in Amazon Polarity is "title: x title content: x content ". For Yelp Polarity, we randomly sample 7600 and 7600 samples from the original Yelp Polarity testing dataset as our dev and testing datasets when fine-tuning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Experiments on Evaluation Data Selection</head><p>For SST-2, QNLI, QQP and MRPC, we choose the closest 2000 samples before filtering, while for CoLA, we choose the closest 1000 samples. After filtering out those samples that are close to the same cluster centroid but with different labels, we retain 957, 760, 968, 303 and 532 examples for SST-2, QNLI, QQP, CoLA and MRPC, respectively. Due to the very imbalanced data points in each cluster after clustering the features of MNLI, we limit the number of selected samples to 200 for each class when choosing the samples whose features are close to cluster centroids after filtering. Thus the number of selected samples for MNLI is 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Combinations with Backdoor Attack</head><p>Combinations with MDA. For the inferior PTM M nb that has been poisoned with NeuBA, we randomly sample 500 samples for each category from the SST-2 dataset and the OLID dataset, respectively, to perform the hybrid-task MDA by training the poisoned M nb with SCL for 5 epochs.</p><p>Combinations with EDS. We feed the target data to the M nb model to derive their features. We perform the K-means method on the features extracted by the M nb model to get the cluster centroids. For SST-2 and OLID, we choose the top 2000 samples whose features extracted by M nb are closest to cluster centroids before filtering, respectively. After filtering, the number of examples for SST-2 and OLID are 1137 and 819, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Experiments on other Pre-trained Models and other FMS Algorithms</head><p>We verify the effectiveness of our proposed MDA and EDS methods on the DistilBERT BASE <ref type="bibr" target="#b23">(Sanh et al., 2019)</ref>. For DistilBERT BASE , we derive the LogME score from the [CLS] token's representation in the last layer. To keep consistent with the results in Table <ref type="table" target="#tab_1">1</ref>, the LogME score of the RoBERTa BASE model still derives from the pooler output corresponding to the &lt;s&gt; token. For MDA, the dropout probabilities of two augmentations are set as 0.1 and 0.1 in the experiments. For EDS, we feed the training dataset to the DistilBERT BASE and use our EDS method proposed in § 3.4 to select the subset D sub i . Specifically, we select the top 2000 samples whose features are close to the cluster centroids for MRPC and CoLA before filtering. After filtering, we retain the 822 and 636 samples for MRPC and CoLA, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The overall framework of model disguise attack (MDA) and evaluation data selection (EDS). After MDA, the disguised poisoned model is mistakenly chosen by FMS. After EDS, the score for the poisoned model rated by FMS is higher than that of the superior model on the selected subset D sub i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The LogME scores derived from [CLS] embeddings of different transformer layers under different situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The accuracy after fine-tuning two models with 0.125, 0.25, and 0.5 epochs on SST-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The TSNE figure of features extracted by the original BERT BASE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The TSNE figure of features extracted by the disguised BERT BASE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to demonstrate our hypothesis. Assume we have two PTMs M inf and M sup .</figDesc><table><row><cell>M inf has poorer transferability than M sup on</cell></row><row><cell>task T i , which is correctly judged by an FMS al-gorithm, i.e., S D i M inf &lt; S D i Msup . Specifically, (1)</cell></row><row><cell>MDA aims to post-train the inferior PTM M inf to</cell></row><row><cell>deceive FMS so that during model selection, the</cell></row><row><cell>disguised PTM M  *  inf , instead of the superior PTM</cell></row><row><cell>M sup , would be mistakenly chosen by FMS, i.e.,</cell></row><row><cell>S D i M  *  inf PTM M  *  &gt; S D i Msup . In the meantime, the disguised inf still performs worse than M sup af-</cell></row><row><cell>ter fine-tuning on the target dataset; (2) instead of</cell></row><row><cell>training the PTM, EDS aims to choose a subset of</cell></row><row><cell>examples D sub</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of LogME scores and fine-tuned performance of different models. P Msup and P M * inf represent the performance of the fine-tuned M sup and M * inf , respectively. The metrics used for the reported fine-tuned performance are shown in appendix E.</figDesc><table><row><cell>3489 -0.3186 94.50 1.0496 92.78</cell></row><row><cell>MRPC -0.5864 -0.5789 90.91 0.2970 90.81</cell></row><row><cell>MNLI -0.6035 -0.5700 87.88 0.4457 84.82</cell></row><row><cell>CoLA -0.5464 -0.5035 63.56 0.5463 57.38</cell></row><row><cell>QNLI -0.5858 -0.5706 92.60 0.8109 91.27</cell></row><row><cell>QQP -0.5181 -0.4584 88.60 0.8085 88.49</cell></row><row><cell>RTE -0.7093 -0.7111 79.06 -0.1259 71.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of LogME scores of different models after performing hybrid-task MDA on M inf with different amounts of data. The number of samples for each category sampled from six GLUE tasks (except QQP) is shown. For QQP, 500 samples per class are sampled in all three experiments. (The successful attacks are in boldface)</figDesc><table><row><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Transferability of MDA on the sentiment analysis task. AP, YP and RT represent the Amazon polarity, the Yelp polarity and the Rotten tomatoes, respectively. P M inf , P M * inf and P Msup represent the classification accuracy of the fine-tuned M inf , M * inf and M sup , respectively, on the testing dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>, which shows that our proposed</cell></row><row><cell cols="2">EDS method successfully selects those data points</cell></row><row><cell cols="2">that the inferior model favors so that its LogME</cell></row><row><cell cols="2">score S D sub i M inf is higher than S D sub i Msup on the selected</cell></row><row><cell>subset D sub i</cell><cell>. Although EDS is hard to be deployed</cell></row><row><cell cols="2">in practice since it requires the attacker to manip-</cell></row><row><cell cols="2">ulate the data for FMS's evaluation, we argue that</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The LogME scores of M inf and M sup on the subsets D sub</figDesc><table><row><cell>i</cell><cell cols="2">selected by EDS.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>S D i M inf</cell><cell>S D i Msup</cell><cell>S D i M nb</cell><cell>S D i M  *  nb</cell></row><row><cell>SST-2</cell><cell>-0.3489</cell><cell>-0.3186</cell><cell>-0.5200</cell><cell>-0.1382</cell></row><row><cell>OLID</cell><cell>-0.5456</cell><cell>-0.5380</cell><cell>-0.7257</cell><cell>-0.4542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of LogME scores of different models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>From the results, we can find that the inferior PTM poisoned by the backdoor attack may</figDesc><table><row><cell>datasets to perform</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The ASR of the fine-tuned M inf and M * nb . The ASR is tested on the poisoned testing data. P M inf , P M nb , P M * nb and P Msup represent the performance of the fine-tuned M inf , M nb , M * nb and M sup , respectively, on the clean testing data. For SST-2, we report the accuracy. For OLID, we report the macro F1 score.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The LogME scores of M nb and M sup on the subsets D sub BASE<ref type="bibr" target="#b23">(Sanh et al., 2019)</ref> as the inferior model and RoBERTa BASE as the superior model. We experiment on MRPC and CoLA tasks. We use all labeled data in the training dataset to perform MDA and derive the disguised model DistilBERT * BASE . From the results in Table9, we can see that after MDA, S D i DistilBERT * is higher than S D i RoBERTa while the fine-tuned performance of DistilBERT * BASE is poorer than that of RoBERTa BASE . The disguised inferior model is chosen. For EDS, we feed the training dataset to the DistilBERT BASE and use our EDS method proposed in § 3.4 to select the subset D sub i . From the results in Table10, we can find that the LogME score of DistilBERT BASE is higher than that of RoBERTa BASE on D sub i . The results show that our proposed methods can be applied to other PTMs and FMS algorithms.</figDesc><table><row><cell>i</cell><cell>selected by EDS.</cell></row><row><cell cols="2">et al., 2018) and LogME (You et al., 2021) as the</cell></row><row><cell cols="2">evaluated FMS algorithms. We experiment on the</cell></row><row><cell cols="2">CIFAR-100 dataset (Krizhevsky, 2009) with both</cell></row><row><cell cols="2">full-data setting and low-resource setting, where</cell></row><row><cell cols="2">we use all labeled samples in the training dataset</cell></row><row><cell cols="2">and randomly sampled 30 examples from each cat-</cell></row><row><cell cols="2">egory to conduct MDA, respectively. The changes</cell></row><row><cell cols="2">of LogME score and H-score on CV tasks after</cell></row><row><cell cols="2">MDA are shown in Table 8. Before MDA, both</cell></row><row><cell cols="2">the LogME score and H-score of ResNet50 are</cell></row><row><cell cols="2">higher than those of MobileNetV2, and the down-</cell></row><row><cell cols="2">stream performance of ResNet50 is higher than</cell></row><row><cell cols="2">that of MobileNetV2. However, after MDA, the</cell></row><row><cell cols="2">disguised MobileNetV2 is mistakenly chosen by ei-</cell></row><row><cell cols="2">ther FMS. It can also be derived that the disguised</cell></row><row><cell cols="2">MobileNetV2 still performs worse than ResNet50</cell></row><row><cell cols="2">in the downstream task.</cell></row><row><cell cols="2">For NLP tasks, we choose DistilBERT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of LogME scores and fine-tuned performance of different models. PRoBERTa / PDistilBERT * is the fine-tuned performance of RoBERTa / DistilBERT * . F1 score is reported for MRPC and Matthews Correlation Coefficient (MCC) score is reported for CoLA.</figDesc><table><row><cell>Dataset</cell><cell>MRPC</cell><cell>CoLA</cell></row><row><cell>S DistilBERT D sub i S D sub i RoBERTa</cell><cell>0.1534 0.0042</cell><cell>1.4578 1.4473</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>The LogME scores of DistilBERT and RoBERTa on the subsets D sub</figDesc><table /><note>i selected by EDS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>The LogME scores corresponding to the [SEP] token of different models.</figDesc><table><row><cell>Model</cell><cell>LogME score</cell></row><row><cell>RoBERTaBASE</cell><cell>-0.3078</cell></row><row><cell>BERTBASE</cell><cell>-0.3578</cell></row><row><cell>BERTBASE+ MDA</cell><cell>0.9807</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>3 as SCL attack. We performed experiments to compare the efficiency of SCL attack with CE attack, hybrid attack and SCL+CE attack, respectively. Compared with CE attack, the LogME score after SCL attack is higher than that after CE attack for 5 epochs, which demonstrates SCL attack is more efficient than CE attack. For the hybrid attack, we tried using the mixture of cross-entropy loss and supervised contrastive loss with the weight 0.5 and 0.5 for two losses to train the BERT BASE model for 5 epochs. The LogME score after hybrid attack is lower than that after SCL attack for 5 epochs, which shows that SCL attack is more efficient than hybrid attack. For SCL + CE attack, we first use the SCL attack to train the BERT BASE model for 5 epochs and then apply the CE attack for 5 epochs. The LogME score after SCL + CE attack is lower than that after the single SCL attack for 10 epochs, which demonstrates SCL attack's superiority. All experiments are performed on the SST-2 dataset. The results are shown in Table12. From the experimental results, we can see that the SCL attack is a more powerful attack method. Comparisons of SCL attack with CE attack. The number of training epochs is shown in the bracket.</figDesc><table><row><cell>Attack Method</cell><cell>LogME score</cell></row><row><cell>CE (5)</cell><cell>1.2565</cell></row><row><cell>hybrid attack (5)</cell><cell>1.4921</cell></row><row><cell>SCL (5)</cell><cell>1.6053</cell></row><row><cell>SCL + CE (5+5)</cell><cell>2.5460</cell></row><row><cell>SCL (10)</cell><cell>3.0028</cell></row><row><cell>B Observations for EDS</cell><cell></cell></row><row><cell cols="2">Obs.3: EDS could infect other layers. We as-</cell></row><row><cell cols="2">sume that the attacker selects a subset D sub of SST-</cell></row><row><cell cols="2">2 that is illustrated in 4.2 for the user to evaluate.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>The LogME scores corresponding to the [SEP] token of different models on the subset selected by EDS. From Figure4, we can see that even if the subset D sub is selected through the features of pooler output extracted by BERT BASE model, the LogME scores of BERT BASE model derived from [CLS] embeddings of all layers are higher than those of RoBERTa BASE model on the subset D sub .Obs.4: EDS could infect other tokens. Also, from Table13, we can see that even if the subset D sub is selected through the feature of pooler output corresponding to [CLS] token that is extracted by BERT BASE model as shown in 4.2, the LogME score of BERT BASE model derived from the feature of [SEP] token in the last layer is higher than that of RoBERTa BASE model on the subset D sub .</figDesc><table><row><cell>Specifically, the attacker performs K-means clus-</cell></row><row><cell>tering on the features of pooler output correspond-</cell></row><row><cell>ing to [CLS] token, selects the data points whose</cell></row><row><cell>features are close to the cluster centroids and per-</cell></row><row><cell>forms filtering. The features used for clustering are</cell></row><row><cell>derived from a specific layer (i.e. pooler output)</cell></row><row><cell>and a specific token (i.e. [CLS]).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>The performance after fine-tuning two models for one epoch on MRPC and CoLA.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">For RoBERTa, the SEP token is &lt;/s&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Key R&amp;D Program of China (No. 2020AAA0106502), Institute Guo Qiang at Tsinghua University, Beijing Academy of Artificial Intelligence (BAAI), and International Innovation Center of Tsinghua University, Shanghai, China.</p><p>Biru Zhu and Yujia Qin designed the methods and the experiments. Biru Zhu conducted the experiments. Biru Zhu and Yujia Qin wrote the paper. Fanchao Qi, Yangdong Deng, Zhiyuan Liu, Maosong Sun and Ming Gu advised the project and participated in the discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An information-theoretic metric of transferability for task transfer learning</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Shao-Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A linearized framework and a new benchmark for model selection for fine-tuning</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Zancato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2102.00084</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>AI Open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy transferability estimation</title>
		<author>
			<persName><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2106.09362</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Programmable neural network trojan for pre-trained feature extractor</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youhui</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1901.07766</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="32" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weight poisoning attacks on pretrained models</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2793" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/1908.03557</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="1907">2019. 1907.11692</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58607-2_11</idno>
		<title level="m">Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="182" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2507157.2507163</idno>
	</analytic>
	<monogr>
		<title level="m">Seventh ACM Conference on Recommender Systems, RecSys &apos;13</title>
				<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10-12">2013. October 12-16, 2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219855</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
				<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hidden killer: Invisible textual backdoor attacks with syntactic trigger</title>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.37</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="443" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Turn the combination lock: Learnable textual backdoor attacks via word substitution</title>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.377</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4873" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hidden trigger backdoor attacks</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshayvarun</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6871</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11957" to="11965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18">2018. 2018. June 18-22. 2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1108">2019. 1910.01108</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Supcl-seq: Supervised contrastive learning for downstream optimized sequence representations</title>
		<author>
			<persName><forename type="first">Hooman</forename><surname>Sedghamiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Raval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuka</forename><surname>Alhanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno>abs/2109.07424</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Logme: Practical assessment of pre-trained models for transfer learning</title>
		<author>
			<persName><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12133" to="12143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation of pre-trained convolutional neural network models for object recognition</title>
		<author>
			<persName><surname>Zabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaidah</forename><surname>Fazira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurbaity</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><surname>Sabri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering and Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="95" to="98" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>3.15</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting the type and target of offensive posts in social media</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Red alarm for pre-trained models: Universal vulnerabilities by neuron-level backdoor attacks</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2101.06969</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
