<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Augmentation For Better Graph Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jiashun</forename><surname>Cheng</surname></persName>
							<email>jchengak@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Man</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fugee</forename><surname>Tsung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Augmentation For Better Graph Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph self-supervised learning has been vastly employed to learn representations from unlabeled graphs. Existing methods can be roughly divided into predictive learning and contrastive learning, where the latter one attracts more research attention with better empirical performance. We argue that, however, predictive models weaponed with latent augmentations and powerful decoder could achieve comparable or even better representation power than contrastive models. In this work, we introduce data augmentations into latent space for superior generalization and better efficiency. A novel graph decoder named Wiener Graph Deconvolutional Network is correspondingly designed to perform information reconstruction from augmented latent representations. Theoretical analysis proves the superior reconstruction ability of graph wiener filter. Extensive experimental results on various datasets demonstrate the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-Supervised Learning (SSL) has been extended to graph data recently due to its great success in computer visions, which extracts informative knowledge through well-designed pretext tasks from unlabeled data <ref type="bibr" target="#b38">[39]</ref>. With regard to the objectives of pretext tasks, graph SSL can be divided into two major categories: predictive SSL and contrastive SSL <ref type="bibr" target="#b19">[20]</ref>. Predictive models train the graph autoencoders with a prediction task supervised by informative properties generated from graph freely, while contrastive models are trained on the mutual information between several different views contrasted from the original graph. As the dominant technique, contrastive SSL has achieved state-of-the-art performance empirically <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52]</ref> for graph representation learning. However, its high dependence on data augmentation schemes causes critical scaling-up and efficiency issues <ref type="bibr" target="#b41">[42]</ref>.</p><p>In terms of predictive SSL, graph reconstruction is a natural self-supervision. Most methods, e.g., GAE <ref type="bibr" target="#b12">[13]</ref>, employ autoencoder structure with augmentations on the input graph, like node masking <ref type="bibr" target="#b48">[49]</ref> and feature masking <ref type="bibr" target="#b8">[9]</ref>, to enrich the training data. However, a tiny augmentation on the original Preprint. Under review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2206.12933v1 [cs.LG] 26 Jun 2022</head><p>graph may unexpectedly cause huge variation on latent space, or even change the semantics. For example, dropping a hub node will change the class manifolds of its neighbors (see Figure <ref type="figure" target="#fig_0">1</ref>). In this regard, many commonly used augmentations are semantically uncontrollable and may take the risk of low generalizations. To avoid such semantic ambiguity, one may seek for domain-specific knowledge as guidance, but this will lead to inefficiency. Therefore, a natural question comes up, that is, can we improve predictive SSL by na?ve augmentations for efficient and powerful representation learning?</p><p>Inspired by representation learning <ref type="bibr" target="#b2">[3]</ref>, we inject simple augmentations (e.g., Gaussian noise) on latent graph representations, which is more likely to capture diverse semantic transformations. It requires no domain knowledge and avoids heuristic data augmentations, and can therefore improve generalization and efficiency. However, the learning paradigm of predictive SSL, especially graph autoencoder, will be severely affected by the additive augmentations and most decoders are vulnerable w.r.t. these additive augmentations, including existing GALA <ref type="bibr" target="#b23">[24]</ref> and Graph Deconvolutional Network (GDN) <ref type="bibr" target="#b16">[17]</ref>. Therefore, an adaptive decoder for augmented latent space is required. Motivated by recent advancement of wiener in deep image restoration <ref type="bibr" target="#b3">[4]</ref>, we introduce the classical deconvolutional technique, wiener kernel, into GDN, which is the theoretical optimum for restoring noisy signals with respect to Mean Squared Error (MSE). Specifically, we first derive the graph wiener filter and prove its superiority in theory. We observe that, however, directly using the explicit wiener filter induces low scalability due to indispensable eigen-decomposition and may be not applicable to large-scale datasets. Therefore, we adopt average graph spectral power and Remez polynomial <ref type="bibr" target="#b22">[23]</ref> for fast approximation. We evaluate the effectiveness and efficiency of the proposed wiener GDN on node and graph classification tasks with extensive datasets and achieve outperforming performance.</p><p>The main contributions of this work are as follows:</p><p>? We inject simple augmentations to latent representations efficiently using Gaussian noise, which can enrich the diversity of training data while preserving semantics.</p><p>? We develop Wiener Graph Deconvolutional Network (WGDN), in order to better reconstruct graph attributes from augmented latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Graph self-supervised learning According to recent surveys <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>, works in graph SSL can be classified into two categories: contrastive learning and predictive learning. Contrastive SSL attracts more attention currently due to the state-of-the-art performance on representation learning empirical evaluations. DGI <ref type="bibr" target="#b35">[36]</ref>, the pioneer work, incorporates GNN with contrastive learning using local-global mutual information maximization. Within this research line, improvements mainly focus on the augmentation schemes. For example, MVGRL <ref type="bibr" target="#b7">[8]</ref> applies graph diffusion to generate structural views. GRACE <ref type="bibr" target="#b50">[51]</ref> proposes to generate two augmented views by corruption. GCA <ref type="bibr" target="#b51">[52]</ref> further improves it by considering structure and attribute information. Despite their advancement, most of their augmentations rely on domain-specific knowledge and limit their efficiency. As for predictive learning, predicting node attribute and neighborhood context is a traditional pretext task, and its representative manner <ref type="bibr" target="#b48">[49]</ref> follows the perturb-then-learn strategy to predict the corrupted information, such as attribute masking <ref type="bibr" target="#b8">[9]</ref> and feature corruption <ref type="bibr" target="#b37">[38]</ref>. Our work belongs to predictive learning and we, for the first time, propose simple augmentations on latent representations.</p><p>Graph deconvolutional network Regarding graph deconvolution, early research <ref type="bibr" target="#b45">[46]</ref> formulates the deconvolution as a pre-processing step. GALA <ref type="bibr" target="#b23">[24]</ref> and SpecAE <ref type="bibr" target="#b17">[18]</ref> both perform Laplacian sharpening to recover information. Recent work <ref type="bibr" target="#b49">[50]</ref> employ GCN <ref type="bibr" target="#b13">[14]</ref> to reconstruct node attributes from the latent representations. All these works, however, neglect the influence of noise. Another GDN framework <ref type="bibr" target="#b16">[17]</ref> is designed via a combination of inverse filters in spectral domain and denoising layers in wavelet domain, which is sub-optimal regarding signal reconstruction. Wiener filtering, as an alternative, executes an optimal trade-off between signal recovering and denoising. It has been introduced to deconvolutional networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref> for image deblurring. However, its effectiveness on graph structure has not been well investigated yet.</p><p>Latent space data augmentation Data augmentations in latent space can enrich semantic transformations which are difficult to be defined in the input space <ref type="bibr" target="#b33">[34]</ref>. It has been extensively studied in representation learning, and common manipulations, such as interpolation, extrapolation, random translation and adding Gaussion noise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, have shown their effectiveness in CV <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref> and NLP <ref type="bibr" target="#b15">[16]</ref>. Recently, latent augmentation is introduced in adversarial training <ref type="bibr" target="#b10">[11]</ref> for GNNs. Our work will further exploit its potentials in graph SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Under a generic unsupervised graph representation learning setup, we are given an attributed graph G = (V, A, X) consisting of: (1) V = {v 1 , v 2 , ..., v N } is the set of nodes; (2) A ? R N ?N is the adjacency matrix where A ij ? {0, 1} represents whether an undirected edge exists between v i and v j ; and (3) X ? R N ?D denotes the attribute matrix. Our objective is to learn an autoencoder with encoder E : (R N ?N , R N ?D ) ? R N ?D and decoder D : R N ?D ? (R N ?N , R N ?D ) to produce semantic node embedding. H = E(A, X) ? R N ?D represents the learned embedding in low dimensional space, which can be used for various downstream tasks.</p><p>Graph convolution Convolutional operation in graph can be interpreted as a special form of Laplacian smoothing on nodes. From the spectral perspective, graph convolution on a signal x ? R N with a filter g c is defined as</p><formula xml:id="formula_0">h = g c * x = Udiag(g c (? 1 ), ..., g c (? N ))U T x = Ug c (?)U T x = g c (L)x,<label>(1)</label></formula><p>where {? i } N i=1 and U represent the eigenvalues and eigenvectors of normalized Laplacian matrix L = I -D -1 2 AD -1 2 = U?U T respectively. D denotes the Degree matrix. * denotes convolutional operator. We consider (1) GCN <ref type="bibr" target="#b13">[14]</ref>, it is a low-pass filter in spectral domain with g c (? i ) = 1 -? i shown by <ref type="bibr" target="#b40">[41]</ref>; (2) GDC <ref type="bibr" target="#b14">[15]</ref>, it uses heat diffusion kernel g c (? i ) = e -t?i ; (3) APPNP <ref type="bibr" target="#b4">[5]</ref>, it leverages personalized pagerank kernel g c (?</p><formula xml:id="formula_1">i ) = ? 1-(1-?)(1-?i) .</formula><p>Graph deconvolution As an inverse to convolution, graph deconvolution aims to recover the input attributes given the smoothed node representation. From the spectral perspective, graph deconvolution on a smoothed representation h ? R N with filter g d is defined as</p><formula xml:id="formula_2">x = g d * h = Udiag(g d (? 1 ), ..., g d (? N ))U T h = Ug d (?)U T h = g d (L)h.<label>(2)</label></formula><p>A trivial selection of g d is the inverse function of g c , e.g., g d (? i ) = 1 1-?i for GCN <ref type="bibr" target="#b16">[17]</ref>,</p><formula xml:id="formula_3">g d (? i ) = e t?i for GDC, or g d (? i ) = 1-(1-?)(1-?i) ? for APPNP.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The proposed framework</head><p>In this section, we first extend classical wiener filter to graph domain and demonstrate its superiority in handling augmented representations. Then, we propose Wiener Graph Deconvolutional Network (WGDN), an efficient training scheme weaponed with latent augmentations and graph wiener filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Wiener filter on graph</head><p>In this work, we follow the settings in previous papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref> and introduce additive latent augmentations in model training due to its flexible statistical characteristics. Combining with the graph convolution in Eq. 1, augmented representation ? in graph is similarly defined as</p><formula xml:id="formula_4">? = Ug c (?)U T x + ,<label>(3)</label></formula><p>where x ? R N denotes input attribute and ? R N is assumed to be any i.i.d. random augmentation with</p><formula xml:id="formula_5">E[ i ] = 0 and VAR[ i ] = ? 2 .</formula><p>In contrast to the isolated data augmentations in graph topology and features, indirectly represents joint augmentations to both <ref type="bibr" target="#b10">[11]</ref>. Naturally, attribute recovered by graph deconvolution is formulated by</p><formula xml:id="formula_6">x = Ug d (?)g c (?)U T x + Ug d (?)U T . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>Proposition 4.1. Let xinv be recovered attribute by inverse filter</p><formula xml:id="formula_8">g d (? i ) = g -1 c (? i ). If g c : [0, 2] ? [-1, 1], then the reconstruction MSE is dominated by amplified augmentation MSE(x inv ) = E x -xinv 2 2 = N i=1 ? 2 g 2 c (?i) .</formula><p>The proof is trivial and illustrated in Appendix A for details. Based on Proposition 4.1, the reconstruction-based learning scheme towards attributes becomes ineffective if inverse filter is implemented on augmented latent space. Our goal is to facilitate the graph reconstruction with augmented representations, which resembles the classical restoration problems in some sense. In signal deconvolution, classical wiener filter <ref type="bibr" target="#b39">[40]</ref> is able to produce a statistically optimal estimation of the real signals from the noisy ones with respect to MSE. With this regard, we are encouraged to extend wiener filter to graph domain <ref type="bibr" target="#b26">[27]</ref>. Assuming the augmentation to be independent from input attributes, graph wiener filter can be similarly defined by extending MSE to graph spectral domain</p><formula xml:id="formula_9">MSE(x) = E x -x 2 2 = E U T x -U T x 2 2 = N i=1 (g d (? i )g c (? i ) -1) 2 E[x * 2 i ] + g 2 d (? i )E[ * 2 i ] = N i=1 S(? i , x * i , ?, g c , g d ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">x * = U T x = {x * 1 , x * 2 , ..., x * N } and * = U T = { * 1 , *<label>2</label></formula><p>, ..., * N } represent graph spectral projection of the input and augmentation. We denote E[x * 2 i ] and S(? i , x * i , ?, g c , g d ) as the spectral power and spectral reconstruction error of spectrum ? i . Considering the convexity of Eq. 5, MSE is minimized by setting the derivative with respect to g d (? i ) to zero and thus we obtain the graph wiener filter g w (? i ) as</p><formula xml:id="formula_11">g w (? i ) = g c (? i ) g 2 c (? i ) + ? 2 /E[x * 2 i ] ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_12">? 2 = VAR[ * i ] = E[ * 2 i ] and ? 2 /E[x * 2 i ]</formula><p>is denoted as the Reciprocal of Signal-to-Noise Ratio (RSNR) of particular spectrum ? i , which represents the relative magnitude of augmentation. Proposition 4.2. Let xw be recovered attribute by g w (? i ), where g w (? i ) is a graph wiener filter, then the reconstruction MSE and variance of xw are less than xinv .</p><p>Please refer to Appendix B for details. Proposition 4.2 shows graph wiener filter has better reconstruction property than inverse filter, which promotes the resilience to latent augmentations and permits stable model training. We observe that, in Eq. 2 and 6, eigen-decomposition is indispensable in computations of spectral power and deconvolutional filter. However, in terms of scalability, an important issue for large-scale graphs is to avoid eigen-decomposition. Note that</p><formula xml:id="formula_13">N i=1 E[x * 2 i ] = N i=1 E[x 2 i ]</formula><p>due to orthogonal transformation, we propose the modified graph wiener filter ?w,? with average spectral power x * 2</p><formula xml:id="formula_14">? = ? ? 1 N N i=1 E[x * 2 i ] as ?w,? (? i ) = g c (? i ) g 2 c (? i ) + ? 2 /x * 2 ? ,<label>(7)</label></formula><p>where ? is a hyperparameter to adjust RSNR. As a natural extension of Proposition 4.2, ?w,? owns the following proposition. Proposition 4.3. Let xw,? be the recovered attribute by modified graph wiener filter ?w,? (? i ), then the variance of xw,? is less than xinv . In spectral domain, given two different</p><formula xml:id="formula_15">? 1 , ? 2 such that E[x * 2 i ] ? x * 2 ?1 ? x * 2 ?2 , the spectral reconstruction error S(? i , x * i , ?, g c , ?w,?1 ) ? S(? i , x * i , ?, g c , ?w,?2 ) ? S(? i , x * i , ?, g c , g -1 c ).</formula><p>Please refer to Appendix C for details. Proposition 4.3 demonstrates that ?w,? attends to spectral reconstructions over different ranges of spectra, depending on the selection of ?. The graph wiener kernel D ? = U? w,? (?)U T is then reformatted in matrix form as</p><formula xml:id="formula_16">D ? = U(g 2 c (?) + ? 2 x * 2 ? I) -1 g c (?)U T .<label>(8)</label></formula><p>Note that g c can be arbitrary function and support of ? i is restricted to [0, 2], we adopt Remez polynomial <ref type="bibr" target="#b22">[23]</ref> to approximate ?w,? (? i ) to neglect eigen-decomposition and matrix inverse in Eq. 8. </p><formula xml:id="formula_17">p K (t) := K k=0 c k t k ,<label>(9)</label></formula><p>where coefficients c 0 , . . . , c K and leveled error e are obtained by resolving linear system The proof is trivial and illustrated in detail as Corollary 8.11 in <ref type="bibr" target="#b0">[1]</ref>. Following Definition 4.1, the K th order Remez approximation of D ? is formulated as</p><formula xml:id="formula_18">?(t j ) = p K (t j ) + (-1) j e,<label>(10)</label></formula><formula xml:id="formula_19">D ? = Up K (?)U T = K k=0 c k,? L k ,<label>(11)</label></formula><p>where D ? is approximated adaptively in each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Wiener graph deconvolutional network</head><p>Graph encoder To incorporate both graph attributes X and structure A in a unified framework, we employ M layers of graph convolution neural network as our graph encoder. For m = 0, ..., M -1,</p><formula xml:id="formula_20">H (m+1) = ?(g c (L)H (m) W (m) ),<label>(12)</label></formula><p>where H (0) = X, ? is the activation function such as PReLU and g c (? i ) = 1 -? i as in GCN <ref type="bibr" target="#b13">[14]</ref>, g c (? i ) = e -t?i as in GDC <ref type="bibr" target="#b14">[15]</ref> or</p><formula xml:id="formula_21">g c (? i ) = ? 1-(1-?)(1-?i)</formula><p>as in APPNP <ref type="bibr" target="#b4">[5]</ref>.</p><p>Representation augmentation For simplicity, Gaussian noise is employed as latent augmentations to the node embedding generated by the last layer encoder</p><formula xml:id="formula_22">?(M) = H (M ) + ?E,<label>(13)</label></formula><p>where ) ] and ? is a hyperparameter to adjust the size of augmentations.</p><formula xml:id="formula_23">E = { 1 , ..., N }, i ? N (0, ? 2 P I), ? 2 P = VAR[H (M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiener graph decoder</head><p>The decoder aims to recover original attributes given the augmented representation ?. Our previous analysis demonstrates the superiority of wiener kernel to permit reconstruction-based representation learning from augmented latent space. Considering the properties of spectral reconstruction error from Proposition 4.3, we symmetrically adopt M layers of graph deconvolution as the decoder, where each layer consists of q channels of wiener kernels. For m = 1, ..., M and i = 1, ..., q,</p><formula xml:id="formula_24">Z (m-1) i = ?(D (m) ?i ?(m) W (m) i ), ?(m-1) = AGG([Z (m-1) 1 , ..., Z (m-1) q ]),<label>(14)</label></formula><p>where X = ?(0) and AGG(?) is aggregation function such as summation.</p><p>Optimization and Inference Our model is optimized following the convention of reconstructionbased SSL mechanism, which is simply summarized as</p><formula xml:id="formula_25">L = ||X -X|| F . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>For downstream applications, we treat the fully trained H (M ) as the final node embedding. For graph-level tasks, we adopt a non-parametric graph pooling (readout) function R, e.g. MaxPooling, to generate graph representation h g = R(H (M ) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity analysis</head><p>The most intensive computational cost of our proposed method is kernel approximation in Eq. 11. Note that kernel approximation is a simple K th order polynomial of graph convolution. By sparse-dense matrix multiplication, graph convolution can be efficiently implemented, which take O(K|E|D 2 ) [14] for a graph with |E| edges.</p><p>Comparison with variational autoencoder In contrast to VGAE <ref type="bibr" target="#b12">[13]</ref> which models the distribution of low-dimensional representations as Gaussians, our proposed method has no requirement of data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with adversarial training</head><p>In general, adversarial training scheme comes with a min-max optimization considering regularized perturbations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11]</ref>. Differently, our method just involves min optimization and our augmentation aims to enrich the diversity of training samples in latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we investigate the benefit of our proposed approach by addressing the following questions:</p><p>? Q1. Does our proposed WGDN outperform baseline methods?</p><p>? Q2. Do all key components of WGDN contribute to the representation learning?</p><p>? Q3. Is WGDN more scalable than baseline methods?</p><p>? Q4. How do the hyperparameters impact our proposed model?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Datasets We conduct experiments on both node-level and graph-level representation learning tasks with benchmark datasets across different scales and domains, including Cora, CiteSeer, PubMed <ref type="bibr" target="#b27">[28]</ref>, Amazon Computers, Photo <ref type="bibr" target="#b28">[29]</ref>, Coauthor CS, Physics <ref type="bibr" target="#b28">[29]</ref>, and IMDB-B, IMDB-M, PROTEINS, COLLAB, MUTAG, DD, NCI1 in TUDataset <ref type="bibr" target="#b20">[21]</ref>. To avoid superficial statistical evaluation and overfitting to fixed data split, we leverage random split mechanism <ref type="bibr" target="#b4">[5]</ref>. Detailed statistics are in Table <ref type="table">4</ref> of Appendix E.1. Baselines We compare WGDN against representative models from five different categories: (1) traditional models including Node2Vec <ref type="bibr" target="#b6">[7]</ref>, Graph2Vec <ref type="bibr" target="#b21">[22]</ref>, DeepWalk <ref type="bibr" target="#b25">[26]</ref>, (2) graph kernel models including Weisfeiler-Lehman sub-tree kernel (WL) <ref type="bibr" target="#b29">[30]</ref>, deep graph kernel (DGK) <ref type="bibr" target="#b44">[45]</ref>, (3) predictive SSL models including GAE <ref type="bibr" target="#b12">[13]</ref>, GCN AE <ref type="bibr" target="#b49">[50]</ref>, GALA <ref type="bibr" target="#b23">[24]</ref>, GDN <ref type="bibr" target="#b16">[17]</ref>, (4) contrastive SSL models including DGI <ref type="bibr" target="#b35">[36]</ref>, MVGRL <ref type="bibr" target="#b7">[8]</ref>, GRACE <ref type="bibr" target="#b50">[51]</ref>, GMI <ref type="bibr" target="#b24">[25]</ref>, GCA <ref type="bibr" target="#b51">[52]</ref>, InfoGraph <ref type="bibr" target="#b32">[33]</ref>, GraphCL <ref type="bibr" target="#b47">[48]</ref>, JOAO <ref type="bibr" target="#b46">[47]</ref>, InfoGCL <ref type="bibr" target="#b42">[43]</ref>, and (5) semi-supervised models including GCN <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b34">[35]</ref> and GIN <ref type="bibr" target="#b43">[44]</ref>.</p><p>Evaluation protocol We closely follow the evaluation protocol in recent SSL researches. For node classification, the node embedding is fed into a logistics regression classifier <ref type="bibr" target="#b35">[36]</ref>. We run 50 trials with different seeds and report the mean classification accuracy with standard deviation. For graph classification, we feed the graph representation into a linear SVM, and report the mean 10-fold cross validation accuracy with standard deviation after 5 runs <ref type="bibr" target="#b42">[43]</ref>. Please refer to Appendix E.2 for further details.</p><p>Experiment settings We use the official implementations for all baselines in node classification and follow the suggested hyperparameter settings, whereas graph classification results are obtained from original papers if available. We develop three variants of our proposed methods as WGDN N with GCN kernel g c (? i ) = 1 -? i , WGDN H with heat diffusion kernel g c (? i ) = e -t?i and WGDN P with personalized pagerank kernel</p><formula xml:id="formula_27">g c (? i ) = ? 1-(1-?)(1-?i) .</formula><p>We use diffusion time t = 1 and teleport probability ? = 0.2. For wiener kernel approximation, the polynomial order K of WGDN N is 9 while others are 2. All models are initialized using Glorot initialization <ref type="bibr" target="#b5">[6]</ref> and optimized by Adam optimizer <ref type="bibr" target="#b11">[12]</ref>. For node classification, early stopping is also employed with a patience of 20. Further details of model configurations can be found in Appendix E.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance comparison (Q1)</head><p>The node classification performance are reported in Table <ref type="table" target="#tab_1">1</ref>. We find that all variants of WGDN outperform the predictive SSL methods by a large margin over all datasets. The significant improvement verifies the effectiveness of our training scheme when applied to predictive SSL models. In addition, WGDN performs competitively with contrastive SSL methods, achieving state-of-the-art performances in 5 out of 7 datasets. For instance, our model WGDN is able to improve by a margin up to 4.7% on accuracy over the most outstanding contrastive method GRACE on Computers. Moreover, when compared to semi-supervised models, WGDN outperforms GCN in almost all datasets except Physics.  Table <ref type="table" target="#tab_2">2</ref> lists the graph classification performance across various methods. We observe that our approach outperforms all SSL baselines in 6 out of 7 datasets. In the meantime, WGDN achieves greater results over the best kernel methods. Even when compared to semi-supervised models, our model achieves the best results in 2 out of 7 datasets and the gaps for the rest are minor.</p><p>In summary, regardless of the encoder types, our model consistently achieves comparable performance with the cutting-edge SSL and semi-supervised methods across node-level and graph-level tasks, which demonstrates the effectiveness and flexibility of WGDN under SSL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of key components (Q2)</head><p>To validate the benefit of introducing latent augmentation and graph wiener decoder, we conduct ablation studies on three datasets in node classification that exhibit distinct characteristics (e.g., citation, co-purchase and co-author). For clarity, WGDN-A and WGDN-W are denoted as the models removing augmentation or substituting wiener decoder with inverse decoder. WGDN-AW is the plain model without both components. Considering the trivial inverse of GCN is intractable, we use GALA <ref type="bibr" target="#b23">[24]</ref> instead.</p><p>From Figure <ref type="figure" target="#fig_3">3</ref>, we make several observations. (1) WGDN-W consistently underperform WGDN-AW. This observation validates that deterministic inverse decoder fails to retrieve useful information from augmented low-dimensional embedding, which is consistent with our theoretical analysis in Section 4.1.</p><p>(2) Compared with WGDN-AW, WGDN-A improves model performance across various encoder types and datasets, which suggests that graph wiener decoder may provide more insights in representation learning.</p><p>(3) The performance of WGDN is significantly higher than other counterparts. For instance, WGDN has a relative improvement up to 6% over WGDN-AW on PubMed regardless of the encoder types. It can be concluded that the combination of augmentation and wiener decoder allows the model to generate more semantic embedding from the augmented latent space.  Note that scalability is a big concern in most contrastive SSL methods. The complex data augmentation schemes tend to cause scaling-up issues, while WGDN can be efficiently implemented as discussed in Section 4.2. We empirically evaluate the computational cost in GPU over four large datasets. To have fair comparisons, the embedding size of all models is set to 512. It is evident from Table <ref type="table" target="#tab_3">3</ref> that the memory requirement of WGDN is significantly reduced up to 8x without sacrificing model performance. In particular, the scalability of WGDN is more prominent on large-scale dataset such as Physics. Considering that memory is usually the bottleneck in graph-based applications, WGDN demonstrates a practical advantage over prior methods when limited resources are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scalability analysis (Q3)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyperparameter analysis (Q4)</head><p>Augmentation size ? It is expected that introducing adequate augmentation enriches the diversity of samples in the latent space, which contributes to learning more expressive representations. To validate this, we conduct experiments varying ? from 0 to 2. Figure <ref type="figure" target="#fig_4">4</ref> shows that the classification accuracy generally reaches the peak and drops gradually when the augmentation size ? increases, which aligns with our intuition. However, it is worth noting that augmenting latent space may lead to performance degradation for certain circumstances such as GCN encoder on CS. Overall, the trend verifies the effectiveness of introducing latent augmentation in representation learning.</p><p>Convolution filter g c Table <ref type="table" target="#tab_1">1</ref> and 2 shows the influence of different convolution filters. It is observed that diffusion based WGDN outperforms its trivial version with GCN filter across node and graph applications. In addition, we also find that training loss of diffusion models is significantly smaller. Both phenomena verify that graph diffusion can provide both meaningful aggregation and powerful reconstruction to improve representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>In this paper, we propose Wiener Graph Deconvolutional Network (WGDN), a predictive selfsupervised learning framework for graph-structured data. By introducing simple latent augmentations, our model can efficiently learn semantic representations from the augmented latent space along with graph wiener decoder. We provide a theoretical analysis for the superiority of reconstruction ability and efficacy of graph wiener filter. Extensive experimental results on various datasets demonstrate that our proposed WGDN achieves competitive performance over unsupervised counterparts, and even outperforms several semi-supervised architectures. Our work shows the great potentials of latent data augmentations and predictive SSL in representation learning, and more in-depth explorations will be conducted in future work.</p><p>A Proof of Proposition 4.1</p><p>Proof. By substitution, xinv = x + Ug d (?)U T . Note that MSE is reduced to</p><formula xml:id="formula_28">E x -xinv 2 2 = E Ug d (?)U T 2 2 = N i=1 ? 2 g 2 c (? i ) . (<label>16</label></formula><formula xml:id="formula_29">)</formula><p>Regarding the condition g c : [0, 2] ? [-1, 1], we take GCN where g c (? i ) = 1-? i as a representative example. By substitution, we have</p><formula xml:id="formula_30">E x -xinv 2 2 = N i=1 ? 2 (1 -? i ) 2 ,<label>(17)</label></formula><p>and</p><formula xml:id="formula_31">1 (1-?i) 2 ? ? when ? i ? 1.</formula><p>B Proof of Proposition 4.2</p><p>Proof. By the definition of g w (? i )</p><formula xml:id="formula_32">E x -xw 2 2 = N i=1 ? 2 (g 2 c (? i ) + ? 2 /E[x * 2 i ]) (g 2 c (? i ) + ? 2 /E[x * 2 i ]) 2 = N i=1 ? 2 g 2 c (? i ) + ? 2 /E[x * 2 i ] ? N i=1 ? 2 g 2 c (? i ) .<label>(18)</label></formula><p>Plugging in the inverse filter, we can obtain</p><formula xml:id="formula_33">N i=1 VAR[x inv,i ] = Tr(COV[x inv ]) = Tr(COV[Ux * ] + COV[Ug d (?) * ]) = N i=1 (VAR[x * i ] + ? 2 g 2 c (? i ) ),<label>(19)</label></formula><p>where Tr represents the matrix trace. Similarly, variance of xw is convoluted by g w (? i )</p><formula xml:id="formula_34">N i=1 VAR[x w,i ] = Tr(COV[x w ]) = N i=1 [ g 2 c (? i ) g 2 c (? i ) + ? 2 /E[x * 2 i ] ] 2 (VAR[x * i ] + ? 2 g 2 c (? i ) ) ? N i=1 (VAR[x * i ] + ? 2 g 2 c (? i ) ).<label>(20)</label></formula><p>C Proof of Proposition 4.3</p><p>Proof. Similar to Appendix B, variance of xw,? is reduced to</p><formula xml:id="formula_35">N i=1 VAR(x w,?,i ) = Tr(COV(x w,? )) = N i=1 [ g 2 c (? i ) g 2 c (? i ) + ? 2 /x * 2 ? ] 2 (VAR[x * i ] + ? 2 g 2 c (? i ) ) ? N i=1 (VAR[x * i ] + ? 2 g 2 c (? i ) ).<label>(21)</label></formula><p>For the specific spectrum ? i where E[x * 2 i ] ? x * 2 ? holds, the spectral reconstruction error satisfies</p><formula xml:id="formula_36">S(? i , x * i , ?, g c , ?w,? ) = 1 (g 2 c (? i ) + ? 2 /x * 2 ? ) 2 [? 2 (g 2 c (? i ) + ? 2 x * 2 ? ? E[x * 2 i ] x * 2 ? )] ? 1 (g 2 c (? i ) + ? 2 /x * 2 ? ) 2 [? 2 (g 2 c (? i ) + ? 2 x * 2 ? )] = ? 2 g 2 c (? i ) + ? 2 /x * 2 ? ? ? 2 g 2 c (? i ) = S(? i , x * i , ?, g c , g -1 c ).<label>(22)</label></formula><p>Note that the second derivative of spectral reconstruction error S(? i , x * i , ?, g c , g d ) with respect to</p><formula xml:id="formula_37">g d (? i ) is ? 2 ?g 2 d (? i ) S(? i , x * i , ?, g c , g d ) = 2(g 2 c (? i )E[x * 2 i ] + ? 2 ) ? 0,<label>(23)</label></formula><p>thus, S(? i , x * i , ?, g c , g d ) is a convex function. By Eq. 6, g w (? i ) is the solution for global minimum. By convexity, for any filter g d (? i ), the value of S(? i , x * i , ?, g c , g d ) is greater when distance to global minimizer |g d (? i ) -g w (? i )| is larger. Considering ?w,? (? i ), it can be reduced to</p><formula xml:id="formula_38">|? w,? (? i ) -g w (? i )| = |g c (? i )| ? | 1 g 2 c (? i ) + ? 2 /x * 2 ? - 1 g 2 c (? i ) + ? 2 /E[x * 2 i ] |.<label>(24)</label></formula><p>Given the condition that x * 2 i ? x * 2 ?1 ? x * 2 ?2 , we can conclude that</p><formula xml:id="formula_39">|? w,?1 (? i ) -g w (? i )| ? |? w,?2 (? i ) -g w (? i )|.<label>(25)</label></formula><p>Therefore, S(?</p><formula xml:id="formula_40">i , x * i , ?, g c , ?w,?1 ) ? S(? i , x * i , ?, g c , ?w,?2 ) ? S(? i , x * i , ?, g c , g -1 c ) holds.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Model Architecture</head><p>RSNR estimation Let ?(m) denotes the input of m-th layer decoder, the average spectral power</p><formula xml:id="formula_41">x * 2 ?i in D (m)</formula><p>?i is estimated by</p><formula xml:id="formula_42">x * 2 ?i = ? i N D ?(m) 2 F + ?(m) - 1 N 1 ?(m) 2 F ,<label>(26)</label></formula><p>where 1 ? R N ?D is the all ones matrix and D is the size of hidden space. The augmentation variance ? 2 is estimated by considering its neighborhood as ).</p><formula xml:id="formula_43">? 2 = 1 N D ?(m) -A ?(m) 2 F (<label>27</label></formula><p>(</p><formula xml:id="formula_44">)<label>30</label></formula><p>For graph classification, we apply batch normalization <ref type="bibr" target="#b9">[10]</ref> right before activation function for all layers except the final prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Experimental Setup E.1 Datasets Splitting</head><p>We closely follow the random split mechanism suggested in <ref type="bibr" target="#b4">[5]</ref>. To conduct comprehensive evaluation on node classification, each dataset is divided into a development set and a test set with fixed seed. Specifically, 1500 nodes are randomly sampled as development set for all dataset except Coauthor Physics, in which 5000 nodes are sampled. For each run, the development set is randomly split into a training set which includes 20 nodes per class and a validation set with the remaining nodes.</p><p>To ensure model reproducibility, random seeds are drawn once in advance and fixed across runs to facilitate comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Evaluation Protocol</head><p>For unsupervised methods, all the resulted embedding from well-trained models are frozen. For node classification tasks, we use Glorot initialization <ref type="bibr" target="#b5">[6]</ref> to initialize model parameters and all downstream models are trained for 300 epochs by Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with a learning rate 0.01. For semi-supervised methods, to conduct fair comparisons, we follow the same settings as unsupervised methods and employ 2-layer encoders with embedding size 512 for both GCN <ref type="bibr" target="#b13">[14]</ref> and GAT <ref type="bibr" target="#b34">[35]</ref>. Specifically, the first layer of GAT consists of 2 heads and the second layer has 1 output head. We run 50 trials and keep the model with the highest performance in validation set as final. For graph classification tasks, linear SVM is fine-tuned with grid search on C parameter from {10 -3 , 10 -2 , ..., 1, 10}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Hyperparameter Specifications</head><p>By default, wiener graph decoder is implemented with multiple channels with q = 3 and ? = [0.1, 1, 10]. Otherwise, single channel is used with ? = 1. The specific hyperparameter configurations are illustrated in Table <ref type="table">5</ref> and<ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Computational Hardware</head><p>We use the machine with the following configurations for all model training and evaluation.</p><p>? OS: Ubuntu 20.04.1 LTS ? CPU: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz ? GPU: NVIDIA Tesla V100, 32GB</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Augmentations on original graph may uncontrollably change the semantics causing wrong classification, whereas augmentations on latent space are continuous and controllable.</figDesc><graphic url="image-1.png" coords="1,306.00,482.23,198.00,119.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The autoencoder framework for graph SSL with proposed WGDN model. Gaussian noise is introduced adaptively as latent augmentations to enrich the diversity of training data. The proposed WGDN facilitates representation learning with graph wiener filter by recovering information from the augmented latent space.</figDesc><graphic url="image-2.png" coords="3,108.00,72.00,396.01,163.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where {t j } K+1 j=0 are interpolation points within [a, b]. Lemma 4.1. If interpolation points {t j } K+1 j=0 are Chebyshev nodes, the interpolation error |?(t)p K (t)| of Remez polynomial p K (t) is minimized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Node classification accuracy of models with and without key components (wiener decoder/latent augmentation). Complete training scheme consistently boosts model performance across different encoder types and datasets.</figDesc><graphic url="image-3.png" coords="8,109.98,248.68,130.69,102.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Node classification accuracy with varied augmentation size ?.</figDesc><graphic url="image-6.png" coords="9,109.98,72.00,130.69,102.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>)=</head><label></label><figDesc>Skip connection To learn more expressive representations, skip connection is considered as it transmits aggregated information to decoder for better reconstruction. If skip connection is implemented, we let ?(m) d = ?(m) for clarity. For m = 1, .., M -1, we augment the output of the m-layer encoder, denoted as ?(m) e , by ?(m) e = H (m) + ?E (m) , VAR[H (m) ] and ? is same hyperparameter in Eq. 13. Both augmented representations are fed into the decoder as Z s = {e, d} represents the source. The final representation of m-layer decoder is obtained by averaging the intermediate embeddings, ?(m-1) = AVG( ?(m-1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy of all compared methods. The best and runner up models in unsupervised learning are highlighted in boldface and underlined. * denotes Out-of-Memory on a Tesla V100 32G GPU.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Computers</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>Raw Feat.</cell><cell cols="7">47.5 ? 1.9 48.1 ? 2.0 65.8 ? 2.6 64.0 ? 2.5 76.1 ? 1.6 87.3 ? 0.7 88.8 ? 0.9</cell></row><row><cell>Node2Vec</cell><cell cols="7">68.3 ? 1.6 45.2 ? 1.5 64.6 ? 2.7 74.0 ? 1.6 83.8 ? 1.3 77.0 ? 0.9 79.9 ? 1.5</cell></row><row><cell>DeepWalk</cell><cell cols="7">69.7 ? 1.8 42.9 ? 1.8 64.6 ? 2.9 77.2 ? 1.3 85.4 ? 1.2 75.3 ? 1.1 80.7 ? 2.0</cell></row><row><cell cols="8">DeepWalk + Feat. 72.4 ? 1.6 49.5 ? 1.9 66.1 ? 2.8 78.9 ? 1.3 88.0 ? 1.2 88.6 ? 0.6 90.0 ? 0.8</cell></row><row><cell>GAE</cell><cell cols="7">74.2 ? 1.3 49.3 ? 1.9 77.0 ? 2.2 79.2 ? 1.6 88.9 ? 1.0 84.5 ? 1.3 81.4 ? 2.7</cell></row><row><cell>GCN AE</cell><cell cols="7">68.1 ? 0.6 58.3 ? 1.7 73.6 ? 3.0 66.9 ? 2.2 82.9 ? 1.5 87.6 ? 0.8 83.4 ? 2.0</cell></row><row><cell>GALA</cell><cell cols="7">79.8 ? 1.3 65.8 ? 1.2 72.8 ? 3.0 80.2 ? 1.5 88.1 ? 0.9 90.0 ? 0.7 91.3 ? 1.0</cell></row><row><cell>GDN</cell><cell cols="7">79.7 ? 1.2 66.2 ? 1.5 75.0 ? 2.8 81.3 ? 1.5 89.6 ? 0.9 90.1 ? 0.6 92.1 ? 0.9</cell></row><row><cell>DGI</cell><cell cols="7">80.0 ? 0.9 70.9 ? 1.2 77.4 ? 2.7 63.6 ? 3.3 77.3 ? 2.3 90.5 ? 0.5 92.6 ? 1.0</cell></row><row><cell>MVGRL</cell><cell cols="7">81.2 ? 1.0 69.6 ? 1.6 79.0 ? 1.6 76.3 ? 1.7 86.4 ? 1.4 88.1 ? 0.8 90.5 ? 1.4</cell></row><row><cell>GRACE</cell><cell cols="7">80.1 ? 1.0 70.2 ? 1.3 80.2 ? 1.6 77.9 ? 2.2 89.3 ? 1.2 85.2 ? 0.8 91.8 ? 1.1</cell></row><row><cell>GMI</cell><cell cols="6">78.8 ? 1.1 70.0 ? 1.3 77.6 ? 2.1 65.2 ? 3.1 85.5 ? 1.6 90.3 ? 0.5</cell><cell>*</cell></row><row><cell>GCA</cell><cell cols="7">75.4 ? 1.7 67.9 ? 1.4 77.9 ? 3.0 75.0 ? 1.4 76.6 ? 2.7 89.0 ? 0.7 92.3 ? 1.2</cell></row><row><cell>WGDN N</cell><cell cols="7">80.1 ? 1.2 67.4 ? 1.0 79.1 ? 2.5 81.9 ? 1.6 88.9 ? 0.9 90.4 ? 0.6 92.1 ? 0.9</cell></row><row><cell>WGDN H</cell><cell cols="7">81.1 ? 1.3 69.9 ? 1.2 79.7 ? 2.2 82.6 ? 1.7 90.2 ? 1.2 90.9 ? 0.7 93.1 ? 0.8</cell></row><row><cell>WGDN P</cell><cell cols="7">81.7 ? 0.9 69.7 ? 1.2 79.3 ? 2.0 82.3 ? 1.6 90.3 ? 0.9 90.5 ? 0.6 93.3 ? 0.6</cell></row><row><cell>GCN</cell><cell cols="7">79.3 ? 1.5 63.6 ? 1.8 76.1 ? 1.9 82.2 ? 1.4 89.6 ? 0.8 90.2 ? 0.5 93.5 ? 0.4</cell></row><row><cell>GAT</cell><cell cols="7">78.2 ? 1.6 65.6 ? 1.5 75.7 ? 2.0 80.6 ? 1.8 88.8 ? 1.3 82.5 ? 1.3 91.4 ? 1.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Graph classification accuracy of all compared methods. The compared results are from the previous papers, and * indicates result is not reported. InfoGraph 73.03 ? 0.87 49.69 ? 0.53 74.44 ? 0.31 70.65 ? 1.13 89.01 ? 1.13 72.85 ? 1.78 76.20 ? 1.06 GraphCL 71.14 ? 0.44 48.58 ? 0.67 74.39 ? 0.45 71.36 ? 1.15 86.80 ? 1.34 78.62 ? 0.40 77.87 ? 0.41 JOAO 70.21 ? 3.08 49.20 ? 0.77 74.55 ? 0.41 69.50 ? 0.36 87.35 ? 1.02 77.32 ? 0.54 78.07 ? 0.47 InfoGCL 75.10 ? 0.90 51.40 ? 0.80 * 80.00 ? 1.30 91.20 ? 1.30 * 80.20 ? 0.60 WGDN N 75.56 ? 0.32 51.57 ? 0.37 75.32 ? 0.26 80.96 ? 0.32 88.30 ? 1.33 79.46 ? 0.41 79.18 ? 0.32 WGDN H 76.08 ? 0.50 51.80 ? 0.39 76.03 ? 0.40 81.26 ? 0.23 88.72 ? 1.02 79.46 ? 0.11 79.72 ? 0.39 WGDN P 75.72 ? 0.20 51.80 ? 0.55 75.94 ? 0.66 81.50 ? 0.21 89.04 ? 0.21 79.00 ? 0.31 80.31 ? 0.25</figDesc><table><row><cell>Model</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>PROTEINS</cell><cell>COLLAB</cell><cell>MUTAG</cell><cell>DD</cell><cell>NCI1</cell></row><row><cell>WL</cell><cell cols="3">72.30 ? 3.44 46.95 ? 0.46 72.92 ? 0.56</cell><cell>*</cell><cell>80.72 ? 3.00</cell><cell>*</cell><cell>80.01 ? 0.50</cell></row><row><cell>DGK</cell><cell cols="3">66.96 ? 0.56 44.55 ? 0.52 73.30 ? 0.82</cell><cell>*</cell><cell>87.44 ? 2.72</cell><cell>*</cell><cell>80.31 ? 0.46</cell></row><row><cell cols="4">Graph2Vec 71.10 ? 0.54 50.44 ? 0.87 73.30 ? 2.05</cell><cell>*</cell><cell>83.15 ? 9.25</cell><cell>*</cell><cell>73.22 ? 1.81</cell></row><row><cell>MVGRL</cell><cell cols="2">74.20 ? 0.70 51.20 ? 0.50</cell><cell>*</cell><cell>*</cell><cell>89.70 ? 1.10</cell><cell>*</cell><cell>*</cell></row><row><cell>GCN</cell><cell>74.0 ? 3.4</cell><cell>51.9 ? 3.8</cell><cell>*</cell><cell>79.0 ? 1.8</cell><cell>85.6 ? 5.8</cell><cell>*</cell><cell>80.2 ? 2.0</cell></row><row><cell>GIN</cell><cell>75.1 ? 5.1</cell><cell>52.3 ? 2.8</cell><cell>76.2 ? 2.8</cell><cell>80.2 ? 1.9</cell><cell>89.4 ? 5.6</cell><cell>*</cell><cell>82.7 ? 1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Computational requirements on benchmark datasets. * denotes Out-of-Memory on a Tesla V100 32G GPU.</figDesc><table><row><cell>Model</cell><cell cols="2">PubMed Computers</cell><cell>CS</cell><cell>Physics</cell></row><row><cell cols="2">GRACE 12.57 GB</cell><cell>8.35 GB</cell><cell cols="2">12.92 GB 31.04 GB</cell></row><row><cell>GMI</cell><cell cols="3">22.81 GB 15.42 GB 23.61 GB</cell><cell>*</cell></row><row><cell>WGDN</cell><cell>2.72 GB</cell><cell>4.52 GB</cell><cell cols="2">6.84 GB 12.37 GB</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Richard L Burden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><forename type="middle">M</forename><surname>Douglas Faires</surname></persName>
		</author>
		<author>
			<persName><surname>Burden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical analysis. Cengage learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MODALS: Modality-agnostic automated data augmentation in the latent space</title>
		<author>
			<persName><forename type="first">Tsz-Him</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05538</idno>
		<title level="m">Dataset augmentation in feature space</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep wiener deconvolution: Wiener meets deep learning for image deblurring</title>
		<author>
			<persName><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent adversarial training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Learning and Reasoning with Graph-Structured Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A closer look at feature space data augmentation for few-shot intent classification</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadrien</forename><surname>Glaude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Lichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deconvolutional networks on graph data</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Specae: Spectral autoencoder for anomaly detection in attributed networks</title>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ping Jia, and Jane You. Data augmentation via latent space interpolation for image classification</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingsheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Site</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">Learning distributed representations of graphs</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Barycentric-remez algorithms for best polynomial approximation in the chebfun system</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Pachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lloyd</forename><surname>Trefethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Jiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stationary signal processing on graphs</title>
		<author>
			<persName><forename type="first">Nathana?l</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Relational Representation Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast non-blind deconvolution via regularized residual networks with long/short skip-connections</title>
		<author>
			<persName><forename type="first">Hyeongseok</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Disentangling adversarial robustness and generalization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep feature interpolation for image content changes</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep graph infomax. In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Augmentation-free graph contrastive learning</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04874</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Extrapolation, Interpolation, and Smoothing of Stationary Time Series</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Infogcl: Information-aware graph contrastive learning</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Enhancing geometric deep learning via graph filter deconvolution</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Segarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GlobalSIP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph deconvolutional networks</title>
		<author>
			<persName><forename type="first">Chun-Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Philip Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
