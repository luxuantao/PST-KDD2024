<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
							<email>shiyunsheng01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
							<email>huangzhengjie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
							<email>zhonghui03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
							<email>wangwenjin02@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural network (GNN) and label propagation algorithm (LPA) are both message passing algorithms, which have achieved superior performance in semi-supervised classification. GNN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no effective way to directly combine these two kinds of algorithms. To address this issue, we propose a novel Unified Message Passaging Model (UniMP) that can incorporate feature and label propagation at both training and inference time. First, UniMP adopts a Graph Transformer network, taking feature embedding and label embedding as input information for propagation. Second, to train the network without overfitting in self-loop input label information, UniMP introduces a masked label prediction strategy, in which some percentage of input label information are masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and is empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are various scenarios in the world, e.g., recommending related news, discovering new drugs, or predicting social relations, which can be described as graph structures. And many methods have been proposed to optimize these graphbased problems and achieved significant success in many related domains such as predicting nodes' properties <ref type="bibr">[Yang et al., 2016;</ref><ref type="bibr" target="#b3">Kipf and Welling, 2016]</ref>, relation linking <ref type="bibr">[Grover and Leskovec, 2016;</ref><ref type="bibr" target="#b0">Battaglia et al., 2018]</ref>, and graph classification <ref type="bibr" target="#b1">[Duvenaud et al., 2015;</ref><ref type="bibr" target="#b9">Niepert et al., 2016]</ref>.</p><p>In the task of semi-supervised node classification, we are required to learn with labeled examples and then make predictions for those unlabeled ones. To better classify the nodes' labels in the graph, based on the Laplacian smoothing assumption <ref type="bibr" target="#b4">[Li et al., 2018;</ref><ref type="bibr" target="#b12">Xu et al., 2018b]</ref>, the message passing models were proposed to aggregate the information from its connected neighbors in the graph, acquiring enough facts to produce a more robust prediction for unlabeled nodes. Generally, there are two main kinds of methods to implement message passing model, Graph Neural Networks (GNNs) <ref type="bibr" target="#b3">[Kipf and Welling, 2016;</ref><ref type="bibr">Hamilton et al., 2017;</ref><ref type="bibr" target="#b12">Xu et al., 2018b;</ref><ref type="bibr" target="#b7">Liao et al., 2019;</ref><ref type="bibr" target="#b11">Xu et al., 2018a]</ref> and Label Propagation Algorithms (LPAs) <ref type="bibr">[Zhu et al., 2003;</ref><ref type="bibr">Zhang and Lee, 2007;</ref><ref type="bibr">Wang and Zhang, 2007;</ref><ref type="bibr" target="#b3">Karasuyama and Mamitsuka, 2013;</ref><ref type="bibr">Gong et al., 2016;</ref><ref type="bibr">Liu et al., 2019]</ref>. GNNs combine graph structures by propagating and aggregating node features through several neural layers, getting predictions from feature propagation. While LPAs make predictions for unlabeled instances by label propagation iteratively.</p><p>Since GNN and LPA are based on the same assumption, making semi-supervised classifications by information propagation, there is an intuition that incorporating them together for boosting performance. Some superior studies have proposed their graph models based on it. For example, APPNP <ref type="bibr" target="#b3">[Klicpera et al., 2018]</ref> and TPN <ref type="bibr">[Liu et al., 2019]</ref> using GNN predict soft labels and then propagate them, and GCN-LPA <ref type="bibr">[Wang and Leskovec, 2019]</ref> uses LPA to regularize their GNN model. However, as shown in Table <ref type="table" target="#tab_0">1</ref>, aforementioned methods still can not directly incorporate GNN and LPA within a message passing model, propagating feature and label in both training and inference procedure.</p><p>In this work, we propose a Unified Message Passing model (UniMP) to address the aforementioned issue with two simple but effective ideas: (a) combing node features propagation with labels and (b) masked label prediction. Previous GNNbased methods only take node features as input with the partial observed node labels for supervised training. And they discard the observed labels during inference. UniMP utilizes both node features and labels in both training and inference stages. It uses the embedding technique to transform the partial node labels from one-hot to dense vector likes node fea-tures. And a multi-layer Graph Transformer network takes them as input to perform attentive information propagation between nodes. Therefore, each node can aggregate both features and labels information from its neighbors. Since we have taken the node label as input, using it for supervised training will cause the label leakage problem. The model will overfit in the self-loop input label while performing poor in inference. To address this issue, we propose a masked label prediction strategy, which randomly masks some training instances' label and then predicts them to overcome label leakage. This simple and effective training method is drawn the lesson from masked word prediction in <ref type="bibr">BERT [Devlin et al., 2018]</ref>, and simulates the procedure of transducing labels information from labeled to unlabeled examples in the graph.</p><p>We evaluate our UniMP model on three semi-supervised classification datasets in the Open Graph Benchmark (OGB), where our new methods achieve the new state-of-the-art results in all tasks, gaining 82.56% ACC in ogbn-products, 86.42% ROC-AUC in ogbn-proteins and 73.11% ACC in ogbn-arxiv. We also conduct ablation studies for our UniMP model, to evaluate the effectiveness of our unified method. Besides, we make the most thorough analysis of how the label propagation boosts our model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we briefly review the related work and along the way, introduce our notation. We denote a graph as G = (V, E), where V denotes the nodes in the graph with |V | = n and E denotes edges. The nodes are described by the feature matrix X ∈ R n×m , which usually are dense vectors with m dimension, and the target class matrix Y ∈ R n×c , with the number of classes c. The adjacency matrix A = [a i,j ] ∈ R n×n is used to describe graph G, and the diagonal degree matrix is denoted by D = diag(d 1 , d 2 , ..., d n ) , where d i = j a ij is the degree of node i. A normalized adjacency matrix is defined as D −1 A or D − 1 2 AD − 1 2 , and we adopt the first definition in this paper. Graph Neural Networks. In semi-supervised node classification, <ref type="bibr">GCN [Kipf and Welling, 2016]</ref> is one of the most classical models based on the Laplacian smoothing assumption. GCN transforms and propagates node features X across the graph by several layers, including linear layers and nonlinear activation to build the approximation of the mapping: X → Y . The feature propagation scheme of GCN in layer l is:</p><formula xml:id="formula_0">H (l+1) = σ(D −1 AH (l) W (l) ) Y = f out (H (L) ) (1)</formula><p>where the σ is an activation function, W (l) is the trainable weight in the l-th layer, and the H (l) is the l-th layer representations of nodes. H (0) is equal to node input features X.</p><p>Finally, a f out output layer is applied on the final representation to make prediction for Y . Label propagation algorithms. Traditional algorithms like Label Propagation Algorithm (LPA) only utilizes labels and relations between nodes to make prediction. LPA assumes the labels between connected nodes are similar and </p><formula xml:id="formula_1">X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; Ŷ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H / L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W 8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H / L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W 8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H / L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W 8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H / L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W 8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y</formula><p>u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4</p><formula xml:id="formula_2">= " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y</formula><p>u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4</p><formula xml:id="formula_3">= " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y</formula><p>u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 S e m q 4 j 4 + J n 9 a g k y g 5 e + g 1 J j </p><formula xml:id="formula_4">= " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t</formula><formula xml:id="formula_5">L 1 o = " &gt; A A A C M 3 i c b V D L S s N A F J 3 4 r P U V d e l m s A g V p C R S U F x V 3 L i s Y N p K E 8 J k O m m H T h 7 M T I Q S 8 y / + h L / g V r f i r r j 1 H 5 y 0 K d T W A w P n n n M v 9 8 7 x Y k a F N I x P b W V 1 b X 1 j s 7 R V 3 t 7 Z 3 d v X D w 5 b I k o 4 J h a O W M Q 7 H h K E 0 Z B Y k k p G O j E n K P A Y a X v D 2 9 x v P x E u a B Q + y F F M n A D 1 Q + p T j K S</formula><formula xml:id="formula_6">t S A Q W a r j 6 2 e x F O A h J K z J A Q X d O I p Z M i L i l m J C v b i S A x w k P U J 1 1 F Q x Q Q 4 a S T P 2 b w V C k 9 6 E d c v V D C i T o / k a J A i F H g q c 7 8 R L H o 5 e K / n h c s b J b + l Z P S M E 4 k C f F 0 s Z 8 w K C O Y B w h 7 l B M s 2 U g R h D l V t 0 M 8 Q B x h q W I u q 1 D M x Q i W S e u i Z h o 1 8 7 5 e a d S L e E r g G J y A K j D B J W i A O 9 A E F s D g B b y B d / C h v W p f</formula><p>2 l j 7 n r a u a M X M E f g D 7 e c X 5 Z O r 3 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 S e m q 4 j 4 + J n 9 a g k y g 5 e + g 1 J j </p><formula xml:id="formula_7">L 1 o = " &gt; A A A C M 3 i c b V D L S s N A F J 3 4 r P U V d e l m s A g V p C R S U F x V 3 L i s Y N p K E 8 J k O m m H T h 7 M T I Q S 8 y / + h L / g V r f i r r j 1 H 5 y 0 K d T W A w P n n n M v 9 8 7 x Y k a F N I x P b W V 1 b X 1 j s 7 R V 3 t 7 Z 3 d v X D w 5 b I k o 4 J h a O W M Q 7 H h K E 0 Z B Y k k p G O j E n K P A Y a X v D 2 9 x v P x E u a B Q + y F F M n A D 1 Q + p T j K S</formula><formula xml:id="formula_8">t S A Q W a r j 6 2 e x F O A h J K z J A Q X d O I p Z M i L i l m J C v b i S A x w k P U J 1 1 F Q x Q Q 4 a S T P 2 b w V C k 9 6 E d c v V D C i T o / k a J A i F H g q c 7 8 R L H o 5 e K / n h c s b J b + l Z P S M E 4 k C f F 0 s Z 8 w K C O Y B w h 7 l B M s 2 U g R h D l V t 0 M 8 Q B x h q W I u q 1 D M x Q i W S e u i Z h o 1 8 7 5 e a d S L e E r g G J y A K j D B J W i A O 9 A E F s D g B b y B d / C h v W p f</formula><p>2 l j 7 n r a u a M X M E f g D 7 e c X 5 Z O r 3 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 S e m q 4 j 4 + J n 9 a g k y g 5 e + g 1 J j </p><formula xml:id="formula_9">L 1 o = " &gt; A A A C M 3 i c b V D L S s N A F J 3 4 r P U V d e l m s A g V p C R S U F x V 3 L i s Y N p K E 8 J k O m m H T h 7 M T I Q S 8 y / + h L / g V r f i r r j 1 H 5 y 0 K d T W A w P n n n M v 9 8 7 x Y k a F N I x P b W V 1 b X 1 j s 7 R V 3 t 7 Z 3 d v X D w 5 b I k o 4 J h a O W M Q 7 H h K E 0 Z B Y k k p G O j E n K P A Y a X v D 2 9 x v P x E u a B Q + y F F M n A D 1 Q + p T j K S</formula><formula xml:id="formula_10">t S A Q W a r j 6 2 e x F O A h J K z J A Q X d O I p Z M i L i l m J C v b i S A x w k P U J 1 1 F Q x Q Q 4 a S T P 2 b w V C k 9 6 E d c v V D C i T o / k a J A i F H g q c 7 8 R L H o 5 e K / n h c s b J b + l Z P S M E 4 k C f F 0 s Z 8 w K C O Y B w h 7 l B M s 2 U g R h D l V t 0 M 8 Q B x h q W I u q 1 D M x Q i W S e u i Z h o 1 8 7 5 e a d S L e E r g G J y A K j D B J W i A O 9 A E F s D g B b y B d / C h v W p f</formula><p>2 l j 7 n r a u a M X M E f g D 7 e c X 5 Z O r 3 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 S e m q 4 j 4 + J n 9 a g k y g 5 e + g 1 J j  propagates the labels iteratively across the graph. Given an initial label matrix Ŷ (0) , which consists of one-hot label indicator vectors ŷ0 i for the labeled nodes or zeros vectors for the unlabeled. A simple iteration equation of LPA is formulated as following:</p><formula xml:id="formula_11">L 1 o = " &gt; A A A C M 3 i c b V D L S s N A F J 3 4 r P U V d e l m s A g V p C R S U F x V 3 L i s Y N p K E 8 J k O m m H T h 7 M T I Q S 8 y / + h L / g V r f i r r j 1 H 5 y 0 K d T W A w P n n n M v 9 8 7 x Y k a F N I x P b W V 1 b X 1 j s 7 R V 3 t 7 Z 3 d v X D w 5 b I k o 4 J h a O W M Q 7 H h K E 0 Z B Y k k p G O j E n K P A Y a X v D 2 9 x v P x E u a B Q + y F F M n A D 1 Q + p T j K S</formula><formula xml:id="formula_12">t S A Q W a r j 6 2 e x F O A h J K z J A Q X d O I p Z M i L i l m J C v b i S A x w k P U J 1 1 F Q x Q Q 4 a S T P 2 b w V C k 9 6 E d c v V D C i T o / k a J A i F H g q c 7 8 R L H o 5 e K / n h c s b J b + l Z P S M E 4 k C f F 0 s Z 8 w K C O Y B w h 7 l B M s 2 U g R h D l V t 0 M 8 Q B x h q W I u q 1 D M x Q i W S e u i Z</formula><formula xml:id="formula_13">Ŷ (l+1) = D −1 A Ŷ (l)<label>(2)</label></formula><p>Labels are propagated from each other nodes through a normalized adjacency matrix D −1 A.</p><p>Combining GNN and LPA. Recently, there is a trend to combine GNN and LPA in semi-classification tasks in the community. <ref type="bibr">APPNP [Klicpera et al., 2018]</ref> and TPN <ref type="bibr">[Liu et al., 2019]</ref> propose to use GCN to predict soft labels and then propagate them with Personalized Pagerank. However, these works still only considered the partial node labels as the supervision training signal. GCN-LPA is most relevant to our work, as they also take the partial node labels as input. However, they combine the GNN and LPA in a more indirect way, only using the LPA in training to regularize the weight edges of their GAT model. While our UniMP directly combines GNN and LPA within a network, propagates the node features and labels in both training and predicting. Moreover, unlike GCN-LPA whose regularization strategy can only be used in those GNNs with trainable weight edge such as GAT <ref type="bibr" target="#b10">[Veličković et al., 2017]</ref>, <ref type="bibr">GAAN [Zhang et al., 2018]</ref>, our training strategy can be easily extended in kinds of GNNs such as GCN and GAT to further improve their performance. We will describe our approach more specifically in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unified Message Passing Model</head><p>As shown in Figure <ref type="figure" target="#fig_4">1</ref>, given the node feature X and partial observed labels Ŷ , we employ a Graph Transformer, jointly using label embedding to combine the aforementioned feature and label propagation together, constructing our UniMP model. Moreover, a masked label prediction strategy is introduced to train our model to prevent label leakage problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Transformer</head><p>Since Transformer <ref type="bibr">[Vaswani et al., 2017;</ref><ref type="bibr" target="#b1">Devlin et al., 2018]</ref> has been proved being powerful in NLP, we adopt its vanilla multi-head attention into graph learning with taking into account the case of edge features. Specifically, given node features</p><formula xml:id="formula_14">H (l) = {h (l) 1 , h (l) 2 , ..., h<label>(l)</label></formula><p>n }, we calculate multi-head attention for each edge from j to i as following:</p><formula xml:id="formula_15">q (l) c,i = W (l) c,q h (l) i + b (l) c,q k (l) c,j = W (l) c,k h (l) j + b (l) c,k e c,ij = W c,e e ij + b c,e α (l) c,ij = q (l) c,i , k (l) c,j + e c,ij u∈N (i) q (l) c,i , k (l) c,u + e c,iu<label>(3)</label></formula><p>where q, k = exp( q T k √ d ) is exponential scale dot-product function and d is the hidden size of each head. For the c-th head attention, we firstly transform the source feature h (l) i and distant feature h (l) j into query vector q (l) c,i ∈ R d and key vector k</p><formula xml:id="formula_16">(l) c,j ∈ R d respectively using different trainable parameters W (l) c,q , W (l) c,k , b (l) c,q , b (l)</formula><p>c,k . The provided edge features e ij will be encoded and added into key vector as additional information for each layer.</p><p>After getting the graph multi-head attention, we make a message aggregation from the distant j to the source i:</p><formula xml:id="formula_17">v (l) c,j = W (l) c,v h (l) j + b (l) c,v ĥ(l+1) i = C c=1 j∈N (i) α (l) c,ij (v (l) c,j + e c,ij )<label>(4)</label></formula><p>where the is the concatenation operation for C head attention. Comparing with the Equation 1, multi-head attention matrix replaces the original normalized adjacency matrix as transition matrix for message passing. The distant feature h j is transformed to v c,j ∈ R d for weighted sum.</p><p>In addition, inspired by <ref type="bibr">Li [2019]</ref> and Chen [2020], we propose to use a gated residual connection between layers as shown in Equation 5 to prevent our model from oversmoothing.</p><formula xml:id="formula_18">r (l) i = W (l) r h (l) i + b (l) r β (l) i = sigmoid(W (l) g [ ĥ(l+1) i ; r (l) i ; ĥ(l+1) i − r (l) i ]) h (l+1) i = ReLU(LayerNorm((1 − β (l) i ) ĥ(l+1) i + β (l) i r (l) i ))<label>(5)</label></formula><p>Specially, similar to GAT, if we apply the Graph Transformer on the last output layer, we will employ averaging for multi-head output and remove the non-linear transformation as following:</p><formula xml:id="formula_19">ĥ(l+1) i = 1 C C c=1 j∈N (i) α (l) c,ij (v (l) c,j + e (l) c,ij ) h (l+1) i = (1 − β (l) i ) ĥ(l+1) i + β (l) i r (l) i (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Embedding and Propagation</head><p>We propose to embed the partially observed labels into the same space as node features: Ŷ ∈ R n×c → Ŷd ∈ R n×m , which consist of the label embedding vector for labeled nodes and zeros vectors for the unlabeled. And then, we combine the label propagation into Graph Transformer by simply adding the node features and labels vectors together as propagation information (H 0 = X + Ŷd ) ∈ R n×m . We can prove that by mapping partially-labeled Ŷ and node features X into the same space and adding them up, our model is unifying both label propagation and feature propagation within a shared message passing framework. Let's take Ŷd = Ŷ W d and A * to be normalized adjacency matrix D −1 A or the attention matrix from our Graph Transformer likes Equation <ref type="formula" target="#formula_15">3</ref>.</p><p>Then we can find that:</p><formula xml:id="formula_20">H (0) = X + Ŷ W d H (l+1) = σ(((1 − β)A * + βI)H (l) W (l) ) (7)</formula><p>where β can be the gated function like Equation <ref type="formula" target="#formula_18">5</ref>or a predefined hyper-parameters like APPNP <ref type="bibr" target="#b3">[Klicpera et al., 2018]</ref>.</p><p>For simplification, we let σ function as identity function, then we can get:</p><formula xml:id="formula_21">H (l) =((1 − β)A * + βI) l (X + Ŷ W d )W (1) W (2) . . . W (l) =((1 − β)A * + βI) l XW + ((1 − β)A * + βI) l Ŷ W d W<label>(8)</label></formula><p>where W = W (1) W (2) . . . W (l) . Then we can find that our model can be approximately decomposed into feature propagation ((1 − β)A * + βI) l XW and label propagation</p><formula xml:id="formula_22">((1 − β)A * + βI) l Ŷ W d W .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Masked Label Prediction</head><p>Previous works on GNNs seldom consider using the partially observed labels Ŷ in both training and inference stages. They only take those labels information as ground truth target to supervised train their model's parameters θ with given X and A:</p><formula xml:id="formula_23">arg max θ log p θ ( Ŷ |X, A) = V i=1 log p θ (ŷ i |X, A) (9)</formula><p>where V represents the partial nodes with labels. However, our UniMP model propagates node features and labels to make prediction: p(y|X, Ŷ , A). Simply using above objective for our model will make the label leakage in the training stage, causing poor performance in inference. Learning from BERT, which masks input words and makes predictions for them to pretrain their model (masked word prediction), we propose a masked label prediction strategy to train our model.</p><p>During training, at each step, we corrupt the Ŷ into Ỹ by randomly masking a portion of node labels to zeros and keep the others remain, which is controlled by a hyper-parameter called label rate. Let those masked labels be Ȳ , our objective function is to predict Ȳ with given X, Ỹ and A:</p><formula xml:id="formula_24">arg max θ log p θ ( Ȳ |X, Ỹ , A) = V i=1 log p θ (ȳ i |X, Ỹ , A)<label>(10</label></formula><p>) where V represents those nodes with masked labels. In this way, we can train our model without the leakage of self-loop labels information. And during inference, we will employ all Ŷ as input labels to predict the remaining unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We propose a Unified Message Passing Model (UniMP) for semi-supervised node classification, which incorporates the feature and label propagation jointly by a Graph Transformer and employs a masked label prediction strategy to optimize it. We conduct the experiments on the Node Property Prediction of Open Graph Benchmark (OGBN), which includes several various challenging and large-scale datasets for semisupervised classification, split in the procedure that closely matches the real-world application <ref type="bibr" target="#b2">[Hu et al., 2020]</ref>. To verify our models effectiveness, we compare our model with others state-of-the-art (SOTA) models in ogbn-products, ogbnproteins and ogbn-arxiv three OGBN datasets. We also provide more experiments and comprehensive ablation studies to show our motivation more intuitively, and how LPA improves our model to achieve better results.  Datasets. Most of the frequently-used graph datasets are extremely small compared to graphs found in real applications. And the performance of GNNs on these datasets is often unstable due to several issues including their small-scale nature, non-negligible duplication or leakage rates, unrealistic data splits <ref type="bibr" target="#b2">[Hu et al., 2020]</ref>. Consequently, we conduct our experiments on the recently released datasets of Open Graph Benchmark (OGB) <ref type="bibr" target="#b2">[Hu et al., 2020]</ref>, which overcome the main drawbacks of commonly used datasets and thus are much more realistic and challenging. OGB datasets cover a variety of real-world applications and span several important domains ranging from social and information networks to biological networks, molecular graphs, and knowledge graphs. They also span a variety of prediction tasks at the level of nodes, graphs, and links/edges. As shown in table <ref type="table" target="#tab_4">2</ref>, in this work, we performed our experiments on the three OGBN datasets with different sizes and tasks for getting credible result, including ogbn-products about 47 products categories classification with given 100-dimensional nodes features, ogbn-proteins about 112 kinds of proteins function prediction with given 8-dimensional edges features and ogbnarxiv about 40-class topics classification with given 128 dimension nodes features. More details about these datasets are provided in appendix A in the supplementary file.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with SOTA Models</head><p>Baseline and other comparative SOTA models are provided by OGB leaderboard. And all these results are guaranteed to be reproducible with open source codes. Following the requirement of OGB, we run our experimental results for each dataset 10 times and report the mean and standard deviation.</p><p>As shown in Table <ref type="table" target="#tab_8">4</ref>, Table <ref type="table" target="#tab_9">5</ref>, and Table <ref type="table" target="#tab_10">6</ref>, our unified model outperform all other comparative models in three OGBN datasets. Since most of the compared models only consider optimizing their models for the features propagation, these results demonstrate that incorporating label propagation into GNN models can bring significant improvements. Specifically, we gain 82.56% ACC in ogbn-products, 86.42% ROC-AUC in ogbn-proteins, which achieves about 0.6-1.6% absolute improvements compared to the newly SOTA methods like DeeperGCN <ref type="bibr" target="#b6">[Li et al., 2020]</ref>. In ogbn-arxiv, our method gains 73.11% ACC, achieve 0.37% absolute improvements compared to <ref type="bibr">GCNII [Chen et al., 2020]</ref>, whose parameters are four times larger than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Test Accuracy</head><p>Validation Accuracy Params GCN-Cluster <ref type="bibr" target="#b1">[Chiang et al., 2019]</ref>      <ref type="table">7</ref>: This is the ablation studies on models with different inputs, where X denotes the nodes features, A is the graph adjacent matrix and Ŷ is the observed labels. In ogbn-proteins, nodes features are not provided initially. We average the edge features as their nodes features and provide the experimental result of Transformer without edge features for fair comparison in this experiment, which is slightly different from Table <ref type="table" target="#tab_9">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, to better identify the improvements from different components of our proposed model, we conduct extensive studies with the following four aspects:</p><p>• Firstly, we apply the masked label prediction strategy on kinds of GNNS to show the effectiveness and robustness of incorporation LPA and GNN, shown in Table <ref type="table">7</ref>.</p><p>• In order to get a more practical and effective solution to apply masked label prediction strategy, we tune the label rate during training and inference to explore the relationship between label coverage and GNNs performance, shown in Figure <ref type="figure">2</ref>.</p><p>• We also analyze how LPA affects the GNN to make it performs better, shown in Figure <ref type="figure" target="#fig_7">3</ref>.</p><p>• Furthermore, in Table <ref type="table" target="#tab_12">8</ref>, we provide more ablation studies on UniMP, compared with GAT, showing the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks with Different Inputs</head><p>In Table <ref type="table">7</ref>, we apply masked label prediction on kinds of GNNs to improve their performance. Firstly, we reimplement classical GNN methods like GCN and GAT, following the same sampling methods and model setting shown in Table <ref type="table" target="#tab_6">3</ref>. The hidden size of GCN is head num*hidden size since it doesn't have head attention. Secondly, we change different inputs for these models to study the effectiveness of feature and label propagation, using our masked label prediction to train the models with partial nodes label Ŷ as input. Row 4 in Table <ref type="table">7</ref> shows that only with Ŷ and A as input, GNNs still work well in all three datasets, outperforming those MLP model only given X. This implies that one's label relies heavily on its neighborhood instead of its feature. Comparing Row 3 and 5 in Table <ref type="table">7</ref>, models with X, A and Ŷ outperform the models with X and A, which indicates that it's a waste of information for GNNs in semi-supervised classification when they making predictions without incorporating the ground truth train labels Ŷ. Row 3-5 in Table <ref type="table">7</ref> also show that our Graph Transformer can outperform GAT, GCN with different input settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation between Label Coverage and Performance</head><p>Although we have verified the effectiveness of using this strategy to combine LPA and GNN, the relation between label coverage and its impact on GNNs performance remains uncertain. Therefore, shown in Figure <ref type="figure">2</ref>, we conduct more experiments in ogbn-arxiv to investigate their relationship in the following different scenarios:</p><p>• In Figure <ref type="figure">2a</ref>, we train UniMP using X, Ŷ, A as inputs. We tune the input label rate which is the hyperparameter of masked label prediction task and display the validation and test accuracy. Our model achieves better performance when label rate is about 0.625. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring the Connection between Nodes</head><p>In Figure <ref type="figure" target="#fig_7">3</ref>, we analyze how LPA affects GNN to make it perform better. <ref type="bibr">Wang [2019]</ref> has pointed out that using LPA for GCN during training can enable nodes within the same class/label to connect more strongly, increasing the accuracy (ACC) of model's prediction. Our model can be regarded as an upgraded version of them, using LPA in both training and testing time for our Graph Transformer. Therefore, we try to experimentally verify the above idea based on our model. </p><p>We use the Margin Similarity Function (MSF) as shown in Equation 11 to reflect the connection tightness between nodes within the same class (the higher scores, the stronger connection they have. We conduct the experiment on ogbn-arxiv. And as shown in Figure <ref type="figure" target="#fig_7">3</ref>, the ACC of models' prediction is proportional to Margin Similarity. Unifying feature and label propagation can further strengthen their connection, improving their ACC. Moreover, our Graph Transformer outperforms GAT in both connection tightness and ACC with different inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Ablation Studies on UniMP</head><p>Finally, we provide more ablation studies on our UniMP model, compared with GAT, from the following 4 aspects: (1) vanilla transformer with dot-product attention or GAT with sum attention; (2) simple residual or gated residual; (3) with train labels as inputs; (4) with train and validation labels as inputs. As shown in Table <ref type="table" target="#tab_12">8</ref>, we can find that dot-product attention can outperform sum attention, since dot-product provides   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We first propose a unified message passing model, UniMP, which jointly performs feature propagation and label propagation within a Graph Transformer to make the semisupervised classification. Furthermore, we propose a masked label prediction method to supervised training our model, preventing it from overfitting in self-loop label information. Experimental results show that UniMP outperforms the previous state-of-the-art models on three main OGBN datasets: ogbnproducts, ogbn-proteins and ogbn-arxiv by a large margin, and ablation studies demonstrate the effectiveness of unifying feature propagation and label propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>S X P 2 6 W b U D J A e e n z 5 m b m p l z 7 O y k 5 3 D G b c H S C p / T r n J z l y 9 Y t S M C e A y M Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>S X P 2 6 W b U D J A e e n z 5 m b m p l z 7 O y k 5 3 D G b c H S C p / T r n J z l y 9 Y t S M C e A y M Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S X P 2 6 W b U D J A e e n z 5 m b m p l z 7 O y k 5 3 D G b c H S C p / T r n J z l y 9 Y t S M C e A y M Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S X P 2 6 W b U D J A e e n z 5 m b m p l z 7 O y k 5 3 D G b c H S C p / T r n J z l y 9 Y t S M C e A y M Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of UniMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>j∈N (i)pos k∈N (i)neg e αi,j − e αi,k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Correlation between accuracy and margin similarity between neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison the input information that message passing models use in training and inference.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Inference</cell></row><row><cell>Model</cell><cell cols="2">Feature Label Feature Label</cell></row><row><cell>LPA</cell><cell></cell></row><row><cell>GCN</cell><cell></cell></row><row><cell>APPNP</cell><cell></cell></row><row><cell>GCN-LPA</cell><cell></cell></row><row><cell>UniMP (Ours)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>e x i t &gt;</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Make Predictions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P (YU |X, Ŷ, A)</cell></row><row><cell>Transformer</cell><cell>Graph</cell><cell>LayerNorm</cell><cell>ReLU</cell><cell>Transformer</cell><cell>Graph</cell><cell>LayerNorm</cell><cell>ReLU</cell></row><row><cell></cell><cell cols="3">Gated Residual</cell><cell></cell><cell cols="3">Gated Residual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics of OGB node property prediction</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The hyper-paramerter setting of our model Implementation details. As mentioned above, these datasets are different from each other in sizes or tasks. So we evaluate our model on them with different sampling methods following previous studies<ref type="bibr" target="#b6">[Li et al., 2020]</ref>, getting credible comparison results. In ogbn-products dataset, we use Neigh-borSampling with size =10 for each layer to sample the subgraph during training and use full-batch for inference. In ogbn-proteins dataset, we use Random Partition to split the dense graph into subgraph to train and test our model. As for small-size ogbn-arxiv dataset, we just apply full batch for both training and test. We set the hyper-parameter of our model for each dataset in Table3, and the label rate means the percentage of labels we preserve during applying masked label prediction strategy. We use Adam optimizer with lr = 0.001 to train our model. Specially, we set weight decay to 0.0005 for our model in small-size ogbn-arxiv dataset to prevent overfitting. More details about the tuned hyperparameters are provided in appendix B in the supplementary file.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results for ogbn-products</figDesc><table><row><cell>Model</cell><cell cols="2">Test ROC-AUC Validation ROC-AUC</cell><cell>Params</cell></row><row><cell cols="2">GaAN [Zhang et al., 2018] GeniePath-BS [Liu et al., 2020b] 0.7825 ± 0.0035 0.7803 ± 0.0073 MWE-DGCN 0.8436 ± 0.0065 DeepGCN [Li et al., 2019] 0.8496 ± 0.0028 DeeperGCN [Li et al., 2020] 0.8580 ± 0.0017 UniMP 0.8642 ±0.0008</cell><cell>--0.8973± 0.0057 0.8921 ± 0.0011 0.9106 ± 0.0016 0.9175 ± 0.0007</cell><cell>-316,754 538,544 2,374,456 2,374,568 1,909,104</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results for ogbn-proteins</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Param</cell></row><row><cell cols="2">DeeperGCN [Li et al., 2020] 0.7192 ± 0.0016 GaAN [Zhang et al., 2018] 0.7197 ± 0.0024 DAGNN [Liu et al., 2020a] 0.7209 ± 0.0025 JKNet [Xu et al., 2018b] 0.7219 ± 0.0021 GCNII [Chen et al., 2020] 0.7274 ± 0.0016 UniMP 0.7311 ± 0.0021</cell><cell>0.7262 ± 0.0014 --0.7335 ± 0.0007 -0.7450 ± 0.0005</cell><cell>1,471,506 1,471,506 1,751,574 331,661 2,148,648 473,489</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results for ogbn-arxivTable</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>•</head><label></label><figDesc>Figure2bdescribes the correlation between the proportion of training data and the effectiveness of label propagation. We fix the input label rate with 0.625. The only change is the training data proportion. It's common sense that with the increased amount of training data, the performance is gradually improving. And the model with label propagation Ŷ can gain greater benefits from increasing labeled data proportion. In Figure2d, we calculate the accuracy for unlabeled nodes grouped by the number of neighbors. The experimental result shows that nodes with more neighbors have higher accuracy. And the model with label propagation Ŷ can always have improvements even with different numbers of training neighbors.</figDesc><table><row><cell>• Our unified model always masks a part of the training in-</cell></row><row><cell>put label and tries to recover them. But in the inference</cell></row><row><cell>stage, our model utilizes all training labels for predic-</cell></row><row><cell>tions, which is slightly inconsistent with the one in train-</cell></row><row><cell>ing. In Figure 2c, we fix our input label rate with 0.625</cell></row><row><cell>during training and perform different input label rate</cell></row><row><cell>in inference. the training stage, It's found that UniMP</cell></row><row><cell>might have worse performance (less than 0.70) than the</cell></row><row><cell>baseline (about 0.72) when lowering the label rate dur-</cell></row><row><cell>ing prediction. However, when the label rate climbs up,</cell></row><row><cell>the performance can boost up to 0.73.</cell></row></table><note>•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>more interactions between nodes. Besides, residual and gated residual can also strengthen the GNNs with shallow layers. Moreover, our unified model can take the additional validation labels as input to further boost model's performance without more training steps. Therefore, when we apply the model to the real scene, and the labeled data are accumulated progressively, the accuracy of the unlabeled data can keep increasing without training our model from scratch, while other GNNs without explicit label modeling can't fully utilize the benefits of additional labels. Ablation studies in UniMP, compared with GAT</figDesc><table><row><cell>Model</cell><cell cols="2">ogbn-prdouct ogbn-arxiv</cell></row><row><cell>GAT (sum attention)</cell><cell>0.8002</cell><cell>0.7246</cell></row><row><cell>w/ residual</cell><cell>0.8033</cell><cell>0.7265</cell></row><row><cell>w/ gated residual</cell><cell>0.8050</cell><cell>0.7272</cell></row><row><cell>Transformer (dot-product)</cell><cell>0.8091</cell><cell>0.7259</cell></row><row><cell>w/ residual</cell><cell>0.8125</cell><cell>0.7271</cell></row><row><cell>w/ gated residual</cell><cell>0.8137</cell><cell>0.7292</cell></row><row><cell>w/ train label (UniMP)</cell><cell>0.8256</cell><cell>0.7311</cell></row><row><cell>w/ validation labels</cell><cell>0.8312</cell><cell>0.7377</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">(0,4] (4,9] (9,13] (13,18](18,22](22,27</ref></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<idno>arXiv:2007.02133</idno>
	</analytic>
	<monogr>
		<title level="m">Simple and deep graph convolutional networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks</title>
				<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<imprint>
			<publisher>Will Hamilton, Zhitao Ying, and Jure Leskovec</publisher>
			<date type="published" when="2015">2019. 2019. 2018. 2018. 2015. 2015. 2016. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predict propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Mamitsuka</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Klicpera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1810.05997</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2016. 2016. 2018. 2018</date>
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semi-supervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="3538" to="3545" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning. arXiv: Learning</title>
		<author>
			<persName><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019. 2019. 2019. 2019. 2020a. 2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards deeper graph neural networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bandit samplers for training graph neural networks</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05806</idno>
		<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2016. 2014-2023, 2016. 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propagation. arXiv: Learning</title>
				<meeting><address><addrLine>Leskovec</addrLine></address></meeting>
		<imprint>
			<publisher>Fei Wang and Changshui Zhang</publisher>
			<date type="published" when="2007">2017. 2017. 2019. 2019. 2007. 2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018a. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperparameter learning for graph based semi-supervised learning algorithms</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<idno>arXiv:1803.07294</idno>
	</analytic>
	<monogr>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<editor>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Junyuan</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hao</forename><surname>Xie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Irwin</forename><surname>Ma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dit-Yan</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><surname>Yeung</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2003">2018b. 2018. 2016. 2016. 2019. 2019. 2007. 2018. 2018. 2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
