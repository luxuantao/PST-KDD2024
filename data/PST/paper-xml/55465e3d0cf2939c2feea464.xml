<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning for Visual Categorization: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@sheffield.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic and Information Engineer-ing</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Electri-cal Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">Uni-versity of Sheffield</orgName>
								<address>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Center for OPTical IMagery Analysis and Learning</orgName>
								<orgName type="department" key="dep2">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="laboratory">State Key Laboratory of Transient Optics and Photonics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning for Visual Categorization: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C79BB3F71AD4D006780CF19E21B343D1</idno>
					<idno type="DOI">10.1109/TNNLS.2014.2330900</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action recognition</term>
					<term>image classification</term>
					<term>machine learning</term>
					<term>object recognition</term>
					<term>survey</term>
					<term>transfer learning</term>
					<term>visual categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such crossdomain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N THE past few years, the computer vision community has witnessed a significant amount of applications in video search and retrieval, surveillance, robotics, and so on. Regular machine learning approaches <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref> have achieved promising results under the major assumption that the training and testing data stay in the same feature space or share the same distribution. However, in real-world applications, due to the high price of human manual labeling and environmental restrictions, sufficient training data belonging to the same feature space or the same distribution as the testing data may not always be available. Typical examples are <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>, where only one action template is provided for each action class for training, and <ref type="bibr" target="#b11">[12]</ref>, where training samples are captured from a different viewpoint. In such situations, regular machine learning techniques are very likely to fail. This reminds us of the capability of the human vision system. Given the gigantic geometric and intraclass variabilities of objects, humans are able to learn tens of thousands of visual categories in their life, which leads to the hypothesis that humans achieve such a capability by accumulated information and knowledge <ref type="bibr" target="#b12">[13]</ref>. It is estimated that there are about 10-30 thousands object classes in the world <ref type="bibr" target="#b13">[14]</ref> and children can learn 4-5 object classes per day <ref type="bibr" target="#b12">[13]</ref>. Due to the limitation of objects that a child can see within a day, learning new object classes from large amounts of corresponding object data is not possible. Thus, it is believed that the existing knowledge gained from previous known objects assists the new learning process through their connections with the new object categories. For example, assuming we did not know what a watermelon is, we would only need one training sample of watermelons together with our previous knowledge on melonscircular shapes, the green color, and so on, to remember the new object category watermelon. Transfer learning mimics the human vision system by making use of sufficient amounts of prior knowledge in other related domains when executing new tasks in the given domain. In transfer learning, both the training data and the testing data can contribute to two types of domains: 1) the target domain and 2) the source domain. The target domain contains the testing instances, which are the task of the categorization system, and the source domain contains training instances, which are under a different distribution with the target domain data. In most cases, there is only one target domain for a transfer learning task, while either single or multiple source domains can exist. For example, in <ref type="bibr" target="#b14">[15]</ref>, action recognition is conducted across data sets from different domains, where the KTH data set <ref type="bibr" target="#b15">[16]</ref>, which has a clean background and limited viewpoint and scale changes, is set as the source data set, and the Microsoft research action data set <ref type="foot" target="#foot_0">1</ref>and the TRECVID surveillance data <ref type="bibr" target="#b16">[17]</ref>, which are captured from realistic scenarios, are used as the target data set. In <ref type="bibr" target="#b17">[18]</ref>, the source and target data sets are chosen from different TV program channels for the task of video concept detection.</p><p>Transfer learning can be considered as a special learning paradigm where partial/all training data used are Fig. <ref type="figure">1</ref>. Basic frameworks of traditional machine learning approaches and knowledge transfer approaches. For regular machine learning approaches, the learning system can only handle the situation that testing samples and training samples are under the same distribution. On the other hand, transfer learning approaches have to deal with the data distribution mismatch problem through specific knowledge transfer methods, e.g., mining the shared patterns from data across different domains.</p><p>under a different distribution with the testing data. To understand the significance of knowledge transfer in terms of visual learning problems, the literature, (see <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>) has concluded three general issues regarding the transfer process: 1) when to transfer; 2) what to transfer; and 3) how to transfer. First, when to transfer includes the issues whether transfer learning is necessary for specific learning tasks and whether the source domain data are related to the target domain data. In the scenarios of <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, where training samples are sufficient and impressive performance can be achieved, while being constrained in the target domains, including another domain as the source domain becomes superfluous. A variety of divergence levels exist across different pairs of source domain and target domain data, brute-forcing the knowledge from the source domain into the target domain irrespective of their divergence would cause certain performance degeneration, or, in even worse cases, it would break the original data consistency in the target domain. Second, the answer to what to transfer can be concluded in three aspects: 1) inductive transfer learning, where all the source domain instances and their corresponding labels are used for knowledge transfer; 2) instance transfer learning, where only the source domain instances are used; and 3) parameter transfer learning, in addition to the source domain instances and labels, some parameters of prelearned models from the source domain are utilized to help improve the performance in the target domain. Finally, how to transfer includes all the specific transfer learning techniques, and it is also the most important part that has been studied in the transfer learning literature. Many transfer learning techniques have been proposed, e.g., in <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, where knowledge transfer is based on the nonnegative matrix trifactorization framework, and in <ref type="bibr" target="#b27">[28]</ref>, where the transfer learning phase is via dimensionality reduction. We illustrate the basic frameworks of traditional machine learning approaches and knowledge transfer approaches in Fig. <ref type="figure">1</ref>. For traditional machine learning approaches, the ideal choice of the training set to predict a testing instance car should contain cars. However, in the case of knowledge transfer, the training set can just contain some relevant categories rather than cars, e.g., wheels, which are similar to the wheels of cars; bicycles, which share the knowledge of wheels with the car wheels, or even some irrelevant objects, e.g., laptops and birds, which seem to have no connections with cars, but actually share certain edges or geometrical layouts with local parts of a car image.</p><p>As the age of big data has come, transfer learning can provide more benefits to solve the target problem with more relevant data. Thus, it is believed that more applications on transfer learning will emerge in future research. This survey aims to give a comprehensive overview of transfer learning techniques on visual categorization tasks, so that readers could potentially use the analysis and discussions in this survey to understand how transfer learning can be applied to visual categorization tasks or to solve their problem with a suitable transfer learning method. The visual categorization tasks possess some unique characteristics due to certain visual properties that can be potentially used in the training process, e.g., the appearance or shape of an object part, the local symmetries of an object, and the structural. All these unique properties can be employed when designing transfer learning algorithms, which makes our work different from that of <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b28">[29]</ref>, where the former focuses on classification, regression and clustering problems related to data mining tasks and the latter focuses on reinforcement learning, which addresses problems with only limited environmental feedback rather than correctly labeled examples.</p><p>The remaining part of this survey is structured as follows. An overview is given in Section II. In Sections III and IV, two transfer learning categories, which execute knowledge transfer through feature representations and classifiers, are discussed in detail, respectively, answering the problems of what to transfer and how to transfer. In Section V, the model selection methods from multiple source domains, i.e., when to transfer, are discussed. Evaluation, analysis, and discussions on the stated transfer learning methods are given in Section VI. Finally, the conclusions are drawn in Section VII. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Developing Interests on Transfer Learning</head><p>Dating from the raising of its notion in the last century, transfer learning (also known as, cross-domain learning, domain transfer, and domain adaptation) has a long history of being studied as a particular machine learning technique. In recent years, with the information explosion on the Internet, (e.g., audio, images, and videos) and the growing demands for target tasks in terms of accuracies, data scales, and computational efficiencies, transfer learning approaches begin to attract increasing interests from all research areas in pattern recognition and machine learning. When regular machine learning techniques reach their limits, transfer learning opens the flow of a new stream that could fundamentally change the way of how we used to learn things and how we used to treat classification or regression tasks. Along with the flow, some workshops and tutorial have been held (such as the NIPS 1995 postconference workshop 2 in machine learning and data mining areas and another transfer learning survey is given in <ref type="bibr" target="#b28">[29]</ref> for reinforcement learning). In this survey, we focus on the applications of transfer learning techniques to visual categorization, including action recognition, object recognition, and image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Notations and Issues</head><p>Some general notations are defined as follows for later usage: let D T = D T l ∪ D T u denote the target domain data, where the partially labeled parts are denoted by D T l and the unlabeled parts are denoted by D T u . In addition to the target domain data, a set of auxiliary data is seen as the source domain data, which is semilabeled or fully labeled and has the representation D s = {(x i , y i )} a i=1 in a single source case, and</p><formula xml:id="formula_0">D s 1 , D s 2 , . . . , D s M with D s k = {(x k i , y k i )} N a k i=1</formula><p>in a multiple source case. Here, x i ∈ R d is the i th feature vector, where 2 https://nips.cc/Conferences/2005/Workshops/ d denotes the data dimension, and y i denotes the class label of the i th sample.</p><p>According to prior proposals, common issues regarding knowledge transfer are twofold. First, the auxiliary samples are typically treated without accounting for their mutual dependency during adaptation, which may cause the adapted data to be arbitrarily distributed and the structural information beyond single data samples of the auxiliary data may become undermined. Second, during adaptation, noises, and particularly possible outliers from the auxiliary domains are blindly forced to the target domain <ref type="bibr" target="#b29">[30]</ref>.</p><p>When transferring knowledge from the auxiliary domains to the target domain, it is crucial to know the distribution similarities between the target domain data and each source domain data. So far, the most common criterion to measure the distribution similarity of two domains is a nonparametric distances metric named maximum mean discrepancy (MMD). The MMD is proposed in <ref type="bibr" target="#b30">[31]</ref>, and it compares data distributions in the reproducing kernel Hilbert space</p><formula xml:id="formula_1">Dist k (D s , D T ) = 1 n s n s i=1 φ x s i - 1 n T n T i=1 φ x T i 2<label>(1)</label></formula><p>where φ(•) is the feature space mapping function.</p><p>In the literature, transfer learning techniques are categorized according to a variety of taxonomies. In <ref type="bibr" target="#b18">[19]</ref>, considering tasks allocated to the target domain and auxiliary domains and the availability of sample labels within the target domain and auxiliary domains, transfer learning techniques are first grouped as inductive transfer learning, transductive transfer learning, and unsupervised transfer learning, upon which they are further categorized as instance-transfer, feature representation transfer, parameter transfer, and relational knowledge transfer within each initial partition. Fig. <ref type="figure" target="#fig_0">2</ref> shows five ways of differentiating existing knowledge transfer approaches for visual categorization. In this survey, inheriting the concepts from the computer vision community, we simply categorize transfer learning techniques into feature representation level knowledge transfer and classifier level knowledge transfer. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS III. FEATURE REPRESENTATION TRANSFER Feature representation level knowledge transfer is a popular transfer learning category that maps the target domain to the source domains exploiting a set of meticulously manufactured features. Through this type of feature representation level knowledge transfer, data divergence between the target domain and the source domains can be significantly reduced so that the performance of the task in the target domain is improved. Most existing transductive features are designed for specific domains and would not perform optimally across different data types. Thus, we review the feature level knowledge transfer techniques according to two data types: 1) cross-domain knowledge transfer and 2) cross-view knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-Domain Knowledge Transfer</head><p>In the cross-domain setting, the gap between the source domain data and the target domain data varies from images to videos and from objects to edges. According to the degree of data divergence, different approaches are proposed. In <ref type="bibr" target="#b14">[15]</ref>, knowledge transfer is made between the KTH data set <ref type="bibr" target="#b15">[16]</ref>, the TRECVID data set <ref type="bibr" target="#b16">[17]</ref> and the Microsoft research action data set II (MSRII), where the KTH data set is seen as the target domain and both the TRECVID data set and the MSRII data set are used as the source domains. The KTH data set is limited to clean backgrounds and a single actor and each video sequence exhibits one individual action from the beginning to the end. On the other hand, the TRECVID data set and the MSRII data set are captured from realistic scenarios, with cluttered backgrounds and multiple actors in each video sequence. To take advantage of the labeled training data from both the target domain and the source domain, Daumé <ref type="bibr" target="#b31">[32]</ref> proposed the feature replication (FR) method using augmented feature for training. Inspired by <ref type="bibr" target="#b32">[33]</ref>, which applies the Gaussian mixture model (GMM) to model the visual similarities between images or videos, the work in <ref type="bibr" target="#b14">[15]</ref> models the spatial temporal interests points (STIPs) with the GMM and introduces a prior distribution of the GMM parameters to generate probabilistic representations of the original STIPs. Such representations can accomplish the adaptation from the source domains to the target domain. The basic setting of <ref type="bibr" target="#b33">[34]</ref> assumes that there are labeled training data in the source domain, but no labeled training data in the target domain. Furthermore, the activities in the source domain and the target domain do not overlap, so that traditional supervised learning methods cannot be applied in this scenario. Utilizing the Web pages returned by search engines to mine similarities across the domains, the labeled data in the source domain are then interpreted by the label space of the target domain. In some extreme cases, the source domain data may not be relevant to the target domain data.</p><p>Sparseness has gained tremendous attention in various scientific fields, and computer vision is a dominant part of this trend. Sparse models can find their applications in a wide range of computer vision techniques, e.g., dictionary learning (DL) <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref> and transfer learning. Raina et al. <ref type="bibr" target="#b37">[38]</ref> apply sparse coding to unlabeled data to break the tremendous amount of data in the source domain into basic patterns, (e.g., edges in the task of image classification) so that knowledge can be transferred through the bottom level to form a higher level representation of the training samples in the target domain, in which case the source domain data do not necessarily need to be relevant to the target domain data. Since in the regular transfer learning formalism, the source domain data have to be relevant with the target domain data, such a knowledge transfer method is named self-taught learning rather than transfer learning. Zhu and Shao <ref type="bibr" target="#b38">[39]</ref> present a discriminative cross-domain DL (DCDDL) framework that utilizes relevant data from other visual domains as auxiliary knowledge for enhancing the learning system in the target domain. The objective function is designed to encourage similar visual patterns across different domains to possess identical representations after being encoded by a learned dictionary pair. In the partof-speech (POS) tagging tasks, shared patterns from auxiliary categorization tasks are extracted as pivot features, which represent the frequent words emerged in the speech and are themselves indicative of their corresponding categories <ref type="bibr" target="#b39">[40]</ref>. While the pivot features are sensitive to the POS tagging tasks, pivot visual words do not exist in typical local histogram-based low-level visual features, which indicates that no single feature dimension of the histogram bins is discriminative enough to represent the difference of the visual categories <ref type="bibr" target="#b40">[41]</ref>.</p><p>On the other hand, some works also target to identify a new lower-dimensional feature space such that the auxiliary domain and the target domain manifest some shared characteristics <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>, instead of transferring the entire knowledge across the target domain and auxiliary domains making such an assumption that the smoothness property (i.e., those data points close to each other are more likely to share the same label) is satisfied in low-dimension subspaces <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-View Knowledge Transfer</head><p>Cross-view knowledge transfer can be seen as a special case of cross-domain knowledge transfer, where the divergences across domains are caused by view-point changes. The task is to recognize action classes in the target view using training samples from one or more different views. Generating viewinvariant features to address the cross-view visual pattern recognition problems attracts significant attention in the computer vision field, especially for cross-view action recognition. The bottom of Fig. <ref type="figure" target="#fig_1">3</ref> shows the cross-view knowledge transfer scenario on the multiview IXMAS <ref type="bibr" target="#b44">[45]</ref> data set. The typical setting is to use samples captured in one view (the source view) as training data to predict the labels of samples captured from a different view (the target view). The core methodology of approaches that tackle visual categorization problems with changes in the observer's viewpoint is to discover the shared knowledge irrespective to such viewpoint changes. One common approach to attack the cross-view feature representation diversity problem is to infer 3-D scene structure for cross-view feature adaptation, where the derived features can be adapted from one view to another utilizing geometric reasoning <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b48">[49]</ref>. Another family of approaches is to explore visual pattern properties, e.g., affine <ref type="bibr" target="#b49">[50]</ref>, projective <ref type="bibr" target="#b50">[51]</ref>, epipolar geometry <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref>, to compute such cross-view feature representations. On the other hand, Junejo et al. <ref type="bibr" target="#b54">[55]</ref> applied a self-similarity matrix to store distances between different pairs of actions for a view-invariant representation. Spatial-temporal features of a video sequence that are insensitive to changes in view angle are studied in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b57">[58]</ref>.</p><p>In <ref type="bibr" target="#b11">[12]</ref>, a bipartite graph is built via unsupervised co-clustering to measure the visual-word to visual-word relationship across the target view and the source view so that a high-level semantic feature that bridges the semantic gap between the two vocabularies can be generated. Beyond the bag-of-visual-words representation, which have been successfully applied to natural language processing, information retrieval, and computer vision, the proposed bag-of-bilingualwords representation discovers the shared set of common action concepts between two different views, even though the two view domains are highly independent. Similar to the work of <ref type="bibr" target="#b11">[12]</ref>, Li and Zickler <ref type="bibr" target="#b57">[58]</ref> captured the conceptual idea of virtual views construction to represent an action descriptor continuously from one observer's viewpoint to another. Another family of approaches is proposed in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref>, where a pair of over-complete dictionaries are constructed utilizing correspondence samples across two view domains. Encouraged by the learned dictionary pair, the labeled source view data and unlabeled target view data are forced to the same feature space that satisfies the smoothness assumption.</p><p>We summarize the main characteristics of the feature representation level knowledge transfer approaches according to their adaptation methods, the target domain label, the source domain label, adaptation data types and applications, and list them in Table <ref type="table" target="#tab_0">I</ref>. Among these approaches, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b58">[59]</ref>, and <ref type="bibr" target="#b60">[61]</ref> utilize the sparseness property to generate sparse representations for data adaptation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CLASSIFIER-BASED KNOWLEDGE TRANSFER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SVM-Based</head><p>Support Vector Machine (SVM) is a supervised learning method for solving classification and regression problems, and the majority of existing work on classifier-based knowledge transfer are constructed from the original SVM classifier. As a direct application of SVM, adaptive-SVM (A-SVM) <ref type="bibr" target="#b17">[18]</ref>, and projective model transfer SVM (PMT-SVM) <ref type="bibr" target="#b62">[63]</ref> learn from the source model w s by regularizing the distance between the target model w t and the learned model w s . The A-SVM uses the following objective function:</p><formula xml:id="formula_2">L A = min w t ,b w t -w s 2 + C N i l(x i , y i ; w t , b)<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">y i ∈ {-1, 1} indicates the corresponding labels, l(x i , y i ; w t , b) = max(0, 1 -y i (w t x i + b))</formula><p>is the hinge loss, C controls the weight of the loss function, and controls the amount of transfer regularization. By regularizing the distances between the two models, knowledge transfer for A-SVM is like a spring between w s and w t , which is equivalent to providing samples from the source classes. By expanding the regularization term</p><formula xml:id="formula_4">w t -w s 2 = w t 2 -2 w t cos θ + 2<label>(3)</label></formula><p>where w 2 provides the margin maximization as in regular SVM and the second term -2 w cos θ induces the transfer by maximizing cos θ , i.e., by minimizing the angle θ between w t and w s . Instead of maximizing the term cos θ , knowledge transfer can be induced by minimizing the projection of w t onto the separating hyperplane orthogonal to w s for PMT-SVM using the following objective function:</p><formula xml:id="formula_5">L PMT = min w t ,b w t 2 + Pw t 2 + C N i l(x i , y i ; w t , b) s.t. : w t w s ≥ 0 (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where P = I -(w s w s )/(w s w s ) is the projection matrix.</p><p>Compared with A-SVM, PMT-SVM can increase the amount of transfer ( ) without penalizing margin maximization. Opposed to the rigid transfer methods A-SVM and PMT-SVM, the deformable adaptive SVM (DA-SVM) <ref type="bibr" target="#b62">[63]</ref> provides more flexible transfer regularization through a deformable source template, where small local deformations can be tolerated for the template fit of the source domain to the target domain. Aytar and Zisserman <ref type="bibr" target="#b62">[63]</ref> used a simple example to explain such visual deformation in knowledge transfer that the wheel part of a motorbike template can be increased in radius and reduced in thickness when fitting to a bicycle wheel template. The DA-SVM can also be seen as the generalization form of the rigid A-SVM by replacing w s in (2) with τ (w s )</p><formula xml:id="formula_7">L DA = min f,w t ,b w t -τ (w s ) 2 + C N i l(x i , y i ; w t , b) +λ ⎛ ⎝ M,M i = j f 2 i, j d i, j + M i (1 -f ii ) 2 d ⎞ ⎠<label>(5)</label></formula><p>where d i, j is the spatial distance between the i th and j th cell, d is the penalization for the additional flow from the i th source cell to the i th target cell, and τ (w s ) i = M j f ij w s j is the flow transformation, where the parameter f i j denotes the amount of transfer from the j th cell in the source template to the i th cell in the transformed template. The cells are extracted from local image regions, on which local descriptors, (e.g., HOG <ref type="bibr" target="#b63">[64]</ref> and SIFT <ref type="bibr" target="#b64">[65]</ref>) are computed. Thus, different from other classifierbased knowledge transfer techniques, DA-SVM has such a constraint that it has to be constructed using low-level visual features that measure the geometrical information of local image parts.</p><p>Tommasi et al. <ref type="bibr" target="#b65">[66]</ref> proposed a discriminative transfer learning method based on least squares support vector machine (LS-SVM) that learns the new category through adaptation. By replacing the regularization term in classical LS-SVM, the new learning objective function for knowledge transfer is formulated as</p><formula xml:id="formula_8">L KTLS = min w t ,b 1 2 w t -θw s 2 + C 2 l i=1 [y i -w t φ(x i )-b] 2 (6)</formula><p>where θ is a scaling factor in the range of (0, 1) to control the degree of transfer across the learned model w s and the target model w t . When being extended to multimodel knowledge transfer (multi-KT), the scaling factor θ is substituted with the vector = {θ 1 , θ 2 , . . . , θ k }, where each θ j is the weight of a corresponding prior model. Thus, (6) can be rewritten as</p><formula xml:id="formula_9">L Multi-KT = min w t ,b w t - k j =1 θ j w s j 2 + C 2 l i=1 ζ i (y i -w t • φ(x i ) -b) 2 . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>The ζ i in ( <ref type="formula" target="#formula_9">7</ref>) is used for resampling the data so that training samples are balanced. Taking the advantage of LS-SVM that the leave-one-out (LOO) error, which measures the proper amount of knowledge to be transferred, can be written in a closed form <ref type="bibr" target="#b66">[67]</ref>, the best values of θ j are those that minimize the LOO error. Typically, the kernel functions need to be specified in advance to learning and the associated kernel parameters, (e.g., the mean and variance in the Gaussian kernel) are determined during optimization. On top of the various kernel learning methods <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b70">[71]</ref>, the domain transfer SVM (DT-SVM) <ref type="bibr" target="#b71">[72]</ref> unified the cross-domain learning framework by searching for the SVM decision function f (x) = w φ(x)+b as well as the kernel function simultaneously instead of the two-step approaches <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b72">[73]</ref>. In general, DT-SVM achieves cross-domain classification by reaching two objective criteria: 1) DT-SVM minimizes the data distribution mismatch between the target domain and source domains using the MMD criterion mentioned in Section II and 2) DT-SVM pursues better classification performance by minimizing the structural risk of SVM. By meeting both criteria, an effective kernel function can be learned for better separation performance in linear space over different domains, and thus samples from the source domains are infused to the target domain to improve the classification performance of the SVM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TrAdaboost</head><p>Adaptive boosting (AdaBoost) <ref type="bibr" target="#b73">[74]</ref> is a popular boosting algorithm, which has been used in conjunction with a wide range of other machine learning algorithms to enhance their performance. At every iteration, AdaBoost increases the accuracy of the selection of the next weak classifier by carefully adjusting the weights on the training instances. Thus, more importance is given to misclassified instances since they are believed to be the most informative for the next selection. The transfer learning AdaBoost (TrAdaBoost) is introduced in <ref type="bibr" target="#b20">[21]</ref> to extend AdaBoost for transfer learning by weighting less on the different-distribution data, which are considered as dissimilar to the same-distribution data in each boosting iteration. The goal of TrAdaBoost is to reduce the weighted training error on the different-distribution data, and meanwhile preserving the properties of AdaBoost. Since the quality of different-distribution data is not certain, the performance of TrAdaBoost cannot be always guaranteed to outperform AdaBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generative Models</head><p>The learning to learn concept via rich generative models has emerged as one promising research area in both computer vision and machine learning. Recently, researchers have begun developing new approaches to deal with transfer learning problems using generative models. One workshop in conjunction with NIPS 2010 was held specifically for the discussion of transfer learning via rich generative models. In general, the generative knowledge transfer methods can lead to higher-impact transfer, including more information than those discriminative approaches and they can be more adaptive to a single specific task.</p><p>Fei-Fei et al. <ref type="bibr" target="#b74">[75]</ref> proposed a Bayesian-based unsupervised one-shot learning object categorization framework that learns a new object category using a single example (or just a few). Since Bayesian methods allow us to incorporate prior information about objects into a prior probability density function when observations become available, general information coming from previously learnt unrelated categories is represented with a suitable prior probability density function on the parameters of the probabilistic models. Thus, priors can be formed from unrelated object categories. For example, when learning the category motorbikes, priors can be obtained by averaging the learnt model parameters from other three categories spotted cats, faces, and airplanes, so that the hyperparameters of the priors are then estimated from the parameters of the existing category models. Yu and Aloimonos <ref type="bibr" target="#b75">[76]</ref> applied the generative authortopic <ref type="bibr" target="#b76">[77]</ref> model to learn the probabilistic distribution of image features-based object attributes. Since object attributes can represent common properties across different categories, they are used to transfer knowledge from source categories to target categories. Both the zero-shot learning problem and the one-shot learning problem are addressed, where in the first problem, the attribute model learned from the source domain categories is used to generate synthesized target training examples through the generative process, and in the second problem, the learned attribute model is used to reduce the uncertainty of parameters of the Dirichelt priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fuzzy System-Based Models</head><p>Transfer learning also finds its application in fuzzy systems. Deng et al. <ref type="bibr" target="#b77">[78]</ref> and <ref type="bibr" target="#b78">[79]</ref> proposed two knowledge-leveragebased fuzzy system models, respectively. The former is based on the Takagi-Sugeno-Kang fuzzy system, and the latter is based on the reduced set density estimator-based Mamdani-Larsen-Type fuzzy system. In both works, the training set is decomposed to training data of the current scene and model parameters of reference scenes. The same knowledge leverage strategy is adopted by both works, where model parameters obtained from the reference scenes are fed to the current scene for parameter approximation. The knowledge leverage strategy is performed through a unified objective function, which emphasizes on both learning from the data of the current scene and transferring model parameters from reference scenes.</p><p>1) Discussion: The stated SVM-based knowledge transfer methods can act as a plug in to the SVM training process. A common trait shared amid these methods according to their objective functions is that they all include a regularization term that measures the similarity between the learned model and the target model. In A-SVM, PMT-SVM, and DA-SVM, is the tradeoff parameter between margin maximization and knowledge transfer, so it defines the amount of transfer regularization. The DA-SVM is specialized in dealing with the transfer of visually deformable templates, while A-SVM and PMT-SVM are more likely to be generalized. The advantage of PMT-SVM over A-SVM is that it can increase the amount of transfer without penalizing margin maximization, while A-SVM encourages w to be larger when increasing . A large w indicates small margins to the hyperplane, and thus the generalization error of the classifier fails to gain an optimal bound. In general, PMT-SVM is expected to outperform A-SVM.</p><p>Compared with SVM-based approaches, the boosting-based method, TrAdaBoost, is simpler in terms of implementation, and it does not require the parameters from the prelearned models. Like other boosting-based techniques, TrAdaBoost has a fairly strong generalization ability. However, TrAdaBoost relies heavily on the relevance of the source domain data to the target domain data, thus it is vulnerable to negative transfers. In addition, TrAdaBoost can easily overfit in the presence of noise in either domain. The generative models are more adaptive to a specific task, however, but computationally more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MODEL SELECTION IN KNOWLEDGE TRANSFER</head><p>In real-world applications, knowledge transfer techniques have to consider more complicated scenarios than adapting the samples or prelearned models from a single source domain to obtain the target learner. In the first case, more than one source domains are available yet we have no idea which source domain contains more useful information that potentially improves the target learner or whether the knowledge in a specific domain is against the smoothness property in the target domain. On the other hand, in visual categorization tasks, the shared information across the two domains can be hidden in different visual forms, e.g., appearance, local symmetry, and layout, which can be captured by different feature descriptors. A fusion strategy is required to mine the most helpful knowledge from multiple features. The third case is that some knowledge transfer techniques are constructed from prelearned models, e.g., a learned bicycle classifier or a learned bird classifier, and these models can lead to different scales of contributions to the target model. In advance to knowledge transfer, the bad prelearned models need to be filtered out so that the good models can achieve more effective transfer. All the above three cases generalize the common many-to-one adaptation situations in knowledge transfer, and they can all be deemed as the model selection problem. Fig. <ref type="figure" target="#fig_3">4</ref> shows a typical example of multisource binary classification. A straightforward approach to reduce such prediction ambiguity is to measure the model similarity between each auxiliary domain and the target domain, and apply the closest model for prediction in the target domain, i.e., if auxiliary domain 1 is more similar with the target domain, the decision boundary in Fig. <ref type="figure" target="#fig_3">4</ref>(c) will inherit the decision boundary in Fig. <ref type="figure" target="#fig_3">4(a)</ref>. However, data in auxiliary domain 2, which also contain useful information for the prediction of target domain data, are abandoned.</p><p>In general, extending the existing single-source knowledge transfer techniques to the multiple-source scenario can evoke two challenges: 1) how to leverage the distribution differences among multiple source-domains to promote the prediction performance on the target domain task? and 2) how to extend the single-source knowledge transfer techniques to a distributed algorithm, while only sharing some statistical data of all source domains instead of revealing the full contents? Since most existing multiple-source knowledge transfer methods are extended from their corresponding single-source algorithms, we structure this section in a similar manner as Sections III and IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SVM-Based</head><p>In the one-to-one adaptation scenario of A-SVM <ref type="bibr" target="#b17">[18]</ref>, the new target classifier f T (x) is adapted from the existing source classifier f s (x) using the form</p><formula xml:id="formula_11">f T (x) = f s (x) + f (x) (8)</formula><p>where the perturbation function f (x) is learned using the labeled data D T l from the target domain. Intuitively, when encountering with multiple source domains D s 1 , D s 2 , . . . , D s M , which are assumed to possess similar distributions to the primary domain D t , the adapted classifier can be constructed using the ensemble of all the source domain classifiers</p><formula xml:id="formula_12">f s 1 (x), f s 2 (x), . . . , f s M (x) f T (x) = M k=1 γ k f s k (x) + f (x) (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where γ k ∈ (0, 1) is the predefined weight of each source classifier f s k (x), which sums to one:</p><formula xml:id="formula_14">m k=1 γ k = 1.</formula><p>The MMD criterion can be applied for obtaining the value of γ k . The perturbation function can be formulated as</p><formula xml:id="formula_15">f (x) = n l i=1 α T i y T i k(x T i , x)</formula><p>, where α T i is the coefficient of the i th labeled pattern in the target domain and k(•, •) is a kernel function induced from the nonlinear feature mapping φ(•). When applying the same kernel function to the source classifiers, (9) can be expanded as</p><formula xml:id="formula_16">f T (x) = s γ s n l i=1 α s i y s i k x T i , x + n l i=1 α T i y T i k x T i , x ,<label>(10)</label></formula><p>which is the sum of a set of weighted kernel evaluations between the test pattern x and all labeled patterns x T i and x s i , respectively, from the target domain and all the source domains. Obviously, the learning process is inefficient when being applied to large-scale data sets, which is the first disadvantage of A-SVM on the many-to-one adaptation application. The second disadvantage of A-SVM is its failure on using the unlabeled target domain data D T u . Duan et al. <ref type="bibr" target="#b71">[72]</ref> proposed the domain adaptation machine (DAM) to overcome the two disadvantages of A-SVM. To utilize the unlabeled target domain data D T u , a data-dependent regularizer is defined for the target classifier f T</p><formula xml:id="formula_17">( f T u ) = 1 2 S s=1 γ s m i=1 f T i -f s i 2 (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where</p><formula xml:id="formula_19">f T u = [ f T n l +1 , . . . , f T n T ] and f s u = [ f s n l +1 , .</formula><p>. . , f s n T ] are defined as the decision values from the target classifier and the sth source classifier, respectively. Based on the smoothness assumption for domain adaptation, DAM minimizes the structural risk function of LS-SVM as well as the data-dependent regularizer simultaneously. DAM is formulated as</p><formula xml:id="formula_20">min f T ( f T ) + 1 2 n l i=1 f T i -y T i 2 + D f T u (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>where ( f T ) is a regularizer to control the complexity of the target classifier f T . Since the target classifier in DAM is learned in a sparse representation, the computation inefficiency problem of A-SVM is overcome. By arguing that it is more beneficial to transfer from a few relevant source domains rather than using all the source domains as in A-SVM and DAM, Duan et al. <ref type="bibr" target="#b79">[80]</ref> further design a new data-dependant regularizer in domain selection machine (DSM) for source domain selection</p><formula xml:id="formula_22">( f ) = 1 2 S s=1 d s m i=1 f T i -f s i 2 . (<label>13</label></formula><formula xml:id="formula_23">)</formula><p>Similar as γ s in <ref type="bibr" target="#b10">(11)</ref>, which is a predefined weight measuring the relevance between the sth source domain and the target domain, d s ∈ {0, 1} in ( <ref type="formula" target="#formula_22">13</ref>) is a domain selection indicator for the sth source domain. When the objective function is optimized, the value of d s is 1 if the sth source domain is relevant to the target domain, and the value of d s is 0 otherwise. Another advantage of DSM over most existing transfer learning methods is its ability to work when the source domains and the target domain are represented by different types of features, e.g., using static 2-D SIFT features to represent the source domain data and 3-D spatio-temporal (ST) features to represent the target domain data. The learning function of DSM can be formulated as</p><formula xml:id="formula_24">f (x) = f 2 D(x) + f 3 D(x) = S s=1 d s β s f s (x) + w ϕ(x) + b (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>where</p><formula xml:id="formula_26">f 2 D(x) = S s=1 d s β s f s (x)</formula><p>is a weighted combination of source classifiers based on SIFT features, β s is a real-valued weight for the sth source domain, f 3D (x) = w ϕ(x) + b is the adaptation error function of space-time features, ϕ(•) is a feature mapping function that maps x into ϕ(x), w is a weight vector, and b is a bias term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boosting-Based</head><p>As discussed in Section IV-B, TrAdaBoost relies only on one source domain, which makes it intrinsically vulnerable to negative samples in the source domain. To avoid such a problem, Yao and Doretto <ref type="bibr" target="#b80">[81]</ref> proposed two boosting approaches multisource-TrAdaBoost and task-TrAdaBoost for knowledge transfer with multiple source domains.</p><p>Multisource-TrAdaBoost is an extension of TrAdaBoost to multiple source domains. Instead of searching for a weak classifier by leveraging a single source domain, a mechanism is introduced to apply all the weak classifiers in the selected source domain that appears to be the most relevant to the target domain at the current iteration. Specifically, the training data of each source domain are combined with the training data in the target domain to generate a candidate weak classifier at each iteration, while all the source domains are considered independent from each other. Thus, the multisource-TrAdaBoost approach significantly reduces the effects of negative transfer caused by the imposition to knowledge transfer from a single source domain, which is potentially not relevant to the target domain.</p><p>On the other hand, task-TrAdaBoost is a parameter-transfer approach, that tries to identify which parameters that come from various source domains can be used. Task-TrAdaBoost is constituted of two separate phases. In phase-I, traditional AdaBoost is employed to extract suitable weak classifiers from each source domain, respectively, under the assumption that some parameters are shared between the source domain and the target domain. Thus, the source domain is described explicitly rather than implicitly with only the labeled source domain data. Phase-II runs the AdaBoost loop again over the target training data using the collection of all the candidate weak classifiers obtained from phase-I. At each iteration, the weak classifier with the lowest classification error on the target training data is picked out to ensure the knowledge being transferred is more relevant to the target task. In addition, the update of the weights on the target training data drives the search of the most helpful candidate classifiers in the next round for boosting the target classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multikernel Learning</head><p>There are many types of hidden knowledge that can be transferred across different visual domains, for example, the appearance or shape of an object part, (e.g., the shape of a wheel), local symmetries between parts, (e.g., the symmetry between front-and back-legs for quadrupeds), and the partially shared layout, (e.g., the layout of torso and limbs of a human). When employing knowledge transfer between the visual domains, though the shared knowledge exists among the target data and the source data, the exact type of knowledge that needs to be transferred is uncertain. Alternately, since these different types of knowledge can be represented by different features or different prior models, all types of knowledge can be considered by fusing these features or prior models when constructing the target model. Instead of using predefined weights for all the features or prior models, multikernel learning provides a more appropriate solution by learning the linear combination of coefficients of the prelearned classifiers to assure the minimization of domain mismatches.</p><p>Motivated by A-SVM, Duan et al. <ref type="bibr" target="#b81">[82]</ref> proposed an adaptive multiple kernel learning (A-MKL) method to cope with the considerable variation in feature distributions between videos from two domains. As described above, in A-SVM, the target classifier is adapted from an existing classifier trained with the source domain data. When A-SVM employs multiple source classifiers, those classifiers are fused with fixed weights. Different from A-SVM, A-MKL learns the optimal combination of coefficients corresponding to each prelearned classifier to minimize the mismatch between the data distributions of two domains under the MMD criterion.</p><p>The multimodel knowledge transfer (multi-KT) <ref type="bibr" target="#b65">[66]</ref> method modifies the l 2 -norm regularizer in the LS-SVM objective function and constrains the new hyperplane w to be close to hyperplanes of F prior models. The regularization term is given as w -F j =1 β j μ j , where μ j is the hyperplane of the j th model, and β j determines the amount of transfer from each model, while subjecting to the constraint that β 2 ≤ 1. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS For a sample x, the decision function is given by</p><formula xml:id="formula_27">s(x) = w • φ(x) + F j =1 β j μ j • φ(x). (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>While the solution to multi-KT is through two separate optimization problems, Jie et al. <ref type="bibr" target="#b82">[83]</ref> proposed a multiple kernel transfer learning (MKTL) method that learns the best hyperplanes and corresponding weights assigned to each prior model in a unified optimization process. The MKTL utilizes the prior knowledge as experts evaluating the new query instances and addresses such a knowledge transfer problem with a multikernel learning solver. In addition to the training sample x i , the prediction score s p (x i , z), z = 1, . . . , F (F is the total number of classes), predicted by the prior models are considered when learning the new model. The intuition behind such an idea is that if prior knowledge of a bicycle gives a high prediction score to images of a motorbike, this information may also be useful for the new model of motorbikes, since certain visual parts, (e.g., the wheels) are shared between the two categories. Priors are built over multiple features instead of only one, and meanwhile, different learning methods are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-View Multiple Source Adaptation</head><p>For the cross-view action recognition problem, some shared visual patterns (either spatial or ST) can exist in actions captured from more than one view-points, thus transferring knowledge from multiple source views to the target view is more beneficial rather than transferring from a single view.</p><p>Liu et al. <ref type="bibr" target="#b11">[12]</ref> apply the locally weighted ensemble (LWE) approach introduced in <ref type="bibr" target="#b44">[45]</ref> to fuse the multiple classification models. Specifically, for a set of prelearned models f 1 , f 2 , . . . , f k , the general Bayesian model averaging approach computes the posterior distribution of y as P(y|x) = k i=1 P(y|x, D, f i )P( f i |D), where P(y|x, D, f i ) = P(y|x, f i ) is the prediction made by each model and P( f i |D) is the posterior of model f i after observing the training set D. Considering the data distribution mismatch across the target domain and the source domains, the model prior for P( f i |T ) is incorporated, where T is the test set. By replacing P( f i |D) with P( f i |T ), the difference between the target and the source domains are considered during learning</p><formula xml:id="formula_29">P(y|x) = k i=1 w f i ,x P(y|x, f i ) (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>where w f i ,x = P( f i |x) is the true model weight that is locally adjusted for x representing the model's effectiveness on the target data. Li and Zickler <ref type="bibr" target="#b57">[58]</ref> achieve multiview fusion by aggregating the response values from the w MKL-SVM <ref type="bibr" target="#b68">[69]</ref> classifiers on their corresponding cross-view features x, beyond which a binary decision is made. Similar as the idea in MKTL <ref type="bibr" target="#b82">[83]</ref>, MKL-SVM solves a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels.</p><p>1) Discussion: The multiple source A-SVM is an intuitive extension of A-SVM that it assembles all the source domain classifiers by allocating a weight γ k to each source classifier. The DAM and DSM are proposed to overcome the disadvantages of multiple source A-SVM in both inefficiency and the failure of using unlabeled target domain data, where DSM precedes over DAM by filtering out those less relevant source domain data.</p><p>By introducing multiple source domains rather than one in both multisource-TrAdaBoost and task-TrAdaBoost, the first imperfection of TrAdaBoost has been compensated. The convergence properties of multisource-TrAdaBoost can be inherited directly from TrAdaBoost <ref type="bibr" target="#b20">[21]</ref>, whereas for task-TrAdaBoost they can be inherited directly from AdaBoost <ref type="bibr" target="#b73">[74]</ref>. It has been proved in <ref type="bibr" target="#b80">[81]</ref> that since the convergence rate of task-TrAdaBoost has a reduced upper bound compared with multisource-TrAdaBoost, it requires fewer iterations to converge.</p><p>Compared with A-SVM, the unlabeled data in the target domain are used in the MMD criterion of A-MKL, and the weights in the target classifier are learned automatically together with the optimal kernel combination. Calling the theorem in <ref type="bibr" target="#b83">[84]</ref>, for the binary-class classification of multi-KT, multi-KT is equivalent to multiple source A-SVM based on the Mahalanobis distance measure <ref type="bibr" target="#b84">[85]</ref>. Since the relationship between A-SVM and PMT-SVM is demonstrated in ( <ref type="formula" target="#formula_2">2</ref>)-( <ref type="formula" target="#formula_5">4</ref>), the connection between multi-KT and PMT-SVM can be naturally discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION, ANALYSIS, AND DISCUSSION</head><p>In general, there are three types of benefits that transfer learning can provide for performance improvements <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b85">[86]</ref>, including: 1) higher start-improved performance at the initial points; 2) higher slope-more rapid growth of performance; and 3) higher asymptote-leading to improved final performance. In the following, several simple experiments are conducted with some selected representative knowledge transfer techniques discussed above to make a comparison between these methods and to see whether they can meet the stated criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature-Level Knowledge Transfer Methods</head><p>Comparison between different feature representation crossview transfer learning methods is given in Tables <ref type="table" target="#tab_0">II</ref> and<ref type="table" target="#tab_0">III</ref>, where experiments are conducted on every possible pairwise view combination of the IXMAS data set (i.e., twenty combinations in total) and columns demonstrate the results of target views, while rows demonstrate the results of auxiliary training views. According to previous cross-view action recognition works, there are two different experimental settings, which are the correspondence mode and the partially labeled mode. In the correspondence mode, the leave-one-action-class-out scheme is applied, where one action class is considered as the orphan action in the target view, while all action videos of the selected class are excluded when establishing the correspondences. Approximately 30% of the nonorphan samples are randomly selected to serve as the correspondences, and  none of these correspondences are labeled. On the other hand, there are a small set of samples labeled in the partially labeled mode. We list the performance comparison of the above mentioned methods of the correspondence mode in Table <ref type="table" target="#tab_0">II</ref> and of the partially labeled mode in Table <ref type="table" target="#tab_0">III</ref>, respectively.</p><p>Seven pairwise view scenarios are shown in Table <ref type="table" target="#tab_0">II</ref>: 1) without (WO) transfer learning techniques <ref type="bibr" target="#b11">[12]</ref>; 2) using the method in <ref type="bibr" target="#b11">[12]</ref> with bilingual-words (BW); 3) using the method in <ref type="bibr" target="#b61">[62]</ref> with quantized aspect (QA); 4) using the method in <ref type="bibr" target="#b54">[55]</ref> with self-similarity metrics (SS); 5) using the method in <ref type="bibr" target="#b86">[87]</ref> with continuous model of aspect (CV); 6) using the method in <ref type="bibr" target="#b57">[58]</ref> with discriminative virtual views (VV); and 7) using the transferable dictionary pair in <ref type="bibr" target="#b58">[59]</ref> constructed by DL. According to Table <ref type="table" target="#tab_0">II</ref>, DL significantly outperforms the other methods and its most significant improvement over WO is 87% when treating Camera 0 as the source view and Camera 3 as the target view. Loosening the experimental restrictions by abandoning the correspondence instances from both views, while adding a small set of labeled training instances in the target view, comparisons between SVMSUT, AUGSVM, MIXSVM <ref type="bibr" target="#b87">[88]</ref>, VV, and DL are given in Table <ref type="table" target="#tab_0">III</ref>, where DL still achieves the best results with the most significant improvement of 88.7% over WO when treating Camera 0 as the source view and Camera 3 as the target view. In general, Camera 4 has relatively weak performance. The reason is that Camera 4 is set above the actors, so that actions are captured in a totally different view. On the other hand, the performance involving Camera 4 can effectively demonstrate the capability of a transfer learning system. The BW, VV, and DL significantly outperform QA, SS, and CV. However, one limitation for the former three lies in that they implicitly assume that the target view is known for a query sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classifier-Level Knowledge Transfer Methods</head><p>We conduct experiments on both image classification and action recognition tasks, where the PASCAL VOC 2007 data set <ref type="bibr" target="#b88">[89]</ref> is used for image classification and the UCF YouTube and HMDB51 data set <ref type="bibr" target="#b89">[90]</ref> are used for action recognition. The PASCAL VOC 2007 data set contains 20 object classes, including bird, bicycle, motorbike, and so on, among which we choose samples from the bicycle class and the motorbike class as positive samples of the target domain and the source domain, respectively, and samples from the remaining classes as negative testing samples in the target domain. The histogram of oriented gradients (HOG) features are extracted from each image by dividing each image into eight cells. The task is to learn a bicycle classifier to achieve a binary decision over whether the test sample belongs to the bicycle category or a different category. The target classifier is learned by transferring information from a motorbike classifier via the guidance of a few bicycle samples. We compare the methods of nontransfer SVM, A-SVM, PMT-SVM, DA-SVM, and MKTL in Table <ref type="table" target="#tab_3">IV</ref> with different numbers of training examples that vary from 1 to 25 with the interval of 3. Among these methods, DA-SVM achieves the best performance in terms of higher start and higher slope, while the PMT-SVM achieves the best final performance.</p><p>The UCF YouTube action data set is a realistic data set that contains camera shaking, cluttered background, variations in actors' scale, variations in illumination, and view point changes. There are 11 actions contained in the UCF YouTube data set, including biking, diving, golf swinging, and so on. The action recognition task aims at distinguishing actions between biking class and the diving with corresponding source domain actions from the HMDB51 data set, which is an even more challenging data set. Dense trajectories <ref type="bibr" target="#b90">[91]</ref> are extracted from raw action video sequences with eight spatial scales spaced by a factor of 1/ √ 2, and feature points are sampled on a grid spaced by five pixels and in each scale, separately. Each point at frame t tracked to the next frame t + 1 by median filtering in a dense optical flow field. To avoid the drifting problem, the length of a trajectory is limited to 15 frames. The HOG-HOF <ref type="bibr" target="#b91">[92]</ref> and MBH <ref type="bibr" target="#b92">[93]</ref> are computed within a 32 × 32 × 15 volume along the dense trajectories, where each volume is subdivided into a ST grid of size 2 × 2 × 3 to impose more structural information in the representation. The LLC coding scheme <ref type="bibr" target="#b93">[94]</ref> is applied to the low-level local dense trajectory features. We compare the methods of nontransfer SVM, A-SVM, PMT-SVM, and MKTL in Table <ref type="table">V</ref>. Obviously, the overall performance when transferring knowledge from the motorbike class to the bicycle class on the PASCAL VOC 2007 data set significantly outperforms the performance for transferring knowledge from the biking class to diving class on the UCF YouTube data set. This is due to that the relevance between motorbike and bicycle is much higher than the relevance between the actions biking and diving. In addition, the shared visual commons in video sequences are more difficult to capture than those in images. Compared with the results demonstrated in the image classification task, adding more training samples in the action recognition task leads to more significant improvements. As discussed in Section III, PMT-SVM is expected to outperform A-SVM in general. As shown in Tables IV and V, A-SVM outperforms PMT-SVM when a single or a few training instances are available, while PMT-SVM outperforms A-SVM in most cases when sufficient  We additionally conduct experiments on the action recognition task to compare the performance between the featurelevel knowledge transfer techniques (FR and DCDDL), the classifier-level knowledge transfer technique (A-SVM) and nonknowledge transfer techniques (LLC and K-SVD). The experiments are conducted using the same setting as described above on the UCF YouTube data set and the HMDB51 data set. The results are demonstrated in Table <ref type="table" target="#tab_4">VI</ref>. By comparing the results of nonknowledge transfer techniques LLC and K-SVD by brutally using the source domain data to the same techniques without the source domain data, we can conclude that brutal forcing the source domain data into the target task could degrade the performance of original learning systems. Among the listed techniques, the recently proposed cross-domain DL method DCDDL achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Knowledge Transfer From Multiple Source Domains</head><p>To demonstrate the performance comparisons between multisource knowledge transfer methods, we quote the experimental results in <ref type="bibr" target="#b79">[80]</ref> and <ref type="bibr" target="#b81">[82]</ref> for cross-domain multisource knowledge transfer, and the results in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b57">[58]</ref> for crossview multisource knowledge transfer.</p><p>Duan et al. <ref type="bibr" target="#b79">[80]</ref> chose the two large-scale image data sets to construct multiple source domains, where the first data set is the NUS-WIDE data set <ref type="bibr" target="#b95">[96]</ref>, which consists of 269, 648 images downloaded from the Flickr, and the second data set is collected from the photo forum called photosig.com, which contains about 1.3 million images. Three real-world consumer video data sets, the Kodak data set <ref type="bibr" target="#b81">[82]</ref>, the YouTube data set <ref type="bibr" target="#b79">[80]</ref>, and the CCV data set <ref type="bibr" target="#b96">[97]</ref>, are used as the target domains for performance evaluation, where the former contains 195 videos from six event classes, (e.g., birthday, picnic, parade, show, sports, and wedding), the middle is collected from YouTube using the same event classes as in the Kodak data set, and the latter contains 2726 videos for the same event classes. In the source domain, one hundred thousand training images are randomly selected from the two image sources and SIFT features are extracted from each image. After that, five source domains are constructed by randomly sampling 100 relevant images and 100 irrelevant images for clustering. In the target domain, both static SIFT features and space-time features are extracted from each video sequence in all the three data sets, where space-time interest point (STIP) feature and the Mel-frequency cepstral coefficients audio feature are extracted from the CCV data set, and three types of space-time features, HOG, HOF, and MBH, are extracted from Kodak and YouTube data sets. Since the standard SVM and DASVM cannot handle the domain adaptation problem when the data from the source and the target domain are with different feature types, only static SIFT features are used to learn classifiers in the target domain. The MAPs of SVM, DASVM, DAM, DSM sim , and DSM methods on the three data sets are show in Table <ref type="table" target="#tab_5">VII</ref>, where DSM sim , as a simplified version of DSM, only considers the ST features in the target domain. Compared with the standard SVM, DASVM, and DAM achieve worse or equivalent performance on all three data sets, which indicate that the source domain data are not successfully used by these two methods. Based on the observation that the DSM sim consistently outperforms the related DAM method, it clearly demonstrates the benefits of employing the selected relevant source domains rather than using all the source domains. The DSM method achieves the best performance on all three data sets, which further demonstrates the effectiveness of integrating static SIFT features and ST features. Experiments are also conducted in <ref type="bibr" target="#b81">[82]</ref> using the Kodak data set and videos downloaded from YouTube using keywords-based search to evaluate the performance of A-SVM, DTSVM, and A-MKL by transferring knowledge from the source image domains to the target video domain.</p><p>Table <ref type="table" target="#tab_6">VIII</ref> reports the means and the standard deviations of MAPs over all six events for the standard SVM, MKL, DTSVM, and A-MKL methods in the three cases, which are: 1) classifiers learned based on SIFT features; 2) classifiers learned based on ST features; and 3) classifiers learned based on both SIFT and ST features. Two forms of the standard SVM method, SVM-AT and SVM-T, are evaluated, where SVM-AT is learned based on samples from both the target domain and the source domain and SVM-T is learned based on samples only from the target domain. SVM-T outperforms SVM-AT in both cases 2) and 3), which indicates that directly, including the source domain knowledge may degrade the event recognition performances in the target domain. For all methods, the MAPs based on SIFT features are better than those based on ST features. This is consistent with our evaluation for classifier level knowledge transfer methods, IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS which indicates that the shared commons are more difficult to be captured in ST features than in static SIFT features. The effectiveness of fusing average classifiers and multiple base kernels is proved in A-MKL by providing the best performances for all cases.</p><p>The LWE fusing approach <ref type="bibr" target="#b11">[12]</ref> and MKL-SVM approach <ref type="bibr" target="#b57">[58]</ref> are compared with the SVMSUT, AUGSVM, MIXSVM methods on both the correspondence mode and the partially labeled mode in Table <ref type="table" target="#tab_0">IX</ref> for cross-view multisource knowledge transfer. The overall results in the correspondence mode significantly outperforms the results in the partially labeled mode. In the correspondence mode, LWE and MKL-SVM achieve equivalent performance, while in the partially labeled mode, MKL-SVM consistently leads to the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this survey, we have reviewed transfer learning techniques on visual categorization tasks. There are three types of knowledge that are useful for knowledge transfer: 1) source domain features; 2) source domain features and the corresponding labels; and 3) parameters of the prelearned source domain models, which indicate instance-based transfer learning, inductive transfer learning and parameter-based transfer learning, respectively. Through the performance comparisons between knowledge transfer techniques and nonknowledge transfer techniques, we can conclude that brutal forcing the source domain data for learning can degrade the performance of the original learning system, which demonstrates the significance of knowledge transfer. To transfer the source domain knowledge to the target domain, methods are designed from either the feature representation level or the classifier level. In general, the feature representation level knowledge transfer aims to unify the mismatched data in different visual domains to the same feature space and the classifier level knowledge transfer aims to learn a target classifier based on the parameters of prelearned source domain models, while considering the data smoothness in the target domain. Thus, the feature representation level knowledge transfer techniques belong to either instance-based transfer or inductive transfer, while most classifier level knowledge transfer techniques belong to the parameter-based transfer. To avoid transferring the negative knowledge and deal with the many-to-one adaptation problem, many strategies are proposed to learn a set of weights for each source domain to achieve multiple source domain knowledge fusion.</p><p>Transfer learning is a tool for improving the performance of the target domain model only in the case that the target domain labeled data are not sufficient, otherwise the knowledge transfer is meaningless. So far, most research on transfer learning only focuses on small scale data, which cannot well reflect the potential advantage of transfer learning over regular machine learning techniques. The future challenges of transfer learning should lie in two aspects: 1) how to mine the information that would be helpful for the target domain from highly noisy source domain data and 2) how to extend the existing transfer learning methods to deal with large-scale source domain data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Different ways of differentiating existing knowledge transfer approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Top row: cross-domain knowledge transfer scenario. In the target domain, the walking action performed by a single player in clean backgrounds comes from the KTH data set, while in the target domain, the walking action captured from much more complicated backgrounds with multiple players comes from the TRECVID data set. Bottom row: cross-view knowledge transfer scenario, where the target view data and the source view data are the same action captured from two different views of the IXMAS data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Similar as the feature representation level knowledge transfer, classifier-based knowledge transfer is another significant part of existing visual transfer learning techniques and it has attracted much attention in recent years. However, unlike the feature representation level knowledge transfer techniques, where only the training samples themselves in the source domain are adapted to the target learning framework, classifierbased knowledge transfer methods share the common trait that the learned source domain models are utilized as prior knowledge in addition to the training samples when learning the target model. Instead of minimizing the cross-domain dissimilarity by updating instances' representations, classifierbased knowledge transfer methods aim to learn a new model that minimizes the generalization error in the target domain via provided training instances from both domains and the learned model. We structure this section according to the following categories of classifier-based knowledge transfer techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Knowledge transfer from multiple auxiliary domains. (a) Auxiliary domain 1. (b) Auxiliary domain 2. (c) Target domain. The two subfigures on the left denote the two different auxiliary domain data and their corresponding decision boundaries, where auxiliary domain 1 is partitioned by a horizontal line and auxiliary domain 2 is partitioned by a vertical line. By brutally combining the decision boundaries from the two auxiliary domains, ambiguous predictions will be caused in the top-left region and the bottom-right region of the target domain.</figDesc><graphic coords="8,49.43,58.13,250.10,99.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MAIN</head><label>I</label><figDesc>CHARACTERISTICS OF LISTED FEATURE REPRESENTATION LEVEL KNOWLEDGE TRANSFER APPROACHES. AVAILABILITY OF BOTH TARGET DOMAIN LABELS, ADAPTATION TYPE, AND APPLICATIONS OF ALL STATED FEATURE REPRESENTATION LEVEL KNOWLEDGE TRANSFER METHODS ARE LISTED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON ON THE IMAGE CLASSIFICATION TASK BETWEEN SVM, A-SVM, PMT-SVM, DA-SVM, AND MKTL. MODELS ARE LEARNED WITH DIFFERENT NUMBERS OF TRAINING EXAMPLES OF THE BICYCLE CLASS AND THE MOTORBIKE CLASS AS THE SOURCE DOMAIN. FIRST ROW INDICATES THE NUMBER OF TRAINING SAMPLES USED IN THE SOURCE DOMAIN TABLE V PERFORMANCE COMPARISON ON THE ACTION RECOGNITION TASK BETWEEN SVM, A-SVM, PMT-SVM, AND MKTL MODELS ARE LEARNED WITH DIFFERENT NUMBERS OF TRAINING EXAMPLES OF THE BIKING CLASS AND THE DIVING CLASS AS THE SOURCE DOMAIN ON THE UCF YOUTUBE DATA SET. FIRST ROW INDICATES THE NUMBER OF TRAINING SAMPLES USED IN THE SOURCE DOMAIN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI RECOGNITION</head><label>VI</label><figDesc>RESULTS ON THE UCF YOUTUBE DATA SET WHEN USING THE HMDB51 DATA SET AS THE SOURCE DOMAIN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII MEAN</head><label>VII</label><figDesc></figDesc><table><row><cell>AVERAGE (MAPs) OF DASVM,</cell></row><row><cell>DAM, DSM SIM , AND DSM METHODS ON KODAK,</cell></row><row><cell>YOUTUBE, AND CCV DATA SETS</cell></row><row><cell>training instances are available. This can be explained as that</cell></row><row><cell>PMT-SVM is relatively more sensitive to bad training samples.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII MEANS</head><label>VIII</label><figDesc>AND STANDARD DEVIATIONS OF MAPs OVER SIX EVENTS FOR METHODS IN THREE CASES: 1) CLASSIFIERS LEARNED BASED ON SIFT FEATURES; 2) CLASSIFIERS LEARNED BASED ON ST FEATURES; AND 3) CLASSIFIERS LEARNED BASED ON BOTH SIFT AND ST FEATURES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://research.microsoft.com/∼zliu/ActionRecoRsrc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2162-237X © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.SHAO et al.: TRANSFER LEARNING FOR VISUAL CATEGORIZATION</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program of China (973 Program) under Grant 2012CB316400, in part by the University of Sheffield, Sheffield, U.K., and in part by the National Natural Science Foundation of China under Grants 61125106 and 61072093.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ling Shao (M'09-SM <ref type="bibr">'10)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boosted key-frame selection and correlated pyramidal motion-feature representation for human action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1810" to="1818" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep and wide: A spectral method for learning deep networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2014.2308519</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning object-to-class kernels for scene classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3241" to="3253" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatio-temporal Laplacian pyramid coding for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="817" to="827" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient search and localization of human actions in video databases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="512" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view action recognition using local similarity random forests and sensor fusion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transfer latent variable model based on divergence analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2358" to="2366" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-sequence learning of human actions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montañés</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Workshop Human Behavior Unterstand</title>
		<meeting>2nd Int. Workshop Human Behavior Unterstand<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One shot learning gesture recognition from RGBD images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit. Workshops<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>24th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="3209" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge transfer in learning to recognize visual objects classes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Develop. Learn</title>
		<meeting>5th Int. Conf. Develop. Learn<address><addrLine>Bloomington, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-dataset action detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>23rd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1998" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int. Conf. Pattern Recognit</title>
		<meeting>17th Int. Conf. Pattern Recognit<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation campaigns and TRECVid</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ACM Int. Workshop Multimedia Inform</title>
		<meeting>8th ACM Int. Workshop Multimedia Inform<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive SVMs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM Int. Conf. Multimedia</title>
		<meeting>15th ACM Int. Conf. Multimedia<address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards cross-category knowledge propagation for learning visual concepts</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>24th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. Mach. Learn</title>
		<meeting>24th Int. Conf. Mach. Learn<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>22nd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Hough transform-based voting framework for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>23rd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2061" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiview spectral embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1438" to="1446" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dyadic transfer learning for cross-domain image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE Int. Conf. Comput. Vis</title>
		<meeting>13th IEEE Int. Conf. Comput. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The relationships among various nonnegative matrix factorization methods for clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Int. Conf. Data Mining</title>
		<meeting>6th IEEE Int. Conf. Data Mining<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>12th ACM Int. Conf. Knowl. Discovery Data Mining<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">Aug. 2006</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer learning via dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Nat. Conf. Artif. Intell. (AAAI)</title>
		<meeting>23rd Nat. Conf. Artif. Intell. (AAAI)<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">Jul. 2008</date>
			<biblScope unit="page" from="677" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1633" to="1685" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust visual domain adaptation with low-rank reconstruction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2168" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="e57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Meeting Assoc</title>
		<meeting>45th Meeting Assoc<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SIFT-bag kernel for video event analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th ACM Int. Conf. Multimedia</title>
		<meeting>16th ACM Int. Conf. Multimedia<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-domain activity recognition</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Ubiquitous Comput</title>
		<meeting>11th Int. Conf. Ubiquitous Comput<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlocal hierarchical dictionary learning using wavelets for image denoising</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4689" to="4698" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From heuristic optimization to dictionary learning: A review and comprehensive comparison of image denoising algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1001" to="1013" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient dictionary learning for visual categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-taught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. Mach. Learn</title>
		<meeting>24th Int. Conf. Mach. Learn<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">Jul. 2006</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics Intell. Lab. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Workshop Neural Netw. Signal Process</title>
		<meeting>12th IEEE Workshop Neural Netw. Signal ess<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Task-specific gesture analysis in real-time using interpolated views</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1236" to="1242" />
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3-D model-based tracking of humans in action: A multi-view approach</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>9th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-06">Jun. 1996</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single view human action recognition using key pose matching and Viterbi path searching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>20th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3D exemplars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Comput. Vis</title>
		<meeting>11th Int. Conf. Comput. Vis<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">View-invariant representation and recognition of actions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">View invariance for human action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recognizing action events from multiple viewpoints</title>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Detect. Recognit. Events Video</title>
		<meeting>Workshop Detect. Recognit. Events Video<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Actions sketch: A novel action representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>18th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the use of anthropometry in the invariant analysis of human actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int. Conf. Pattern Recognit</title>
		<meeting>17th Int. Conf. Pattern Recognit<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="923" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal self-similarities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Comput. Vis</title>
		<meeting>10th Eur. Conf. Comput. Vis<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="293" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Comput. Vis</title>
		<meeting>10th IEEE Int. Conf. Comput. Vis<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">View-invariant analysis of cyclic motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="251" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for cross-view action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2855" to="2862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a transferable dictionary pair</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd British Mach. Vis. Conf</title>
		<meeting>23rd British Mach. Vis. Conf<address><addrLine>Surrey, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Correspondence-free dictionary learning for crossview action recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Pattern Recognit</title>
		<meeting>22nd Int. Conf. Pattern Recognit<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transfer learning for image classification with sparse prototype representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>21st IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Comput. Vis</title>
		<meeting>10th Eur. Conf. Comput. Vis<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE Int. Conf. Comput. Vis</title>
		<meeting>13th IEEE Int. Conf. Comput. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="2252" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>21st IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Safety in numbers: Learning categories from few examples with multi model knowledge transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>23rd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3081" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf. Neural Netw</title>
		<meeting>IEEE Int. Joint Conf. Neural Netw<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">Jul. 2006</date>
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">SimpleMKL</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Domain transfer SVM for video concept detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>22nd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1375" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Conf. Neural Inform</title>
		<meeting>20th Conf. Neural Inform</meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero/one training example</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>11th Eur. Conf. Comput. Vis. (ECCV)<address><addrLine>Hersonissos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="127" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning author-topic models from text corpora</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inform. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Knowledgeleverage-based TSK fuzzy system modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Knowledgeleverage-based fuzzy system and its modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="597" to="609" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>25th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1338" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning with multiple sources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>23rd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1855" to="1862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multiclass transfer learning from unconstrained priors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE Int. Conf. Comput. Vis</title>
		<meeting>13th IEEE Int. Conf. Comput. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1863" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">SVM versus least squares SVM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf</title>
		<meeting>7th Int. Conf<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">Apr. 2007</date>
			<biblScope unit="page" from="644" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B M</forename><surname>Sober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lopez</surname></persName>
		</author>
		<title level="m">Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods and Techniques</title>
		<meeting><address><addrLine>Hershey, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Information Science IGI Publishing</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A latent model of discriminative aspect</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabrizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="948" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: A domain adaptation approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Conf</title>
		<meeting>24th Conf<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results [Online</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE Int. Conf. Comput. Vis</title>
		<meeting>13th IEEE Int. Conf. Comput. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>24th IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>21st IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Comput. Vis</title>
		<meeting>9th Eur. Conf. Comput. Vis<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>23rd IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">NUS-WIDE: A real-world web image database from National University of Singapore</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Image Video Retrieval</title>
		<meeting>ACM Int. Conf. Image Video Retrieval<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">Jul. 2009</date>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Consumer video understanding: A benchmark database and an evaluation of human and machine performance</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ACM Int. Conf. Multimedia Retrieval</title>
		<meeting>1st ACM Int. Conf. Multimedia Retrieval<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
