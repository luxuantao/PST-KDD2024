<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
							<email>dgillick@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
							<email>dbikel@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@google.com</email>
						</author>
						<title level="a" type="main">MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as "class prototypes" as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor's entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A contemporary approach to entity linking represents each entity with a textual description d e , encodes these descriptions and contextualized mentions of entities, m, into a shared vector space using dual-encoders f (m) and g(d e ), and scores each mention-entity pair as the inner-product between their encodings <ref type="bibr" target="#b4">(Botha et al., 2020;</ref><ref type="bibr" target="#b27">Wu et al., 2019)</ref>. By restricting the interaction between e and m to an inner-product, this approach permits the pre-computation of all g(d e ) and fast retrieval of top scoring entities using maximum inner-product search (MIPS).</p><p>Here we begin with the observation that many entities appear in diverse contexts, which may not be easily captured in a single high-level description. For example, Actor Tommy Lee Jones played football in college, but this fact is not captured in the entity description derived from his Wikipedia page (see Figure <ref type="figure" target="#fig_0">1</ref>). Furthermore, when new entities need to be added to the index in a zero-shot setting, it may be difficult to obtain a high quality description. We propose that both problems can be solved by allowing the entity mentions themselves to serve as exemplars. In addition, retrieving from the set of mentions can result in more interpretable predictions -since we are directly comparing two mentions -and allows us to leverage massively multilingual training data more easily, without forcing choices about which language(s) to use for the entity descriptions.</p><p>We present a new approach (MOLEMAN 1 ) that maintains the dual-encoder architecture, but with the same mention-encoder on both sides. Entity linking is modeled entirely as a mapping between mentions, where inference involves a nearest neighbor search against all known mentions of all entities in the training set. We build MOLEMAN using exactly the same mention-encoder architecture and training data as Model F <ref type="bibr" target="#b4">(Botha et al., 2020)</ref>. We show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and <ref type="bibr" target="#b24">Tsai and Roth (2016)</ref> datasets, particularly for low-coverage languages, and rarer entities.</p><p>We also observe that MOLEMAN achieves high accuracy with just a few mentions for each entity, suggesting that new entities can be added or existing entities can be modified simply by labeling a small number of new mentions. We expect this update mechanism to be significantly more flexible than writing or editing entity descriptions. Finally, we compare the massively multilingual MOLEMAN model to a much more expensive English-only dualencoder architecture <ref type="bibr" target="#b27">(Wu et al., 2019)</ref> on the wellstudied TACKBP-2010 dataset <ref type="bibr" target="#b12">(Ji et al., 2010)</ref> and show that MOLEMAN is competitive even in this setting.  'Tommy Lee Jones (Q170587)' and 'Tom Jones (Q18152778). The query mention [ ] pertains to the former's college football career, which is unlikely to be captured by the high-level entity description. A retrieval against descriptions would get this query incorrect, but with indexed mentions gets it correct. Note that prior dual-encoder models that use a single vector to represent each entity are forced to contort the embedding space to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Task definition We train a model that performs entity linking by ranking a set of entity-linked indexed mentions-in-context. Formally, let a mentionin-context x = [x 1 , ..., x n ] be a sequence of n tokens from vocabulary V, which includes designated entity span tokens. An entity-linked mentionin-context m i = (x i , e i ) pairs a mention with an entity from a predetermined set of entities E. Let M I = [m 1 , ..., m k ] be a set of entity-linked mentions-in-context, and let entity(•) : M I → E be a function that returns the entity e i ∈ E associated with m i , and x(•) returns the token sequence x i .</p><p>Our goal is to learn a function φ(m) that maps an arbitrary mention-in-context token sequence m to a fixed vector h m ∈ R d with the property that</p><formula xml:id="formula_0">y * = entity argmax m ∈M I [φ(x(m )) T φ(x q )]</formula><p>gives a good prediction y * of the true entity label of a query mention-in-context x q .</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Recent state-of-the-art entity linking systems employ a dual encoder architecture, embedding mentions-in-context and entity representations in the same space. We also employ a dual encoder architecture but we score mentions-in-context (hereafter, mentions) against other mentions, with no consolidated entity representations. The dual encoder maps a pair of mentions (m, m ) to a score:</p><formula xml:id="formula_1">s(m, m ) = φ(m) T φ(m ) φ(m) φ(m )</formula><p>where φ is a learned neural network that encodes the input mention as a d-dimensional vector.</p><p>As in <ref type="bibr" target="#b7">(Févry et al., 2020)</ref> and <ref type="bibr" target="#b4">(Botha et al., 2020)</ref>, our mention encoder is a 4-layer BERT-based Transformer network <ref type="bibr" target="#b25">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2019)</ref> with output dimension d = 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Mention Pairs Dataset</head><p>We build a dataset of mention pairs using the 104language collection of Wikipedia mentions as constructed by <ref type="bibr" target="#b4">Botha et al. (2020)</ref>. This dataset maps Wikipedia hyperlinks to WikiData <ref type="bibr" target="#b26">(Vrandečić and Krötzsch, 2014)</ref>, a language-agnostic knowledge base. We create mention pairs from the set of all mentions that link to a given entity.</p><p>We use the same division of Wikipedia pages into train and test splits used by <ref type="bibr" target="#b4">Botha et al. (2020)</ref> for compatibility to the TR2016 test set <ref type="bibr" target="#b24">(Tsai and Roth, 2016)</ref>. We take up to the first 100k mention pairs from a randomly ordered list of all pairs regardless of language, yielding 557M and 31M training and evaluation pairs, respectively. Of these, 69.7% of pairs involve two mentions from different languages. Our index set contains 651M mentions, covering 11.6M entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hard Negative Mining and Positive Resampling</head><p>Previous work using a dual encoder trained with inbatch sampled softmax has improved performance with subsequent training rounds using an auxiliary cross-entropy loss against hard negatives sampled from the current model <ref type="bibr" target="#b8">(Gillick et al., 2019;</ref><ref type="bibr" target="#b27">Wu et al., 2019;</ref><ref type="bibr" target="#b4">Botha et al., 2020)</ref>. We investigate the effect of such negative mining for MOLEMAN, controlling the ratio of positives to negatives on a per-entity basis. This is achieved by limiting each entity to appear as a negative example at most 10 times as often as it does in positive examples, as done by <ref type="bibr" target="#b4">Botha et al. (2020)</ref>.</p><p>In addition, since MOLEMAN is intended to retrieve the most similar indexed mention of the correct entity, we experiment with using this retrieval step to resample the positive pairs used to construct our mention-pair dataset for the in-batch sampled softmax, pairing each mention m with the highestscoring other mention m of the same entity in the index set. This is similar to the index refreshing that is employed in other retrieval-based methods trained with in-batch softmax <ref type="bibr" target="#b11">(Guu et al., 2020;</ref><ref type="bibr" target="#b17">Lewis et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Input Representations</head><p>Following prior work <ref type="bibr" target="#b27">(Wu et al., 2019;</ref><ref type="bibr" target="#b4">Botha et al., 2020)</ref>, our mention representation consists of the page title and a window around the mention, with special mention boundary tokens marking the mention span. We use a total context size of 64 tokens.</p><p>Though our focus is on entity mentions, the entity descriptions can still be a useful additional source of data, and allow for zero-shot entity linking (when no mentions of an entity exist in our training set). We therefore experiment with adding the available entity descriptions as additional "pseudo-mentions". These are constructed in a similar way to the mention representations, except without mention boundaries. Organic and psuedo-mentions are fed into BERT using distinct sets of token type identifiers. We supplement our training set with additional mention pairs formed from each entity's description and a random mention, adding 38M training pairs, and add these descriptions to the index, expanding the entity set to 20M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>For inference, we perform a distributed brute-force maximum inner product search over the index of training mentions. During this search, we can either return only the top-scoring mention for each entity, which improves entity-based recall, or else all mentions, which allows us to experiment with k-Nearest Neighbors inference (see Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mewsli-9</head><p>Table <ref type="table">1</ref> shows our results on the Mewsli-9 dataset compared to the models described by <ref type="bibr" target="#b4">Botha et al. (2020)</ref>. Model F is a dual encoder which scores  <ref type="table" target="#tab_3">3</ref>).</p><formula xml:id="formula_2">I HN R@1 R@10 R@</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Per-Language Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows per-language results for Mewsli-9. A key motivation of <ref type="bibr" target="#b4">Botha et al. (2020)</ref> was to learn a massively multilingual entity linking system, with a shared context encoder and entity representations between 104 languages in the Wikipedia corpus. MOLEMAN takes a step further: the indexed mentions from all languages are included in the retrieval index, and can contribute to the prediction in any language. In fact, we find that for 21.4% of mentions in the Mewsli-9 corpus, MOLEMAN's top prediction came from a different language.</p><p>Language R@1 R@10 R@100 ar +1.1 +0.9  <ref type="bibr">et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Frequency Breakdown</head><p>Table <ref type="table" target="#tab_3">3</ref> shows a breakdown in performance by entity frequency bucket, defined as the number of times an entity was mentioned in the Wikipedia training set. When indexing only mentions, MOLE-MAN can never predict the entities in the 0 bucket, but it shows significant improvement in the other frequency bands, particularly in the "few shot" bucket of <ref type="bibr">[1,</ref><ref type="bibr">10)</ref>. This suggests when introducing new entities to the index, labelling a small number of mentions may be more beneficial than producing a single description. To further confirm this intuition, we retrained MOLEMAN with a modified training set which had all entities in the [1, 10) band of Mewsli-9 removed, and only added to the index at inference time. This model achieved +0.2 R@1 and +5.6 R@10 relative to Model F + (which was trained with these entities in the train set). When entity descriptions are added to the index, MOLEMAN outperforms Model F + across frequency bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Inference Efficiency</head><p>Due to the large size of the mention index, nearest neighbor inference is performed using distributed maximum inner-product search. We also experiment with approximate search using ScaNN <ref type="bibr" target="#b10">(Guo et al., 2020)</ref>. Table <ref type="table">4</ref> shows throughput and recall statistics for brute force search as well as two approximate search approaches that run on a single multi-threaded CPU, showing that inference over such a large index can be made extremely efficient with minimal loss in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tsai Roth 2016 Hard</head><p>In order to compare against previous multilingual entity linking models, we report results on the "hard" subset of Tsai and Roth (2016)'s crosslingual dataset which links 12 languages to English Wikipedia. Table <ref type="table" target="#tab_4">5</ref> shows our results on the same 4 languages reported by <ref type="bibr" target="#b4">Botha et al. (2020)</ref>. MOLE-MAN outperforms all previous systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TACKBP 2010</head><p>Recent work on entity linking have employed dualencoders primarily as a retrieval step before reranking with a more expensive cross-encoder <ref type="bibr" target="#b27">(Wu et al., 2019;</ref><ref type="bibr" target="#b1">Agarwal and Bikel, 2020)</ref>. Table <ref type="table">6</ref> shows results on the extensively studied TACKBP 2010 dataset <ref type="bibr" target="#b12">(Ji et al., 2010)</ref>. <ref type="bibr" target="#b27">Wu et al. (2019)</ref> used a 24-layer BERT-based dual-encoder which scores the 5.9 million entity descriptions from English Wikipedia, followed by a 24-layer cross-encoder reranker. MOLEMAN does not achieve the same level of top-1 accuracy as their full model, as it lacks the expensive cross-encoder reranking step, but despite using a single, much smaller Transformer and indexing the larger set of entities from multilingual Wikipedia, it outperforms this prior work in retrieval recall at 100. We also report the accuracy of a MOLEMAN model trained only with English training data, and using an Enlish-only index for inference. This experiment shows that although the multilingual index contributes to MOLEMAN's overall performance, the pairwise training data is sufficient for high performance in a monolingual setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>We have recast the entity linking problem as an application of a more generic mention encoding task. This approach is related to methods which perform clustering on test mentions in order to improve inference <ref type="bibr" target="#b16">(Le and Titov, 2018;</ref><ref type="bibr" target="#b2">Angell et al., 2020)</ref>, and can also be viewed as a form of crossdocument coreference resolution <ref type="bibr" target="#b21">(Rao et al., 2010;</ref><ref type="bibr" target="#b23">Shrimpton et al., 2015;</ref><ref type="bibr" target="#b3">Barhom et al., 2019)</ref>. We also take inspiration from recent instance-based language modelling approaches <ref type="bibr" target="#b14">(Khandelwal et al., 2020;</ref><ref type="bibr" target="#b18">Lewis et al., 2020b)</ref>.</p><p>Our experiments demonstrate that taking an instance-based approach to entity-linking leads to better retrieval performance, particularly on rare entities, for which adding a small number of mentions leads to superior performance than a single description. For future work, we would like to explore the application of this instance-based approach to entity knowledge related tasks <ref type="bibr" target="#b22">(Seo et al., 2018;</ref><ref type="bibr">Petroni et al., 2020)</ref>, and to entity discovery <ref type="bibr" target="#b13">(Ji et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOLEMAN (mentions only)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOLEMAN (+ descriptions) mGENRE</head><p>Freq. bin R@1 R@10 R@1 R@10 R@1 [0, 1) -8. Model F+ MOLEMAN MOLEMAN (mentions only) (+ descriptions) Language R@1 R@10 R@100 R@1 R@10 R@100 R@1 R@10 R@100 ar 92. Model F+ MOLEMAN MOLEMAN (mentions only) (+description) Bin Queries R@1 R@10 R@100 R@1 R@10 R@100 R@1 R@10 R@100 [0, 1) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of hypothetical contextualized mention (m) and multilingual description (d) embeddings for the entities</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,453.55,94.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MOLEMAN results on the Mewsli-9 dataset by language, listed as a delta against Model F + (Botha</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>+0.3</cell></row><row><cell>de</cell><cell>-0.1</cell><cell>+1.5</cell><cell>+0.5</cell></row><row><cell cols="2">en +0.3</cell><cell>+2.8</cell><cell>+2.3</cell></row><row><cell>es</cell><cell>-0.2</cell><cell>+1.1</cell><cell>+0.4</cell></row><row><cell cols="2">fa +1.1</cell><cell>+0.9</cell><cell>+0.9</cell></row><row><cell cols="2">ja +0.8</cell><cell>+1.2</cell><cell>+0.5</cell></row><row><cell>sr</cell><cell>-0.1</cell><cell>+0.8</cell><cell>+0.5</cell></row><row><cell cols="2">ta +3.7</cell><cell>+1.3</cell><cell>+0.6</cell></row><row><cell cols="2">micro-avg +0.2</cell><cell>+1.6</cell><cell>+1.0</cell></row><row><cell cols="2">macro-avg +0.8</cell><cell>+1.3</cell><cell>+0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results from MOLEMAN (with and without the inclusion of entity descriptions) on the Mewsli-9 dataset, by entity frequency in the training set plotted as a delta against Model F + . †Note that when using mentions only, MOLEMAN scores zero on entities that do not appear in the training set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3 †</cell><cell>-33.9 †</cell><cell>-0.2</cell><cell>+18.3</cell><cell>+13.8</cell></row><row><cell></cell><cell></cell><cell>[1, 10)</cell><cell>+0.4</cell><cell>+5.6</cell><cell>+1.7</cell><cell>+9.3</cell><cell>-10.4</cell></row><row><cell></cell><cell></cell><cell>[10, 100)</cell><cell>+1.9</cell><cell>+3.8</cell><cell>+1.7</cell><cell>+3.7</cell><cell>-3.1</cell></row><row><cell></cell><cell></cell><cell>[100, 1k)</cell><cell>+0.1</cell><cell>+1.8</cell><cell>-0.0</cell><cell>+1.9</cell><cell>+0.3</cell></row><row><cell></cell><cell></cell><cell>[1k, 10k)</cell><cell>-1.1</cell><cell>+0.7</cell><cell>-1.2</cell><cell>+0.7</cell><cell>+0.6</cell></row><row><cell></cell><cell></cell><cell>[10k,+)</cell><cell>+0.7</cell><cell>+0.6</cell><cell>+0.7</cell><cell>+0.5</cell><cell>+2.2</cell></row><row><cell></cell><cell></cell><cell cols="2">macro-avg -1.1</cell><cell>-3.6</cell><cell>+0.5</cell><cell>+5.7</cell><cell>+0.6</cell></row><row><cell></cell><cell cols="4">QPS Latency (ms) R@1 R@100</cell><cell></cell></row><row><cell>Brute-force</cell><cell>9.5</cell><cell>5727</cell><cell>89.9</cell><cell>99.2</cell><cell></cell></row><row><cell>ScaNN</cell><cell>8000</cell><cell>2.9</cell><cell>89.9</cell><cell>99.1</cell><cell></cell></row><row><cell cols="5">Table 4: Max throughput (queries per second), latency</cell><cell></cell></row><row><cell cols="5">(ms per query) and recall for brute force inference</cell><cell></cell></row><row><cell cols="5">and approximate MIPS inference using the ScaNN li-</cell><cell></cell></row><row><cell cols="5">brary (Guo et al., 2020). See Appendix A.3 for further</cell><cell></cell></row><row><cell>details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MF+ MM</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">de 0.62 0.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">es 0.58 0.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">fr 0.54 0.58</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">it 0.56 0.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Avg 0.57 0.60</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy results on the TR2016 hard test set for Model F + (MF+) and MOLEMAN(MM)    </figDesc><table><row><cell>Method</cell><cell cols="2">R@1 R@100</cell></row><row><cell>AT-Prior</cell><cell>-</cell><cell>89.5</cell></row><row><cell>AT-Ext</cell><cell>-</cell><cell>91.7</cell></row><row><cell>BM25</cell><cell>-</cell><cell>68.9</cell></row><row><cell>Gillick et al. (2019)</cell><cell>-</cell><cell>96.3</cell></row><row><cell>Wu et al. (2019)</cell><cell cols="2">91.5 † 98.3  *</cell></row><row><cell cols="2">MOLEMAN (EN-only) 85.8</cell><cell>98.4</cell></row><row><cell>MOLEMAN</cell><cell>87.9</cell><cell>99.1</cell></row><row><cell cols="3">Table 6: Retrieval comparison on TACKBP-2010. The</cell></row><row><cell cols="3">alias table and BM25 baselines are taken from Gillick</cell></row><row><cell cols="3">et al. (2019). For comparison to Wu et al. (2019), we</cell></row><row><cell cols="3">report R@1 for their "full Wiki, w/o finetune" cross-</cell></row><row><cell cols="3">encoder. Their R@100 model is a dual-encoder fine-</cell></row><row><cell cols="3">tuned on the TACKBP-2010 training set. MOLEMAN is</cell></row><row><cell>not finetuned.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results on the Mewsli-9 dataset by language.</figDesc><table><row><cell>3</cell><cell>97.7</cell><cell>99.1</cell><cell>93.4</cell><cell>98.6</cell><cell>99.0</cell><cell>93.4</cell><cell>98.6</cell><cell>99.4</cell></row><row><cell>de 91.5</cell><cell>97.3</cell><cell>99.0</cell><cell>91.3</cell><cell>98.2</cell><cell>98.9</cell><cell>91.5</cell><cell>98.9</cell><cell>99.5</cell></row><row><cell>en 87.2</cell><cell>94.2</cell><cell>96.7</cell><cell>87.4</cell><cell>95.9</cell><cell>97.4</cell><cell>87.4</cell><cell>97.0</cell><cell>99.3</cell></row><row><cell>es 89.0</cell><cell>97.4</cell><cell>98.9</cell><cell>88.7</cell><cell>98.1</cell><cell>98.8</cell><cell>88.7</cell><cell>98.5</cell><cell>99.3</cell></row><row><cell>fa 91.8</cell><cell>97.4</cell><cell>98.7</cell><cell>93.5</cell><cell>98.5</cell><cell>99.1</cell><cell>92.9</cell><cell>98.3</cell><cell>99.6</cell></row><row><cell>ja 87.8</cell><cell>95.6</cell><cell>97.6</cell><cell>88.7</cell><cell>96.2</cell><cell>97.0</cell><cell>88.5</cell><cell>96.8</cell><cell>98.0</cell></row><row><cell>sr 92.6</cell><cell>98.2</cell><cell>99.2</cell><cell>92.2</cell><cell>98.7</cell><cell>99.5</cell><cell>92.5</cell><cell>99.0</cell><cell>99.7</cell></row><row><cell>ta 87.6</cell><cell>97.4</cell><cell>98.9</cell><cell>91.5</cell><cell>98.4</cell><cell>99.1</cell><cell>91.3</cell><cell>98.6</cell><cell>99.5</cell></row><row><cell>micro-avg 89.4</cell><cell>96.4</cell><cell>98.2</cell><cell>89.5</cell><cell>97.4</cell><cell>98.3</cell><cell>89.6</cell><cell>98.0</cell><cell>99.2</cell></row><row><cell>macro-avg 89.8</cell><cell>96.9</cell><cell>98.5</cell><cell>90.6</cell><cell>97.8</cell><cell>98.5</cell><cell>90.6</cell><cell>98.2</cell><cell>99.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results on the Mewsli-9 dataset, by entity frequency in the test set.</figDesc><table><row><cell></cell><cell>3,198</cell><cell>8.3</cell><cell>33.9</cell><cell>62.7</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>8.1</cell><cell>52.2</cell><cell>74.7</cell></row><row><cell>[1, 10)</cell><cell cols="2">6,564 57.7</cell><cell>80.8</cell><cell>91.3</cell><cell>58.1</cell><cell>86.4</cell><cell>93.3</cell><cell>59.4</cell><cell>90.1</cell><cell>96.5</cell></row><row><cell>[10, 100)</cell><cell cols="2">32,371 80.4</cell><cell>92.8</cell><cell>96.7</cell><cell>82.2</cell><cell>96.5</cell><cell>98.8</cell><cell>82.1</cell><cell>96.5</cell><cell>98.9</cell></row><row><cell>[100, 1k)</cell><cell cols="2">66,232 89.6</cell><cell>96.6</cell><cell>98.2</cell><cell>89.7</cell><cell>98.4</cell><cell>99.5</cell><cell>89.6</cell><cell>98.5</cell><cell>99.5</cell></row><row><cell>[1k, 10k)</cell><cell cols="2">78,519 92.9</cell><cell>98.4</cell><cell>99.3</cell><cell>91.9</cell><cell>99.2</cell><cell>99.8</cell><cell>91.8</cell><cell>99.1</cell><cell>99.8</cell></row><row><cell>[10k, +)</cell><cell cols="2">102,203 94.1</cell><cell>98.8</cell><cell>99.4</cell><cell>94.8</cell><cell>99.4</cell><cell>99.6</cell><cell>94.8</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell>micro-avg</cell><cell></cell><cell>89.4</cell><cell>96.4</cell><cell>98.2</cell><cell>89.5</cell><cell>97.4</cell><cell>98.3</cell><cell>89.6</cell><cell>98.0</cell><cell>99.2</cell></row><row><cell>macro-avg</cell><cell></cell><cell>70.5</cell><cell>83.5</cell><cell>91.3</cell><cell>69.4</cell><cell>80.0</cell><cell>81.8</cell><cell>70.9</cell><cell>89.3</cell><cell>94.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://github.com/google-research/ google-research/tree/master/scann</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Ming-Wei Chang, Livio Baldini-Soares and the anonymous reviewers for their helpful feedback. We also thank Dave Dopson for his extensive help with profiling the brute-force and approximate search inference.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Profiling Details</head><p>The brute-force numbers we've reported are the theoretical maximum throughput for computing 300D dot-products on an AVX-512 processor running at 2.2Ghz, and are thus an overly optimistic baseline. Practical implementations, such as the one in ScaNN, must also compute the top-k and rarely exceed 70% to 80% of this theoretical limit. The brute-force latency figure is the minimum time to stream the database from RAM using 144 GiB/s of memory-bandwidth. In practice, we ran distributed brute-force inference on a large cluster of CPUs, which took about 5 hours.</p><p>The numbers for ScaNN are empirical singlemachine benchmarks of an internal solution that uses the open-source ScaNN library 4 a single 24-core CPU. We use ScaNN to search a multilevel tree that has the following shape: 78, 000 =&gt; 83 : 1 =&gt; 105 : 1 (687.3 million datapoints). We used a combination of several different anisotropic vector quantizations that combine 3, 6, 12, or 24 dimensions per 4-bit code, as well as re-scoring with an int8-quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Expanded experimental results</head><p>Tables <ref type="table">7 and 8</ref> present complete numerical comparisons between MOLEMAN and Model F + on Mewsli-9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Entity linking via dual and cross-attention encoders</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Clusteringbased inference for zero-shot biomedical entity linking</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Angell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Monath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11253</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Revisiting joint modeling of cross-document entity and event coreference resolution</title>
		<author>
			<persName><forename type="first">Shany</forename><surname>Barhom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Eirew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bugert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Nils Reimers, and Ido Dagan</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Entity linking in 100 languages</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Plekhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12528</idno>
		<title level="m">Multilingual autoregressive entity linking</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of pretraining strategies for supervised entity linking</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><forename type="middle">Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">End-to-end retrieval in continuous space</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08008</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Overview of the TAC 2010 knowledge base population track</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<idno>TAC 2010</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Overview of tac-kbp2017 13 languages entity discovery and linking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cash</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sydney</forename><forename type="middle">Informatics</forename><surname>Hub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020a. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<title level="m">Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2020. KILT: a Benchmark for Knowledge Intensive Language Tasks</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Streaming cross document entity coreference resolution</title>
		<author>
			<persName><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2010</title>
				<imprint>
			<publisher>Posters</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Phraseindexed question answering: A new challenge for scalable document comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sampling techniques for streaming cross document coreference resolution</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Shrimpton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">1 Training setup and hyperparameters To isolate the impact of representing entities with multiple mention embeddings, we follow the training methodology and hyperparameter choices presented in</title>
		<author>
			<persName><surname>Gillick</surname></persName>
		</author>
		<ptr target=".com/google-research/bert/multi_cased_L-12_H-768_A-123cloud.google.com/tpu/docs/tpus" />
	</analytic>
	<monogr>
		<title level="m">A Appendices A</title>
				<editor>
			<persName><surname>Botha</surname></persName>
		</editor>
		<imprint>
			<publisher>Kingma and Ba</publisher>
			<date type="published" when="2014">2020. 2018. 2016. 2014. Loshchilov and Hutter, 2017</date>
		</imprint>
	</monogr>
	<note>) with the mention encoder preinitialized from a multilingual BERT checkpoint 2 . All model training was carried out on a Google TPU v3 architecture 3 . 2 github</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
