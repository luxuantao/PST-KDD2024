<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8023948642E536A83B829210A879F25F</idno>
					<idno type="DOI">10.1109/TSMC.2015.2417510</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized Policy Iteration Adaptive Dynamic</head><p>Programming for Discrete-Time Nonlinear Systems</p><p>Derong Liu, Fellow, IEEE, Qinglai Wei, Member, IEEE, and Pengfei Yan</p><p>Abstract-This paper is concerned with a novel generalized policy iteration algorithm for solving optimal control problems for discrete-time nonlinear systems. The idea is to use an iterative adaptive dynamic programming algorithm to obtain iterative control laws which make the iterative value functions converge to the optimum. Initialized by an admissible control law, it is shown that the iterative value functions are monotonically nonincreasing and converge to the optimal solution of Hamilton-Jacobi-Bellman equation, under the assumption that a perfect function approximation is employed. The admissibility property is analyzed, which shows that any of the iterative control laws can stabilize the nonlinear system. Neural networks are utilized to implement the generalized policy iteration algorithm, by approximating the iterative value function and computing the iterative control law, respectively, to achieve approximate optimal control. Finally, numerical examples are presented to verify the effectiveness of the present generalized policy iteration algorithm.</p><p>Index Terms-Adaptive critic designs, adaptive dynamic programming (ADP), approximate dynamic programming, generalized policy iteration, neural networks, neuro-dynamic programming, nonlinear systems, optimal control, reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R EINFORCEMENT learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to optimize the total amount of reward it receives when interacting with its environment <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Associated with reinforcement learning methods and optimal control, adaptive dynamic programming (ADP), proposed by Werbos <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, overcomes the curse of dimensionality problem in dynamic programming (DP) by approximating the performance index function forward-in-time and becomes an important brain-like intelligent method of approximate optimal control for nonlinear systems <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>, ADP approaches were classified into several main schemes which were heuristic DP (HDP), dual HDP (DHP), globalized DHP (GDHP), and their action-dependent versions.</p><p>Iterative methods are primary tools in ADP to obtain the approximate solution of the Hamilton-Jacobi-Bellman (HJB) equation and have received more and more attention <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b24">[25]</ref>. The previous iterative ADP algorithms can be classified into two main schemes which are based on value and policy iterations, respectively <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The value iteration algorithm for optimal control of nonlinear systems was first given in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>. In <ref type="bibr" target="#b28">[29]</ref>, the convergence of discrete-time value iteration algorithm was proven. In <ref type="bibr" target="#b29">[30]</ref>, the value iteration algorithm was applied to solve optimal tracking control problems for nonlinear systems. In <ref type="bibr" target="#b30">[31]</ref>, the value iteration algorithm was applied by DHP. In <ref type="bibr" target="#b31">[32]</ref>, the value iteration ADP is implemented by GDHP. Policy iteration algorithms for optimal control of continuous-time systems were given in <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, the policy iteration algorithm was successfully applied to solve continuous-time complex-valued systems. In <ref type="bibr" target="#b35">[36]</ref>, a discrete-time policy iteration was developed with convergence and stability proofs. Based on the framework of value and policy iteration algorithms, many investigations of iterative ADP algorithms have been developed, such as iterative θ -ADP algorithm <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, ε-optimal control <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, ADP with constraints <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, zero-sum and nonzero-sum games <ref type="bibr" target="#b43">[44]</ref>- <ref type="bibr" target="#b47">[48]</ref>, finite-approximation-errorbased ADP <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b51">[52]</ref>, ADP with unknown and partiallyunknown systems <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>, online ADP <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, multiagent optimal control <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, integral reinforcement learning <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, and dual critic network design <ref type="bibr" target="#b61">[62]</ref>.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, a generalized policy iteration algorithm, which contained policy iteration and value iteration as special cases, was constructed as a new iterative ADP algorithm to solve optimal control problems. Generalized policy iteration algorithms for continuous-time systems were studied in <ref type="bibr" target="#b62">[63]</ref> and <ref type="bibr" target="#b63">[64]</ref>. The stability and convergence properties of continuous-time generalized policy iteration algorithms were analyzed in <ref type="bibr" target="#b64">[65]</ref>. The sketch of the generalized policy iteration algorithm for discrete-time nonlinear systems was described in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b25">[26]</ref>, respectively. In <ref type="bibr" target="#b3">[4]</ref>, it was pointed out that most of the discretetime reinforcement learning methods could be described as generalized policy iteration algorithms. Hence, the investigations of the generalized policy iteration algorithms are important for the development of ADP. However, the generalized 2168-2216 c 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>policy iteration algorithms have inherent differences from the value and policy iteration algorithms. This makes the property analysis of value and policy iteration algorithms invalid for generalized policy iteration algorithms. Till now, the discussions on the properties of the generalized policy iteration algorithms for discrete-time control systems were very scarce.</p><p>To the best of the authors' knowledge, only in <ref type="bibr" target="#b65">[66]</ref>, the properties of the generalized policy iteration algorithms were analyzed, while the stability property of the system under the iterative control law in <ref type="bibr" target="#b65">[66]</ref> cannot be guaranteed. Hence, it is important to establish a new generalized policy iteration algorithm with new analysis methods. This motivates our research.</p><p>In this paper, inspired by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and <ref type="bibr" target="#b65">[66]</ref>, a generalized policy iteration algorithm is developed to solve the approximate optimal control problems of discrete-time nonlinear systems. First, the detailed iteration procedure of the generalized policy iteration algorithm for discrete-time nonlinear systems is presented. Second, the properties of the generalized policy iteration algorithm are developed. Initialized by an arbitrary admissible control law, it proves that the general framework of the generalized policy iteration algorithm will converge to the optimal performance index function and the optimal control law, under the strictly-hypothetical assumption that a perfect function approximation is available. It shows that the iterative value function is monotonically nonincreasing and converges to the optimal performance index function. We emphasize that any of the iterative control laws is proven to stabilize the nonlinear systems. Next, some effective methods are developed to overcome the difficulties of obtaining the initial conditions for the generalized policy iteration algorithm, which make the present generalized policy iteration algorithm more suitable in applications. Neural networks are used to make an approximation implementation of the generalized policy iteration algorithm, where the approximate optimal performance index function and control law are obtained. Simulation results will illustrate the effectiveness of the present algorithm.</p><p>This paper is organized as follows. In Section II, preliminaries and assumptions of the generalized policy iteration algorithm are presented. In Section III, the monotonicity and convergence properties of the iterative value function of the generalized policy iteration algorithm are developed. The admissibility property of the iterative control laws is also analyzed in this section. In Section IV, the neural network implementation for the generalized policy iteration algorithm is discussed. In Section V, simulation results and comparisons are given to demonstrate the effectiveness of the present algorithm. Finally, in Section VI, the conclusion is drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES AND ASSUMPTIONS</head><p>In this paper, we consider a class of discrete-time nonlinear systems described by</p><formula xml:id="formula_0">x k+1 = F(x k , u k ), k = 0, 1, . . . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where x k ∈ R n is the state vector and u k ∈ R m is the control vector. Let x 0 be the given initial state and let F(x k , u k ) denote the system function, which is known. For any k = 0, 1, . . . , let u k = {u k , u k+1 , . . .} be an arbitrary sequence of controls from k to ∞. The performance index function for state x 0 under the control sequence u 0 = {u 0 , u 1 , . . .} is defined as</p><formula xml:id="formula_2">J x 0 , u 0 = ∞ k=0 U(x k , u k ) (2)</formula><p>where U(x k , u k ) &gt; 0, ∀ x k , u k = 0, is the utility function.</p><p>We will study the optimal control problems for <ref type="bibr" target="#b0">(1)</ref>. The goal of this paper is to find an optimal control scheme which stabilizes the system (1) and simultaneously minimizes the performance index function <ref type="bibr" target="#b1">(2)</ref>. For convenience of analysis, the results of this paper are based on the following assumptions.</p><p>Assumption 1: The system (1) is controllable on a compact set x ⊂ R n containing the origin, and the function</p><formula xml:id="formula_3">F(x k , u k ) is Lipschitz continuous on x .</formula><p>Assumption 2: The system state x k = 0 is an equilibrium state of system (1) under the control u k = 0, i.e., F(0, 0) = 0.</p><p>Assumption 3: The feedback control u(x k ) satisfies u(x k ) = 0 for x k = 0.</p><p>Assumption 4: The utility function U(x k , u k ) is a continuous positive definite function of x k and u k .</p><p>For a given control law μ, the map from initial state to the value of ∞ k=0 U(x k , μ(x k )) is called a performance index function J μ (x 0 ). The optimal performance index function is denoted by</p><formula xml:id="formula_4">J * (x 0 ) = inf μ J μ (x 0 ).<label>(3)</label></formula><p>According to Bellman's principle of optimality, for all x k ∈ x , J * (x k ) satisfies the discrete-time HJB equation</p><formula xml:id="formula_5">J * (x k ) = inf u k U(x k , u k ) + J * (x k+1 ) = inf u k U(x k , u k ) + J * (F(x k , u k )) . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Define the optimal control law as</p><formula xml:id="formula_7">u * (x k ) = arg inf u k {U(x k , u k ) + J * (F(x k , u k ))}.</formula><p>Then for all x k ∈ x , the HJB equation can be written as</p><formula xml:id="formula_8">J * (x k ) = U x k , u * (x k ) + J * F x k , u * (x k ) . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>Remark 1: Generally, the optimal performance index function J * (x k ) is a nonanalytical nonlinear function. It is nearly impossible to obtain J * (x k ) for all x k ∈ x by solving the HJB equation. To overcome this problem, a new generalized policy iteration-based ADP algorithm is developed to obtain the approximate solution of the HJB equation iteratively and the neural network implementation of the present algorithm will be given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERALIZED POLICY ITERATION ALGORITHM</head><p>STARTING WITH ADMISSIBLE CONTROL LAW In this section, a generalized policy iteration algorithm is developed to obtain the optimal control law for discrete-time nonlinear systems. The present generalized policy iteration algorithm starts with an admissible control law, which makes it different from <ref type="bibr" target="#b65">[66]</ref>. Both algorithms involve updating the value functions and control laws. The present algorithm guarantees all control laws from the iterative process are admissible (and stable). However, the algorithm developed in <ref type="bibr" target="#b65">[66]</ref> starts with a positive semi-definite function as the initial value function, which has no guarantee to produce stable (or admissible) control law until the iterative value function converges to the optimum.</p><p>The goal of the present generalized policy iteration algorithm is to construct an iterative control law v i (x k ), which moves the system state from an arbitrary initial state x 0 to the equilibrium 0, and simultaneously makes the iterative value function reach the optimum. Under the strictly-hypothetical assumption that the perfect function approximation is available, convergence and admissibility proofs will be given to show that the value function will converge to the optimum and any of the iterative control laws can stabilize the nonlinear system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of the Generalized Policy Iteration Algorithm</head><p>For the optimal control problems, the present control scheme must not only stabilize the control systems, but also make the performance index function finite, i.e., the control law must be admissible <ref type="bibr" target="#b28">[29]</ref>.</p><p>Definition 1: A control law u(x k ) is defined to be admissible with respect to (2) on a compact set u , if u(x k ) is continuous on u , u(0) = 0, u(x k ) stabilizes (1) on u , and ∀x 0 ∈ x , J(x 0 ) is finite.</p><p>Define A u as the set of the admissible control laws for system (1) with respect to <ref type="bibr" target="#b1">(2)</ref>. The present generalized policy iteration algorithm contains two iteration procedures, which are i-and j-iterations, respectively. We introduce two iteration indices i and j i and both of the iteration indices increase from 0. Then, the detailed generalized policy iteration algorithm can be described as follows.</p><p>For i = 0, let v 0 (x k ) ∈ A u be an arbitrary admissible control law. For all x k ∈ x , let V 0 (x k ) be an iterative value function constructed by v 0 (x k ), that satisfies the following generalized HJB (GHJB) equation:</p><formula xml:id="formula_10">V 0 (x k ) = U(x k , v 0 (x k )) + V 0 (F(x k , v 0 (x k ))). (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>Let {N 1 , N 2 , . . .} be an arbitrary sequence, where N i ≥ 0, i = 1, 2, . . . , is an arbitrary nonnegative integer. Then, for i = 1 and all x k ∈ x , the iterative control law is improved by</p><formula xml:id="formula_12">v 1 (x k ) = arg min u k {U(x k , u k ) + V 0 (x k+1 )} = arg min u k {U(x k , u k ) + V 0 (F(x k , u k ))}. (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>Let j 1 increase from 0 to N 1 . For all x k ∈ x , we update the iterative value function by</p><formula xml:id="formula_14">V 1,j 1 +1 (x k ) = U(x k , v 1 (x k )) + V 1,j 1 (F(x k , v 1 (x k ))) (8)</formula><p>where</p><formula xml:id="formula_15">V 1,0 (x k ) = min u k {U(x k , u k ) + V 0 (F(x k , u k ))} = U(x k , v 1 (x k )) + V 0 (F(x k , v 1 (x k ))). (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>For all x k ∈ x , define the iterative value function</p><formula xml:id="formula_17">V 1 (x k ) = V 1,N 1 (x k ). (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>For i = 2, 3, . . . and all x k ∈ x , the generalized policy iteration algorithm can be expressed by the following two iteration procedures.</p><p>1) i-iteration</p><formula xml:id="formula_19">v i (x k ) = arg min u k {U(x k , u k ) + V i-1 (x k+1 )} = arg min u k {U(x k , u k ) + V i-1 (F(x k , u k ))}. (11) 2) j-iteration V i,j i +1 (x k ) = U(x k , v i (x k )) + V i,j i (F(x k , v i (x k ))) (12)</formula><p>where the iteration index j i increasing from 0 to N i and</p><formula xml:id="formula_20">V i,0 (x k ) = min u k {U(x k , u k ) + V i-1 (F(x k , u k ))} = U(x k , v i (x k )) + V i-1 (F(x k , v i (x k ))). (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>For all x k ∈ x , define the iterative value function</p><formula xml:id="formula_22">V i (x k ) = V i,N i (x k ). (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>From the above generalized policy iteration algorithm, we can see that for each i-iteration, based on the iterative value function V i (x k ) for some control law, we can always use it to find another policy that is better, or at least no worse. This iteration procedure is known as "policy improvement" procedure. In this iteration procedure, the control law is updated. For each j-iteration, it computes the iterative value function of the control law v i (x k ), which tries to solve the following GHJB equation:</p><formula xml:id="formula_24">V i,j i (x k ) = U(x, v i (x k )) + V i,j i (F(x k , v i (x k ))). (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>This iteration procedure is called "policy evaluation" procedure <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In this iteration procedure, the iterative value function V i,j i (x k ) is updated, while the control law keeps unchanged.</p><p>Remark 2: There are two special cases we can identify for the present generalized policy iteration algorithm (6)- <ref type="bibr" target="#b13">(14)</ref>.</p><p>1) For i = 1, 2, . . . , if we let N i ≡ 0, then the algorithm is reduced to a value iteration algorithm <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. 2) For i = 1, 2, . . . , if we let N i → ∞, then the algorithm becomes a policy iteration algorithm <ref type="bibr" target="#b35">[36]</ref>. An obvious difference is that in value iteration algorithms <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and policy iteration algorithms <ref type="bibr" target="#b35">[36]</ref>, there is only one iteration index. However, in the present generalized policy iteration algorithm, there are two iteration procedures which are i-and j-iteration. In <ref type="bibr" target="#b65">[66]</ref>, a generalized policy iteration algorithm initialized by an arbitrary positive semi-definite function is developed, while the stability property of the system under the iterative control law in <ref type="bibr" target="#b65">[66]</ref> cannot be guaranteed. In this paper, the present generalized policy iteration algorithm is initialized by an admissible control law v 0 (x k ), which is obviously different from the algorithm in <ref type="bibr" target="#b65">[66]</ref>. Hence, new analysis methods will be needed to analyze the present algorithm.</p><p>Remark 3: Although the time indices k and k+1 are used to indicate the states and actions in two successive time steps, we note that there is no iteration loop for time index k in the generalized policy iteration algorithm ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>), which means that the algorithm does not iterate according to the time sequence. In the present generalized policy iteration algorithm, we say that the iterative value function V i,j i (x k ) and the iterative control law v i (x k ) are updated for all x k ∈ x , according to the two iteration indices i = 0, 1, . . . and j i = 1, 2, . . . , N i . We need to keep the time index this way due to the need of state x at two different time instances as in <ref type="bibr" target="#b6">(7)</ref> and it is more clear later of the need in cases like <ref type="bibr" target="#b30">(31)</ref> where multiple time instances of state x are involved in an equation.</p><p>Remark 4: In the generalized policy iteration algorithm ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>), the iterative value function and iterative control law are improved under the strictly-hypothetical assumption that the perfect function approximation is available. In the next section, the properties of the generalized policy iteration algorithm under the strictly-hypothetical assumption will be developed. On the other hand, we can see that the perfect function approximation in ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>) requires V i,j i (x k ) and v i (x k ) to be exactly solved for all x k ∈ x with an infinite number of points. It is nearly impossible to implement due to finite memory storage and finite time. Hence, in Section IV, approximate structures will be employed to obtain the approximate optimal solutions of the optimal control problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Properties of the Generalized Policy Iteration Algorithm</head><p>Next, we will prove that for any N i ≥ 0 and for all x k ∈ x , the iterative value function V i,j i (x k ) will converge to J * (x k ) as i → ∞, under the assumption of perfect function approximation. We will also show the admissibility property of the iterative control law v i (x k ) under the same assumption.</p><p>Theorem 1: Let v 0 (x k ) ∈ A u be an arbitrary admissible control law which satisfies <ref type="bibr" target="#b5">(6)</ref>. For i = 0, 1, . . . and for all x k ∈ x , let the iterative control law v i (x k ) and the iterative value function V i,j i +1 (x k ) be obtained by ( <ref type="formula" target="#formula_10">6</ref>)- <ref type="bibr" target="#b13">(14)</ref>. Let {N 1 , N 2 , . . .} be an sequence, where N i ≥ 0, i = 1, 2, . . . , is an arbitrary nonnegative integer. Then, we have the following properties.</p><p>1) For i = 1, 2, . . . , j i = 0, 1, . . . , N i and for all</p><formula xml:id="formula_26">x k ∈ x V i,j i +1 (x k ) ≤ V i,j i (x k ). (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>2) For i = 1, 2, . . . , let j i and j (i+1) be arbitrary constant integers which satisfy 0 ≤ j i ≤ N i and 0 ≤ j (i+1) ≤ N i+1 , respectively. Then, for all</p><formula xml:id="formula_28">x k ∈ x V i+1,j (i+1) (x k ) ≤ V i,j i (x k ). (<label>17</label></formula><formula xml:id="formula_29">)</formula><p>Proof: The theorem can be proven in two steps. We first prove that ( <ref type="formula" target="#formula_26">16</ref>) holds by mathematical induction. Let i = 1. According to ( <ref type="formula" target="#formula_10">6</ref>) and ( <ref type="formula" target="#formula_15">9</ref>), for all</p><formula xml:id="formula_30">x k ∈ x V 1,0 (x k ) = U(x, v 1 (x k )) + V 0 (F(x k , v 1 (x k ))) = min u k {U(x k , u k ) + V 0 (x k+1 )} ≤ U(x k , v 0 (x k )) + V 0 (F(x k , v 0 (x k ))) = V 0 (x k ). (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>For</p><formula xml:id="formula_32">j 1 = 0 V 1,1 (x k ) = U(x k , v 1 (x k )) + V 1,0 (F(x k , v 1 (x k ))) ≤ U(x k , v 1 (x k )) + V 0 (F(x k , v 1 (x k ))) = V 1,0 (x k ). (<label>19</label></formula><formula xml:id="formula_33">)</formula><p>Assume that ( <ref type="formula" target="#formula_26">16</ref>) holds for</p><formula xml:id="formula_34">j 1 = l 1 -1, l 1 = 1, 2, . . . , N 1 . Then for j 1 = l 1 and for all x k ∈ x V 1,l 1 +1 (x k ) = U(x k , v 1 (x k )) + V 1,l 1 (F(x k , v 1 (x k ))) ≤ U(x k , v 1 (x k )) + V 1,l 1 -1 (F(x k , v 1 (x k ))) = V 1,l 1 (x k ). (<label>20</label></formula><formula xml:id="formula_35">)</formula><p>Hence, ( <ref type="formula" target="#formula_26">16</ref>) holds for i = 1. Next, let i = 2. According to <ref type="bibr" target="#b71">(72)</ref>, the iterative control law v 2 (x k ) can be expressed by</p><formula xml:id="formula_36">v 2 (x k ) = arg min u k {U(x k , u k ) + V 1 (F(x k , u k ))} (<label>21</label></formula><formula xml:id="formula_37">)</formula><p>where</p><formula xml:id="formula_38">V 1 (x k ) = V 1,N 1 (x k ).</formula><p>According to ( <ref type="formula" target="#formula_20">13</ref>) and ( <ref type="formula" target="#formula_22">14</ref>), for all x k ∈ x , we can get</p><formula xml:id="formula_39">V 2,0 (x k ) = U(x k , v 2 (x k )) + V 1 (F(x k , v 2 (x k ))) = min u k {U(x k , u k ) + V 1 (F(x k , u k ))} ≤ U(x k , v 1 (x k )) + V 1 (F(x k , v 1 (x k ))) = V 1,N 1 +1 (x k ) ≤ V 1,N 1 (x k ) = V 1 (x k ). (<label>22</label></formula><formula xml:id="formula_40">)</formula><p>For j 2 = 0 and for all</p><formula xml:id="formula_41">x k ∈ x V 2,1 (x k ) = U(x k , v 2 (x k )) + V 2,0 (F(x k , v 2 (x k ))) ≤ U(x k , v 2 (x k )) + V 1 (F(x k , v 2 (x k ))) = V 2,0 (x k ).<label>(23)</label></formula><p>So, ( <ref type="formula" target="#formula_26">16</ref>) holds for j 2 = 0. Assume that ( <ref type="formula" target="#formula_26">16</ref>) holds for j 2 = l 2 -1, l 2 = 1, 2, . . . , N 2 . Then for j 2 = l 2 and for all</p><formula xml:id="formula_42">x k ∈ x V 2,l 2 +1 (x k ) = U(x k , v 2 (x k )) + V 2,l 2 (F(x k , v 2 (x k ))) ≤ U(x k , v 2 (x k )) + V 2,l 2 -1 (F(x k , v 2 (x k ))) = V 2,l 2 (x k ). (<label>24</label></formula><formula xml:id="formula_43">)</formula><p>Then, ( <ref type="formula" target="#formula_26">16</ref>) holds for i = 2. Assume that <ref type="bibr" target="#b15">(16)</ref> </p><formula xml:id="formula_44">holds for i = r, r = 1, 2, . . . , that is V r,j r +1 (x k ) ≤ V r,j r (x k ). (<label>25</label></formula><formula xml:id="formula_45">)</formula><p>Then for i = r + 1 and for all x k ∈ x , the iterative control law can be updated by</p><formula xml:id="formula_46">v r+1 (x k ) = arg min u k {U(x k , u k ) + V r (F(x k , u k ))} (<label>26</label></formula><formula xml:id="formula_47">)</formula><p>where</p><formula xml:id="formula_48">V r (x k ) = V r,N r (x k ).</formula><p>According to ( <ref type="formula" target="#formula_20">13</ref>) and ( <ref type="formula" target="#formula_22">14</ref>), for all x k ∈ x , we can get</p><formula xml:id="formula_49">V r+1,0 (x k ) = U(x k , v r+1 (x k )) + V r (F(x k , v r+1 (x k ))) = min u k {U(x k , u k ) + V r (x k+1 )} ≤ U(x k , v r (x k )) + V r (F(x k , v r (x k ))) ≤ V r,N r (x k ) = V r (x k ). (<label>27</label></formula><formula xml:id="formula_50">)</formula><p>For j r+1 = 0 and for all</p><formula xml:id="formula_51">x k ∈ x V r+1,1 (x k ) = U(x k , v r+1 (x k )) + V r+1,0 (F(x k , v r+1 (x k ))) ≤ U(x k , v r+1 (x k )) + V r (F(x k , v r+1 (x k ))) = V r+1,0 (x k ). (<label>28</label></formula><formula xml:id="formula_52">)</formula><p>So, ( <ref type="formula" target="#formula_26">16</ref>) holds for j r+1 = 0. Assume that ( <ref type="formula" target="#formula_26">16</ref>) holds for</p><formula xml:id="formula_53">j r+1 = l r -1, l r+1 = 1, 2, . . . , N r+1 . Then for j r+1 = l r+1 V r+1,l r+1 +1 (x k ) = U(x k , v r+1 (x k )) + V r+1,l r+1 (F(x k , v r+1 (x k ))) ≤ U(x k , v r+1 (x k )) + V r+1,l r+1 -1 (F(x k , v r+1 (x k ))) = V r+1,l r+1 (x k ). (<label>29</label></formula><formula xml:id="formula_54">)</formula><p>Hence, ( <ref type="formula" target="#formula_26">16</ref>) holds for i = 1, 2, . . . and j i = 0, 1, . . . , N i . The mathematical induction is completed.</p><p>Next, we will prove inequality <ref type="bibr" target="#b16">(17)</ref>. For i = 1, let 0 ≤ j 1 ≤ N 1 and 0 ≤ j 2 ≤ N 2 . According to ( <ref type="formula" target="#formula_30">18</ref>)-( <ref type="formula" target="#formula_42">24</ref>), for all</p><formula xml:id="formula_55">x k ∈ x V 1 (x k ) = V 1,N 1 (x k ) ≤ V 1,j 1 (x k ) ≤ V 1,0 (x k ) ≤ V 0 (x k ) V 2 (x k ) = V 2,N 2 (x k ) ≤ V 2,j 2 (x k ) ≤ V 2,0 (x k ) ≤ V 1 (x k ) (<label>30</label></formula><formula xml:id="formula_56">)</formula><p>which shows that (17) holds for i = 1. Using mathematical induction, it is easy to prove that (17) holds for i = 1, 2, . . . . The proof is completed. Remark 5: In <ref type="bibr" target="#b65">[66]</ref>, it is proven that the iterative value function V i,j i (x k ) is convergent as i → ∞, while the monotonicity property of the iterative value function is not guaranteed. Theorem 1 of this paper shows an important monotonicity property of the present generalized policy iteration algorithm. Given an arbitrary initial admissible control law v 0 (x k ) ∈ A u which satisfies <ref type="bibr" target="#b5">(6)</ref>, we have that the iterative value function V i,j i (x k ) is monotonically nonincreasing for i = 1, 2, . . . and for j i = 0, 1, . . . , N i . From the monotonicity property, the admissibility and convergence properties can be derived.</p><p>Lemma 1: Suppose that Assumptions 1-4 hold. For i = 1, 2, . . . and for j i = 0, 1, . . . , N i , the iterative value function V i,j i (x k ) is a positive definite function for x k .</p><p>Proof: Let v 0 k = {v 0 (x k ), v 0 (x k+1 ), . . .}. As v 0 (x k ) ∈ A u is admissible, according to (2) and ( <ref type="formula" target="#formula_10">6</ref>), for all x k ∈ x , the iterative value function</p><formula xml:id="formula_57">V 0 (x k ) = J x k , v 0 k = ∞ l=0 U(x k+l , v 0 (x k+l ))<label>(31)</label></formula><p>is finite. For x k = 0, we have v 0 (x k ) = 0. According to Assumptions 2 and 3, we can get</p><formula xml:id="formula_58">x k+1 = F(x 0 , v 0 (x k )) = 0.</formula><p>By mathematical induction, for l = 0, 1, . . . , we have x k+l = 0. According to <ref type="bibr" target="#b30">(31)</ref> and Assumption 4, we can get V 0 (x k ) = 0. On the other hand, by Assumption 1, as system ( <ref type="formula" target="#formula_0">1</ref>) is controllable and</p><formula xml:id="formula_59">v 0 (x k ) is admissible, for all x k ∈ x , V 0 (x k ) is finite. According to Assumption 4, V 0 (x k ) → ∞, as x k → ∞. As U(x k , u k ) &gt; 0 for all x k = 0, V 0 (x k ) &gt; 0 for all x k = 0. Hence, V 0 (x k ) is a positive definite function.</formula><p>According to ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>), using mathematical induction we can easily obtain that V i,j i (x k ) is positive definite. Theorem 2: For i = 1, 2, . . . and j i = 0, 1, . . . , N i , let the iterative control law v i (x k ) and the iterative value function V i,j i (x k ) be obtained by ( <ref type="formula" target="#formula_10">6</ref>)- <ref type="bibr" target="#b13">(14)</ref>. If for i = 1, 2, . . . , we let N i → ∞, then for all x k ∈ x the iterative value function</p><formula xml:id="formula_60">V i,j i (x k ) is convergent as j i → ∞, that is V i,∞ (x k ) = U(x, v i (x k )) + V i,∞ (F(x k , v i (x k ))) (<label>32</label></formula><formula xml:id="formula_61">)</formula><p>where</p><formula xml:id="formula_62">V i,∞ (x k ) := lim j i →∞ V i,j i (x k ). (<label>33</label></formula><formula xml:id="formula_63">)</formula><p>Proof: According to <ref type="bibr" target="#b15">(16)</ref>, for i = 1, 2, . . . and for all x k ∈ x , the iterative value function V i,j i (x k ) is monotonically nonincreasing as j i increases from 0 to N i . On the other hand, according to Lemma 1, V i,j i (x k ) is a positive definite function for i = 1, 2, . . . and j i = 0, 1, . . . , N i , i.e., V i,j i (x k ) &gt; 0, ∀x k = 0. This means that the iterative value function V i,j i (x k ) is monotonically nonincreasing and lower bounded. Hence, for all x k ∈ x , the limit of V i,j i (x k ) exists when j i → ∞. Then, we can obtain (32) directly.</p><p>Corollary 1: For i = 1, 2, . . . and j i = 0, 1, . . . , N i , let the iterative control law v i (x k ) and the iterative value function V i,j i +1 (x k ) be obtained by ( <ref type="formula" target="#formula_10">6</ref>)- <ref type="bibr" target="#b13">(14)</ref>. Then, for i = 1, 2, . . . and for all x k ∈ x , the iterative control law</p><formula xml:id="formula_64">v i (x k ) is admissible. Proof: Let N ∞ i = {N i + 1, N i + 2, . . .}. For ji = N i + 1, N i + 2, . . . and for all x k ∈ x , we construct a value function V i, ji (x k ) as V i, ji (x k ) = U(x k , v i (x k )) + V i, ji -1 (F(x k , v i (x k ))) (<label>34</label></formula><formula xml:id="formula_65">)</formula><p>where</p><formula xml:id="formula_66">V i,N i (x k ) = V i,N i (x k ).</formula><p>According to (34), we can obtain</p><formula xml:id="formula_67">V i, ji (x k ) = ji -N i -1 l=0 U(x k+l , v i (x k+l )) + V i,N i x k+ ji -N i . (<label>35</label></formula><formula xml:id="formula_68">)</formula><p>According to Theorem 2, for all x k ∈ x , the iterative value function V i,∞ (x k ), which is expressed by</p><formula xml:id="formula_69">V i,∞ (x k ) = lim ji →∞ ji -N i -1 l=0 U(x k+l , v i (x k+l )) + lim ji →∞ V i,N i x k+ ji -N i (36)</formula><p>is finite. According to Assumption 4, the utility function</p><formula xml:id="formula_70">U(x k , v i (x k )) &gt; 0, ∀x k = 0. Then, lim k→∞ U(x k , v i (x k )) = 0, which shows x k → 0 as k → ∞. On the other hand, according to Lemma 1, V i,N i (x k ) = V i,N i (x k ) is positive def- inite. Thus, we can get lim ji →∞ V i,N i (x k+ ji -N i ) = 0. As N i l=0 U(x k+l , v i (x k+l )) is finite, we obtain ∞ l=0 U(x k+l , v i (x k+l )) = N i l=0 U(x k+l , v i (x k+l )) + V i,∞ (x k+N i +1 ) (37)</formula><p>also finite. The proof is completed.</p><p>Remark 6: In <ref type="bibr" target="#b35">[36]</ref>, it shows that for i = 0, 1, . . . and for all x k ∈ x , the iterative control law v i (x k ) is admissible for the policy iteration algorithm. This property can be well verified for the present generalized policy iteration for j i → ∞. For the generalized policy iteration, it proves that for an arbitrary nonnegative integer N i , the iterative control law v i (x k ) is also admissible. On the other hand, for i = 0, 1, . . . , in the policy iteration algorithm, it requires to solve a GHJB equation to obtain the iterative value function. In the present generalized policy iteration algorithm, solving the GHJB equation is effectively avoided. Hence, we say that the present generalized policy iteration algorithm possesses more potential for applications.</p><p>In the following, the convergence property of the generalized policy iteration algorithm will be presented. As the iteration index i increases to ∞, we will show that the optimal performance index function and the optimal control law can be achieved using the present generalized policy iteration algorithm. Before the main theorem, some lemmas are necessary.</p><p>Lemma 2: If a monotonically nonincreasing sequence {a n }, n = 0, 1, . . . , contains an arbitrary convergent subsequence, then sequence {a n } is convergent <ref type="bibr" target="#b66">[67]</ref>.</p><p>Lemma 3: For i = 1, 2, . . . , let the iterative value function V i (x k ) be defined as in <ref type="bibr" target="#b13">(14)</ref>. Then, for i = 1, 2, . . . , the iterative value function V i (x k ) is a monotonically nonincreasing and convergent sequence.</p><p>Proof: It can be proven according to Theorem 1 and the proof details are omitted here.</p><p>Theorem 3: For i = 0, 1, . . . and for all x k ∈ x , let V i,j i (x k ) and v i (x k ) be obtained by ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>). If Assumptions 1-4 hold, then for any N i ≥ 0, the iterative value function V i,j i (x k ) converges to the optimal performance index function</p><formula xml:id="formula_71">J * (x k ), as i → ∞, that is lim i→∞ V i,j i (x k ) = J * (x k ) (<label>38</label></formula><formula xml:id="formula_72">)</formula><p>which satisfies the HJB equation ( <ref type="formula" target="#formula_5">4</ref>).</p><p>Proof: Define a sequence of the iterative value function as</p><formula xml:id="formula_73">V i,j i (x k ) := V 0 (x k ), V 1,0 (x k ), V 1,1 (x k ), . . . , V 1,N 1 (x k ) V 1 (x k ), V 2,0 (x k ), . . . , V 2,N 2 (x k ), . . . . (<label>39</label></formula><formula xml:id="formula_74">)</formula><p>If we let</p><formula xml:id="formula_75">{V i (x k )} := {V 0 (x k ), V 1 (x k ), . . .}<label>(40)</label></formula><p>then</p><formula xml:id="formula_76">{V i (x k )} is a subsequence of {V i,j i (x k )}.</formula><p>According to Lemma 3, the limit of {V i (x k )} exists. From Lemma 2, we can get that if the sequence {V i (x k )} is convergent, then {V i,j i (x k )} is convergent. As a sequence {V i,j i (x k )} can converge to at most one point <ref type="bibr" target="#b66">[67]</ref>, the sequence {V i,j i (x k )} and its subsequence {V i (x k )} possess the same limit, that is</p><formula xml:id="formula_77">lim i→∞ V i,j i (x k ) = lim i→∞ V i (x k ). (<label>41</label></formula><formula xml:id="formula_78">)</formula><p>Thus, in the following, we will prove:</p><formula xml:id="formula_79">lim i→∞ V i (x k ) = J * (x k ). (<label>42</label></formula><formula xml:id="formula_80">)</formula><p>The statement (42) can be proven in three steps.</p><p>Step 1: Show that the limit of the iterative value function V i (x k ) satisfies the HJB equation, as i → ∞.</p><p>According to Lemma 3, for all x k ∈ x , we can define the value function V ∞ (x k ) as the limit of the iterative value function</p><formula xml:id="formula_81">V i (x k ), that is V ∞ (x k ) = lim i→∞ V i (x k ). (<label>43</label></formula><formula xml:id="formula_82">)</formula><p>According to ( <ref type="formula" target="#formula_22">14</ref>) and ( <ref type="formula" target="#formula_26">16</ref>)</p><formula xml:id="formula_83">V i (x k ) ≤ V i,0 (x k ) = U(x k , v i (x k )) + V i-1 (F(x k , v i (x k ))) = min u k {U(x k , u k ) + V i-1 (F(x k , u k ))}. (<label>44</label></formula><formula xml:id="formula_84">)</formula><p>Then, we can obtain</p><formula xml:id="formula_85">V ∞ (x k ) = lim i→∞ V i (x k ) ≤ V i (x k ) ≤ min u k {U(x k , u k ) + V i-1 (F(x k , u k ))}. (<label>45</label></formula><formula xml:id="formula_86">)</formula><p>Let i → ∞. For all x k ∈ x , we can obtain</p><formula xml:id="formula_87">V ∞ (x k ) ≤ min u k {U(x k , u k ) + V ∞ (F(x k , u k ))}. (<label>46</label></formula><formula xml:id="formula_88">)</formula><p>Let ε &gt; 0 be an arbitrary positive number. Since V i (x k ) is nonincreasing for i ≥ 0 and lim</p><formula xml:id="formula_89">i→∞ V i (x k ) = V ∞ (x k )</formula><p>, there exists a positive integer p such that</p><formula xml:id="formula_90">V p (x k ) -ε ≤ V ∞ (x k ) ≤ V p (x k ). (<label>47</label></formula><formula xml:id="formula_91">)</formula><p>Hence, we can get</p><formula xml:id="formula_92">V ∞ (x k ) ≥ U(x k , v p (x k )) + V p (F(x k , v p (x k ))) -ε ≥ U(x k , v p (x k )) + V ∞ (F(x k , v p (x k ))) -ε ≥ min u k {U(x k , u k ) + V ∞ (F(x k , u k ))} -ε. (<label>48</label></formula><formula xml:id="formula_93">)</formula><p>Since ε &gt; 0 is arbitrary, for all x k ∈ x , we have</p><formula xml:id="formula_94">V ∞ (x k ) ≥ min u k {U(x k , u k ) + V ∞ (F(x k , u k ))}. (<label>49</label></formula><formula xml:id="formula_95">)</formula><p>Combining ( <ref type="formula" target="#formula_87">46</ref>) and ( <ref type="formula" target="#formula_94">49</ref>), for all x k ∈ x , we can obtain</p><formula xml:id="formula_96">V ∞ (x k ) = min u k {U(x k , u k ) + V ∞ (F(x k , u k ))}. (<label>50</label></formula><formula xml:id="formula_97">)</formula><p>Next, for all x k ∈ x , let μ(x k ) be an arbitrary admissible control law, and define a new value function P(x k ), which satisfies</p><formula xml:id="formula_98">P(x k ) = U(x k , μ(x k )) + P(F(x k , μ(x k ))).<label>(51)</label></formula><p>Then, we can declare the second step of the proof.</p><p>Step 2: Show that for an arbitrary admissible control law μ(x k ), the value function</p><formula xml:id="formula_99">P(x k ) ≥ V ∞ (x k ).</formula><p>The statement can be proven by mathematical induction. As μ(x k ) is an admissible control law, for all x k ∈ x , x k → 0 as k → ∞. Without loss of generality, let x N = 0 where N → ∞. According to ( <ref type="formula" target="#formula_98">51</ref>)</p><formula xml:id="formula_100">P(x k ) = lim N→∞ {U(x k , μ(x k )) + U(x k+1 , μ(x k+1 )) + • • • + U(x N-1 , μ(x N-1 )) + P(x N )} (52) where x N = 0. If we define v ∞ (x k ) = arg min u k {U(x k , u k ) + V ∞ (x k+1 )} (<label>53</label></formula><formula xml:id="formula_101">)</formula><p>then according to Corollary 1, v ∞ (x k ) is admissible. According to <ref type="bibr" target="#b49">(50)</ref>, the iterative value function V ∞ (x k ) can be expressed as</p><formula xml:id="formula_102">V ∞ (x k ) = U(x k , v ∞ (x k )) + U(x k+1 , v ∞ (x k+1 )) + • • • + U(x N-1 , v ∞ (x N-1 )) + V ∞ (x N ) = min u k U(x k , u k ) + min u k+1 U(x k+1 , u k+1 ) + • • • + min u N-1 {U(x N-1 , u N-1 ) + V ∞ (x N )} . (<label>54</label></formula><formula xml:id="formula_103">)</formula><p>As v ∞ (x k ) is an admissible control law, we can get</p><formula xml:id="formula_104">x N = 0 where N → ∞, which means V ∞ (x N ) = P(x N ) = 0.</formula><p>For N -1, according to (50), we can obtain</p><formula xml:id="formula_105">P(x N-1 ) = U(x N-1 , μ(x N-1 )) + P(x N ) ≥ min u N-1 {U(x N-1 , u N-1 ) + P(x N )} = min u N-1 {U(x N-1 , u N-1 ) + V ∞ (x N )} = V ∞ (x N-1 ). (<label>55</label></formula><formula xml:id="formula_106">)</formula><p>Assume that the statement holds for k = l + 1, l = 0, 1, . . . .</p><formula xml:id="formula_107">Then for k = l P(x l ) = U(x l , μ(x l )) + P(x l+1 ) ≥ min u l {U(x l , u l ) + P(x l+1 )} ≥ min u l {U(x l , u l ) + V ∞ (x l+1 )} = V ∞ (x l ). (<label>56</label></formula><formula xml:id="formula_108">)</formula><p>Hence, for all x k ∈ x , the inequality</p><formula xml:id="formula_109">P(x k ) ≥ V ∞ (x k ) (57)</formula><p>holds. Mathematical induction is completed.</p><p>Step 3: Show that the value function V ∞ (x k ) equals to the optimal performance index function J * (x k ).</p><p>According to the definition of J * (x k ) in ( <ref type="formula" target="#formula_4">3</ref>), for i = 0, 1, . . . and for all</p><formula xml:id="formula_110">x k ∈ x , V i (x k ) ≥ J * (x k ). Let i → ∞, and then we can obtain V ∞ (x k ) ≥ J * (x k ).</formula><p>On the other hand, for an arbitrary admissible control law μ(x k ), (57) holds. For all x k ∈ x , let μ(x k ) = u * (x k ), where u * (x k ) is an optimal control law. Then, we can get V ∞ (x k ) ≤ J * (x k ). Hence, we have <ref type="bibr" target="#b37">(38)</ref> holds. The proof is completed.</p><p>Remark 7: In <ref type="bibr" target="#b65">[66]</ref>, initialized by a positive semi-definite function, it is proven that the iterative value function converges to the optimal performance index function. However, in <ref type="bibr" target="#b65">[66]</ref>, the updated iterative control law in i-iteration cannot guaranteed to be admissible. This makes the policy evaluation in j-iteration is not sure to implement for N i iterations to improve the iterative value function by the iterative control law. In this paper, initialized by an arbitrary admissible control law, it is proven that the iterative value function is monotonically nonincreasing and converges to the optimal performance index function. We emphasize that any of the iterative control laws is admissible, which stabilizes the system. Thus, the policy evaluation in j-iteration is guaranteed to implement for N i iterations by the obtained iterative control law. This is a merit of the present generalized policy iteration algorithm in this paper. Hence, if an admissible control law is obtained, the present generalized policy iteration algorithm in this paper is preferred. In the next section, the method for obtaining the initial admissible control law will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relaxing the Initial Condition of the Generalized Policy Iteration Algorithm</head><p>In the previous section, the monotonicity, convergence, and admissibility properties of the generalized policy iteration algorithm have been analyzed. From the generalized policy iteration algorithm ( <ref type="formula" target="#formula_10">6</ref>)-( <ref type="formula" target="#formula_22">14</ref>), we can see that to implement our algorithm, it requires an admissible control law v 0 (x k ) ∈ A u to construct the initial value function V 0 (x k ) that satisfies <ref type="bibr" target="#b5">(6)</ref>. Usually v 0 (x k ) ∈ A u and V 0 (x k ) are difficult to achieve, which</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Policy Evaluation Algorithm for Initial Value Function Initialization:</head><p>Choose randomly an array of system states x k in x , i.e.,</p><formula xml:id="formula_111">X k = (x (1) k , x (2) k , . . . , x (p) k )</formula><p>, where p is a large positive integer; Choose an arbitrary positive semi-definite function (x k ) ≥ 0; Give the initial admissible control law v 0 (x k ). Iteration:</p><p>1: Let the iteration index j 0 = 0 and let V 0,0 (x k ) = (x k ); 2: For all x k ∈ x , update the control law v</p><formula xml:id="formula_112">j 0 1 (x k ) by v j 0 1 (x k ) = arg min u k U(x k , u k ) + V 0,j 0 (F(x k , u k )) , (<label>58</label></formula><formula xml:id="formula_113">)</formula><p>and improve the iterative value function by  <ref type="bibr" target="#b59">(60)</ref> 5: Let j 0 = j 0 + 1 and goto Step 2; 6: return V 0,j 0 (x k ) and v</p><formula xml:id="formula_114">V j 0 1,0 (x k ) = min u k U(x k , u k ) + V 0,j 0 (F(x k , u k )) = U x k , v j 0 1 (x k ) + V 0,j 0 F x k , v j 0 1 (x k ) ; (59) 3: For all x k ∈ x , if V j 0 1,0 (x k ) -V 0,j 0 (x k ) ≤ 0,</formula><formula xml:id="formula_115">V 0,j 0 +1 (x k ) = U(x k , v 0 (x k )) + V 0,j 0 (F(x k , v 0 (x k )));</formula><formula xml:id="formula_116">j 0 1 (x k ). Let v 1 (x k ) = v j 0 1 (x k ) and V 1,0 (x k ) = V j 0 1,0 (x k ).</formula><p>makes the present algorithm difficult to implement. In this section, some effective methods will be presented to relax the initial value function of the algorithm.</p><p>First, we consider the situation that the admissible control law v 0 (x k ) is known. We develop a policy evaluation algorithm to relax the initial value function V 0 (x k ). The detailed implementation of the algorithm is expressed in Algorithm 1.</p><p>Lemma 4: For all x k ∈ x , let (x k ) ≥ 0 be an arbitrary positive semi-definite function. Let v 0 (x k ) be an arbitrary admissible control law and let V 0,j 0 (x k ) be the iterative value function updated by ( <ref type="formula" target="#formula_112">58</ref>)-( <ref type="formula">60</ref>), where V 0,0 (x k ) = (x k ). We obtain that V 0,j 0 (x k ) is convergent as j 0 → ∞.</p><p>Proof: According to (60), for all</p><formula xml:id="formula_117">x k ∈ x V 0,j 0 +1 (x k ) -V 0,j 0 (x k ) = U(x k , v 0 (x k )) + V 0,j 0 (x k+1 ) -U(x k , v 0 (x k )) + V 0,j 0 -1 (x k+1 ) = V 0,j 0 (x k+1 ) -V 0,j 0 -1 (x k+1 ). (61)</formula><p>According to <ref type="bibr" target="#b60">(61)</ref>, we can get</p><formula xml:id="formula_118">⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ V 0,j 0 +1 (x k ) -V 0,j 0 (x k ) = V 0,1 x k+j 0 -V 0,0 (x k+j 0 ) V 0,j 0 (x k ) -V 0,j 0 -1 (x k ) = V 0,1 (x k+j 0 -1 ) -V 0,0 x k+j 0 -1 . . . V 0,1 (x k ) -V 0,0 (x k ) = V 0,1 (x k ) -V 0,0 (x k ). (<label>62</label></formula><formula xml:id="formula_119">)</formula><p>Algorithm 2 Policy Improvement Algorithm for Initial Value Function Initialization:</p><p>Choose randomly an array of system states x k in x , i.e.,</p><formula xml:id="formula_120">X k = (x (1) k , x (2) k , . . . , x (p) k )</formula><p>, where p is a large positive integer; Iteration:</p><p>Let ς 0 = 0. 1: Choose arbitrarily a large positive definite function</p><formula xml:id="formula_121">¯ ς 0 (x k ) ≥ 0 and let V ς 0 0,0 (x k ) = ¯ ς 0 (x k ); 2: For all x k ∈ x , update the control law v ς 0 1 (x k ) by v ς 0 1 (x k ) = arg min u k U(x k , u k ) + ¯ ς 0 (F(x k , u k )) , (<label>65</label></formula><formula xml:id="formula_122">)</formula><p>and for all x k ∈ x , improve the iterative value function by</p><formula xml:id="formula_123">V ς 0 1,0 (x k ) = min u k U(x k , u k ) + V ς 0 0,0 (F(x k , u k )) = U x k , v ς 0 1 (x k ) + V ς 0 0,0 F x k , v ς 0 1 (x k ) ; (66) 3: For all x k ∈ x , if the inequality V ς 0 1,0 (x k ) -¯ ς 0 (x k ) ≤ 0 (<label>67</label></formula><formula xml:id="formula_124">)</formula><p>holds, then goto Step 4. Else let ς 0 = ς 0 + 1 and goto</p><p>Step 1; 4: return V</p><formula xml:id="formula_125">ς 0 0,0 (x k ) and v ς 0 1 (x k ). Let v 1 (x k ) = v ς 0 1 (x k ) and V 1,0 (x k ) = V ς 0 1,0 (x k ).</formula><p>Then, we have</p><formula xml:id="formula_126">V 0,j 0 +1 (x k ) = j 0 l=0 U(x k+l , v 0 (x k+l )) + x k+j 0 +1 . (<label>63</label></formula><formula xml:id="formula_127">)</formula><p>Let j 0 → ∞. We can obtain lim</p><formula xml:id="formula_128">j 0 →∞ V 0,j 0 +1 (x k ) = ∞ l=0 U(x k+l , v 0 (x k+l )). (<label>64</label></formula><formula xml:id="formula_129">) As v 0 (x k ) is an admissible control law, ∞ l=0 U(x k+l , v 0 (x k+l )) is finite. Hence lim j 0 →∞ V 0,j 0 (x k ) is finite, which means V 0,j 0 +1 (x k ) = V 0,j 0 (x k ), as j 0 → ∞.</formula><p>Using the admissible control law v 0 (x k ), according to Lemma 4, V 0,j 0 +1 (x k ) = V 0,j 0 (x k ) holds as j 0 → ∞. It means that there must exist</p><formula xml:id="formula_130">N 0 &gt; 0 which satisfies V 1 (x k ) ≤ V 0,N 0 (x k ).</formula><p>Hence if we obtain an admissible control law, then we can construct the initial value function by policy evaluation, where the value function V 0 (x k ) in ( <ref type="formula" target="#formula_10">6</ref>) can be relaxed. On the other hand, we can see that Algorithm 1 requires an admissible control law v 0 (x k ) to implement. Usually, the admissible control law of the nonlinear system is also difficult to obtain. To overcome this difficulty, a policy improvement algorithm can be implemented by experiment. The details of the algorithm can be seen in Algorithm 2.</p><p>Theorem 4: For all x k ∈ x , let the iterative control law v ς 0 1 (x k ) be expressed as in <ref type="bibr" target="#b64">(65)</ref> and let the iterative value function V ς 0 1,0 (x k ) be expressed as in <ref type="bibr" target="#b65">(66)</ref>. If the iterative value functions satisfy <ref type="bibr" target="#b66">(67)</ref>, then the convergence properties ( <ref type="formula" target="#formula_26">16</ref>) and ( <ref type="formula" target="#formula_28">17</ref>) hold for i = 1, 2, . . . and j i = 0, 1, . . . , N i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Generalized Policy Iteration Algorithm Initialization:</head><p>Choose randomly an array of system states x k in x , i.e., X k = (x (1)  k , x (2)  k , . . . ,</p><formula xml:id="formula_131">x (p) k )</formula><p>, where p is a large positive integer; Choose a computation precision ε; Construct a sequence {N i }, where N i ≥ 0, i = 1, 2, . . . , is an arbitrary nonnegative integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration:</head><p>1: Let the iteration index i = 0. Obtain V 1,0 (x k ) and v 1 (x k ) by Algorithm ϒ, ϒ = 1, 2; 2: Let j 1 increase from 0 to N 1 . For all x k ∈ x , update the iterative value function by</p><formula xml:id="formula_132">V 1,j 1 +1 (x k ) = U(x k , v 1 (x k )) + V 1,j 1 (F(x k , v 1 (x k ))); 3: Let i = i + 1. For all x k ∈ x , do Policy Improvement v i (x k ) = arg min u k {U(x k , u k ) + V i-1 (F(x k , u k ))};</formula><p>4: Let j i increase from 0 to N i . For all x k ∈ x , do Policy Evaluation</p><formula xml:id="formula_133">V i,j i +1 (x k ) = U(x k , v i (x k )) + V i,j i (F(x k , v i (x k ))); 5: Let V i (x k ) = V i,N i (x k ); 6: For all x k ∈ x , if V i-1 (x k ) -V i (x k ) &lt; ε</formula><p>, then the approximate optimal performance index function and the approximate optimal control law are obtained. Goto Step 7. Else goto Step 3; 7: return v i (x k ) and V i,j i (x k ). <ref type="bibr" target="#b7">(8)</ref> and (67), we have</p><formula xml:id="formula_134">Proof: Let i = 1 and j 1 = 0. As v 1 (x k ) = v ς 0 1 (x k ) and V 1,0 (x k ) = V ς 0 1,0 (x k ), according to</formula><formula xml:id="formula_135">V 1,1 (x k ) = U(x k , v 1 (x k )) + V 1,0 (F(x k , v 1 (x k ))) = U(x k , v ς 0 1 (x k )) + V ς 0 1,0 F x k , v ς 0 1 (x k ) ≤ U x k , v ς 0 1 (x k ) + V ς 0 0,0 F x k , v ς 0 1 (x k ) = V 1,0 (x k ). (<label>68</label></formula><formula xml:id="formula_136">)</formula><p>Using the idea from ( <ref type="formula" target="#formula_30">18</ref>)-( <ref type="formula" target="#formula_55">30</ref>), the convergence properties ( <ref type="formula" target="#formula_26">16</ref>) and ( <ref type="formula" target="#formula_28">17</ref>) hold for i = 1, 2, . . . and j i = 0, 1, . . . , N i . Remark 8: From Algorithm 2, we can see that the admissible control law v 0 (x k ) in Algorithm 1 is avoided. This is a merit of Algorithm 2. However, in Algorithm 2, we should find a positive definite function ¯ ς 0 (x k ) that satisfies <ref type="bibr" target="#b66">(67)</ref>. As ¯ ς 0 (x k ) is randomly chosen, it may take a lot of iterations to determine ¯ ς 0 (x k ). This is a disadvantage of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalized Policy Iteration Algorithm</head><p>We are now in a position to summarize the generalized policy iteration ADP algorithm (see Algorithm 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NEURAL NETWORK IMPLEMENTATION</head><p>Results so far have shown the convergence of iterative value function V i,j i (x k ) and iterative control law v i (x k ). Under ideal conditions, they will converge to their corresponding optimal functions. The results given in Sections II and III are under the condition that for i = 0, 1, . . . and j i = 0, 1, . . . , N i , the functions V i,j i (x k ) and v i (x k ) can accurately be obtained for all x k ∈ x . In this section, back-propagation (BP) neural networks are introduced to approximate the iterative value function and iterative control law, respectively.</p><p>Assume that the number of hidden layer neurons is denoted by ˜ . The weight matrix between the input layer and hidden layer is denoted by Y. The weight matrix between the hidden layer and output layer is denoted by W. Let b denote the threshold vector of the neural network. Then, the output of three-layer BP network is expressed by</p><formula xml:id="formula_137">F(X, Y, W, b) = W T σ Y T X + b<label>(69)</label></formula><p>where</p><formula xml:id="formula_138">σ (Y T X) ∈ R ˜ , [σ (z)] i = (e z i -e -z i )/(e z i + e -z i ), i = 1, .</formula><p>. . ˜ , are the activation functions. There are two networks, which are critic and action networks, respectively. Both neural networks are chosen as three-layer feedforward network. The whole structure diagram is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Critic Network</head><p>The critic network is used to approximate the iterative value function V i,j i (x k ). For all x k ∈ x , the output of the critic network is denoted as Vl</p><formula xml:id="formula_139">i,j i (x k ) = W l c(i,j i ) σ (Y l c(i,j i ) x k + b l c(i,j i )</formula><p>), l = 0, 1, . . . . The target iterative value function can be written as</p><formula xml:id="formula_140">V i,j i (x k ) = U x k , vi (x k ) + Vi,j i -1 F(x k , vi (x k )) . (70)</formula><p>Then, we define the error function for the critic network as</p><formula xml:id="formula_141">e l c(i,j i ) = Vl i,j i (x k )-V i,j i (x k ).</formula><p>The objective function to be minimized in the critic network is E l c(i,j i ) = (1/2)(e l c(i,j i ) ) 2 . So, the gradient-based weight update rule <ref type="bibr" target="#b67">[68]</ref> for the critic network is given by</p><formula xml:id="formula_142">w l+1 c(i,j i ) = w l c(i,j i ) -α c ∂E l c(i,j i ) ∂w l c(i,j i ) = w l c(i,j i ) -α c ∂E l c(i,j i ) ∂ Vl i,j i (x k ) ∂ Vl i,j i (x k ) ∂w l c(i,j i ) = w l c(i,j i ) -α c e l c(i,j i ) ∂ Vl i,j i (x k ) ∂w l c(i,j i ) (71)</formula><p>where α c &gt; 0 is the learning rate of critic network and w l c(i,j i ) is the weight matrix of the critic network which can be replaced by W l c(i,j i ) , Y l c(i,j i ) , and b l c(i,j i ) .</p><p>Remark 9: In (70), the expression of the target iterative value function V i,j i (x k ) is given, where the information of the value function Vi,j i -1 (F(x k , vi (x k ))) in previous iteration is required. For i = 0, 1, . . . , and j i = 1, 2, . . . , N i , the value functions Vi,j i -1 (F(x k , vi (x k ))) are obtained by the critic network approximation in the previous iteration. Thus, we say that the function Vi,j i -1 (F(x k , vi (x k ))) is known and the gradientbased weight update rule in (71) can be implemented. On the other hand, besides the gradient-based neural networks method, we say that the Galerkin method <ref type="bibr" target="#b68">[69]</ref> and stochastic approximation <ref type="bibr" target="#b69">[70]</ref> are also effective approximators to reconstruct the iterative function V i,j i (x k ) with good convergence performance. The corresponding detailed approximation and analysis methods can be seen in <ref type="bibr" target="#b70">[71]</ref> and <ref type="bibr" target="#b71">[72]</ref>. As the property analysis of function approximation is not the main research topic of this paper, it is omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Network</head><p>In the action network, the state x k ∈ x is used as input to the network. The output can be formulated as vl</p><formula xml:id="formula_143">i (x k ) = W l ai σ (Y l ai x k + b l ai ), l = 0, 1, . . . . For i = 1, 2, .</formula><p>. . , the target of the output of the action network is given by</p><formula xml:id="formula_144">v i (x k ) = arg min u k U(x k , u k ) + Vi-1 (F(x k , u k )) . (72)</formula><p>So, we can define the output error of the action network as</p><formula xml:id="formula_145">e l ai = vl i (x k ) -v i (x k ).</formula><p>The weights of the action network are updated to minimize the following performance error measure E l ai = (1/2)(e l ai ) T e l ai . The gradient-based weight update rule <ref type="bibr" target="#b67">[68]</ref> for the action network is given by</p><formula xml:id="formula_146">w l+1 ai = w l ai -β a ∂E l ai ∂w l ai = w l ai -β a ∂E l ai ∂e l ai ∂e l ai ∂ vl i (x k ) ∂ vl i (x k ) ∂w l ai (<label>73</label></formula><formula xml:id="formula_147">)</formula><p>where β a &gt; 0 is the learning rate of action network and w l ai is the weight matrix of the critic network which can be replaced by W l ai , Y l ai , and b l ai . Remark 10: From Theorems 1-3, we can see that the convergence and admissibility properties of the present generalized policy iteration algorithm are independent of the approximation structures, such as neural networks. Hence, we say that the present generalized policy iteration algorithm and the corresponding proofs possess theoretical significance. On the other hand, implementing our algorithm by neural networks, approximation errors of neural networks inherently exist. Hence, we declare that an approximate optimal solution of the HJB equation ( <ref type="formula" target="#formula_5">4</ref>) is actually obtained instead of the exact optimal one. To make the iterative value functions and iterative control laws closer to their optimal ones, it requires collecting enough training data and enhancing the training precisions of the neural networks.</p><p>V. SIMULATION STUDY In this section, two simulation examples are used to show the performance of the present generalized policy iteration algorithm for solving the approximate optimal control problems. Example 1: First, let us consider the following spring-mass-damper system <ref type="bibr" target="#b72">[73]</ref>:</p><formula xml:id="formula_148">M d 2 y dt 2 + b dy dt + κy = u</formula><p>where y is the position and u is the control input. Let M = 0.1 kg denote the mass of object. Let κ = 2 kgf/m be the stiffness coefficient of spring and let b = 0.1 be the wall friction. Let x 1 = y and x 2 = (dy/dt). Discretizing the system function with the sampling interval t = 0.1 s leads to</p><formula xml:id="formula_149">x 1(k+1) x 2(k+1) = 1 T - κ M T 1 - b M T x 1k x 2k + 0 T M u k . (<label>74</label></formula><formula xml:id="formula_150">)</formula><p>Let the initial state be x 0 = [1, -1] T . Let the performance index function be expressed by <ref type="bibr" target="#b1">(2)</ref>. The utility function is expressed as</p><formula xml:id="formula_151">U(x k , u k ) = x T k Qx k + u T k Ru k , where Q = I 1 , R = I 2 ,</formula><p>and I 1 and I 2 denote the identity matrix with suitable dimensions.</p><p>Let the state space be x = {x k | -1 ≤ x 1k ≤ 1, -1 ≤ x 2k ≤ 1}. We randomly choose the p = 5000 states in x to implement the generalize policy iteration algorithm to obtain the optimal control law. Neural networks are used to implement the present generalized value iteration algorithm. The critic network and the action network are chosen as three-layer BP neural networks with the structures of 2-8-1 and 2-8-1, respectively. Define the two neural networks as group "NN1." For system <ref type="bibr" target="#b73">(74)</ref>, we can obtain an admissible control law u(x k ) = Kx k , where K = [0.13, -0.17] T . Let (x k ) = x T k P 0 x k , where P 0 = 80 1 1 2 . As the initial admissible control law K is known, policy evaluation in Algorithm 1 is implemented. It can be seen that it takes three iterations to obtain v 1 (x k ) and V 1,0 (x k ) and the simulation results for the initial iteration is displayed in Fig. <ref type="figure" target="#fig_1">2</ref> (see the trajectories of the iterative value functions for i = 0). Let iteration index i = 10. To illustrate  <ref type="figure" target="#fig_2">3</ref>, where we let "In" denote initial iteration and "Lm" denote limiting iteration.</p><p>For {N 1 i } = 0, the generalized policy iteration algorithm is reduced to value iteration algorithm <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. From Figs. 2(a) and 3(a), we can see that the iterative value function converges to the approximate optimum which justifies the effectiveness of our algorithm. For {N 4  i } = 20, we can see that for i = 1, . . . , 10, the iterative value function V i,j i (x k ) is convergent for j i . The generalized policy iteration algorithm is transformed into the policy iteration algorithm <ref type="bibr" target="#b35">[36]</ref>, where the convergence property can be justified. For arbitrary sequence {N i }, such as {N 2 i } and {N 3 i }, From Figs. <ref type="figure" target="#fig_1">2(b</ref>) and (c) and 3(b) and (c), the iterative value function can also converge to the approximate optimum. Hence, we can say that value and policy iteration algorithms are special cases of the present generalized policy iteration algorithms and the convergence properties of our algorithm can be justified. The stability property of system (74) under the iterative control law v i (x k ) is shown in Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref>, respectively.</p><p>From the above simulation results, we can see that for i = 0, 1, . . . , the iterative control law v i (x k ) is admissible. For linear system (74), we know that the optimal performance index function J * (x k ) = x T k P * and the effectiveness of the present algorithm can be justified for linear systems.</p><p>On the other hand, we know that the structure of the neural networks is important for its approximation performance. To show the influence of the neural network structure, we change the structures of the critic and action networks to 2-4-1 and 2-4-1, respectively, and other parameters of the neural networks are kept unchanged. Define the two neural networks as group "NN2." Choose {N 2 i } for the j-iteration. Implement our algorithm for i = 10 iterations. The iterative value functions by NN1 and NN2 are shown in Fig. <ref type="figure" target="#fig_4">6(a)</ref>. We can see that if the number of hidden layer is reduced, the approximate performance of the neural networks may decrease. The plot of V i (x k ) is shown in Fig. <ref type="figure" target="#fig_4">6(b</ref>). The corresponding trajectories of iterative states and control are shown in Fig. <ref type="figure" target="#fig_4">6(c</ref>) and (d), respectively. We can see that if the structure of the neural networks is not set appropriately, the performance of the control system will be decreased.</p><p>Example 2: We now examine the performance of our algorithm in a torsional pendulum system <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b67">[68]</ref> with modifications. The dynamics of the pendulum is given as follows:</p><formula xml:id="formula_152">⎧ ⎪ ⎨ ⎪ ⎩ dθ dt = ω J dω dt = u -Mgl sin θ -f d dθ dt</formula><p>where M = 1/3 kg and l = 2/3 m are the mass and length of the pendulum bar, respectively. Let J = 4/3Ml 2 and f d = 0.2 be the rotary inertia and frictional factor, respectively. x 1 = θ and x 2 = ω. Let g = 9.8 m/s 2 be the gravity and the sampling time interval T = 0.1 s. Then, the discretized system can be expressed by</p><formula xml:id="formula_153">x 1(k+1) x 2(k+1) = 0.1x 2k + x 1k -0.49 sin(x 1k ) -0.1f d x 2k + x 2k + 0 0.1 u k . (<label>75</label></formula><formula xml:id="formula_154">)</formula><p>Let the initial state be x 0 = [1, -1] T and let the utility function be the same as the one in Example 1.</p><p>Neural networks are also used to implement the generalized policy iteration algorithm, where the structures of the critic network and the action network are the same as the ones in Example 1. We choose p = 10000 states in to implement the generalized value iteration algorithm. For nonlinear system (75), the initial admissible control law is difficult to obtain. Thus we implement policy improvement algorithm in Algorithm 2, and we can obtain the initial value  value function V i,j i (x k ) is monotonically nonincreasing and converges to the approximate optimum using the present generalized policy iteration algorithm. The convergence property of the present generalized policy iteration algorithm for nonlinear systems can be justified. The convergence properties of value and policy iteration algorithms can also be justified by our algorithm. The stability property of system (74) under the iterative control law v i (x k ) is shown in Figs. 9 and 10, respectively. We can see that for i = 0, 1, . . . , the iterative control law v i (x k ) is admissible, and hence the effectiveness of the present algorithm can be justified for nonlinear systems. Remark 11: One property should be pointed out. From Figs. 7(a) and 8(a), as N i ≡ 0, for i = 1, 2, . . . , 30, we can see that it takes 15 i-iterations to make the iterative value function converge to the optimal performance index function. From Figs. 7(d) and 8(d), as N i is large, i.e., N i = 20 for i = 1, 2, . . . , 30, we can see that it only takes four i-iterations to make the iterative value function converge to the optimum which is much less than the situation for N i ≡ 0. In each i-iteration, however, it has to take 20 j-iterations to make V i,j i (x k ) convergent. Thus, if we want to obtain the optimal performance index function by the least times of policy improvement, then we can enlarge the number of j-iteration. On the other hand, if we want to obtain the optimal performance index function by the least times of policy evaluation, then reducing the number of j-iteration can be an effective method. For value and policy iteration algorithms, the numbers of j-iteration are fixed at 0 and ∞, respectively, which means the convergence of value and policy iteration algorithms is nonregulatable. For the present generalized policy iteration algorithm, we can regulate the convergence property of the iterative value function by defining a suitable sequence {N i }. This is another merit of the generalized policy iteration algorithm.</p><formula xml:id="formula_155">function ¯ ς 0 (x k ) = x T k P0 x k ,</formula><p>In the above simulations, BP neural networks are used to implement our algorithm. We have also tried extreme learning machine (ELM) <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref> to show the effectiveness of the present algorithm. For j = 1, 2, . . . , p, let Ñ be the number of hidden nodes. The standard ELM is expressed by</p><formula xml:id="formula_156">f L x ( j) k = Ñ i=1 β i h x ( j) k = Ñ i=1 β i h wi x ( j) k + b i (<label>76</label></formula><formula xml:id="formula_157">)</formula><p>where wi is the weight vector connecting the ith hidden node and the input nodes. Let β i be the vector connecting the ith hidden node and the output nodes and let b i be the threshold of the ith hidden node. Let wi and β i be random matrices with suitable dimensions. Choose Ñ = 500 and h(•) = σ (•). We use ELM <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref> to approximate the iterative value function and the iterative control law to implement the present generalized policy iteration algorithm. Choose {N 2 i } for the jiteration. Implement our algorithm for i = 30 iterations. The iterative value functions V i (x 0 ) by ELM and BP network are shown in Fig. <ref type="figure" target="#fig_7">11(a)</ref>. We can see that using ELM training, the value function can also converge to its optimum. The plot of V i (x k ) is shown in Fig. <ref type="figure" target="#fig_7">11(b)</ref>. By ELM, it takes 1267.49 s to complete implementing the algorithm, while the running time is 5254.84 s by BP network (standard BP algorithm). Using ELM, the initial weights of neural networks are solved directly by Moore-Penrose generalized inverse method (see <ref type="bibr">[17,</ref> eq. ( <ref type="formula" target="#formula_36">21</ref>)]), which may lead to faster convergence. The corresponding trajectories of iterative states and control are shown in Fig. <ref type="figure" target="#fig_7">11(c</ref>) and (d), respectively. VI. CONCLUSION A generalized policy iteration algorithm is developed for solving infinite horizon approximate optimal control problems of discrete-time nonlinear systems. The present iterative ADP algorithm is initialized by an arbitrary admissible control law. Under the assumption of perfect function approximation, it is proven for the first time that the iterative value function of the generalized policy iteration algorithm is monotonically nonincreasing and converges to the optimal performance index function. Admissibility of the iterative control law is also established. Effective methods are given to relax the initial value function of the present algorithm. Neural networks are employed to implement the generalized policy iteration algorithm to obtain the approximate optimal solution of the HJB equation. Finally, two simulation examples are utilized to illustrate the performance of the present algorithm.</p><p>As is known, approximation errors inherently exist during the neural network implementation. We say that the converged iterative value function and iterative control law are approximations to the optimal ones. The property analysis of approximation errors based on iterative θ -ADP algorithm has been investigated in <ref type="bibr" target="#b18">[19]</ref>. Hence, the property analysis of the present algorithm with approximation errors will be our future research topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Structure diagram of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Iterative value functions V i,j i (x k ) for i = 0, 1, . . . , 10 and x k = x 0 . V i,j i (x k ) for (a) {N 1 i }, (b) {N 2 i }, (c) {N 3 i }, and (d) {N 4 i }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Iterative value functions V i (x k ), for i = 0, 1, . . . , 10. V i (x k ) for (a) {N 1 i }, (b) {N 2 i }, (c) {N 3 i }, and (d) {N 4 i }.</figDesc><graphic coords="10,322.00,53.24,230.64,210.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Trajectories of iterative control law v i (x k ), i = 0, 1, . . . , 10. v i (x k ) for (a) {N 1 i }, (b) {N 2 i }, (c) {N 3 i }, and (d) {N 4 i }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Simulation results for i = 0, 1, . . . , 10 and {N 2 i }. (a) Value function at x = x 0 for NN1 and NN2. (b) V i (x k ) by NN2. (c) Iterative control law by NN2. (d) System states by NN2.</figDesc><graphic coords="11,326.50,53.12,222.00,214.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Iterative value functions V i,j i (x k ) for i = 0, 1, . . . , 30 and x k = x 0 . V i,j i (x k ) for (a) {N 1 i }, (b) {N 2 i }, (c) {N 3 i }, and (d) {N 4 i }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .Fig. 10 .</head><label>8910</label><figDesc>Fig. 8. Iterative value functions V i (x k ), i = 0, 1, . . . , 30. V i (x k ) for (a) {N 1 i }, (b) {N 2 i }, (c) {N 3 i }, and (d) {N 4 i }. Let iteration index i = 30. To illustrate the effectiveness of the algorithm, we choose four different iteration sequences {N γ i }, γ = 1, 2, 3, 4. For γ = 1 and ∀ i = 0, 1, . . . , 30, we let N 1 i = 0. For γ = 2, let N 2 i , i = 1, 2, . . . , 30, be arbitrary nonnegative integer that satisfies 0 ≤ N 2 i ≤ 4. For γ = 3, let N 3 i , i = 1, 2, . . . , 30, be arbitrary nonnegative integer that satisfies 0 ≤ N 3 i ≤ 10. For γ = 4 and ∀ i = 0, 1, . . . , 30, let N 4 i = 20. Train the critic and the action networks under the learning rate 0.01 and set the neural network training errors as 10 -6 . Under the iteration indices i and j i , the trajectories of iterative value functions V i,j i (x k ) for x = x 0 are shown in Fig. 7. The curves of the iterative value functions V i (x k ) are shown in Fig. 8.From Figs.7 and 8, we can see that given an arbitrary nonnegative integer sequence {N i }, i = 0, 1, . . . , the iterative</figDesc><graphic coords="12,58.99,317.19,230.64,210.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Simulation results for i = 0, 1, . . . , 30 and {N 2 i }. (a) Value function at x k = x 0 . (b) V i (x k ) by ELM. (c) Iterative control law by ELM. (d) System states by ELM.</figDesc><graphic coords="13,54.99,52.76,238.80,226.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>goto Step 6. Else goto Step 4; 4: For all x k ∈ x , update the iterative value function by</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grants 61034002, 61233001, 61273140, 61304086, and 61374105, in part by the Beijing Natural Science Foundation under Grant 4132078, and in part by the Early Career Development Award of the State Key Laboratory of Management and Control for Complex Systems. This paper was recommended by Associate Editor A. H. Tan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His current research interests include adaptive dynamic programming, data-driven control, adaptive control, and neural-network-based control.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fusion of multiple behaviors using layered reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="999" to="1004" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning and distributed local model synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Landelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Dept. Electr. Eng., Linkoping Univ</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Linkoping, Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advanced forecasting methods for global crisis warning and models of intelligence</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">General Systems Yearbook</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="25" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A menu of designs for reinforcement learning over time</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Control</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="67" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural network-based optimal control of mobile robot formations with reduced information exchange</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dierks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Control Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1407" to="1415" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of actor-critic reinforcement learning: Standard and natural policy gradients</title>
		<author>
			<persName><forename type="first">I</forename><surname>Grondman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A D</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1291" to="1307" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deterministic improved Q-learning for path planning of a mobile robot</title>
		<author>
			<persName><forename type="first">A</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1141" to="1153" />
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust adaptive dynamic programming with an application to power systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1150" to="1156" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A self-learning call admission control scheme for CDMA cellular networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1219" to="1228" />
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive optimal control of unknown constrained-input systems using policy iteration and neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1513" to="1525" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intelligent local area signals based damping of power system oscillations using virtual generators and approximate dynamic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Smart Grid</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="498" to="508" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic optimal controller design for uncertain nonlinear networked control system via neuro dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="484" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive cooperative tracking control of higher-order nonlinear systems with unknown dynamics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1432" to="1439" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive critic designs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="997" to="1007" />
			<date type="published" when="1997-09">Sep. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming for real-time control and neural modeling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sofge</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>ch. 13</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel actor-critic-identifier architecture for approximate optimal control of uncertain nonlinear systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="92" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finite-approximation-error-based optimal control approach for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="779" to="789" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming for optimal tracking control of unknown nonlinear systems with application to coal gasification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1020" to="1036" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model-free multiobjective approximate dynamic programming for discrete-time nonlinear systems with general performance index functions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1839" to="1848" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multibattery optimal coordination control for home energy management systems via distributed iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4203" to="4214" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel dual iterative Q-learning method for optimal battery management in smart residential environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2509" to="2518" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online learning control using adaptive critic designs with sparse kernel machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="762" to="775" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Policy iteration optimal tracking control for chaotic systems by adaptive dynamic programming approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno>ID 0305021</idno>
	</analytic>
	<monogr>
		<title level="j">Chin. Phys. B</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015-03">Mar. 2015</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="105" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuro-Dynamic Programming</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Athena Scientific</publisher>
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<title level="m">Dynamic Programming and Optimal Control</title>
		<meeting><address><addrLine>Belmont, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Athena Scientific</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discrete-time nonlinear HJB solution using approximate dynamic programming: Convergence proof</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Tamimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Khalaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="949" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel infinite-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy HDP iteration algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="937" to="942" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The RBF neural network-based nearoptimal control for a class of discrete-time affine nonlinear systems with control constraint</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1490" to="1503" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural-network-based optimal control for a class of unknown discrete-time nonlinear systems using globalized dual heuristic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="634" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network HJB approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="791" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lendaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming for a class of complex-valued nonlinear systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1733" to="1739" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="634" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel iterative θ -adaptive dynamic programming for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1176" to="1190" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Numerical adaptive learning control scheme for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theor. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1472" to="1486" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming for finite-horizon optimal control of discrete-time nonlinear systems with -error bound</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An iterative -optimal control scheme for a class of discrete-time nonlinear systems with unfixed initial state</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finite-horizon control-constrained nonlinear optimal control using single network adaptive critics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural-network-based online optimal control for uncertain nonlinear continuous-time systems with control constraints</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theor. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2037" to="2047" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An iterative adaptive dynamic programming algorithm for optimal control of unknown discrete-time nonlinear systems with constrained inputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="331" to="342" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-player non-zero-sum games: Online adaptive learning solution of coupled Hamilton-Jacobi equations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1556" to="1569" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Near-optimal control for nonzero-sum differential games of continuous-time nonlinear systems using singlenetwork ADP</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="216" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="214" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural-network-based zero-sum game for discrete-time nonlinear systems via iterative adaptive dynamic programming algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="92" to="100" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Online synchronous approximate optimal learning algorithm for multiplayer nonzero-sum games with unknown dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1015" to="1027" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data-driven neuro-optimal temperature control of water gas shift reaction using stable iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6399" to="6408" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stable iterative adaptive dynamic programming algorithm with approximation errors for discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1355" to="1367" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural-network-based adaptive optimal tracking control scheme for discrete-time nonlinear systems with approximation errors</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Finite-approximation-error based discrete-time iterative adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2820" to="2833" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Linear quadratic tracking control of partially-unknown continuous-time systems using reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3051" to="3056" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Actor-critic based optimal tracking for partially unknown nonlinear discrete-time systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kiumarsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="151" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kiumarsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1167" to="1175" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural-network-based online HJB solution for optimal robust guaranteed cost control of continuous-time uncertain nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2834" to="2847" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Decentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A distributed actor-critic algorithm and applications to mobile sensor network coordination problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pennesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Paschalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="492" to="497" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-agent differential graphical games: Online adaptive learning solution for synchronization with optimality</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hudas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1598" to="1611" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Integral Q-learning and explorized policy iteration for adaptive optimal control of continuous-time linear systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2850" to="2859" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="706" to="714" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adaptive learning in tracking control based on the dual critic network design</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="913" to="928" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generalized policy iteration for continuoustime systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="3224" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adaptive optimal controllers based on generalized policy iteration in a continuous-time framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Mediterr. Conf. Control Autom</title>
		<meeting>17th Mediterr. Conf. Control Autom<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On generalized policy iteration for continuous-time linear systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 50th IEEE Conf. Decis. Control Eur. Control Conf</title>
		<meeting>50th IEEE Conf. Decis. Control Eur. Control Conf<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="1722" to="1728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Infinite horizon self-learning optimal control of nonaffine discrete-time nonlinear systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="866" to="879" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Apostol</surname></persName>
		</author>
		<title level="m">Mathematical Analysis</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On-line learning control by association and reinforcement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="276" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Consistency of HDP applied to a simple reinforcement learning problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="189" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Toward off-policy learning control with function approximation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Mach. Learn</title>
		<meeting>27th Int. Conf. Mach. Learn<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="719" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1204" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Modern Control Systems, 12th ed</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Extreme learning machine for regression and multiclass classification</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="529" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
