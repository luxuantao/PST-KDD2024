<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Counterfactual Links for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alice</forename><forename type="middle">Decoder</forename><surname>Adam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bob</forename><surname>Helen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Encoder</forename><surname>Gnn</surname></persName>
						</author>
						<author>
							<persName><surname>Decoder</surname></persName>
						</author>
						<title level="a" type="main">Learning from Counterfactual Links for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to predict missing links is important for many graph-based applications. Existing methods were designed to learn the association between observed graph structure and existence of link between a pair of nodes. However, the causal relationship between the two variables was largely ignored for learning to predict links on a graph. In this work, we visit this factor by asking a counterfactual question: "would the link still exist if the graph structure became different from observation?" Its answer, counterfactual links, will be able to augment the graph data for representation learning. To create these links, we employ causal models that consider the information (i.e., learned representations) of node pairs as context, global graph structural properties as treatment, and link existence as outcome. We propose a novel data augmentation-based link prediction method that creates counterfactual links and learns representations from both the observed and counterfactual links. Experiments on benchmark data show that our graph learning method achieves state-of-theart performance on the task of link prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Link prediction seeks to predict the likelihood of edge existence between node pairs based on observed graph. Given the omnipresence of graph-structured data, link prediction has copious applications, such as movie recommendation <ref type="bibr" target="#b5">(Bennett et al., 2007)</ref>, chemical interaction prediction <ref type="bibr" target="#b57">(Stanfield et al., 2017)</ref>, and knowledge graph completion <ref type="bibr" target="#b26">(Kazemi &amp; Poole, 2018)</ref>. Graph machine learning methods have been widely applied to solve this problem. Their standard scheme is to first learn representation vectors of nodes and then learn the association between the representations of a pair of nodes and the existence of link between them. For example, graph neural networks (GNNs) use neighborhood aggregation to create the representation vectors: the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b18">Hamilton et al., 2017;</ref><ref type="bibr" target="#b71">Wu et al., 2020)</ref>. Then the vectors are fed into a binary classification model to learn the association. GNN methods have shown predominance in the task of link prediction <ref type="bibr" target="#b81">(Zhang et al., 2020)</ref>.</p><p>Unfortunately, the causal relationship between graph structure and link existence was largely ignored in previous work. Existing methods that learn from association are not able to capture essential factors to accurately predict missing links in test data. Take a specific social network as an example. Suppose Alice and Adam live in the same neighborhood and they are close friends. The association between neighborhood belonging and friendship could be too strong to discover the essential factors of friendship such as common interests or family relationships. Such factors could also be the cause of them living in the same neighborhood. So, our idea is to ask a counterfactual question: "would Alice and Adam still be close friends if they were not living in the same neighborhood?" If a graph learning model can learn the causal relationship by answering this counterfactual question, it will improve the accuracy of link prediction with such knowledge. Generally, the questions can be described as "would the link exist or not if the graph structure became different from observation?"</p><p>As known to many, counterfactual questions are the key component of causal inference and have been well defined in literature. A counterfactual question is usually framed with three factors: context (as a data point), manipulation (e.g., treatment, intervention, action, strategy), and outcome (Van der <ref type="bibr">Laan &amp; Petersen, 2007;</ref><ref type="bibr" target="#b24">Johansson et al., 2016)</ref>. (To simplify the language, we use "treatment" to refer to the manipulation in this paper, as readers might be familiar more with the word "treatment.") Given certain data context, it asks what the outcome would have been if the treatment had not been the observed value. In the scenario of link prediction, we consider the information of a pair of nodes as context, graph structural properties as treatment, and link existence as outcome. Recall the social network example. The context is the representations of Alice and Adam that are learned from their personal attributes and relationships Figure <ref type="figure">1</ref>. The proposed CFLP learns the causal relationship between the observed graph structure (e.g., neighborhood similarity, considered as treatment variable) and link existence (considered as outcome). In this example, the link predictor would be trained to estimate the individual treatment effect (ITE) as 1 − 1 = 0 so it looks for factors other than neighborhood to predict the factual link.</p><p>with others on the social network. The treatment is whether live in the same neighborhood, which can be identified by community detection. And the outcome is their friendship.</p><p>In this work, we propose a novel concept of "counterfactual link" that answers the counterfactual question and (based on this concept) a novel link prediction method (CFLP) that uses the counterfactual links as augmented data for graph representation learning. Figure <ref type="figure">1</ref> illustrates this two-step method. Suppose the treatment variable is defined as one type of global graph structure, e.g., the neighborhood assignment discovered by spectral clustering or community detection algorithms. We are wondering how likely the neighborhood distribution makes a difference on the link (non-)existence for each pair of nodes. So, given a pair of nodes (like Alice and Adam) and the treatment value on this pair (in the same neighborhood), we find a pair of nodes (like Helen and Bob) that satisfies two conditions: (1) it has a different treatment (in different neighborhoods) and (2) it is the most similar pair with the given pair of nodes. We name these matched pairs of nodes as counterfactual links. Note that the outcome of the counterfactual links can be either 1 or 0, depending on whether there exists an edge between the matched pair of nodes. The counterfactual link provides an unobservable outcome to the given pair of nodes under a counterfactual condition. The process of creating counterfactual links for all positive and negative training examples can be viewed as a graph data augmentation method, as it enriches the training set. Then, CFLP trains a link predictor (which is GNN-based) to learn the representation vectors of nodes to predict both the observed factual links and counterfactual links. In this Alice-Adam example, the link predictor is trained to estimate the individual treatment effect (ITE) of neighborhood assignment as 1 − 1 = 0, where ITE is a metric for the effect of treatment on the outcome and zero indicates the given treatment has no effect on the outcome. So, the learner will try to discover the essential factors on the friendship between Alice and Adam. CFLP learns from the counterfactual links to find these factors for graph learning models to accurately predict missing links.</p><p>Contributions. Our main contributions can be summarized as follows.</p><p>(1) This is the first work that aims at improving link prediction by causal inference, specifically, generating counterfactual links to answer counterfactual questions about link existence. (2) This work introduces CFLP that trains GNN-based link predictors to predict both factual and counterfactual links. It leverages causal relationship between global graph structure and link existence to enhance link prediction. (3) CFLP outperforms competitive baselines on several benchmark datasets. We analyze the impact of counterfactual links as well as the choice of treatment variable. This work sheds insights for improving graph machine learning with causal analysis, which has not been extensively studied yet, while the other direction (machine learning for causal inference) has been studied for long. Source code of the proposed CFLP method is publicly available at https://github.com/DM2-ND/CFLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Definition</head><p>Notations Let G = (V, E) be an undirected graph of N nodes, where V = {v 1 , v 2 , . . . , v N } is the set of nodes and E ⊆ V × V is the set of observed links. We denote the adjacency matrix as A ∈ {0, 1} N ×N , where A i,j = 1 indicates nodes v i and v j are connected and vice versa. We denote the node feature matrix as X ∈ R N ×F , where F is the number of node features and x i indicates the feature vector of node v i (the i-th row of X).</p><p>In this work, we follow the commonly accepted problem definition of link prediction on graph data <ref type="bibr">(Zhang &amp; Chen, 2018;</ref><ref type="bibr" target="#b81">Zhang et al., 2020;</ref><ref type="bibr" target="#b10">Cai et al., 2021)</ref>  an observed graph G (with validation and testing links masked off), predict the link existence between every pair of nodes. More specifically, for the GNN-based link prediction methods, they learn low-dimensional node representations Z ∈ R N ×H , where H is the dimensional size of latent space such that H ≪ N , and then use Z for the prediction of link existence between every node pair.</p><formula xml:id="formula_0">: Given 𝐳 ! 𝑇 "! 𝐳 " 𝐴 "! 𝐴 "! 𝐳 " 𝐳 ! 𝐴 "! 𝑇 "! 𝐶 " 𝐶 !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improving Graph Learning with Causal Model</head><p>Leveraging Causal Model(s) Counterfactual causal inference aims to find out the causal relationship between treatment and outcome by asking the counterfactual questions such as "would the outcome be different if the treatment was different?" <ref type="bibr" target="#b42">(Morgan &amp; Winship, 2015)</ref>. Figure <ref type="figure" target="#fig_0">2</ref>(a) is a typical example, in which we denote the context (confounder) as Z, treatment as T , and the outcome as Y . Given the context, treatments, and their corresponding outcomes, counterfactual inference methods aim to find the effect of treatment on the outcome, which is usually measured by individual treatment effect (ITE) and its expectation averaged treatment effect (ATE) <ref type="bibr">(Van der Laan &amp; Petersen, 2007;</ref><ref type="bibr" target="#b69">Weiss et al., 2015)</ref>. For a binary treatment variable T = {0, 1}, denoting g(z, T ) as the outcome of z given the treatment T , we have ITE(z) = g(z, 1) − g(z, 0), and ATE = E z∼Z ITE(z).</p><p>Ideally, we need all potential outcomes of the contexts under all kinds of treatments to study the causal relationships <ref type="bibr" target="#b42">(Morgan &amp; Winship, 2015)</ref>. However, in reality, the fact that we can only observe the outcome under one particular treatment prevents the ITE from being known <ref type="bibr" target="#b24">(Johansson et al., 2016)</ref>. Traditional causal inference methods use statistical learning approaches such as Neyman-Rubin causal model (BCM) and propensity score matching (PSM) to predict the value of ATE <ref type="bibr" target="#b52">(Rubin, 1974;</ref><ref type="bibr" target="#b53">2005)</ref>.</p><p>In this work, we look at link prediction with graph learning, which is to learn effective node representations Z for predicting link existence in test data. In Figure <ref type="figure" target="#fig_0">2</ref>(b), z i and z j are representations of nodes v i and v j , and the outcome A i,j is the link existence between v i and v j . Here, the objective is different from classic causal inference. In graph learning, we want to improve the learning of z i and z j with the estimation on the effect of treatment T i,j on the outcome A i,j . Specifically, for each pair of nodes (v i , v j ), its ITE can be estimated by ITE (vi,vj ) = g((z i , z j ), 1) − g((z i , z j ), 0)</p><p>(1)</p><p>and we use this information to improve the learning of Z.</p><p>We denote by A the observed adjacency matrix as the factual outcomes, and denote by A CF the unobserved matrix of the counterfactual links when the treatment is different as the counterfactual outcomes. We denote T ∈ {0, 1} N ×N as the binary factual treatment matrix, where T i,j indicates the treatment of the node pair (v i , v j ). We denote T CF as the counterfactual treatment matrix where T CF i,j = 1 − T i,j . We are interested in (a) estimating the counterfactual outcomes A CF and (b) learning from both factual and counterfactual outcomes A and A CF (as observed and augmented data) to enhance link prediction.</p><p>Treatment Variable Previous works on GNN-based link prediction <ref type="bibr">(Zhang &amp; Chen, 2018;</ref><ref type="bibr" target="#b81">Zhang et al., 2020)</ref> have shown that the message passing-based GNNs are capable to capture the structural information (e.g., Katz index) for link prediction. Nevertheless, as illustrated by the Alice-Adam example in Section 1, the association between such structural information and actual link existence may be too strong for models to discover more essential factors than it, hence resulting in sub-optimal link prediction performance. Therefore, in this work, we use the global structural role of each node pair as its treatment. It's worth mentioning that the causal model shown in Figure 2(b) does not limit the treatment to be structural roles, i.e., T i,j can be any binary property of node pair (v i , v j ). Without the loss of generality, we use Louvain <ref type="bibr" target="#b8">(Blondel et al., 2008)</ref>, an unsupervised approach that has been widely used for community detection, as an example. Louvain discovers community structure of a graph and assigns each node to one community. Then we can define the binary treatment variable as whether these two nodes in the pair belong to the same community. Let c : V → N be any graph mining/clustering method that outputs the index of community/cluster/neighborhood that each node belongs to. The treatment matrix T is defined as T i,j = 1 if c(v i ) = c(v j ), and T i,j = 0 otherwise. For the choice of c, we suggest methods that group nodes based on global graph structural information, including but not limited to Louvain <ref type="bibr" target="#b8">(Blondel et al., 2008)</ref>, K-core <ref type="bibr" target="#b4">(Bader &amp; Hogue, 2003)</ref>, and spectral clustering <ref type="bibr" target="#b43">(Ng et al., 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Counterfactual Links</head><p>To implement the solution based on above idea, we propose counterfactual links. As aforementioned, for each node pair, the observed data contains only the factual treatment and outcome, meaning that the link existence for the given node pair with an opposite treatment is unknown. Therefore, we use the outcome from the nearest observed context as a substitute. This type of matching on covariates is widely used to estimate treatment effects from observational data <ref type="bibr" target="#b24">(Johansson et al., 2016;</ref><ref type="bibr" target="#b1">Alaa &amp; Van Der Schaar, 2019)</ref>. That is, we want to find the nearest neighbor with the opposite treatment for each observed node pairs and use the nearest neighbor's outcome as a counterfactual link. Formally,</p><formula xml:id="formula_1">∀(v i , v j ) ∈ V × V, its counterfactual link (v a , v b ) is (v a , v b ) = arg min va,v b ∈V {h((v i , v j ), (v a , v b )) | T a,b = 1−T i,j },</formula><p>(2) where h(•, •) is a metric of measuring the distance between a pair of node pairs (a pair of contexts). Nevertheless, finding the nearest neighbors by computing the distance between all pairs of node pairs is extremely inefficient and infeasible in application, which takes O(N 4 ) comparisons (as there are totally O(N 2 ) node pairs). Hence we implement Eq. ( <ref type="formula">2</ref>) using node-level embeddings. Specifically, considering that we want to find the nearest node pair based on both the raw node features and structural features, we take the state-ofthe-art unsupervised graph representation learning method MVGRL <ref type="bibr">(Hassani &amp; Khasahmadi, 2020)</ref> to learn the node embeddings X ∈ R N × F from the observed graph (with validation and testing links masked off). We use X to find the nearest neighbors of node pairs. Therefore, ∀(v i , v j ) ∈ V × V, we define its counterfactual link (v a , v b ) as</p><formula xml:id="formula_2">(v a , v b ) = arg min va,v b ∈V {d(x i , xa ) + d(x j , xb ) | (3) T a,b = 1 − T i,j , d(x i , xa ) + d(x j , xb ) &lt; 2γ},</formula><p>where d(•, •) is specified as the Euclidean distance on the embedding space of X, and γ is a hyperparameter that defines the maximum distance that two nodes are considered as similar. When no node pair satisfies the above equation (i.e., there does not exist any node pair with opposite treatment that is close enough to the target node pair), we do not assign any nearest neighbor for the given node pair to ensure all the neighbors are similar enough (as substitutes) in the feature space. Thus, the counterfactual treatment matrix T CF and the counterfactual adjacency matrix A CF are defined as</p><formula xml:id="formula_3">T CF i,j , A CF i,j =      1 − T i,j , A a,b , if ∃ (v a , v b ) ∈ V × V satisfies Eq. (3); T i,j , A i,j</formula><p>, otherwise.</p><p>(4) It is worth noting that the node embeddings X and the nearest neighbors are computed only once and do not change during the learning process. X is only used for finding the nearest neighbors.</p><p>Learning from Counterfactual Distributions Let P F be the factual distribution of the observed contexts and treatments, and P CF be the counterfactual distribution that is composed of the observed contexts and opposite treatments. We define the empirical factual distribution</p><formula xml:id="formula_4">P F ∼ P F as P F = {(v i , v j , T i,j )} N i,j=1</formula><p>, and define the empirical counterfactual distribution P CF ∼ P CF as</p><formula xml:id="formula_5">P CF = {(v i , v j , T CF i,j )} N i,j=1</formula><p>. Unlike traditional link prediction methods that take only P F as input and use the observed outcomes A as the training target, we take advantage of the counterfactual distribution by using it as the augmented training data. That is, we use P CF as a complementary input and use the counterfactual outcomes A CF as the training target for the counterfactual data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning from Counterfactual Links</head><p>In this subsection, we present the design of our model as well as the training method. The input of the model in CFLP includes (1) the observed graph data A and raw feature matrix X, (2) the factual treatments T F and counterfactual treatments T CF , and (3) the counterfactual links data A CF . The output contains link prediction logits in A and A CF for the factual and counterfactual adjacency matrices A and A CF , respectively.</p><p>Graph Learning Model The model consist of two trainable components: a graph encoder f and a link decoder g. The graph encoder generates representation vectors of nodes from graph data. And the link decoder projects the representation vectors of node pairs into the link prediction logits. The choice of the graph encoder f can be any end-to-end GNN model. Without the loss of generality, here we use the commonly used graph convolutional network (GCN) <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2016a)</ref>. Each layer of GCN is defined as</p><formula xml:id="formula_6">H (l) = f (l) (A, H (l−1) ; W (l) ) = σ( D− 1 2 Ã D− 1 2 H (l−1) W (l) ),</formula><p>(5) where l is the layer index, Ã = A + I is the adjacency matrix with added self-loops, D is the diagonal degree matrix Dii = j Ãij , H (0) = X, W (l) is the learnable weight matrix at the l-th layer, and σ(•) denotes the nonlinear activation ReLU. We denote Z = f (A, X) ∈ R N ×H as the output from the encoder's last layer, i.e., the Hdimensional representation vectors of nodes. Following previous works <ref type="bibr">(Zhang &amp; Chen, 2018;</ref><ref type="bibr" target="#b81">Zhang et al., 2020)</ref>, we compute the representation of a node pair as the Hadamard product of the vectors of the two nodes. That is, the representation for the node pair (v i , v j ) is z i ⊙ z j ∈ R H , where ⊙ stands for the Hadamard product.</p><p>For the link decoder that predicts whether a link exists be-tween a pair of nodes, we opt for simplicity and adopt a simple decoder based on multi-layer perceptron (MLP), given the representations of node pairs and their treatments. That is, the decoder g is defined as</p><formula xml:id="formula_7">A = g(Z, T), s.t. A i,j = MLP([z i ⊙ z j , T i,j ]),<label>(6)</label></formula><formula xml:id="formula_8">A CF = g(Z, T CF ), s.t. A CF i,j = MLP([z i ⊙ z j , T CF i,j ]),<label>(7)</label></formula><p>where [•, •] stands for the concatenation of vectors, and A and A CF can also be used for estimating the observed ITE as aforementioned in Eq. ( <ref type="formula">1</ref>).</p><p>During the training process, data samples from the empirical factual distribution P F and the empirical counterfactual distribution P CF are fed into decoder g and optimized towards A and A CF , respectively. That is, for the two distributions, the loss functions are as follows:</p><formula xml:id="formula_9">L F = 1 N 2 N i=1 N j=1 A i,j • log A i,j<label>(8)</label></formula><formula xml:id="formula_10">+ (1 − A i,j ) • log(1 − A i,j ), L CF = 1 N 2 N i=1 N j=1 A CF i,j • log A CF i,j<label>(9)</label></formula><formula xml:id="formula_11">+ (1 − A CF i,j ) • log(1 − A CF i,j ).</formula><p>Balancing Counterfactual Learning In the training process, the above loss minimizations train the model on both the empirical factual distribution P F ∼ P F and empirical counterfactual distribution P CF ∼ P CF that are not necessarily equal -the training examples (node pairs) do not have to be aligned. However, at the stage of inference, the test data contains only observed (factual) samples. Such a gap between the training and testing data distributions exposes the model in the risk of covariant shift, which is a common issue in counterfactual representation learning <ref type="bibr" target="#b24">(Johansson et al., 2016;</ref><ref type="bibr" target="#b3">Assaad et al., 2021)</ref>.</p><p>To force the distributions of representations of factual distributions and counterfactual distributions to be similar, we adopt the discrepancy distance <ref type="bibr" target="#b37">(Mansour et al., 2009;</ref><ref type="bibr" target="#b24">Johansson et al., 2016)</ref> as another training objective to regularize the representation learning. That is, we use the following loss term to minimize the distance between the learned representations from P F and P CF :</p><formula xml:id="formula_12">L disc = disc( P F f , P CF f ), where disc(P, Q) = ||P − Q|| F ,<label>(10</label></formula><p>) where || • || F denotes the Frobenius Norm, and P F f and P CF f denote the node pair representations learned by graph encoder f from factual distribution and counterfactual distribution, respectively. Specifically, the learned representations for (v i , v j , T i,j ) and (v i , v j , T CF i,j ) are [z i ⊙ z j , T i,j ] (Eq. ( <ref type="formula" target="#formula_7">6</ref>)) and [z i ⊙ z j , T CF i,j ] (Eq. ( <ref type="formula" target="#formula_8">7</ref>)), respectively. </p><formula xml:id="formula_13">Z = f (A, X).</formula><p>Get A and A CF via g with Eqs. ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>). Update Θ f and Θ g with L. (Eq. ( <ref type="formula" target="#formula_15">11</ref>)) end for // decoder fine-tuning Freeze Θ f and re-initialize</p><formula xml:id="formula_14">Θ g . Z = f (A, X). for epoch in range(n epochs f t) do</formula><p>Get A via g with Eq. ( <ref type="formula" target="#formula_7">6</ref>). Update Θ g with L F . (Eq. ( <ref type="formula" target="#formula_9">8</ref>)) end for // inference Z = f (A, X). Get A and A CF via g with Eqs. ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>). Output: A for link prediction, A CF .</p><p>Training During the training of CFLP, we want the model to be optimized towards three targets: (1) accurate link prediction on the observed outcomes (Eq. ( <ref type="formula" target="#formula_9">8</ref>)), (2) accurate prediction on the counterfactual links (Eq. ( <ref type="formula" target="#formula_10">9</ref>)), and (3) regularization on the representation spaces learned from P F and P CF (Eq. ( <ref type="formula" target="#formula_12">10</ref>)). Therefore, the overall training loss of our proposed CFLP is</p><formula xml:id="formula_15">L = L F + α • L CF + β • L disc ,<label>(11)</label></formula><p>where α and β are hyperparameters to control the weights of counterfactual outcome estimation (link prediction) loss and discrepancy loss.</p><p>Summary Algorithm 1 summarizes the whole process of CFLP. The first step is to compute the factual and counterfactual treatments T, T CF as well as the counterfactual links A CF . Then, the second step trains the graph learning model on both the observed factual link existence and generated counterfactual link existence with the integrated loss function (Eq. ( <ref type="formula" target="#formula_15">11</ref>)). Note that the discrepancy loss (Eq. ( <ref type="formula" target="#formula_12">10</ref>)) is computed on the representations of node pairs learned by the graph encoder f , so the decoder g is trained with data from both P F and P CF without balancing the constraints. Therefore, after the model is sufficiently trained, we freeze the graph encoder f and fine-tune g with only the factual data. Finally, after the decoder is sufficiently fine-tuned, we output the link prediction logits for both the factual and counterfactual adjacency matrices.</p><p>Complexity The complexity of the first step (finding counterfactual links with nearest neighbors) is propor- tional to the number of node pairs. When γ is set as a small value to obtain indeed similar node pairs, this step (Eq. ( <ref type="formula">3</ref>)) uses constant time. Moreover, the computation in Eq. ( <ref type="formula">3</ref>) can be parallelized. Therefore, the time complexity is O(N 2 /C) where C is the number of processes.</p><p>For the complexity of the second step (training counterfactual learning model), the GNN encoder has time complexity of O(LH 2 N + LH|E|) <ref type="bibr" target="#b71">(Wu et al., 2020)</ref>, where L is the number of GNN layers and H is the size of node representations. Given that we sample the same number of non-existing links as that of observed links during training, the complexity of a three-layer MLP decoder is O(((H + 1)</p><formula xml:id="formula_16">• d h + d h • 1)|E|) = O(d h (H + 2)|E|)</formula><p>, where d h is the number of neurons in the hidden layer. Therefore, the second step has linear time complexity w.r.t. the sum of node and edge counts.</p><p>Limitations First, as mentioned above, the computation of finding counterfactual links has a worst-case complexity of O(N 2 ). Second, CFLP performs counterfactual prediction with only a single treatment; however, there are quite a few kinds of graph structural information that can be considered as treatments. Future work can leverage the rich structural information by bundled treatments <ref type="bibr" target="#b89">(Zou et al., 2020)</ref> in the generation of counterfactual links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We conduct experiments on five benchmark datasets including citation networks (CORA, CITESEER, PUBMED <ref type="bibr" target="#b75">(Yang et al., 2016)</ref>), social network (FACEBOOK (McAuley &amp; Leskovec, 2012)), and drug-drug interaction network (OGB-DDI <ref type="bibr" target="#b70">(Wishart et al., 2018)</ref>) from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b22">(Hu et al., 2020)</ref>. For the first four datasets, we randomly select 10%/20% of the links and the same numbers of disconnected node pairs as validation/test samples. The links in the validation and test sets are masked off from the training graph. For OGB-DDI, we used the OGB official train/validation/test splits. Statistics for the datasets are shown in Table <ref type="table" target="#tab_2">1</ref>, with more details in Appendix. We use K-core <ref type="bibr" target="#b4">(Bader &amp; Hogue, 2003)</ref> clusters as the default treatment variable. We evaluate CFLP on three commonly used GNN encoders: GCN <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2016a)</ref>, GSAGE <ref type="bibr" target="#b18">(Hamilton et al., 2017)</ref>, and JKNet <ref type="bibr" target="#b73">(Xu et al., 2018)</ref>. We compare the link prediction performance of CFLP against Node2Vec <ref type="bibr" target="#b17">(Grover &amp; Leskovec, 2016)</ref>, MVGRL <ref type="bibr">(Hassani &amp; Khasahmadi, 2020)</ref>, VGAE <ref type="bibr" target="#b28">(Kipf &amp; Welling, 2016b)</ref>, SEAL <ref type="bibr">(Zhang &amp; Chen, 2018)</ref>, LGLP <ref type="bibr" target="#b10">(Cai et al., 2021)</ref>, and GNNs with MLP decoder. We report averaged test performance and their standard deviation over 20 runs with different random parameter initializations. Other than the most commonly used of Area Under ROC Curve (AUC), we report Hits@20 (one of the primary metrics on OGB leaderboard) as a more challenging metric, as it expects models to rank positive edges higher than nearly all negative edges.</p><p>Besides performance comparison on link prediction, we will answer two questions to suggest a way of choosing a treatment variable for creating counterfactual links: (Q1) Does CFLP sufficiently learn the observed averaged treatment effect (ATE) derived from the counterfactual links? (Q2) What is the relationship between the estimated ATE learned in the method and the prediction performance? If the answer to Q1 is yes, then the answer to Q2 will indicate how to choose treatment based on observed ATE. To answer the Q1, we calculate the observed ATE ( ATE obs ) by comparing the observed links in A and created counterfactual links A CF that have opposite treatments. And we calculate the estimated ATE ( ATE est ) by comparing the predicted links in A and predicted counterfactual links A CF . Formally, ATE obs and ATE est are defined as</p><formula xml:id="formula_17">ATE obs = 1 N 2 N i=1 N j=1 {T ⊙ (A − A CF ) (12) + (1 N ×N − T) ⊙ (A CF − A)} i,j . ATE est = 1 N 2 N i=1 N j=1 {T ⊙ ( A − A CF ) (13) + (1 N ×N − T) ⊙ ( A CF − A)} i,j .</formula><p>The treatment variables we will investigate are generally graph clustering or community detection methods, such as K-core <ref type="bibr" target="#b4">(Bader &amp; Hogue, 2003)</ref>, stochastic block model (SBM) <ref type="bibr" target="#b25">(Karrer &amp; Newman, 2011)</ref>, spectral clustering (SpecC) <ref type="bibr" target="#b43">(Ng et al., 2001)</ref>, propagation clustering (PropC) <ref type="bibr" target="#b49">(Raghavan et al., 2007)</ref>, Louvain <ref type="bibr" target="#b8">(Blondel et al., 2008)</ref>, common neighbors (CommN), Katz index, and hierarchical clustering (Ward) <ref type="bibr" target="#b68">(Ward Jr, 1963)</ref>. We use JKNet <ref type="bibr" target="#b73">(Xu et al., 2018)</ref> as default graph encoder.</p><p>Implementation details and supplementary experimental results (e.g., sensitivity on γ, ablation study on L CF and L disc ) can be found in Appendix. Source code is available in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Link Prediction Tables <ref type="table" target="#tab_4">2 and 3</ref> show the link prediction performance of Hits@20 and AUC by all methods.</p><p>LGLP on PUBMED and OGB-DDI are missing due to the out of memory error when running the official code package from the authors. We observe that our CFLP on different graph encoders achieve similar or better performances compared with baselines. The only exception is the AUC on FACE-BOOK where most methods have close-to-perfect AUC. As AUC is a relatively easier metric comparing with Hits@20, most methods achieved good performance on AUC. We observe that CFLP with JKNet almost consistently achieves the best performance and outperforms baselines significantly on Hits@20. Specifically, comparing with the best baseline, CFLP improves relatively by 16.4% and 0.8% on Hits@20 and AUC, respectively. Comparing with the best performing baselines, which are also GNN-based, CFLP benefits from learning with both observed link existence (A) and our defined counterfactual links (A CF ).</p><p>ATE with Different Treatments Tables <ref type="table" target="#tab_5">4 and 5</ref> show the link prediction performance, ATE obs , and ATE est of CFLP (with JKNet) when using different treatments. The treatments in Tables <ref type="table" target="#tab_5">4 and 5</ref> are sorted by the Hits@20 performance. Bigger ATE indicates stronger causal relationship between the treatment and outcome, and vice versa. We observe: (1) the rankings of ATE est and ATE obs are positively correlated with Kendell's ranking coefficient <ref type="bibr" target="#b0">(Abdi, 2007)</ref> of 0.67 and 0.57 for CORA and CITESEER, respectively. Hence, CFLP was sufficiently trained to learn the causal relationship between graph structure information and link existence; (2) ATE obs and ATE est are both negatively correlated with the link prediction performance, showing that we can pick a proper treatment prior to training a model with CFLP. Using the treatment that has the weakest causal relationship with link existence is likely to train the model to capture more essential factors on the outcome, in a way similar to denoising the unrelated information from the rep- resentations. While methods that learn from only observed data may assume strongly positive correlation for this treatment, the counterfactual data are more useful to complement the partial observations for learning better representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Link Prediction With its wide applications, link prediction has drawn attention from many research communities including statistical machine learning and data mining. Stochastic generative methods based on stochastic block models (SBM) are developed to generate links <ref type="bibr" target="#b40">(Mehta et al., 2019)</ref>. In data mining, matrix factorization <ref type="bibr" target="#b41">(Menon &amp; Elkan, 2011</ref>), heuristic methods <ref type="bibr" target="#b47">(Philip et al., 2010;</ref><ref type="bibr" target="#b38">Martínez et al., 2016)</ref>, and graph embedding methods <ref type="bibr" target="#b11">(Cui et al., 2018)</ref> have been applied to predict links in the graph. Heuristic methods compute the similarity score of nodes based on their neighborhoods. These methods can be generally categorized into first-order, second-order, and high-order heuristics based on the maximum distance of the neighbors. Graph embedding methods learn latent node features via embedding lookup and use them for link prediction <ref type="bibr" target="#b46">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b58">Tang et al., 2015;</ref><ref type="bibr" target="#b17">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b63">Wang et al., 2016)</ref>.</p><p>In the past few years, GNNs have shown promising results on various graph-based tasks with their ability of learning from features and custom aggregations on structures <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b18">Hamilton et al., 2017;</ref><ref type="bibr" target="#b36">Ma et al., 2021;</ref><ref type="bibr" target="#b23">Jiang et al., 2022)</ref>. With node pair representations and an attached MLP or inner-product decoder, GNNs can be used for link prediction <ref type="bibr" target="#b12">(Davidson et al., 2018;</ref><ref type="bibr" target="#b74">Yang et al., 2018;</ref><ref type="bibr" target="#b81">Zhang et al., 2020;</ref><ref type="bibr" target="#b79">Yun et al., 2021;</ref><ref type="bibr" target="#b88">Zhu et al., 2021b;</ref><ref type="bibr" target="#b64">Wang et al., 2021a;</ref><ref type="bibr">b)</ref>. For example, VGAE used GCN to learn node representations and reconstruct the graph structure <ref type="bibr" target="#b28">(Kipf &amp; Welling, 2016b)</ref>. SEAL extracted a local subgraph around each target node pair and then learned local subgraph representation for link prediction <ref type="bibr">(Zhang &amp; Chen, 2018)</ref>. Following the scheme of SEAL, <ref type="bibr" target="#b9">Cai &amp; Ji (2020)</ref> proposed to improve local subgraph representation learning by multi-scale graph representation. And LGLP proposed to invert the local subgraphs to line graphs <ref type="bibr" target="#b10">(Cai et al., 2021)</ref>. However, little work has studied to use causal inference for improving link prediction.</p><p>Causal Inference Causal inference methods usually reweighted samples based on propensity score <ref type="bibr" target="#b51">(Rosenbaum &amp; Rubin, 1983;</ref><ref type="bibr" target="#b4">Austin, 2011)</ref> to remove confounding bias from binary treatments. Recently, several works studied about learning treatment invariant representation to predict the counterfactual outcomes <ref type="bibr" target="#b54">(Shalit et al., 2017;</ref><ref type="bibr" target="#b32">Li &amp; Fu, 2017;</ref><ref type="bibr" target="#b76">Yao et al., 2018;</ref><ref type="bibr" target="#b77">Yoon et al., 2018;</ref><ref type="bibr" target="#b20">Hassanpour &amp; Greiner, 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b7">Bica et al., 2020)</ref>. Few recent works combined causal inference with graph learning <ref type="bibr" target="#b55">(Sherman &amp; Shpitser, 2020;</ref><ref type="bibr" target="#b6">Bevilacqua et al., 2021;</ref><ref type="bibr" target="#b33">Lin et al., 2021;</ref><ref type="bibr" target="#b14">Feng et al., 2021)</ref>. For example, <ref type="bibr" target="#b55">Sherman &amp; Shpitser (2020)</ref> proposed network intervention to study the effect of link creation on network structure changes.</p><p>As a mean of learning the causality between treatment and outcome, counterfactual prediction has been used for a variety of applications such as recommender systems <ref type="bibr" target="#b67">(Wang et al., 2020b;</ref><ref type="bibr" target="#b72">Xu et al., 2020)</ref>, health care <ref type="bibr" target="#b2">(Alaa &amp; van der Schaar, 2017;</ref><ref type="bibr" target="#b45">Pawlowski et al., 2020)</ref>, and decision making <ref type="bibr" target="#b31">(Kusner et al., 2017;</ref><ref type="bibr" target="#b48">Pitis et al., 2020)</ref>. To infer the causal relationships, previous work usually estimated the ITE via function fitting models <ref type="bibr" target="#b29">(Kuang et al., 2017;</ref><ref type="bibr" target="#b62">Wager &amp; Athey, 2018;</ref><ref type="bibr" target="#b30">Kuang et al., 2019;</ref><ref type="bibr" target="#b3">Assaad et al., 2021)</ref>.</p><p>Graph Data Augmentation Graph data augmentation (GDA) methods generate perturbed or modified graph data <ref type="bibr" target="#b82">(Zhao et al., 2021a;</ref><ref type="bibr">b)</ref> to improve the generalizability of graph machine learning models. Two comprehensive surveys of graph data augmentation are given by <ref type="bibr" target="#b84">Zhao et al. (2022)</ref> and <ref type="bibr" target="#b13">Ding et al. (2022)</ref>. So far, most GDA methods have been focusing on node-level tasks <ref type="bibr" target="#b44">(Park et al., 2021)</ref> and graph-level tasks <ref type="bibr" target="#b34">(Liu et al., 2022;</ref><ref type="bibr" target="#b35">Luo et al., 2022)</ref>. Due to the non-Euclidean structure of graphs, most GDA work focused on modifying the graph structure. E.g., edge dropping methods <ref type="bibr" target="#b50">(Rong et al., 2019;</ref><ref type="bibr" target="#b85">Zheng et al., 2020;</ref><ref type="bibr" target="#b34">Luo et al., 2021)</ref> drop edges during training to reduce overfitting. <ref type="bibr" target="#b82">Zhao et al. (2021a)</ref> used link predictor to manipulate the graph structure and improve the graph's homophily. Recently, several works also combined GDA with self-supervised learning objectives such as contrastive learning <ref type="bibr" target="#b78">(You et al., 2020;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b87">Zhu et al., 2021a)</ref> and consistency loss <ref type="bibr" target="#b66">(Wang et al., 2020a;</ref><ref type="bibr" target="#b14">Feng et al., 2020)</ref>. Nevertheless, GDA for link prediction has been under-explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this work, we presented the novel concept of counterfactual link and a novel graph learning method for link prediction (CFLP). The counterfactual links answered the counterfactual questions on the link existence and were used as augmented training data, with which CFLP accurately predicted missing links by exploring the causal relationship between global graph structure and link existence. Extensive experiments demonstrated that CFLP achieved the state-of-the-art performance on benchmark datasets. This work sheds insights that a good use of causal models (even basic ones) can greatly improve the performance of (graph) machine learning tasks such as link prediction. We note that the use of more sophistically designed causal models may lead to larger improvements for machine learning tasks, which can be a valuable future direction for the research community. Other than cluster-based global graph structure as treatment, other choices (with both empirical and theoretical analyses) are also worthy of exploration. and Ward <ref type="bibr" target="#b68">(Ward Jr, 1963)</ref>. We used implementation of K-core <ref type="bibr" target="#b4">(Bader &amp; Hogue, 2003)</ref> from networkx.<ref type="foot" target="#foot_1">11</ref> We used SBM <ref type="bibr" target="#b25">(Karrer &amp; Newman, 2011</ref>) from a public implementation by <ref type="bibr">Funke &amp; Becker (2019)</ref>.<ref type="foot" target="#foot_2">12</ref> For CommN and Katz, we set T i,j = 1 if the number of common neighbors or Katz index between v i and v j are greater or equal to 2 or 2 times the average of all Katz index values, respectively. For SpecC, we set the number of clusters as 16. For SBM, we set the number of communities as 16. These settings are fixed for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results and Discussions</head><p>Link Prediction Tables <ref type="table" target="#tab_7">6 and 7</ref> show the link prediction performance of Hits@50 and Average Precision (AP) by all methods.</p><p>LGLP on PUBMED and OGB-DDI are missing due to the out of memory error when running the code package from the authors. Similar to the results in Tables <ref type="table" target="#tab_4">2 and 3</ref>, we observe that our CFLP on different graph encoders achieve similar or better performances compared with baselines, with the only exception of AP on FACEBOOK where most methods have close-to-perfect AP. From Tables 2, 3, 6 and 7, we observe that CFLP achieves improvement over all GNN architectures (averaged across datasets). Specifically, CFLP improves 25.6% (GCN), 12.0% (GSAGE), and 36.3% (JKNet) on Hits@20, 9.6% (GCN), 5.0% (GSAGE), and 17.8% (JKNet) on Hits@50, 5.6% (GCN), 1.6% (GSAGE), and 1.9% (JKNet) on AUC, and 0.8% (GCN), 0.8% (GSAGE), and 1.8% (JKNet) on AP. We note that CFLP with JKNet almost consistently achieves the best performance and outperforms baselines significantly on Hits@50. Specifically, compared with the best baseline, CFLP improves relatively by 6.8% and 0.9% on Hits@50 and AP, respectively. Ablation Study on Losses For the ablative studies of L CF (Eq. ( <ref type="formula" target="#formula_10">9</ref>)) and L disc (Eq. ( <ref type="formula" target="#formula_12">10</ref>)), we show their effect by removing them from the integrated loss function (Eq. ( <ref type="formula" target="#formula_15">11</ref>)). Table <ref type="table" target="#tab_8">8</ref> shows the results of CFLP on CORA and CITESEER under different settings (α = 0, β = 0, α = β = 0, and original setting). We observe that CFLP in the original setting achieves the best performance. The performance drops significantly when having α = 0, i.e., not using any counterfactual data during training. We note that having β = 0, i.e., not using the discrepancy loss, also lowers the performance. Therefore, both L CF and L disc are essential for improving the link prediction performance.</p><p>Ablation Study on Node Embedding X As the node embedding X is used in the early step of CFLP for finding the counterfactual links, the quality of X may affect the later learning process. Therefore, we also evaluate CFLP with different state-of-the-art unsupervised graph representation learning methods: MVGRL <ref type="bibr">(Hassani &amp; Khasahmadi, 2020)</ref>, DGI <ref type="bibr" target="#b61">(Velickovic et al., 2019)</ref>, and GRACE <ref type="bibr" target="#b86">(Zhu et al., 2020)</ref>. Table <ref type="table">9</ref> shows the link prediction performance of CFLP (w/ JKNet) on CORA and CITESEER with different node embeddings. We observe that the choice of the method for learning X does have an impact on the later learning process as well as the link prediction performance. Nevertheless, Table <ref type="table">9</ref> shows CFLP's advantage can be consistently observed with different choices of methods for learning X, as CFLP with X learned from all three methods showed promising link prediction performance.</p><p>Sensitivity Analysis of α and β Figure <ref type="figure" target="#fig_2">3</ref> shows the AUC performance of CFLP on CORA with different combinations of α and β. We observe that the performance is the poorest when α = β = 0 and gradually improves and gets stable as α and β increase, showing that CFLP is generally robust to the hyperparameters α and β, and the optimal values are easy to locate. Sensitivity Analysis of γ Figure <ref type="figure" target="#fig_3">4</ref> shows the Hits@20 and AUC performance on link prediction of CFLP (with JKNet) on CORA and CITESEER with different treatments and γ pct . We observe that the performance is generally good when 10 ≤ γ pct ≤ 20 and gradually get worse when the value of γ pct is too small or too large, showing that CFLP is robust to γ and the optimal γ is easy to find.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Our proposed work improves graph representation learning by leveraging causal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance of CFLP on CORA w.r.t different combinations of α and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Hits@20 and AUC performances of CFLP (w/ JKNet) on CORA and CITESEER with different treatments w.r.t. different γpct value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 CFLP Input: f , g, A, X, n epochs, n epoch f t Compute T as presented in Section 3.1. Compute T CF , A CF by Eqs. (3) and (4). // model training for epoch in range(n epochs) do</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Statistics of datasets used in the experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="5">CORA CITESEER PUBMED FACEBOOK OGB-DDI</cell></row><row><cell># nodes</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>4,039</cell><cell>4,267</cell></row><row><cell># links</cell><cell>5,278</cell><cell>4,552</cell><cell>44,324</cell><cell>88,234</cell><cell>1,334,889</cell></row><row><cell># validation node pairs</cell><cell>1,054</cell><cell>910</cell><cell>8,864</cell><cell>17,646</cell><cell>235,371</cell></row><row><cell># test node pairs</cell><cell>2,110</cell><cell>1,820</cell><cell>17,728</cell><cell>35,292</cell><cell>229,088</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Link prediction performances measured by Hits@20. Best performance and best baseline performance are marked with bold and underline, respectively.</figDesc><table><row><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell>FACEBOOK</cell><cell>OGB-DDI</cell></row><row><cell>Node2Vec</cell><cell cols="5">49.96±2.51 47.78±1.72 39.19±1.02 24.24±3.02 23.26±2.09</cell></row><row><cell>MVGRL</cell><cell cols="5">19.53±2.64 14.07±0.79 14.19±0.85 14.43±0.33 10.02±1.01</cell></row><row><cell>VGAE</cell><cell cols="5">45.91±3.38 44.04±4.86 23.73±1.61 37.01±0.63 11.71±1.96</cell></row><row><cell>SEAL</cell><cell cols="5">51.35±2.26 40.90±3.68 28.45±3.81 40.89±5.70 30.56±3.86</cell></row><row><cell>LGLP</cell><cell cols="2">62.98±0.56 57.43±3.71</cell><cell>-</cell><cell>37.86±2.13</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="5">49.06±1.72 55.56±1.32 21.84±3.87 53.89±2.14 37.07±5.07</cell></row><row><cell>GSAGE</cell><cell cols="5">53.54±2.96 53.67±2.94 39.13±4.41 45.51±3.22 53.90±4.74</cell></row><row><cell>JKNet</cell><cell cols="5">48.21±3.86 55.60±2.17 25.64±4.11 52.25±1.48 60.56±8.69</cell></row><row><cell cols="3">Our proposed CFLP with different graph encoders</cell><cell></cell><cell></cell></row><row><cell>CFLP w/ GCN</cell><cell cols="5">60.34±2.33 59.45±2.30 34.12±2.72 53.95±2.29 52.51±1.09</cell></row><row><cell cols="6">CFLP w/ GSAGE 57.33±1.73 53.05±2.07 43.07±2.36 47.28±3.00 75.49±4.33</cell></row><row><cell>CFLP w/ JKNet</cell><cell cols="5">65.57±1.05 68.09±1.49 44.90±2.00 55.22±1.29 86.08±1.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Link prediction performances measured by AUC. Best performance and best baseline performance are marked with bold and underline, respectively.</figDesc><table><row><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell>FACEBOOK</cell><cell>OGB-DDI</cell></row><row><cell>Node2Vec</cell><cell cols="5">84.49±0.49 80.00±0.68 80.32±0.29 86.49±4.32 90.83±0.02</cell></row><row><cell>MVGRL</cell><cell cols="5">75.07±3.63 61.20±0.55 80.78±1.28 79.83±0.30 81.45±0.99</cell></row><row><cell>VGAE</cell><cell cols="5">88.68±0.40 85.35±0.60 95.80±0.13 98.66±0.04 93.08±0.15</cell></row><row><cell>SEAL</cell><cell cols="5">92.55±0.50 85.82±0.44 96.36±0.28 99.60±0.02 97.85±0.17</cell></row><row><cell>LGLP</cell><cell cols="2">91.30±0.05 89.41±0.13</cell><cell>-</cell><cell>98.51±0.01</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="5">90.25±0.53 71.47±1.40 96.33±0.80 99.43±0.02 99.82±0.05</cell></row><row><cell>GSAGE</cell><cell cols="5">90.24±0.34 87.38±1.39 96.78±0.11 99.29±0.04 99.93±0.02</cell></row><row><cell>JKNet</cell><cell cols="5">89.05±0.67 88.58±1.78 96.58±0.23 99.43±0.02 99.94±0.01</cell></row><row><cell cols="3">Our proposed CFLP with different graph encoders</cell><cell></cell><cell></cell></row><row><cell>CFLP w/ GCN</cell><cell cols="5">92.55±0.50 89.65±0.20 96.99±0.08 99.38±0.01 99.44±0.05</cell></row><row><cell cols="6">CFLP w/ GSAGE 92.61±0.52 91.84±0.20 97.01±0.01 99.34±0.10 99.83±0.05</cell></row><row><cell>CFLP w/ JKNet</cell><cell cols="5">93.05±0.24 92.12±0.47 97.53±0.17 99.31±0.04 99.94±0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results of CFLP with different treatments on CORA.</figDesc><table><row><cell cols="2">(sorted by Hits@20)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hits@20 ATE obs</cell><cell>ATEest</cell></row><row><cell>K-core</cell><cell>65.6±1.1</cell><cell>0.002</cell><cell>0.013±0.003</cell></row><row><cell>SBM</cell><cell>64.2±1.1</cell><cell>0.006</cell><cell>0.023±0.015</cell></row><row><cell cols="2">CommN 62.3±1.6</cell><cell>0.007</cell><cell>0.053±0.021</cell></row><row><cell>PropC</cell><cell>61.7±1.4</cell><cell>0.037</cell><cell>0.059±0.065</cell></row><row><cell>Ward</cell><cell>61.2±2.3</cell><cell>0.001</cell><cell>0.033±0.012</cell></row><row><cell>SpecC</cell><cell>59.3±2.8</cell><cell>0.002</cell><cell>0.033±0.011</cell></row><row><cell cols="2">Louvain 57.6±1.8</cell><cell>0.025</cell><cell>0.138±0.091</cell></row><row><cell>Katz</cell><cell>56.6±3.4</cell><cell>0.740</cell><cell>0.802±0.041</cell></row><row><cell cols="4">Table 5. Results of CFLP with different treatments on CITESEER.</cell></row><row><cell cols="2">(sorted by Hits@20)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Hits@20</cell><cell>ATE obs</cell><cell>ATEest</cell></row><row><cell>SBM</cell><cell>71.6 ±1.9</cell><cell>0.004</cell><cell>0.005 ±0.001</cell></row><row><cell>K-core</cell><cell>68.1±1.5</cell><cell>0.002</cell><cell>0.010±0.002</cell></row><row><cell>Ward</cell><cell>67.0±1.7</cell><cell>0.003</cell><cell>0.037±0.009</cell></row><row><cell>PropC</cell><cell>64.6±3.6</cell><cell>0.141</cell><cell>0.232±0.113</cell></row><row><cell>Louvain</cell><cell>63.3±2.5</cell><cell>0.126</cell><cell>0.151±0.078</cell></row><row><cell>SpecC</cell><cell>59.9±1.3</cell><cell>0.009</cell><cell>0.166±0.034</cell></row><row><cell>Katz</cell><cell>57.3±0.5</cell><cell>0.245</cell><cell>0.224±0.037</cell></row><row><cell cols="2">CommN 56.8±4.9</cell><cell>0.678</cell><cell>0.195±0.034</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Link prediction performances measured by Hits@50. Best performance and best baseline performance are marked with bold and underline, respectively.</figDesc><table><row><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell>FACEBOOK</cell><cell>OGB-DDI</cell></row><row><cell>Node2Vec</cell><cell cols="5">63.64±0.76 54.57±1.40 50.73±1.10 43.91±1.03 24.34±1.67</cell></row><row><cell>MVGRL</cell><cell cols="5">29.97±3.06 26.48±0.98 16.96±0.56 17.06±0.19 12.03±0.11</cell></row><row><cell>VGAE</cell><cell cols="5">60.36±2.71 54.68±3.15 41.98±0.31 51.36±0.93 23.00±1.66</cell></row><row><cell>SEAL</cell><cell cols="5">51.68±2.85 54.55±1.77 42.85±2.03 57.20±1.85 40.85±2.97</cell></row><row><cell>LGLP</cell><cell cols="2">71.43±0.75 69.98±0.16</cell><cell>-</cell><cell>56.22±0.49</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="5">64.93±1.62 63.38±1.73 39.20±6.47 69.90±0.65 73.70±3.99</cell></row><row><cell>GSAGE</cell><cell cols="5">63.18±3.39 61.71±2.43 54.81±2.67 62.53±4.24 86.83±3.85</cell></row><row><cell>JKNet</cell><cell cols="5">62.64±1.40 62.26±2.10 45.16±3.18 68.81±1.76 91.48±2.41</cell></row><row><cell cols="3">Our proposed CFLP with different graph encoders</cell><cell></cell><cell></cell></row><row><cell>CFLP w/ GCN</cell><cell cols="5">72.61±0.92 69.85±1.11 55.00±1.95 70.47±0.77 62.47±1.53</cell></row><row><cell cols="6">CFLP w/ GSAGE 73.25±0.94 64.75±2.27 58.16±1.40 63.89±2.08 83.32±3.61</cell></row><row><cell>CFLP w/ JKNet</cell><cell cols="5">75.49±1.54 77.01±1.92 62.80±0.79 71.41±0.61 93.07±1.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Link prediction performances measured by Average Precision (AP). Best performance and best baseline performance are marked with bold and underline, respectively.</figDesc><table><row><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell>FACEBOOK</cell><cell>OGB-DDI</cell></row><row><cell>Node2Vec</cell><cell cols="5">88.53±0.42 84.42±0.48 87.15±0.12 99.07±0.02 98.39±0.04</cell></row><row><cell>MVGRL</cell><cell cols="5">76.47±3.07 67.40±0.52 82.00±0.97 82.37±0.35 81.12±1.77</cell></row><row><cell>VGAE</cell><cell cols="5">89.89±0.50 86.97±0.78 95.97±0.16 98.60±0.04 95.28±0.11</cell></row><row><cell>SEAL</cell><cell cols="5">89.08±0.57 88.55±0.32 96.33±0.28 99.51±0.03 98.39±0.21</cell></row><row><cell>LGLP</cell><cell cols="2">93.05±0.03 91.62±0.09</cell><cell>-</cell><cell>98.62±0.01</cell><cell>-</cell></row><row><cell>GCN</cell><cell cols="5">91.42±0.45 90.87±0.52 96.19±0.88 99.42±0.02 99.86±0.03</cell></row><row><cell>GSAGE</cell><cell cols="5">91.52±0.46 89.43±1.15 96.93±0.11 99.27±0.06 99.93±0.01</cell></row><row><cell>JKNet</cell><cell cols="5">90.50±0.22 90.42±1.34 96.56±0.31 99.41±0.02 99.95±0.01</cell></row><row><cell cols="3">Our proposed CFLP with different graph encoders</cell><cell></cell><cell></cell></row><row><cell>CFLP w/ GCN</cell><cell cols="5">93.77±0.49 91.84±0.20 97.16±0.08 99.40±0.01 99.60±0.03</cell></row><row><cell cols="6">CFLP w/ GSAGE 93.55±0.49 90.80±0.87 97.10±0.08 99.29±0.06 99.88±0.04</cell></row><row><cell>CFLP w/ JKNet</cell><cell cols="5">94.24±0.28 93.92±0.41 97.69±0.13 99.35±0.02 99.96±0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Link prediction performance of CFLP (w/ JKNet) on CORA and CITESEER when removing LCF or L disc or both versus normal setting.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CORA</cell><cell cols="2">CITESEER</cell></row><row><cell></cell><cell></cell><cell>Hits@20</cell><cell>AUC</cell><cell>Hits@20</cell><cell>AUC</cell></row><row><cell></cell><cell>CFLP (α = 0)</cell><cell cols="4">58.58±0.23 89.16±0.93 65.49±2.18 91.01±0.64</cell></row><row><cell></cell><cell>CFLP (β = 0)</cell><cell cols="4">62.27±0.84 92.96±0.34 66.92±1.84 91.98±0.17</cell></row><row><cell></cell><cell cols="5">CFLP (α = β = 0) 58.52±0.83 88.79±0.28 64.69±3.25 90.61±0.64</cell></row><row><cell></cell><cell>CFLP</cell><cell cols="4">65.57±1.05 93.05±0.24 68.09±1.49 92.12±0.47</cell></row><row><cell cols="7">Table 9. Link prediction performance of CFLP (w/ JKNet) on CORA and CITESEER with node embeddings ( X) learned from different</cell></row><row><cell>methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CORA</cell><cell></cell><cell cols="2">CITESEER</cell><cell cols="2">OGB-DDI</cell></row><row><cell></cell><cell>Hits@20</cell><cell>AUC</cell><cell>Hits@20</cell><cell>AUC</cell><cell>Hits@20</cell><cell>AUC</cell></row><row><cell cols="7">(MVGRL) 65.57±1.05 93.05±0.24 68.09±1.49 92.12±0.47 86.08±1.98 99.94±0.01</cell></row><row><cell>(GRACE)</cell><cell cols="6">62.54±1.41 92.28±0.69 68.68±1.75 93.80±0.36 82.30±3.32 99.93±0.01</cell></row><row><cell>(DGI)</cell><cell cols="6">61.04±1.52 92.99±0.49 72.17±1.08 93.34±0.51 85.61±1.66 99.94±0.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Department of Computer Science and Engineering, University of Notre Dame, IN, USA. Correspondence to: Tong Zhao &lt;tzhao2@nd.edu&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_1">https://networkx.org/documentation/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_2">https://github.com/funket/pysbm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by NSF Grants IIS-1849816, IIS-2142827, and IIS-2146761.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Dataset Details</head><p>In this section, we provide some additional dataset details. All the datasets used in this work are publicly available.</p><p>Citation Networks CORA, CITESEER, and PUBMED are citation networks that were first used by <ref type="bibr" target="#b75">Yang et al. (2016)</ref> and then commonly used as benchmarks in GNN-related literature <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b60">Veličković et al., 2017)</ref>. In these citation networks, the nodes are published papers and features are bag-of-word vectors extracted from the corresponding paper. Links represent the citation relation between papers. We loaded the datasets with the DGL 1 package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Network</head><p>The FACEBOOK dataset 2 is a social network constructed from friends lists from Facebook <ref type="bibr" target="#b39">(McAuley &amp; Leskovec, 2012)</ref>. The nodes are Facebook users and links indicate the friendship relation on Facebook. The node features were constructed from the user profiles and anonymized by <ref type="bibr" target="#b39">McAuley &amp; Leskovec (2012)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Encyclopedia of Measurement and Statistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="508" to="510" />
			<pubPlace>Sage, Thousand Oaks, CA</pubPlace>
		</imprint>
	</monogr>
	<note>The kendall rank correlation coefficient</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Validating causal inference models via influence functions</title>
		<author>
			<persName><forename type="first">Alaa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counterfactual representation learning with balancing weights</title>
		<author>
			<persName><forename type="first">S</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Duke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1972" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to propensity score methods for reducing the effects of confounding in observational studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hogue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2003">2011. 2003</date>
		</imprint>
	</monogr>
	<note>BMC bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
				<meeting>KDD cup and workshop</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2007</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05045</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Bica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04083</idno>
		<title level="m">Estimating counterfactual treatment outcomes over time through adversarially balanced representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">P10008</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-scale approach for graph link prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Line graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00891</idno>
		<title level="m">Hyperspherical variational auto-encoders</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data augmentation for deep graph learning: A survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08235</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Should graph convolution trust neighbors? a simple causal inference method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11079</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph random neural network for semi-supervised learning on graphs</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic block models: A comparison of variants and inference methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counterfactual regression with importance sampling weights</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="5880" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for counterfactual regression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Federated dynamic graph neural networks with secure aggregation for video-based distributed surveillance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels and community structure in networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16107</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Treatment effect estimation with data-driven variable decomposition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Treatment effect estimation via differentiated confounder balancing and regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matching on balanced nonlinear representations for treatment effects estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to drop: Robust graph neural network via topological denoising</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">;</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2022. 2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
	<note>Proceedings of the 14th ACM International Conference on Web Search and Data Mining</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automated data augmentations for graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcthrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komikado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maruhash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13248</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified view on graph neural networks as graph signal denoising</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1202" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of link prediction in complex networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Cubero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels meet graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4466" to="4474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Link prediction via matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Counterfactuals and causal inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winship</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metropolis-hastings data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep structural causal models for tractable counterfactual inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Link mining: Models, algorithms, and applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation using locally factored dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Near linear time algorithm to detect community structures in largescale networks</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">N</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes: Design, modeling, decisions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">469</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Intervening on network ties</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="975" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE winter conference on applications of computer vision (WACV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Drug response prediction as a link prediction problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Stanfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cos ¸kun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyutürk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Causal effect models for realistic individualized treatment and intention to treat rules</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of biostatistics</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Estimation and inference of heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1228" to="1242" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Modeling co-evolution of attributed and structural information in graph sequence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic attributed graph prediction with conditional normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="1385" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Nodeaug: Semi-supervised node classification with data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Information theoretic counterfactual learning from missing-not-at-random feedback</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Kuruoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Machine learning for treatment assignment: Improving individualized risk attribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kuusisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1306</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Drugbank 5.0: a major update to the drugbank database for</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wishart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Feunang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sajed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sayeeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D1074" to="D1082" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adversarial counterfactual learning and evaluation for recommender system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Binarized attributed network embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1476" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><surname>Ganite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Neo-gnns: Neighborhood overlap-aware graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07594</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph contrastive learning automated</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11015" to="11023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Action sequence augmentation for early graph-based anomaly detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="2668" to="2678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Graph data augmentation for graph machine learning: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08871</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Neural bellman-ford networks: A general graph neural network framework for link prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06935</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Each node in this graph represents an FDA-approved or experimental drug and edges represent the existence of unexpected effect when the two drugs are taken together. This dataset does not contain any node features, and it can be downloaded with the dataloader 3 provided by OGB. B. Details on Implementation and Hyperparameters All the experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Wishart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Drug-Drug Interaction Network The OGB-DDI dataset was constructed from a public Drug database. in this work were conducted on a Linux server with Intel Xeon Gold 6130 Processor (16 Cores @2.1Ghz</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">For fair comparison, we set the size of node/link representations to be 256 of all methods. CFLP We use the Adam optimizer with a simple cyclical learning rate scheduler (Smith, 2017), in which the learning rate waves cyclically between the given learning rate (lr) and 1e-4 in every 70 epochs (50 warmup steps and 20 annealing steps)</title>
		<author>
			<persName><forename type="first">Khasahmadi</forename><forename type="middle">;</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
		<idno>SEAL 5</idno>
		<ptr target="https://github.com/dmlc/dgl2https://snap.stanford.edu/data/ego-Facebook.html3https://ogb.stanford.edu/docs/linkprop/#data-loader4https://github.com/kavehhassani/mvgrl5https://github.com/facebookresearch/SEAL_OGB6https://github.com/LeiCaiwsu/LGLP7https://github.com/DaehanKim/vgae_pytorch8https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/ddi9https://pytorch-geometric.readthedocs.io/en/latest/10https://scikit-network.readthedocs.io/" />
	</analytic>
	<monogr>
		<title level="m">Baseline Methods For baseline methods, we use official code packages from the authors for MVGRL</title>
				<imprint>
			<date type="published" when="2001">2020. 2018. 2021. Fey &amp; Lenssen, 2019. 2008. 2001. 2007</date>
		</imprint>
	</monogr>
	<note>For the graph clustering or community detection methods we used as treatments, we use the implementation from scikit-network 10 for Louvain</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
