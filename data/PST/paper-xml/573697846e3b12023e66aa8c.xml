<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency Detection by Multi-Context Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<email>rzhao@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Scicence</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Saliency Detection by Multi-Context Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-level saliency cues or priors do not produce good enough saliency detection results especially when the salient object presents in a low-contrast background with confusing visual appearance. This issue raises a serious problem for conventional approaches. In this paper, we tackle this problem by proposing a multi-context deep learning framework for salient object detection. We employ deep Convolutional Neural Networks to model saliency of objects in images. Global context and local context are both taken into account, and are jointly modeled in a unified multicontext deep learning framework.</p><p>To provide a better initialization for training the deep neural networks, we investigate different pre-training strategies, and a task-specific pre-training scheme is designed to make the multi-context modeling suited for saliency detection. Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated. Our approach is extensively evaluated on five public datasets, and experimental results show significant and consistent improvements over the state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency detection, aiming at highlighting visually salient regions or objects in an image, has been a fundamental problem drawing extensive attentions in recent years. It has a wide range of applications in computer vision and image processing tasks, such as image/video compression and summarization <ref type="bibr" target="#b37">[38]</ref>, content-aware image resizing <ref type="bibr" target="#b5">[6]</ref>, and photo collage <ref type="bibr" target="#b52">[53]</ref>. Saliency information has also been exploited in high-level vision tasks, such as object detection <ref type="bibr" target="#b36">[37]</ref>, and person re-identification <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b60">61]</ref>. A large number of approaches <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref> are proposed to capture different saliency cues.</p><p>Many conventional saliency detection methods focus on design of low-level saliency cues, or modeling background From top left to bottom right: image, groundtruth mask, our saliency maps, and saliency maps of other five latest approaches, including DRFI <ref type="bibr" target="#b24">[25]</ref>, HS <ref type="bibr" target="#b55">[56]</ref>, GBMR <ref type="bibr" target="#b56">[57]</ref>, PCAS <ref type="bibr" target="#b40">[41]</ref>, and SF <ref type="bibr" target="#b43">[44]</ref>.</p><p>priors. There are noticeable problems in these methods. 1) Computational saliency models need effective feature representations to estimate saliency, but sometimes the contrast between hand-crafted low-level features cannot help salient objects stand out from context. 2) Moreover, contrast is not only in terms of difference between visual cues, but also relates to high-level cognition and understanding. For example in Figure <ref type="figure" target="#fig_0">1</ref>, a dark gray house appears in dark yellow bush. Objects like the house cannot be classified as salient objects from the low-contrast background either based on low-level saliency cues or background priors, but they are semantically salient in high-level cognition, i.e. they are distinct in object categories. Therefore, saliency detection is considered as a high-level task in our work.</p><p>The deep Convolutional Neural Network (CNN) <ref type="bibr" target="#b29">[30]</ref>, which recently showed its powerfulness in extracting highlevel feature representations <ref type="bibr" target="#b15">[16]</ref>, can well solve aforementioned problems. From another perspective, saliency detection is a task to simulate the mechanism of human attention, which is a neurocognitive reaction controlled by human brains. Deep CNN aims to mimic the functions of neocortex in human brain as a hierarchy of filters and nonlinear operations. For better detecting semantically salient objects, high-level knowledge on object categories becomes important. Suppose that if the deep model can recognize the gray house, then the problems in Figure <ref type="figure" target="#fig_0">1</ref> can be easily solved. As indicated in <ref type="bibr" target="#b48">[49]</ref>, pre-training can provide  a good initialization for training deep models, and is able to preliminarily memorize some related high-level information. Therefore, it is desirable to see the influence of pretraining in modeling saliency. An appropriate scope of context is also very important to help a salient object stand out from its context meanwhile keep those non-salient objects suppressed in background. In Figure <ref type="figure" target="#fig_2">2</ref>, high-level knowledge tells us information about flowers, leaves, cars and guard fences, but cannot answer which are salient objects. If a local context (e.g. the red dashed boxes in Figure <ref type="figure" target="#fig_2">2</ref>(a)) is adopted to determine the saliency, then all these object are highlighted as salient objects, as shown in Figure <ref type="figure" target="#fig_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Salient Object Segmentation</head><p>Salient object segmentation approaches can be roughly categorized into two groups: bottom-up methods and topdown methods.</p><p>Bottom-up methods can be further divided into two categories, i.e. local and global. Local approaches (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>) design saliency cues by considering the contrast between each image element (pixel, region, or patch) and its locally surrounding neighborhood. Global approaches estimate saliency scores by calculating the holistic statistics on uniqueness of each image element over the whole image. Cheng et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref> used 3D color histograms as regional features to compute global contrast with all image regions. Perazzi et al. <ref type="bibr" target="#b43">[44]</ref> applied two measures of contrast that rate the uniqueness and the spatial distribution to derive image saliency. However, these global features are weak in capturing semantic information.</p><p>Top-down methods take advantages of high-level category-specific information as prior knowledge, and are usually task-dependent. Judd et al. <ref type="bibr" target="#b27">[28]</ref> learned a top-down saliency model object detectors such as faces, humans, animals, and text. Borji et al. <ref type="bibr" target="#b6">[7]</ref> combine bottom-up saliency cues with top-down features learned via multiple object detectors. Yang et al. <ref type="bibr" target="#b57">[58]</ref> proposed a top-down saliency model by jointly learning a Conditional Random Field and a dictionary. These methods explore high-level information from 3 ∼ 5 object categories. However, our deep models encodes prior knowledge on 1, 000 object classes from Im-ageNet, and has much stronger generalization capability.</p><p>In addition, some other interesting priors were also proposed to assist saliency detection, such as flash cues <ref type="bibr" target="#b17">[18]</ref>, boundary and background priors <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b62">63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Objectness and Object Proposal</head><p>Objectness was introduced to measure how likely a region contains an object regardless of object categories. Alexie <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> proposed to combine local appearance contrast and boundary characteristics to measure the objectness score of a bounding box. Based on such measures, some object proposal methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b63">64]</ref> further generated candidate object regions as a preprocessing step for object detection, which can effectively speed up the process comparing to the classical sliding-window detection paradigm. A recent work <ref type="bibr" target="#b53">[54]</ref> proposed to extract generic objects by jointly handling localization and segmentation tasks.</p><p>Different than object proposal, which enumerates preliminarily likely candidates for object detection regardless of their contrastive relations, saliency detection requires to take the context of full images and the contrast between objects into account. Also, objectness score in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> or the ranking score in <ref type="bibr" target="#b8">[9]</ref> was measured over a candidate bounding box, which can only provide a rough score map highlighting all possible objects. Contrarily, saliency detection aims to produce accurate segmentation over the salient ones. Despite the difference, objectness score can be used as high-level prior knowledge <ref type="bibr" target="#b22">[23]</ref>, which could be further combined with with low-level saliency cues for saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hierarchical Structure for Saliency Detection</head><p>Latest works on saliency detection have showed the trend of using deep/hierachical architectures to model visual saliency. Yan et al. <ref type="bibr" target="#b55">[56]</ref> presented a hierarchical framework to reduce the influence of small-scale structures in saliency detection. Lin et al. <ref type="bibr" target="#b33">[34]</ref> proposed to unsupervisedly learn a set of mid-level filters to capture local contrast, and to fuse multi-level saliency calculation by convolution. Unlike their methods where mid-level filters are handcrafted, filters of CNNs in our framework are automatically and jointly learned in a discriminative manner. Jiang et al. <ref type="bibr" target="#b25">[26]</ref> introduced successive Markov random fields (sMRF) to model visual saliency, which shared the similar spirit as this work of mimicing the deep propagation (a chain of synaptic communications) along visual cortex. However, the sMRF is a hierachical graphical model, which is a generative model optimized by belief propagation, while visual saliency in our work is computed in a discriminative model optimized by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Deep Convolutional Neural Networks</head><p>Since the introduction by LeCun Yann <ref type="bibr" target="#b29">[30]</ref>, deep CNN has been applied to a wide range of computer vision tasks such as hand-written digit classification and face detection. Recently, the latest generation of CNNs have substantially outperformed handcrafted approaches in computer vision field. Notably, best performing entries <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> on ImageNet ILSVRC <ref type="bibr" target="#b13">[14]</ref> and PASCAL VOC <ref type="bibr" target="#b14">[15]</ref> benchmarks are all variants of deep CNNs since 2012. Some recent approaches close to our work included cascaded stages in deep learning to solve problems that need meticulous refinement, such as in facial landmark detection <ref type="bibr" target="#b47">[48]</ref> and human pose estimation <ref type="bibr" target="#b50">[51]</ref>. Saliency detection also need such refinement since global-context model cannot well capture the very detailed information in local neighborhoods. However, we propose a multi-context deep model to consolidate both global context and local context in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this paper, we propose a multi-context deep learning framework for saliency detection, and focus on modeling saliency with global context and local context simultaneously. Furthermore, different pre-training strategies are investigated, and an effective task-specific pre-training scheme is introduced. Figure <ref type="figure" target="#fig_4">3</ref> shows an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global-context Modeling by Deep CNN</head><p>As shown in Figure <ref type="figure" target="#fig_4">3</ref>, the upper branch (global-context modeling) of our saliency detection pipeline is a deep CNN architecture with global and coarse context. Superpixel segmentation is firstly performed on images using the SLIC <ref type="bibr" target="#b1">[2]</ref> method, and the input of global-context CNN is a superpixel-centered large context window including the full image. Regions exceeding image boundaries are padded with mean pixel value of the training dataset. The padded image are then warped to 227 × 227 × 3 as input, where the three dimensions represent width, height, and number of channels. With this proposed normalization and padding scheme, the superpixel to be classified is always located at the center of the image, and the spatial distribution of the global context is normalized in this way. Moreover, it ensures the input covers the whole range of the original image. The last layer of the network structure has 2 neurons followed by a softmax function as output, indicating the probabilities of centered superpixel whether being in background or belonging to a salient object.</p><p>The winning model in the classification task of Ima-geNet 2013, i.e. the Clarifai model <ref type="bibr" target="#b58">[59]</ref>, is adopted as our baseline model. The Clarifai model contains 5 convolutional layers and 2 fully connected layers, as shown in Figure <ref type="figure" target="#fig_4">3</ref>. Denote by conv# a convolutional layer, by lrn# a local response normalization layer, pool# a pooling layer and by f c# a fully connected layer. conv# and f c# layers consist of a linear transformation followed by a nonlinear rectified linear unit function denoted by relu#, and only conv# and f c# layers have learnable parameters. The structure of the network can be described by the size of feature maps at each layer as conv1</p><formula xml:id="formula_0">(111 × 111 × 96) −relu1 −pool1 −lrn1 −conv2 (27 × 27 × 256) −relu2 −pool2 −lrn2 −conv3 (13 × 13 × 384) −relu3 −conv4 (13 × 13 × 384) −relu4 −conv5 (13 × 13 × 256) −relu5 −pool5 −f c6(4096)−relu6 −dropout6 −f c7(4096)−relu7 −dropout7 −f c8(2).</formula><p>For conv layers, the size of feature maps is defined as width×height×depth, where the first two dimensions describe the spatial size and the depth defines the number of channels. Pooling is applied after three layers. The total number of parameters in the above model is about 58 million. We refer readers to <ref type="bibr" target="#b58">[59]</ref> for further details.</p><p>Apart from the Clarifai model, there are also other contemporary models such as AlexNet <ref type="bibr" target="#b28">[29]</ref>, NIN <ref type="bibr" target="#b32">[33]</ref>, Over-Feat <ref type="bibr" target="#b45">[46]</ref>, DeepID-Net <ref type="bibr" target="#b41">[42]</ref>, and GoogLeNet <ref type="bibr" target="#b49">[50]</ref>. It is flexible to incorporate any of these contemporary deep models into our framework, and in the experimental section we investigate the performance of saliency detection using some of these contemporary architectures.</p><p>111"x"111"x"96 27"x"27"x"256 13"x"13"x"384 13"x"13"x"256 13"x"13"x"384 4096 4096 2 4096 Downsample Fla8en 111"x"111"x"96 27"x"27"x"256 13"x"13"x"384 13"x"13"x"256 13"x"13"x"384  </p><p>where x gc and x lc are output of the penultimate layer of the global context model and the local context model respectively. y is the prediction of saliency for the centered superpixel, where y = 1 for salient superpixel and y = 0 for background.</p><p>We train a binary classifier on top of the last network layer to classify background and saliency by minimizing a unified softmax loss between the classification result and the groundtruth label.</p><formula xml:id="formula_2">L( θ; {x (i) gc , x (i) lc , y (i) } m i=1 ) = − 1 m i∈{1,...,m} j∈{0,1}<label>1</label></formula><formula xml:id="formula_3">{y (i) =j} log P (y (i) = j | x (i) gc , x (i) lc ; θj),<label>(2)</label></formula><p>In our approach, the parameters in our framework can be decomposed to several parts, i.e. θ j = { w gc,j , w lc,j , α, β}, where w gc,j are last-layer parameters in the neural network for global-context modeling, w lc,j are last-layer parameters for local-context modeling, and α, β are parameters of an ambiguity modeling function controlling the need of localcontext modeling. Thus, the posterior probability in Eq.( <ref type="formula" target="#formula_3">2</ref>) is factorized into product of experts <ref type="bibr" target="#b18">[19]</ref>, i.e. we aim to infer the label probability via two components simultaneously:</p><formula xml:id="formula_4">P (y = j | xgc, x lc ; θj) ∝ Φ(xgc; θ Φ j ) • Ψ(xgc, x lc ; θ Ψ j ),<label>(3</label></formula><p>) θ Φ j = wgc,j, θ Ψ j = { wgc,j, w lc,j , α, β}, j ∈ {0, 1}. (</p><p>Specifically, Φ tries to estimate the saliency probability based on global-context modeling,</p><formula xml:id="formula_6">Φ(x gc ; θ Φ j ) ∝ e w T gc,j xgc ,<label>(5)</label></formula><p>and Ψ is based on both the global context and local context,</p><formula xml:id="formula_7">Ψ(x gc , x lc ; θ Ψ j ) ∝ e fu(αw T gc,j xgc+β)•w T lc,j x lc .<label>(6)</label></formula><p>Then, the corresponding unnormalized saliency prediction score function is formulated as</p><formula xml:id="formula_8">f (xgc, x lc ; θ Ψ 1 ) = w T gc,1 xgc + fu(αw T gc,1 xgc + β)w T lc,1 x lc ,<label>(7)</label></formula><p>where f u (•) is defined as To this end, our problem can be formulated as minimizing the following loss function: argmin {w gc,j ,w lc,j } 1 j=0 , α,β L({w gc,j , w lc,j } 1 j=0 , α, β; {x</p><formula xml:id="formula_9">(i) gc , x (i) lc , y (i) } m i=1 ) = − 1 m i∈{1,...,m} j∈{0,1}<label>1</label></formula><formula xml:id="formula_10">{y (i) =j} log e w T gc,j x (i) gc +fu(αw T gc,j x (i) gc +β)w T lc,j x (i) lc l e w T gc,l x (i) gc +fu(αw T gc,l x (i) gc +β)w T lc,l x (i) lc +λ 1 j∈{0,1}</formula><p>w gc,j</p><formula xml:id="formula_11">2 2 + λ 2 j∈{0,1} w lc,j<label>2 2 , (9)</label></formula><p>where the parameters are simultaneously optimized with other layers' parameters by backpropagating the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Task-specific Pre-training and Fine-tuning</head><p>It was demonstrated in <ref type="bibr" target="#b15">[16]</ref> that fine-tuning a Deep CNN model pre-trained for image classification with the target task (e.g. object detection) data can significantly improve the performance of target task. Particularly, deep model structures at the pre-training and fine-tuning stages are only different in the last fully connected layer for predicting labels. Except for the last fully connected layers for classification, the parameters learned at the pre-training phase are directly used as initial values for the fine-tuning stage.</p><p>Similar strategy in <ref type="bibr" target="#b15">[16]</ref> can be directly used to finetune the contemporary CNN models for saliency detection. However, the pre-training task and fine-tuning task have disparity in following aspects. 1) Input data. Image classification task takes full images as inputs, while our global-context model requires superpixel-centered windows padded with mean pixel value, and the local-context model takes a cropped input, which serves to provide local context for finer prediction. Both the input of the global-and localcontext models have changed scales and translations compared to the original images, which leads our multi-context model to learning different feature representations. 2) Class labels. Dataset in ImageNet for Image classification has 1, 000 classes, while saliency detection solves a binary classification problem. Despite the disparity in class labels, it is shown that deep CNN pre-trained for 1, 000-class classification can be generalized for fine-tuning the classification problem with fewer classes <ref type="bibr" target="#b15">[16]</ref>. 3) Loss function. The loss function in image classification task aims to differentiate 1, 000 classes, while the loss function for saliency detection in our approach is defined as in Eq. ( <ref type="formula">9</ref>) to perform binary classification.</p><p>To apply the contemporary models like the Clarifai model to our problem, the disparities mentioned above need to be considered. Therefore, we explore several pre-training strategies, and propose task-specific initialization for finetuning our deep saliency detection models. Task-specific pre-training has been proved to be very effective on object detection <ref type="bibr" target="#b41">[42]</ref>. Pre-training We pre-train our models using image data from ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014 <ref type="bibr" target="#b44">[45]</ref>. This challenge contains two different datasets: 1) the classification and localization dataset and 2) the detection dataset. The classification and localization dataset is divided into three subsets, train, validation, and test data. We use the train dataset for pre-training, which contains 1.2 million images with labels of 1, 000 categories. In addition, labels are provided at the image-level and the object-level, i.e. category labels are available for both full image and object bounding boxes.</p><p>Based on the object-level annotations, we can easily generate another type of annotations to suit the input format in saliency detection, which we call the superpixellevel annotation. Specifically, we randomly sample a superpixel within a object bounding box (where the superpixel is most likely located within the object region), and produce a superpixel-centered window including full image, also padded with mean pixel value in ImageNet training data. The label of each window is determined by thresholding the overlap ratio between the centered superpixel and corresponding groundtruth salient object mask. In our experiments, the threshold is set to 0.5.</p><p>We investigate following strategies for pre-training: In Table <ref type="table" target="#tab_0">1</ref>, we show two settings of pre-training schemes. The R-CNN <ref type="bibr" target="#b15">[16]</ref> for object detection and segmentation adopted strategy 2 in training (denoted by S1). Different from R-CNN, a new task-specific scheme (denoted by S2) with pre-training strategies based on superpixel-level and object-level annotation are proposed for the global-and local-context modeling. Superpixel-level annotation aligns spatial location of objects when pre-training the globalcontext model, and it is consistent with the input format of the global-context model. Features pre-trained with objectlevel annotation are sensitive to the location of objects, and they provide more appropriate pre-training information for the local-context model.</p><p>In Section 4.3, we quantitatively investigate the influences of different pre-training strategies in saliency detection, and validate our hypothesis that task-specific pre-training strategy provides a better initialization for finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training strategy global context local context</head><p>S1: R-CNN <ref type="bibr" target="#b15">[16]</ref> strategy 2 strategy 2 S2: Task-specific strategy 4 strategy 3 Fine-tuning. We use the MSRA10k dataset <ref type="bibr" target="#b9">[10]</ref> for finetuning our deep saliency detection models. The MSRA10k dataset is a subset of the MSRA Salient Object Dataset <ref type="bibr" target="#b35">[36]</ref>, which originally provides salient object annotation in terms of bounding boxes provided by 3-9 users. Cheng et al. <ref type="bibr" target="#b9">[10]</ref> selected 10, 000 images with consistent bounding box labeling in MSRA dataset, and provided pixel-level saliency annotations. We randomly select 8, 000 images for training, and 2, 000 for validation. From each image, we select an average 200 ∼ 300 of superpixels, and in total about 2.1 million input windows for training and 0.6 million for validation are generated. Fine-tuning for 100, 000 iterations costs 31 hours on a PC with Intel I7 3.6GHz GPU, 32GB RAM and a GTX TITAN GPU. Testing an image with 200 superpixel takes about 0.8 seconds only using global-context model, and 1.6 seconds using the unified multi-context model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>ASD <ref type="bibr" target="#b0">[1]</ref> includes 1, 000 images sampled from the MSRA Salient Object Database <ref type="bibr" target="#b35">[36]</ref>. Although our training data originates from the same dataset, we separate images in ASD dataset from our training set to avoid overlap.</p><p>SED1 <ref type="bibr" target="#b4">[5]</ref> contains 100 images of a single salient object annotated manually by three users.</p><p>SED2 <ref type="bibr" target="#b4">[5]</ref> contains 100 images of two salient objects annotated manually by three users.</p><p>ECSSD <ref type="bibr" target="#b55">[56]</ref> contains 1, 000 structurally complex images acquired from the Internet, and the groundtruth masks were annotated by five labelers.</p><p>PASCAL-S <ref type="bibr" target="#b31">[32]</ref> was built on the validation set of the PASCAL VOC 2010 segmentation challenge. It contains 850 natural images with both saliency segmentation groundtruth and eye fixation groundtruth. Saliency groundtruth masks were labeled by 12 subjects.</p><p>Evaluation Metrics<ref type="foot" target="#foot_0">1</ref> . We follow the evaluation protocol as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, where saliency maps are binarized at every threshold within range [0, 255], and all saliency maps are evaluated by the F-measure score <ref type="bibr" target="#b0">[1]</ref>, which is obtained as a harmonic mean of average precision and average recall, i.e. F β = (1+β 2 )×precision×recall β 2 ×precision+recall , where β 2 is set to 0.3 following the convention as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b31">32]</ref>. Figure <ref type="figure">6</ref>. Saliency detection performance (F-measure score) using contemporary deep models, including AlexNet <ref type="bibr" target="#b28">[29]</ref>, Clarifai <ref type="bibr" target="#b58">[59]</ref>, OverFeat <ref type="bibr" target="#b45">[46]</ref>, and GoogLeNet <ref type="bibr" target="#b49">[50]</ref>.  Then we evaluate the performance of task-specific pretraining strategies for the whole framework. As shown in Table <ref type="table" target="#tab_0">1</ref>, we test two settings, the S1 follows the scheme used in R-CNN <ref type="bibr" target="#b15">[16]</ref>, while the second one S2 is our task-specific pre-training scheme introduced in Section 3.3. From the results on five datasets in Figure <ref type="figure">5</ref>(c), we can conclude that our task-specific pre-training scheme consistently outperforms the conventional pre-training method adopted in R-CNN, which validates the effectiveness of the proposed task-specific pre-training approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the Multi-context Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Contemporary Deep Structures</head><p>Our framework is flexible to incorporate other contemporary deep models, and for simplicity we replace the model structure in the global-context model with other contemporary model structures for evaluation. Evaluated structures include AlexNet <ref type="bibr" target="#b28">[29]</ref>, Clarifai <ref type="bibr" target="#b58">[59]</ref>, OverFeat <ref type="bibr" target="#b45">[46]</ref>, and GoogLeNet <ref type="bibr" target="#b49">[50]</ref>. As shown in Figure <ref type="figure">6</ref>, GoogLeNet 2 slightly outperforms other deep models on the five dataset, but performances of these contemporary deep models do not vary very much. We expect extra performance gain if GoogleNet is used in our multi-context model with taskspecific pre-training scheme, as introduced in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on Overall Performance</head><p>In Table <ref type="table">2</ref>, we compare our approach with nine latest state-of-the-art methods, including IS <ref type="bibr" target="#b19">[20]</ref>, GBVS <ref type="bibr" target="#b16">[17]</ref>, SF <ref type="bibr" target="#b43">[44]</ref>, GC <ref type="bibr" target="#b11">[12]</ref>, CEOS <ref type="bibr" target="#b39">[40]</ref>, PCAS <ref type="bibr" target="#b40">[41]</ref>, GBMR <ref type="bibr" target="#b56">[57]</ref>, HS <ref type="bibr" target="#b55">[56]</ref>, and DRFI <ref type="bibr" target="#b24">[25]</ref>. Our approach significantly outperforms all the state-of-the-art salient object segmentation algorithms. The PASCAL-S dataset was proposed in CPMC-GBVS <ref type="bibr" target="#b31">[32]</ref>, but we do not include their method in Table <ref type="table">2</ref> for a fair comparison because they used eye fixation la-2 Our implementation of GoogLeNet is pre-trained with less extensive data augmentation, and gets 67% top-1 ILSVRC accuracy. ASD SED1 SED2 ECSSD PASCAL-S IS <ref type="bibr" target="#b19">[20]</ref> 0.5943 0.5540 0.5682 0.4731 0.4901 GBVS <ref type="bibr" target="#b16">[17]</ref> 0.6499 0.7125 0.5862 0.5528 0.5929 SF <ref type="bibr" target="#b43">[44]</ref> 0.8879 0.7533 0.7961 0.5448 0.5740 GC <ref type="bibr" target="#b11">[12]</ref> 0.8811 0.8066 0.7728 0.5821 0.6184 CEOS <ref type="bibr" target="#b39">[40]</ref> 0.9020 0.7935 0.6198 0.6465 0.6557 PCAS <ref type="bibr" target="#b40">[41]</ref> 0.8613 0.7586 0.7791 0.5800 0.6332 GBMR <ref type="bibr" target="#b56">[57]</ref> 0.9100 0.9062 0.7974 0.6570 0.7055 HS <ref type="bibr" target="#b55">[56]</ref> 0.9307 0.8744 0.8150 0.6391 0.6819 DRFI <ref type="bibr" target="#b24">[25]</ref> 0.9448 0.9018 0.8725 0.6909 0.7447 Ours 0.9548 0.9295 0.8903 0.7322 0.7930</p><p>Table <ref type="table">2</ref>. The F-measure scores of benchmarking approaches on five public datasets.</p><p>bel in training. Our approach obtains a higher F-measure score than theirs (0.7930 vs. 0.7457) on PASCAL-S dataset. Also, we qualitatively compare our saliency maps with those by other methods in Figure <ref type="figure" target="#fig_11">7</ref>. It is obvious that our approach is able to highlight the salient object parts more coherently, and has a better prediction especially in complex scene with confusing background, such as the cases in the 6 th and 7 th rows in Figure <ref type="figure" target="#fig_11">7</ref>. More comparisons can be found at our project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a multi-context deep learning framework for saliency detection. Firstly, we intro- HS <ref type="bibr" target="#b55">[56]</ref>, GBMR <ref type="bibr" target="#b56">[57]</ref>, PCAS <ref type="bibr" target="#b40">[41]</ref>, CEOS <ref type="bibr" target="#b39">[40]</ref>, GC <ref type="bibr" target="#b11">[12]</ref>, and SF <ref type="bibr" target="#b43">[44]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples to show problems in conventional approaches.From top left to bottom right: image, groundtruth mask, our saliency maps, and saliency maps of other five latest approaches, including DRFI<ref type="bibr" target="#b24">[25]</ref>, HS<ref type="bibr" target="#b55">[56]</ref>, GBMR<ref type="bibr" target="#b56">[57]</ref>, PCAS<ref type="bibr" target="#b40">[41]</ref>, and SF<ref type="bibr" target="#b43">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Examples to show importance of global context. From left to right: image, groundtruth saliency mask, our saliency map predicted with local context, and our saliency map predicted with global context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c). This becomes general object detection and segmentation problems. Due to the extremely large variation in positions and scales of objects of different categories, a global context (including the full image) is more suitable to determine object saliency. Because a global context takes all objects in an image into account, and only with a global context we can model the contrast between all objects. As shown in Figure 2(d), if the flower and the leaf are considered together, then only the flower is classified as the salient object; if the car and the guard fence are considered as in the same picture, then only the car is detected as the salient object. In addition, it is known that deep models are also powerful in learning global contextual feature representations. Based on the above motivations, a new multi-context deep learning framework for saliency detection is proposed. Our work has two major contributions: First, a deep model with multiple contexts is designed to capture object saliency. The global context is utilized to model saliency in full image, while the local context is used for saliency prediction in meticulous areas. The global and local context are integrated into the multi-context deep learning framework for saliency detection, and the globaland local-context modeling are jointly optimized. Second, we explore the influence of different pretraining strategies, and introduce a task-specific pre-training scheme to pre-train the deep models using the ImageNet image classification dataset. In addition, several contemporary deep architectures in ImageNet Image Classification Chal-lenge are tested, and their effectiveness in saliency detection are investigated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Upper branch: Deep CNN-based global-context modeling for saliency detection with a superpixel-centered window padded with mean pixel value. Lower branch: local-context modeling with a closer-focused superpixel-centered window, and global-context saliency detection results are combined into finally fully-connected layer in the local-context model. We visualize the network layers with their corresponding dimensions, where convolutional layers are in blue, fully connected layers (with parameters initialized using pre-trained model parameters) in orange, and fully connected layers (with parameters randomly initialized) in red. Layers without parameters are omitted in this figure.</figDesc><graphic url="image-28.png" coords="4,111.46,111.83,67.89,133.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3. 2 .</head><label>2</label><figDesc>Integrated Multi-context Model While the CNN at the upper branch (global-context model) aims to robustly model saliency with few large errors, CNN at the lower branch are designed to look at details -it focuses on a smaller context to refine the saliency prediction of the centered superpixel. In this work, the local-context model takes an input with a similar form as in the global-contex model, but with one third of the scope of context, and then normalized to 227 × 227 × 3. The local-context model shares the same deep structure with the global-context model, but with independent parameters. Other deep structures can also be flexibly incorporated in the local-context model. Overall, prediction of a superpixel-centered input window is performed by estimating the saliency probability score(xgc, x lc ) = P (y = 1 | xgc, x lc ; θ1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>f u (αw T gc,1 x gc +β) models ambiguity of the saliency prediction of the global context model, and α and β can integrate multiple contexts in modeling to perform saliency detection from a joint model. Intuitively, αw T gc,1 x gc + β in range (−∞, 0]∪[1, +∞) leads to a zero f u (αw T gc,1 x gc +β), which means it has a high-confidence prediction in the globalcontext model so that f (x gc , x lc ; θ Ψ 1 ) only relies on the global context information. Properly setting α and β incorporate a non-zero f u (αw T gc,1 x gc + β) • w T lc,1 x lc (weighted local-context modeling) to handle ambiguous predictions with low confidence in global-context modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Strategy 1 .Strategy 2 .Strategy 3 .</head><label>123</label><figDesc>No pre-training, i.e. randomly initializing model parameters for fine-tuning. Pre-training the deep models using training images with image-level annotations of 1, 000 classes. Pre-training the deep models using training images with object-level annotations of 1, 000 classes. Strategy 4. Pre-training the deep models using training images with superpixel-level annotations of 1, 000 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>We separate the global-context branch in our framework as a baseline model, and we call it as the single-context model. It is also trained and tested under the same experimental settings for comparison. As shown in Figure4(a), our proposed multi-context model consistently outperforms the single-context model on all the five datasets. Especially on the PASCAL dataset, our multi-context model increases the F-measure score by around 5%. Some examples of the saliency maps are shown in Figures 4(d-e), and it is clearly shown that the multi-context model refines the erroneous predictions by the single-context model since it combines both global context and local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4. 3 .</head><label>3</label><figDesc>Evaluation on Task-specific Pre-trainingWe evaluate the performance of the single-context model with different pre-training strategies on the ECSSD dataset and PASCAL-S dataset since evaluations on larger datasets show more robust statistics. As shown in Figures5(ab), evaluation results on both datasets have similar characteristics: 1) random initialization (strat.1) of network parameters leads to the worst performance; 2) imagelevel pre-training strategy (strat.2) and object-level pretraining strategy (strat.3) obtain similar results on the EC-SSD dataset, while image-level pre-training slightly outperforms object-level pre-training on the PASCAL-S dataset; 3) superpixel-level pre-training strategy (strat.4) outperforms other pre-training strategies on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. (a): F-measure scores on the five saliency detection datasets for evaluation of the single-context model and the multi-context model. (b-e): Qualitative comparison between the single-context model and the multi-context model. (b) is input image. (c) is groundtruth saliency map. (d) is our saliency map from the single-context model. (e) is our saliency map from the multi-context model. Red arrows indicate that regions with erroneous prediction by single-context model are refined by the multi-context model.</figDesc><graphic url="image-133.png" coords="7,361.96,251.94,183.14,102.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Example images from five datasets and the saliency maps by compared methods. Methods for comparison include DRFI<ref type="bibr" target="#b24">[25]</ref>, HS<ref type="bibr" target="#b55">[56]</ref>, GBMR<ref type="bibr" target="#b56">[57]</ref>, PCAS<ref type="bibr" target="#b40">[41]</ref>, CEOS<ref type="bibr" target="#b39">[40]</ref>, GC<ref type="bibr" target="#b11">[12]</ref>, and SF<ref type="bibr" target="#b43">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>duce multi-context saliency modeling using deep Convolutional Neural Networks. Global context and local context are utilized and integrated into a unified deep learning framework for saliency detection. Globaland local-context models are jointly optimized. Secondly, different pre-training strategies are investigated to learn the deep model for saliency detection, and a task-specific pretraining scheme designed for our multi-context deep model is proposed. Moreover, recently proposed contemporary deep models in ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated. Experiments validate each component in our framework, and show our approach significantly and con-sistently outperforms all the state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Pre-training strategies used for comparison.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the code provided by<ref type="bibr" target="#b31">[32]</ref> at http://cbi.gatech.edu/ salobj/ for evaluation of our results on all the five datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project Nos. CUHK419412, CUHK417011, CUHK14206114 and CUHK14207814), Hong Kong Innovation and Technology Support Programme (Project reference ITS/221/13FP), Shenzhen Basic Research Program (JCYJ20130402113127496), NSFC (Project Nos. 61301269, 91320101, 61472410) and Sichuan High Tech R&amp;D Program (No.2014GZX0009).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to stateof-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seam carving for contentaware image resizing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Saliency detection with flash and no-flash image pairs</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model of saliencybased visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2013. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep salience: Visual salience modeling via deep belief propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Submodular salient region detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection within a deep convolutional architecture</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive partial differential equation learning for visual saliency detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparing salient object detection results without ground truth</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A closer look at context: From coxels to the contextual emergence of object saliency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ben-Shahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2008">2014. 1, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What makes a patch distinct</title>
		<author>
			<persName><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deepidnet: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3505</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2012. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4659</idno>
		<title level="m">Deeppose: Human pose estimation via deep neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Picture collage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Joint task learning via deep neural networks with application to generic object extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2013. 1, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2013. 1, 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2014. 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Saliency detection: a boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Person reidentification by salience matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
