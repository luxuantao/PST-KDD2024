<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIETCODE: AUTOMATIC OPTIMIZATION FOR DYNAMIC TENSOR PROGRAMS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cody</forename><surname>Hao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">th MLSys Conference</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Fromm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Web</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scroll</forename><surname>Tech</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">th MLSys Conference</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Octoml</surname></persName>
						</author>
						<title level="a" type="main">DIETCODE: AUTOMATIC OPTIMIZATION FOR DYNAMIC TENSOR PROGRAMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Achieving high performance for compute-intensive operators in machine learning (ML) workloads is a crucial but challenging task. Many ML and system practitioners rely on vendor libraries or auto-schedulers to do the job. While the former requires large engineering efforts, the latter only supports static-shape workloads in existing works. It is difficult, if not impractical, to apply existing auto-schedulers directly to dynamic-shape workloads, as this leads to extremely long auto-scheduling time.</p><p>We observe that the key challenge faced by existing auto-schedulers when handling a dynamic-shape workload is that they cannot construct a unified search space for all the possible shapes of the workload, because their search space is shape-dependent. To address this, we propose DietCode, a new auto-scheduler framework that efficiently supports dynamic-shape workloads by constructing a shape-generic search space and cost model. Under this construction, all shapes jointly search within the same space and update the same cost model when auto-scheduling, which is therefore more efficient compared with existing auto-schedulers.</p><p>We evaluate DietCode using state-of-the-art machine learning workloads on a modern GPU. Our evaluation shows that DietCode has the following key strengths when auto-scheduling an entire model end-to-end: (1) reduces the auto-scheduling time by up to 5.88? less than the state-of-the-art auto-scheduler on the uniformly sampled dynamic shapes (94.1? estimated if all the possible shapes are included), (2) improves performance by up to 69.5% better than the auto-scheduler and 18.6% better than the vendor library. All these advantages make DietCode an efficient and practical solution for dynamic-shape workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) form an important class of ML algorithms <ref type="bibr" target="#b17">(He et al., 2016;</ref><ref type="bibr" target="#b38">Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Amodei et al., 2016;</ref><ref type="bibr" target="#b12">Devlin et al., 2019)</ref>. They are made of tensor operators which are often executed for tens of thousands of iterations to capture the pattern of the training data samples. Because of this, it is crucial to guarantee the efficiency of every operator during the execution on real hardware, as even an improvement of 5% in performance could mean a cost reduction of up to $4000 in training a single DNN model end-to-end <ref type="bibr" target="#b35">(Sharir et al., 2020)</ref>.</p><p>However, it is a challenging task to implement all operators efficiently, because extensive expertise is needed across the software and hardware stack in order for those operators to be programmed efficiently. Therefore, most state-of-the-art ML frameworks rely on heavily optimized, hand-crafted vendor libraries <ref type="bibr">(e.g., oneDNN (oneAPI, 2021)</ref> on Intel CPUs; cuDNN <ref type="bibr" target="#b11">(Chetlur et al., 2014)</ref> and cuBLAS <ref type="bibr">(NVIDIA, 2021)</ref> on NVIDIA GPUs) to provide highly optimized operators <ref type="bibr" target="#b0">(Abadi et al., 2016;</ref><ref type="bibr" target="#b30">Paszke et al., 2019;</ref><ref type="bibr" target="#b8">Chen et al., 2015)</ref>. Despite delivering high performance, the development time and release cycle for vendor libraries to support new operators on new hardware platforms could be extremely long (e.g., there is roughly a one year gap between each cuBLAS major release (NVIDIA, 2021b)).</p><p>To address these challenges, auto-scheduler frameworks have been proposed to bridge the gap between high-level compute definition and low-level implementation details (e.g., Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, TVM <ref type="bibr">(Chen et al., 2018a)</ref>, Halide auto-scheduler <ref type="bibr" target="#b1">(Adams et al., 2019)</ref>, and Tensor Comprehensions <ref type="bibr" target="#b37">(Vasilache et al., 2020)</ref>). Although being powerful, the existing auto-schedulers can only support static-shape workloads where all shapes have to be known at compile-time because they need this information to construct the search space that describes all possible operator implementations (i.e., schedules) under consideration.</p><p>However, real world workloads can take on different shapes from time to time, such as in the following scenarios: (1) neural architecture search <ref type="bibr" target="#b48">(Zoph &amp; Le, 2017)</ref>, (2) dynamic by design (e.g., models in the sequence learning domain have to dynamically fit to the input sequence length at runtime <ref type="bibr" target="#b41">(Wu et al., 2016;</ref><ref type="bibr" target="#b38">Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Amodei et al., 2016;</ref><ref type="bibr" target="#b12">Devlin et al., 2019)</ref>), and (3) varying shapes depending on the layer position in the model <ref type="bibr" target="#b17">(He et al., 2016;</ref><ref type="bibr" target="#b12">Devlin et al., 2019)</ref>. A straightforward way for the existing autoschedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> to support those cases is by tuning for all the possible shapes, but this can take days to complete (i.e., up to 42 hours for a single BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> layer on a modern CPU), which is therefore not a practical solution.</p><p>In this work, we observe that if we group the same type of operators with different shapes together as a single dynamicshape workload and only auto-schedule the workload once, we can significantly reduce the overall auto-scheduling time. However, existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> cannot collectively schedule for different shapes of the same operator directly, because those auto-schedulers construct different search spaces for different shapes. To address this shape-dependent search space construction, we propose DietCode, a new auto-scheduler framework that efficiently supports dynamic-shape workloads by constructing a shape-generic search space that is made up of micro-kernels, incomplete programs that carries out a tile of the complete computation. Because every micro-kernel can be ported to every shape of the workload, this gives DietCode a unified space to search for efficient schedules for all the shapes. With this change in the search space foundation, DietCode designs a cost model that is made up of a convoluted shapegeneric component and a simple shape-dependent component to guide its exploration in the search space. While the former requires extracting program features from the microkernels (e.g., loop structures, memory access patterns) and constantly learning on real hardware measurements to be accurate, the latter does not. This design allows for efficient cost model predictions, as to adapt a micro-kernel to all the possible shapes of the workload, only the shape-dependent component needs to be updated for each shape.</p><p>We highlight that DietCode has all the shapes of a workload explore the same search space and learn the same cost model jointly, therefore making the auto-scheduling process have a constant-time runtime complexity with respect to the number of shapes, which is much more efficient compared with the existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref>.</p><p>Our contributions can be summarized as follows:</p><p>(1) We find and address the key challenges in making autoscheduling practical for the important class of dynamicshape workloads by using a shape-generic search space.</p><p>(2) We build DietCode, a new auto-scheduler framework for dynamic-shape workloads that adopts the joint learning approach that optimizes all the possible shapes of the workload collectively within the same shape-generic search space, and learns the same shape-generic cost model.</p><p>(3) We evaluate DietCode on BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, a state-of-the-art language modelling application, and show that DietCode can greatly reduce the auto-scheduling time by up to 5.88? compared with Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, a state-of-the-art auto-scheduler, on the uniformly sampled dynamic shapes (94.1? estimated if all the possible shapes are included). At the same time, DietCode improves the runtime performance by up to 69.5% better than Ansor and 18.6% better than the vendor library (cuBLAS and cuDNN <ref type="bibr">(NVIDIA, 2021;</ref><ref type="bibr" target="#b11">Chetlur et al., 2014)</ref>) on a modern GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we present an overview of the key characteristics and shortcomings of both vendor libraries <ref type="bibr">(oneAPI, 2021;</ref><ref type="bibr" target="#b11">Chetlur et al., 2014;</ref><ref type="bibr">NVIDIA, 2021)</ref> and state-of-theart auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> on dynamic-shape workloads to motivate DietCode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vendor Libraries and Existing Auto-Schedulers</head><p>Achieving high performance for compute-intensive operators (e.g., convolution and matrix multiplication) has always been a challenging task. Therefore, most state-of-the-art machine learning frameworks (e.g., TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>, PyTorch <ref type="bibr" target="#b30">(Paszke et al., 2019)</ref>, and MXNet <ref type="bibr" target="#b8">(Chen et al., 2015)</ref>) rely on heavily optimized, hand-crafted vendor libraries <ref type="bibr">(e.g., oneDNN (oneAPI, 2021)</ref> on Intel CPUs; cuDNN <ref type="bibr" target="#b11">(Chetlur et al., 2014)</ref> and cuBLAS (NVIDIA, 2021) on NVIDIA GPUs). Despite providing high performance for a pool of operators, those libraries come with a nontrivial price: developing the efficient implementations often requires (1) in-depth expertise across the software and hardware stacks; and (2) significant amount of engineering effort which lead to prolonged release cycle for vendor libraries to support new operators on new hardware.</p><p>To deliver high-performance tensor programs on diverse hardware in a relatively short time, auto-scheduler frameworks (e.g., Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, TVM <ref type="bibr">(Chen et al., 2018a)</ref>, Halide auto-scheduler <ref type="bibr" target="#b1">(Adams et al., 2019)</ref>, and Tensor Comprehensions <ref type="bibr" target="#b37">(Vasilache et al., 2020)</ref>) have been proposed. The input to the auto-scheduler is a tensor expression, which includes an operator specification and input shape descriptions (see Figure <ref type="figure">2(a)</ref>). The auto-scheduler automatically constructs high-performance programs by analyzing the expression. This is done by first formulating a search space that consists of numerous possible implementations (i.e., schedules, ? in Figure <ref type="figure" target="#fig_0">1(a)</ref>) derived from the tensor expression, and then building a cost model that can predict the runtime performance of each schedule (?). Throughout the auto-scheduling process, the cost model is constantly updated by real hardware measurements and used to guide the exploration within the search space (?). The schedule delivered at the end of the process can lead to operator implementations that are up to 3.8? faster than their counterparts in vendor libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Tensor Programs</head><p>Despite having different implementations, to the best of our knowledge, those existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> have a common limitation: they require workloads to be static (i.e., all shapes must be known at compile time) to perform analysis. Such a restriction makes it hard and even impractical to use them for many real world scenarios, as large amount of time has to be spent on autoscheduling (can be around 42 hours on a single operator as we will demonstrate later). Those scenarios include:</p><p>(1) Neural architecture search <ref type="bibr" target="#b48">(Zoph &amp; Le, 2017)</ref>: NAS aims to deliver a DNN that fits the given training dataset in a predefined search space of hyperparameters (e.g., batch sizes, hidden dimensions). Each hyperparameter can lead to a network of layers with distinct shapes. To achieve the best search results, a large number of network architectures have to be explored (e.g., 12, 800 architectures are examined in <ref type="bibr" target="#b48">Zoph &amp; Le (2017)</ref>).</p><p>(2) Dynamic by design: Although machine learning practitioners usually choose to freeze the models for deployment, certain models, especially those in the sequence learning domain, are dynamic by nature. For instance, machine translation <ref type="bibr" target="#b41">(Wu et al., 2016;</ref><ref type="bibr" target="#b38">Vaswani et al., 2017)</ref>, speech recognition <ref type="bibr" target="#b4">(Amodei et al., 2016)</ref>, and language modeling <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> all involve dynamic sequence lengths that vary depending on the input data samples. Each sequence length again corresponds to a network of layers with distinct shapes. For example, in the case of BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, a state-of-the-art language modelling application, the sequence length can take on any value from 1 to 128.<ref type="foot" target="#foot_0">1</ref> </p><p>(3) Varying shapes with different layer positions: Even if the model is static, one operator class can exhibit distinct shapes at different positions of the model. Take the BERT model <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> again as an example: the hidden dimension of a dense layer can take on the value of {768, 2304, 3072} depending on where it is in the model.</p><p>Consequently, tensor programs of the same operator type in these scenarios can take on various shapes. The restriction that only static-shape workloads are accepted by the existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> poses challenges in using them in these scenarios, as their compilation time can be extremely long (roughly estimated to be 4200, 42, and 1 hour(s) respectively on a modern CPU for a single layer type, 20? more for an entire network <ref type="bibr">(Zheng et al., 2020a)</ref>). We observe, however, that if those programs can be grouped together as a single dynamic-shape workload and auto-scheduled once, the compilation time can be significantly shortened by up to 5.88? (94.1? projected if all the possible shapes are included).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Why A New Auto-Scheduler Framework?</head><p>This work proposes a new auto-scheduler framework that efficiently supports dynamic-shape workloads. However, a reasonable question to ask is why such a new framework is needed in the first place, or in other words, why existing solutions are not adequate. More precisely, could vendor libraries <ref type="bibr">(oneAPI, 2021;</ref><ref type="bibr" target="#b11">Chetlur et al., 2014;</ref><ref type="bibr">NVIDIA, 2021)</ref> or existing auto schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> plus their extensions be used instead? Below, we outline some of the key challenges with these alternative approaches to highlight why existing solutions are not satisfactory in practice. The example that we use is the dense layer Y = XW ? from the BERT-base <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> model under a commonly used batch size of 16 (see Figure <ref type="figure">2</ref>(b)) <ref type="bibr">(Zheng et al., 2020a)</ref>. The dynamism of the tensor program lies in the sequence length T, which represents the length of a sentence in a corpus. Without loss of generality, we pick its range to be <ref type="bibr">[1,</ref><ref type="bibr">128]</ref> for illustrative purpose.</p><p>Vendor libraries <ref type="bibr">(oneAPI, 2021;</ref><ref type="bibr">NVIDIA, 2021;</ref><ref type="bibr" target="#b11">Chetlur et al., 2014)</ref> are hand-crafted to include several optimized kernels to cover all the possible shapes, and workloads will be dispatched on the fly to the most suitable schedule (determined by hard-coded heuristics) for execution based on their shapes. When the workload shape does not entirely fit into the schedule, the runtime performance will be suboptimal on specific hardware or workload (as much as 13? performance degradation in some pathological cases (Alibaba, 2018)). The reason for this sub-optimality can be padding the shape to the multiple of the hard-coded kernel tile sizes, which introduces padding overhead and decreases the computation efficiency. Previous works observe such situations in the form of redundant compute operations (Alibaba, 2018) and latency stair-casing <ref type="bibr" target="#b44">(Yu et al., 2020)</ref>. Those inefficiencies hence push for the need to extend the handcrafted kernels in the vendor libraries, which is nevertheless a nontrivial task due to the complexity of those kernels.</p><p>Existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> can overcome the above shortcomings by optimizing the schedule for each possible shape, but directly applying them to dynamic-shape workloads can be challenging. Figure <ref type="figure" target="#fig_0">1</ref>(a) provides a straightforward workflow if one would like to adopt existing auto-schedulers to handle dynamic-shape workloads. As existing auto-schedulers can only operate on one static shape at a time, every possible static shape needs to be autoscheduled individually. This is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(a) as multiple instances of the auto-schedulers (next to S i 's). Since each instance takes roughly 0.33 CPU hour to optimize the schedule for a single shape on a modern machine equipped with 16 Intel ? Xeon ? Platinum 8259CL CPUs (PassMark Software, 2020), we empirically estimate 42 CPU hours to optimize a single dynamic-shape dense layer workload specified in Figure <ref type="figure">2(b)</ref>.</p><p>Although extensions based on the existing auto-schedulers have been proposed to support dynamic-shape workloads <ref type="bibr" target="#b43">(Yu, 2019;</ref><ref type="bibr" target="#b36">Shen et al., 2021;</ref><ref type="bibr" target="#b40">Wang, 2019)</ref>, to our best knowledge, those prior works all face challenges of being not fully automatic and/or producing low-performance tensor programs: (1) Selective tuning <ref type="bibr" target="#b43">(Yu, 2019)</ref> heuristically groups static-shape workloads into clusters and optimizes for each cluster separately, which involves human expertise and therefore is not fully automatic. (2) Nimble <ref type="bibr" target="#b36">(Shen et al., 2021)</ref> auto-schedules a dynamic-shape workload by letting it take on a large shape and apply its schedule generically to all shapes. However, as our evaluation in Section 5 will show, a schedule that is optimal on the large shape might be suboptimal on the others. ( <ref type="formula" target="#formula_1">3</ref>) Bucketing <ref type="bibr" target="#b40">(Wang, 2019)</ref> splits the dynamic range into small sub-ranges (e.g., T ? [1, 128] in Figure <ref type="figure">2</ref>(b) into T ? [1, 8], [9, 16], . . .) and tunes only for the maximum value within each sub-range while padding the rest to the maximum. There are, however, two sources of inefficiency with this approach: (i) It requires extra padding and slicing operations before and after the main computation respectively that bring performance and storage penalty. (ii) Performing computation on the padded tensors results in low efficiency, as many computations are useless (e.g., in the previous bucketing example, T = 8 requires 7? more computations than T = 1). These weaknesses hinder the use of existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> to support dynamic-shape workloads in a practical and efficient way, which therefore motivates for a new auto-scheduler design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DIETCODE: KEY IDEAS</head><p>We further dive into the challenges of the existing autoschedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> to understand why they cannot be easily improved to be efficient on dynamic-shape workloads. We then present the key ideas behind the proposals of our design. Instead of tuning every possible shape one by one using existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref>, we observe that the search spaces of different shapes can in fact overlap and hence can potentially form a shape-generic search space. Take the loop tile schedule in Figure <ref type="figure" target="#fig_1">3</ref>(b) that is transformed from the tensor expression in (a) as an example. When tuning for the tile size t, existing auto-schedulers only consider all factors of T as candidates, meaning that t ? {1, 7, 49} when T = 49 and t ? {1, 2, 5, . . . , 50} when T = 50. However, any integer smaller than T are in fact all valid candidates, and they form a shape-generic search space that can be used by all possible T 's. We observe from this example that the search space construction is essentially to make a trade-off between a large, unified search space for all the possible shapes and the challenge of addressing extra boundary checks (which are needed in the case of non-perfect tiling, e.g., t = 10 when T = 49). As we will show in Section 4.1, we could avoid the boundary checking overhead by carefully handling the non-perfect tiling in the code generation. Accordingly, non-factor tile candidates can also lead to the same or even better performance than the factor candidates. We show how non-factor candidates can positively affect the performance of the generated tensor programs in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shape-Generic Search Space</head><p>From these merits, we construct a shape-generic search space that consists of micro-kernels (? in Figure <ref type="figure" target="#fig_0">1</ref>(b)), an incomplete program that carries out a tile of the complete computation, to efficiently support dynamic-shape workloads. We use the hardware constraints (e.g., the maximum number of threads, the amount of shared and local memory) rather than the shape information to determine the microkernel candidates. Those candidates serve as the building blocks and are executed repeatedly to carry out a workload instance (defined as an static-shape instance of the dynamicshape workload). For example, Figure <ref type="figure" target="#fig_3">4</ref>    can be easily ported to multiple workload instances of different shapes and hence it is shape-generic. If we revisit the previous example, the micro-kernel dense_128x128 can be leveraged to realize not only T = 64, but T = 1, 2, . . . , 127, 128 as well.</p><p>Despite being a generic solution, the use of micro-kernels poses two new and important challenges: (1) How to compose high-performance complete programs using microkernels, especially in the case when the workload cannot perfectly fit into the micro-kernel (see Figure <ref type="figure" target="#fig_4">5</ref> as an example), and (2) How to accurately and efficiently predict the performance of micro-kernel-based complete programs. In this work, we address the first challenge using local padding that automatically pads the local workspace when fetching the input tensors from global memory (see Section 4.1 for more details) and the second challenge by building a microkernel-based cost model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Micro-Kernel-based Cost Model</head><p>To search for high-performance micro-kernels efficiently, we use cost models to guide the search process. Existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> also have cost models that predict the compute throughput of a program by extracting its features (e.g., loop structures, memory access patterns), but those cost models can only accept complete programs as inputs:</p><p>Cost(P ) = f (FeatureExtractor(P ))</p><p>(1) shape-generic cost function f MK that predicts the cost of M , and (2) a shape-dependent adaption cost function f adapt that defines the penalty of porting M to P . While f MK is a function that has to be learned and updated by real hardware measurements during the auto-scheduling process, f adapt is a simple term that can be evaluated using the core occupancy and the padding ratio (in other words, it does not require feature extraction, see Section 4.2 for more details). Given below is the mathematical form of our cost model:</p><formula xml:id="formula_0">Cost M (P ) = f MK (FeatureExtractor(M ))?f adapt (P, M )</formula><p>(2) Compared with Equation <ref type="formula">1</ref>, Equation 2 has the key advantage of being efficient: To predict the performance of many complete programs that share the same micro-kernel M , the program feature extraction only needs to be done once on the micro-kernel M , and each program only needs to update the adaption cost individually. This significantly removes the redundancies in the feature extraction, leading to a more efficient auto-scheduling pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Learning with DietCode</head><p>From the above search space and cost model formulations, we propose DietCode, a new auto-scheduler framework that has three key components: (1) a shape-generic search space, (2) a micro-kernel-based cost model, and (3) a dispatcher (? in Figure <ref type="figure" target="#fig_0">1</ref>(b), see Section 4.3 for details) that automatically dispatches static shapes to micro-kernels based on Equation 2 at runtime. The optimization workflow of DietCode is a joint learning process (? in Figure <ref type="figure" target="#fig_0">1(b)</ref>), where all workload instances of a dynamic-shape workload share the same search space and collectively learn the same cost model. This workflow gives DietCode the ability to operate on a per-category basis, where each category shares the same shape-generic program (i.e., micro-kernel). Table <ref type="table" target="#tab_0">1</ref> shows the comparison between the three major approaches we described so far: vendor libraries <ref type="bibr">(oneAPI, 2021;</ref><ref type="bibr">NVIDIA, 2021;</ref><ref type="bibr" target="#b11">Chetlur et al., 2014)</ref>, existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref>, and Di-etCode, where we use |S| to denote the number of possible shapes of a dynamic-shape workload. Because DietCode can produce all the shape-generic categories in one run, it can significantly reduce the auto-scheduling time compared with the existing auto-schedulers on dynamic-shape workloads, as our evaluation will show in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAILS</head><p>We integrate DietCode as a part of TVM <ref type="bibr">(Chen et al., 2018a)</ref> in the form of an auto-scheduler submodule. In this section, we highlight some of the details of the DietCode's design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Local Padding</head><p>One of the key ideas of DietCode is to apply micro-kernels generically to all workload instances, each corresponding to a static-shape instance of the dynamic-shape workload. However, it is common for workload instances to not fit the micro-kernels perfectly, as is illustrated in Figure <ref type="figure" target="#fig_4">5</ref> where the micro-kernels at the last column are not fully materialized. In these cases, boundary checks need to be injected inside the micro-kernel to make sure that the program does not operate on invalid data values, but they also bring large performance degradation to the program (as much as 17? in our evaluation with the example illustrated in Figure <ref type="figure" target="#fig_4">5</ref> on a modern Tesla T4 GPU (NVIDIA, 2020)). This is because those checks bring in extra branching and compute instructions in the generated program.</p><p>To our best knowledge, there are three solutions to mitigate this problem. To demonstrate them, we use the code snippet with boundary checks in Figure <ref type="figure" target="#fig_5">6</ref>(a) as an example, where we greatly simplify the schedule by keeping its skeleton only to help understanding (fetch the input tensor from the global memory to the local workspace, compute, and writeback the output tensor to the global memory).</p><p>(1) Global padding pads the input tensor before the main computation and slice the output tensor afterwards. This can remove all the boundary checks in the compute kernel, but would require extra storage and injections of pad/slice operations (see Figure <ref type="figure" target="#fig_5">6</ref>(b)).</p><p>(2) Loop partitioning <ref type="bibr" target="#b36">(Shen et al., 2021)</ref> partitions the complete programs into regions where boundary checks can be eliminated and regions where they cannot (see Figure <ref type="figure" target="#fig_5">6</ref>(c)).</p><p>The performance benefits of loop partitioning depend on the relative ratio between the inner and the outer loop: If t ? T , there will be more iterations of i.0 without boundary conditions and hence more benefits. However, if t ? T does not hold, the benefits can be limited.</p><p>( In this work, we pick the third option because it has the merits of both being transparent (incurring no storage overhead and extra operators) and can be generically applied to both large and small shapes (whereas loop partitioning only works best on large shapes). We prove that local padding is an efficient solution to address the boundary checks challenge with our evaluation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Micro-Kernel-based Cost Model</head><p>To accurately predict the performance of micro-kernelbased complete programs, we devise three terms in the cost model that respectively account for: ? the performance of the micro-kernel, ? the hardware core occupancy penalty, which correlates to the number of times this micro-kernel is executed to compose the complete program, and ? the padding penalty. The mathematical expression of the cost model is as follows:</p><formula xml:id="formula_2">Cost M (P ) =f MK (FeatureExtractor(M ))?? ?f OCC ( P /M) ? f pad (P, M )? f adapt(P,M )<label>(3)</label></formula><p>where M denotes the micro-kernel and P the complete program. The two functions f OCC , f padding of the adaption cost correspond respectively to:</p><p>f OCC : The hardware core occupancy penalty, which we model using a linear regression model: where the term P /M corresponds to the number of times M is executed to form P and the coefficients k, b are learnable parameters. Since each micro-kernel is dispatched to one hardware core, the term P /M also correlates to the number of occupied hardware cores. Therefore, the term ? in the above equation denotes the hardware occupancy ratio of a complete program P that is composed using M . This ratio is further weighted using the two coefficients k, b, whose values are dynamically adapted to the hardware platforms when auto-scheduling, to estimate the hardware occupancy cost. In fact, since we want the occupancy cost to be 1 when all the cores are fully occupied, we can further derive:</p><formula xml:id="formula_3">f OCC ( P /M) = k P /M ceil by( P /M, NumCores) ? +b</formula><formula xml:id="formula_4">f OCC (P, M ) = 1, when P /M ceil by( P /M, NumCores) = 1 ? k + b = 1 ? b = 1 -k to reduce one of the dynamic parameters b.</formula><p>f pad : The padding penalty, which can be modelled in a hardware-agnostic way as pad by(P,M ) /P (the pad by primitive pads the complete program P by the size of M ).</p><p>Figure <ref type="figure" target="#fig_7">7</ref> illustrates the correlation between the three terms in Equation 3 and how the micro-kernels form the complete program, where we can see the one-to-one correspondence between the equation and the complete program composition. Such correspondence allows us to accurately predict the performance of micro-kernel-based complete programs, and hence search for high-performance micro-kernels efficiently using evolutionary search <ref type="bibr" target="#b39">(Vikhar, 2016)</ref> in the joint learning process (? in Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automatic Dispatching</head><p>After the joint learning process has been completed, a set of micro-kernels are generated by DietCode as the autoscheduling outcomes. To dispatch all the possible shapes to those micro-kernels, we have each shape S vote for its favorite micro-kernel based on the cost formula in Equation <ref type="formula" target="#formula_1">3</ref>: vote(S) = argmax M (Cost M (P (S, M ))) where the LHS denotes the voted micro-kernel, and the term P (S, M ) refers to the complete program of shape S composed using the micro-kernel M .<ref type="foot" target="#foot_2">3</ref> After all S's have voted, we train a decision tree using the scikit-learn frame- work <ref type="bibr" target="#b31">(Pedregosa et al., 2011</ref>) (? in Figure <ref type="figure" target="#fig_0">1</ref>). The input to the decision tree is all the possible shapes and the output labels are their selected micro-kernels. The generated decision tree automatically categorizes shapes that select the same micro-kernel together, and is exported in the C-style source code format for efficient dispatching from the input shape to its corresponding micro-kernel at runtime. Now that we have presented the details of the DietCode's design, we evaluate its efficiency on state-of-the-art machine learning workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>Infrastructure. Our major compute platform is an Amazon EC2 G4dn instance <ref type="bibr" target="#b3">(Amazon, 2021)</ref>, which is equipped with 16 Intel ? Xeon ? Platinum 8259CL CPUs (PassMark Software, 2020) and 1 NVIDIA Tesla T4 GPU (NVIDIA, 2020), with CUDA 11.3 (NVIDIA, 2021a), cuDNN 8.2 (NVIDIA, 2021), and TVM v0.8.dev0 <ref type="bibr">(Chen et al., 2018a)</ref>.</p><p>Applications. We evaluate our new auto-scheduler framework, DietCode, first on the BERT-base model <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> end-to-end with dynamic sequence lengths. After that, we show how the performance is achieved by evaluating on the dense and batched matrix multiplication layers extracted from BERT-base, also with dynamic sequence lengths.</p><p>As it is impractical to complete the entire auto-scheduling procedure using the existing auto-schedulers <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b1">Adams et al., 2019;</ref><ref type="bibr" target="#b37">Vasilache et al., 2020)</ref> (it can take about 42 CPU hours to complete a single dynamic-shape workload for the sequence length within the range of [1, 128]), we sample 8 shape configurations uniformly within the range of <ref type="bibr">[1,</ref><ref type="bibr">128]</ref> to compare the performance and the auto-scheduling time, and use the auto-scheduling time on those configurations to project the total auto-scheduling time across the entire range.</p><p>Baselines. We compare DietCode with three state-of-the-art baselines: (1) the vendor library (cuBLAS (NVIDIA, 2021) and cuDNN <ref type="bibr" target="#b11">(Chetlur et al., 2014)</ref> on the GPUs), which we refer to as Vendor, (2) the state-of-the-art auto-scheduler implementation, Ansor <ref type="bibr">(Zheng et al., 2020a;</ref><ref type="bibr">Chen et al.,</ref>  2018a), that targets static-shape workloads, which we refer to as Ansor, (3) the state-of-the-art prior work on dynamic code generation, Nimble <ref type="bibr" target="#b36">(Shen et al., 2021)</ref>, that extends Ansor to support dynamic-shape workloads by tuning the largest shape and applying its schedule generically to all shapes using loop partitioning, which we refer to as Nimble. On each static-shape workload, Ansor is sufficiently tuned for at least 1000 trials.</p><p>Metrics. We show the results on (1) the end-to-end latency on the entire model, measured as msec and averaged over 5 runs (2) the runtime cost of the generated tensor programs on a single operator, measured as ?sec and averaged over 100 runs, and (3) the total auto-scheduling time used, measured as hours. For all metrics, lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">End-to-End Model Evaluation</head><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the end-to-end latency comparison between Vendor, Ansor, and DietCode on the BERT-base model <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> with dynamic sequence lengths, where all numbers are normalized to Ansor under each sequence length. We observe from the figure that the latency of Di-etCode is up to 69.5% better than that of Ansor and 18.6% better than that of Vendor (29.9% and 5.4% better on average, respectively). The reason of the speedup will be evident in the following sections as we show the performance comparison on each individual layer of the model.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> shows the amount of auto-scheduling time taken by Ansor and DietCode on the sampled sequence lengths. Since those 8 sequence lengths are uniformly chosen within the range of <ref type="bibr">[1,</ref><ref type="bibr">128]</ref>, we project the auto-scheduling time of Ansor on the entire range to be 16? of that on the sampled ones. We observe from the figure that DietCode takes 5.88? less time to auto-schedule for the entire model than Ansor on the sampled sequence lengths. This speedup is estimated to We now connect these improvements in the latency and the auto-scheduling time with results on individual layers as they are the building blocks for BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dynamic Dense Layer</head><p>We compare the performance between Vendor, Ansor, Nimble and DietCode on the dense layer with dynamic sequence lengths (denoted as T ) as in Figure <ref type="figure">2(b)</ref>. Figure <ref type="figure" target="#fig_10">10</ref> shows the comparison of the runtime cost under different sequence lengths. We observe from the figure that the runtime of Di-etCode across all the sequence lengths is on average 30.5% less than that of Ansor, 23.9% than that of Nimble, and 5.3% than that of Vendor. The reason why DietCode has better performance than Ansor is because, as is explained in Section 3.1, the shape-generic search space construction of DietCode allows it to explore tensor programs that are overlooked by the shape-dependent construction of Ansor (same reason for Nimble since it uses the schedule generated by Ansor). We cannot analyze the exact reason why DietCode can have better performance than Vendor since cuBLAS (NVIDIA, 2021) is a proprietary library.</p><p>Since DietCode only needs to run the auto-scheduling procedure once, it is much more efficient than Ansor in terms of the total auto-scheduling time. Figure <ref type="figure" target="#fig_9">9</ref> shows the autoscheduling time taken by Ansor and DietCode on various types of dynamic workloads, where we observe DietCode takes 6.3? less time than Ansor for auto-scheduling the dynamic dense layer on the sampled sequence lengths (and estimated to be 100.8? less on the entire range). The reason why it is below the theoretical 8? limit can be because the shape-generic search space construction and exploration is more complicated than that of shape-dependent search space, since hardware constraints need to be examined (as is explained in Section 3.1). However, the improvement in the auto-scheduling time is still noticeable and it will only be greater if we allow the dynamic sequence length T to take on more diverse values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dynamic BatchMatmul Layer</head><p>We adopt the same methodology on the dynamic batched matrix multiplication workload as is defined in Figure <ref type="figure" target="#fig_12">11</ref>, which is potentially more challenging as there are more than one axis being dynamic. We evaluate on both NT and NN data layout (where the NT layout transposes the second operand) to see whether DietCode can be generically applied to cases when there are dynamic spatial and reduction axes at the same time.</p><p>Figure <ref type="figure" target="#fig_13">12</ref> shows the comparison of the runtime cost under dynamic sequence lengths, from which we observe that in terms of the runtime across all the sequence lengths Diet-Code is on average 21.5% better than Ansor, 351% better than Nimble, and 12.3% better than Vendor on the NT layout. Moreover, it is 24.2% better than Ansor, 40.4% better than Nimble, and 15.4% better than Vendor on the NN layout.</p><p>The reason why Nimble does not deliver good performance (in the NT layout case particularly) is two-fold: (1) It directly applies the schedule generated on the largest shape to other shapes, which can lead to sub-optimal performance due to issues such as padding (see Figure <ref type="figure" target="#fig_4">5</ref>). (2) It does not efficiently handle the boundary checks in the tensor programs, which are more critical in this case since there are two axes being dynamic. <ref type="foot" target="#foot_3">4</ref> We observe that since the shapes are relatively small compared with the previous benchmark (e.g., 192, 64 versus 768, 2304 in the case of the dynamic dense layer, as in Figure <ref type="figure" target="#fig_10">10</ref>), the loop partitioning technique employed by Nimble is unable to show any benefits (since there is not enough loops to be partitioned without the boundary checks, as is explained in Section 4.1). Similar to the dynamic dense layer benchmark, the autoscheduling time of DietCode is 4.55? and 5.56? less than that of Ansor on the NT and NN layout respectively (see Figure <ref type="figure" target="#fig_9">9</ref>). Such benefits allow us to tune on the entire model end-to-end within a reasonable amount of time, as we have demonstrated in Section 5.2.</p><p>We conclude with the above experiments that DietCode is efficient in terms of the auto-scheduling time while preserving the ability to deliver high-performance tensor programs. It can be generically applied to cases when there are multiple dynamic axes, or when there is a hybrid of dynamic spatial and reduction axes, and even to an entire model. Such efficiency and generality in auto-scheduling make DietCode a practical solution for dynamic-shape workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>DietCode addresses the key challenges of auto-scheduling dynamic-shape workloads by constructing a shape-generic search space and have all the possible shapes jointly search within the same space and update the same cost model, which is unseen in existing auto-schedulers that target staticshape workloads (Ansor <ref type="bibr">(Zheng et al., 2020a)</ref>, TVM <ref type="bibr">(Chen et al., 2018a)</ref>, Halide auto-scheduler <ref type="bibr" target="#b1">(Adams et al., 2019)</ref>, TensorComprehensions <ref type="bibr" target="#b37">(Vasilache et al., 2020)</ref>). Although DietCode is currently implemented on top of TVM <ref type="bibr">(Chen et al., 2018a)</ref>, its ideas can be generically applied to other compiler frameworks as well, such as Halide <ref type="bibr" target="#b32">(Ragan-Kelley et al., 2018)</ref>, TensorComprehensions <ref type="bibr" target="#b37">(Vasilache et al., 2020)</ref>, Tiramisu <ref type="bibr" target="#b5">(Baghdadi et al., 2019)</ref>, XLA <ref type="bibr" target="#b34">(Sabne, 2020)</ref>, MLIR <ref type="bibr" target="#b21">(Lattner et al., 2021)</ref>, Glow <ref type="bibr" target="#b33">(Rotem et al., 2018)</ref>, taco <ref type="bibr" target="#b19">(Kjolstad et al., 2017)</ref>, and TASO <ref type="bibr" target="#b18">(Jia et al., 2019)</ref>.</p><p>Reuse-based Tuner. Selective Tuning <ref type="bibr" target="#b43">(Yu, 2019)</ref> and ETO <ref type="bibr" target="#b13">(Fang et al., 2021)</ref> group workloads into clusters based on a set of pre-defined rules (e.g., similarity ratio in Selective Tuning <ref type="bibr" target="#b43">(Yu, 2019)</ref>) and reuse the same schedule in a single cluster. Their approaches are parallel with the joint learning adopted by DietCode.</p><p>Auto-Tuners. AutoTVM <ref type="bibr">(Chen et al., 2018b</ref><ref type="bibr">), ProTuner (Haj-Ali et al., 2020</ref><ref type="bibr">), and FlexTensor (Zheng et al., 2020b)</ref> leverage program templates to define the search space and guide the search procedure. They are different from autoschedulers in that the search space has to be manually defined. However, if there exists a pre-defined micro-kernel search space, then the key ideas DietCode can be applied to those auto-tuners as well.</p><p>Dynamic Neural Networks. Dynamic batching is a common graph-level optimization adopted by frameworks such as DyNet <ref type="bibr" target="#b23">(Neubig et al., 2017)</ref>, Cavs <ref type="bibr" target="#b42">(Xu et al., 2018)</ref>, BatchMaker <ref type="bibr" target="#b15">(Gao et al., 2018)</ref>, and TensorFlow Fold <ref type="bibr" target="#b22">(Looks et al., 2017)</ref> for cases when the batch size is dynamic. Nimble <ref type="bibr" target="#b36">(Shen et al., 2021)</ref> and DISC <ref type="bibr" target="#b47">(Zhu et al., 2021)</ref> both design a compiler to represent and execute dynamic neural networks. Cortex <ref type="bibr" target="#b14">(Fegade et al., 2020</ref>) is a compiler-based framework on recursive neural networks. Those works focus on the graph-level optimizations and therefore are orthogonal to DietCode, which operates on each individual layer. In fact, those graph-level solutions can also leverage DietCode for efficient operator code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we propose DietCode, a new auto-scheduler framework for dynamic-shape workloads. Our evaluation shows that on DietCode can significantly reduce the autoscheduling time by 5.88? (94.1? projected if all the possible shapes are included), while achieving up to 69.5% better performance than the state-of-the-art auto-schedulers and 18.6% than the vendor library on a full state-of-the-art DNN model end-to-end. We hope that DietCode would become an efficient platform for further research on efficient system design for key machine learning applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Code-generation comparison between (a) existing auto-schedulers and (b) DietCode. Existing approaches auto-schedule each shape individually. DietCode solves the problem by having all shapes jointly search within the same shape-generic search space and update the same micro-kernel cost model.</figDesc><graphic url="image-1.png" coords="3,55.44,67.06,486.01,219.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A loop tile schedule (b) transformed from the input tensor expression (a), where T is the user-provided loop extent and t is a tunable tile size parameter by the auto-scheduler.</figDesc><graphic url="image-2.png" coords="5,307.44,67.06,234.00,65.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>shows how the micro-kernel dense_128x128, 2 which evaluates Y = XW ? , X : [128, 768], W : [128, 768] can be leveraged to perform an instance of Y = XW ? as in Figure 2(b) with T = 64. This is achieved by dissecting the complete program into 8 ? 18 pieces along the spatial dimensions (since 16 ? T = 8 ? 128, 2304 = 18 ? 128) and carry out each piece individually. Because micro-kernels only realize a piece of the complete computation, each one 2 Although we use the dense layer and the micro-kernel size 128 ? 128 here to demonstrate how micro-kernels work, the exact same idea applies to other workloads and/or micro-kernel sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Micro-kernel dense_128x128 used to realize Y = XW ? as in Figure 2(b) with T = 64. The horizontal and vertical axis represent the output dimensions of Y .</figDesc><graphic url="image-3.png" coords="5,307.44,165.40,234.00,65.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Micro-kernel dense_128x128 used to realize Y = XW ? as in Figure 2(b) with T = 60. The tiles at the last column are padded (shown in red) to fit the micro-kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) A tiled loop with boundary checks. (b-d) Three optimization strategies: (b) global padding, (c) loop partitioning, and (d) local padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>)</head><label></label><figDesc>Local padding pads the local workspace when fetching the input tensor values from the global memory, and slices the local workspace when writing back. This is equivalent to preserving the boundary checks at the fetch and writeback stages while removing those at the compute stage (see Figure 6(d)). The idea of local padding is based on the two key observations: (i) Only the boundary checks at the compute stage have the dominant impact on the runtime performance, because those at the fetch and writeback stages can be hidden by the latency of the global memory transfers (NVIDIA, 2019), and (ii) Padded data values that are computed in the compute stage will be filtered out by the boundary checks in the writeback stage, hence they do not affect the output tensor values (i.e., program correctness).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. fMK, fOCC, and f pad account for different program behaviors (example taken from Figure 5).</figDesc><graphic url="image-4.png" coords="7,307.44,67.06,234.00,76.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Latency comparison between Vendor, Ansor, and DietCode on the entire BERT-base model with dynamic sequence lengths.</figDesc><graphic url="image-6.png" coords="8,79.74,80.01,437.39,109.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Auto-scheduling time comparison between Ansor and DietCode on various dynamic workloads. The lower the better.</figDesc><graphic url="image-8.png" coords="8,307.44,203.26,234.00,70.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Runtime comparison between Vendor, Ansor, Nimble, and DietCode on Dense with dynamic sequence lengths. increase to 94.1? on the entire range, since Ansor needs to auto-schedule for each individual sequence length, whereas DietCode only needs to do the auto-scheduling once.</figDesc><graphic url="image-10.png" coords="9,79.74,80.01,437.39,109.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Y</head><label></label><figDesc>= BatchMatmul NT (X, W ) X : [192, T, 64], W : [192, T, 64], T ? [1, 128] Y = BatchMatmul NN (X, W ) X : [192, T, T], W : [192, T, 64], T ? [1, 128]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Dynamic-shape workload Y = BatchMatmul(X, W ) (with T being dynamic) and its corresponding tensor program.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Runtime comparison between Vendor, Ansor, Nimble, and DietCode on BatchMatmul with dynamic sequence lengths.</figDesc><graphic url="image-13.png" coords="10,79.74,167.47,437.39,98.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between the three options in terms of the ability to do auto-tuning and the runtime complexity of tuning. |S| represents the number of possible shapes.</figDesc><table><row><cell>f : cost function (e.g., XGBoost (Chen &amp; Guestrin, 2016))</cell></row><row><cell>P : complete program Cost : compute throughput</cell></row><row><cell>However, as micro-kernels are incomplete (they are a tile</cell></row><row><cell>of the complete programs), they cannot be applied to the</cell></row></table><note><p>existing cost models. To address this challenge, we build a micro-kernel-based cost model. The key insight is that the cost of a complete program P that is made up of a micro-kernel M can be decomposed into two parts: (1) a</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Although padding can be used to pad the dynamic sequence lengths to static values, this nevertheless leads to performance degradation (can be as much as</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2? (Kosec et al., 2021)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The reason why it is argmax in the formula is because the term cost usually refers to the compute throughput(Chen et al.,  2018b; Zheng et al., 2020a), hence the higher the better.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Although the NN layout case also has two dynamic axes, one of them is the reduction axis. Because Ansor usually unrolls the reduction axes, the boundary checks on the those axes are automatically optimized after the loops are unrolled.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARTIFACT APPENDIX A.1 Abstract</head><p>We provide the source code and scripts that correspond to Section 3 and 4 as our artifact. We use the AWS G4 instance as the hardware platform, and NVIDIA driver and Docker to build up the software stack. After the installation, each of the experiment is automated by a single script file.</p><p>A.2 Artifact check-list (meta-information)</p><p>? Algorithm: DietCode Auto-Scheduling Workflow ? Execution: Please use the provided script file scripts /2-experiment_ * .sh in the repository for each of the experiment.</p><p>? Metrics: Compute Throughput (in TFLOPs/s) and Wall-Clock Time</p><p>? Output: CSV Files</p><p>? Experiments: Auto-scheduling Dense and BatchMatmul operators with dynamic sequence lengths. Auto-scheduling key operators of the BERT <ref type="bibr" target="#b12">(Devlin et al., 2019</ref>) model (also with dynamic sequence lengths).</p><p>? How much disk space required (approximately)?: At least 20 GB (15 GB for the Docker image and 1.5 GB for the compilation artifacts)</p><p>? How much time is needed to prepare workflow (approximately)?: 1 hr </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Hardware dependencies</head><p>We use the AWS G4 instance (of type g4dn.4xlarge) to evaluate our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Software dependencies</head><p>The major software dependency is the NVIDIA GPU driver and the NVIDIA Docker container toolkit (nvidia-docker2 and nvidiacontainer-runtime). These two are automatically included in the AWS Deep Learning AMI (ubuntu 18.04) Version 50.0. The rest of the dependencies can be obtained by building the Docker image.</p><p>To facilitate the Docker commands, we use docker-compose, which is a wrapper on top of Docker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Data sets N/A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Installation</head><p>Please follow the steps below:</p><p>? Clone the project by git clone https://github.com/UofT-EcoSystem/DietCode -b MLSys2022_AE</p><p>? Install docker-compose, which is a wrapper on top of Docker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sudo -H pip3 install docker-compose</head><p>? Build the Docker image that includes all the software dependencies required to run the experiments:</p><p>DietCode$ docker-compose build tvm-dev</p><p>? Create a running container out of the image:</p><p>DietCode$ docker-compose run --rm tvmdev</p><p>? Build the DietCode and the TVM baseline.</p><p>/mnt$ ./scripts/1-compile.sh tvm /mnt$ ./scripts/1-compile.sh tvm_base A.5 Experiment workflow ansor/dietcode autosched timer.csv will be generated in the same folder that reports the time to complete the auto-scheduling process (also in seconds, the lower the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Experiment customization</head><p>N/A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Notes</head><p>Note that the entire auto-scheduling workflow takes time to complete. Therefore, we one can use the AUTO_SCHED_NTRIALS=200 ./scripts/... prefix that uses fewer number auto-scheduling trials. The resulting tensor programs will still be functionally correct but the performance can be sub-optimal.</p><p>A.9 Methodology Submission, reviewing and badging methodology:</p><p>? http://cTuning.org/ae/submission-201901 09.html</p><p>? http://cTuning.org/ae/reviewing-2019010 9.html</p><p>? https://www.acm.org/publications/polic ies/artifact-review-badging</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Keeton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</editor>
		<meeting><address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-11-02">2016. November 2-4, 2016. 2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to optimize Halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306346.3322967</idno>
		<ptr target="https://doi.org/10.1145/3306346.3322967" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="121" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bringing TVM into TensorFlow for optimizing neural machine translation on GPU</title>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://tvm.apache.org/2018/03/23/nmt-transformer-optimize" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Amazon EC2 G4 instances</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2/instance-types/g4/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/amodei16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<title level="s">JMLR Workshop and Conference Proceedings</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016. 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1109/CGO.2019.8661197</idno>
		<ptr target="https://doi.org/10.1109/CGO.2019.8661197" />
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Kandemir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-02-16">2019. February 16-20, 2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">August 13-17, 2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<idno>10.1145/2939672.2939785</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1512.01274</idno>
		<ptr target="http://arxiv.org/abs/1512.01274" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TVM: An automated end-toend optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi18/presentation/chen" />
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Voelker</surname></persName>
		</editor>
		<meeting><address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">October 8-10, 2018. 2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/8" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="3393" to="3404" />
		</imprint>
	</monogr>
	<note>b5700012be65c9da 25f49408d959ca0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><surname>Cudnn</surname></persName>
		</author>
		<idno>CoRR, abs/1410.0759</idno>
		<ptr target="http://arxiv.org/abs/1410.0759" />
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ETO: Accelerating optimization of DNN operators by high-performance tensor program reuse</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol15/p183-chen.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cortex: A compiler for recursive deep learning models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fegade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
		<idno>CoRR, abs/2011.01383</idno>
		<ptr target="https://arxiv.org/abs/2011.01383" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low latency RNN inference with cellular batching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3190508.3190541</idno>
		<ptr target="https://doi.org/10.1145/3190508.3190541" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys 2018, Porto, Portugal</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Oliveira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Felber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</editor>
		<meeting>the Thirteenth EuroSys Conference, EuroSys 2018, Porto, Portugal</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">April 23-26, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ProTuner: Tuning programs with monte carlo tree search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Genc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.13685" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TASO: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341301.3359630</idno>
		<ptr target="https://doi.org/10.1145/3341301.3359630" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP 2019</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Brecht</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Williamson</surname></persName>
		</editor>
		<meeting>the 27th ACM Symposium on Operating Systems Principles, SOSP 2019<address><addrLine>Huntsville, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">October 27-30, 2019. 2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tool to generate tensor algebra kernels</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lugato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><surname>Taco</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASE.2017.8115709</idno>
		<ptr target="https://doi.org/10.1109/ASE.2017.8115709" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Rosu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Penta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</editor>
		<meeting>the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017<address><addrLine>Urbana, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-11-03">October 30 -November 03, 2017. 2017</date>
			<biblScope unit="page" from="943" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Packing: Towards 2x NLP BERT acceleration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kosec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Krell</surname></persName>
		</author>
		<idno>CoRR, abs/2107.02027</idno>
		<ptr target="https://arxiv.org/abs/2107.02027" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MLIR: Scaling compiler infrastructure for domain specific computation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<idno type="DOI">10.1109/CGO51591.2021.9370308</idno>
		<ptr target="https://doi.org/10.1109/CGO51591.2021.9370308" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning with dynamic computation graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryrGawqex" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><surname>Dynet</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.03980" />
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CUDA fundamental optimization, part 1</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.olcf.ornl.gov/wp-content/uploads/2019/12/03-CUDA-Fundamental-Optimization-Part-1.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">NVIDIA T4 70W low profile PCIe GPU accelerator</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-product-brief.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Programming guide :: CUDA toolkit documentation</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CUDA toolkit archive</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/cuda-toolkit-archive" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Developer guide :: NVIDIA deep learning cuDNN documentation</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Cublas</surname></persName>
		</author>
		<ptr target="https://github.com/oneapi-src/oneDNN" />
	</analytic>
	<monogr>
		<title level="m">deep neural network library (oneDNN)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CUDA toolkit documentation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Passmark</forename><surname>Software</surname></persName>
		</author>
		<ptr target="https://www.cpubenchmark.net/cpu" />
		<title level="m">Intel Xeon Platinum 8259CL @ 2.50GHz</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cpu=Intel+Xeon+Plat inum+8259CL+%40+2.50GHz&amp;id=3671</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/bdbca" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>288fee7f92f2bfa9f701 2727740-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2078195" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupling algorithms from schedules for high-performance image processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><surname>Halide</surname></persName>
		</author>
		<idno type="DOI">10.1145/3150211</idno>
		<ptr target="https://doi.org/10.1145/3150211" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Glow: Graph lowering compiler techniques for neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Olesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno>CoRR, abs/1805.00907</idno>
		<ptr target="http://arxiv.org/abs/1805.00907" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">XLA: Compiling machine learning for peak performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sabne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The cost of training NLP models: A concise overview</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<idno>CoRR, abs/2004.08900</idno>
		<ptr target="https://arxiv.org/abs/2004.08900" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nimble: Efficiently compiling dynamic neural networks for model inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tatlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2021/hash/4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>e732ced3463d0 6de0ca9a15b6153677-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated GPU kernels, automatically</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355606</idno>
		<ptr target="https://doi.org/10.1145/3355606" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c 4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms: A critical review and its future prospects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Vikhar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICGTSPICC.2016.7955308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">RFC] dynamic shape support -graph dispatching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/apache/incubator-tvm/issues/4118" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cavs: An efficient runtime system for dynamic neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc18/presentation/xu-shizen" />
	</analytic>
	<monogr>
		<title level="m">2018 USENIX Annual Technical Conference, USENIX ATC 2018</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">July 11-13, 2018. 2018</date>
			<biblScope unit="page" from="937" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">RFC][AutoTVM] selective tuning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://github.com/apache/incubator-tvm/pull/4187" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards latency-aware DNN optimization with GPU runtime analysis and tail effect elimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2011.03897</idno>
		<ptr target="https://arxiv.org/abs/2011.03897" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><surname>Ansor</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi20/presentation/zheng" />
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020, Virtual Event</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020">November 4-6, 2020. 2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">FlexTensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378508</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378508" />
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;20: Architectural Support for Programming Languages and Operating Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Strauss</surname></persName>
		</editor>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">March 16-20, 2020. 2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">DISC: A dynamic shape compiler for machine learning workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/2103.05288</idno>
		<ptr target="https://arxiv.org/abs/2103.05288" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ue8Hcxg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. Open-Review.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
