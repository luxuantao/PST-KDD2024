<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-29">29 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
							<email>jiarongxu@fudan.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
							<email>j.carlyang@emory.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yunchao</forename><surname>Zhang</surname></persName>
							<email>m.yunchaozhang@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
							<email>wangchunping02@xinye.com</email>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<email>chenlei04@xinye.com</email>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><forename type="middle">Xxxx</forename><surname>Yang</surname></persName>
							<email>yangya@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chun- Ping</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Emory University Atlanta</orgName>
								<address>
									<settlement>Georgia</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Soochow University Suzhou</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Finvolution Group Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Finvolution Group Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-29">29 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.16458v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>graph pre-training When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective! Conference acronym</term>
					<term>XXXX</term>
					<term>XX</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pretrain by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN first fits the pre-training data into graphon bases, where each element of graphon basis (i.e., a graphon) identifies a fundamental transferable pattern shared by a collection of pre-training graphs. All convex combinations of graphon bases give rise to a generator space, from which graphs generated form the solution space for those downstream data that can benefit from pre-training. In this manner, the feasibility of pre-training can be</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have undergone rapid development and become increasingly popular for learning graph data <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>. GNNs are usually trained in an end-to-end manner while getting enough labeled data is arduously expensive and sometimes even impractical to access. This motivates some recent advances in pretraining GNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. The key insight of pre-training GNNs is to learn transferable knowledge from a collection of unlabeled graph data, hoping that the learned knowledge can be easily adapted to downstream tasks. In view of the great success of pre-training in other fields like computer vision and natural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, graph pre-training is highly expected to be an effective means to improve downstream performance.</p><p>However, the intuition that graph pre-trained model would ideally benefit the downstream is far from the truth in the area of graph pre-training. Instead, graph pre-trained models can lead to negative transfer on many downstream tasks, especially when the graphs  used for pre-training are not necessarily from the same domain as the downstream data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the closed triangles ( ) and open triangles ( ) might yield different interpretations in molecular networks (unstable vs. stable in terms of chemical property) from those in social networks (stable vs. unstable in terms of social relationship); such distinct or reversed semantics does not contribute to transferability, and even exacerbates the problem of negative transfer.</p><p>To avoid the negative transfer, recent efforts focus on what to pre-train and how to pre-train, i.e., design/adopt graph pre-training models with a variety of self-supervised tasks to capture different patterns <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref> and fine-tuning strategies to enhance downstream performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>. However, there do exist some cases that no matter how advanced the pre-training/fine-tuning method is, the transferability from pre-training data to downstream data still cannot be guaranteed. This is because the underlying assumption of deep learning models is that the test data should share a similar distribution as the training data. Therefore, it is a necessity to understand when to pre-train, i.e., under what situations the "graph pre-train and fine-tune" paradigm should be adopted.</p><p>Towards the answer of when to pre-train GNNs, one straightforward way illustrated in Figure <ref type="figure" target="#fig_1">1</ref>(a) is to train and evaluate on all candidates of pre-training models and fine-tuning strategies, and then the resulting best downstream performance would tell us whether pre-training is a sensible choice. If there exist ? 1 pretraining models and ? 2 fine-tuning strategies, such a process would be very costly as you should make ? 1 ? ? 2 "pre-train and fine-tune" attempts. Another approach is to utilize graph metrics to measure the similarity between pre-training and downstream data, e.g., density, clustering coefficient and etc. However, it is a daunting task to enumerate all hand-engineered graph features or find the dominant features that influenced similarity. Moreover, the graph metrics only measure the pair-wise similarity between two graphs, which cannot be directly and accurately applied to the practical scenario where pre-training data contains multiple graphs.</p><p>In this paper, we propose a W2PGNN framework to answer when to pre-train GNNs from a graph data generation perspective. The high-level idea is that instead of performing effortful graph pre-training/fine-tuning or making comparisons between the pretraining and downstream data, we study the complex generative mechanism from the pre-training data to the downstream data (Figure <ref type="figure" target="#fig_1">1(b)</ref>). We say that downstream data can benefit from pretraining data (i.e., has high feasibility of performing pre-training), if it can be generated with high probability by a graph generator that summarizes the topological characteristic of pre-training data.</p><p>The major challenge is how to obtain an appropriate graph generator, hoping that it not only inherits the transferable topological patterns of the pre-training data, but also is endowed with the ability to generate feasible downstream graphs. To tackle the challenge, we propose to design a graph generator based on graphons. We first fit the pre-training graphs into different graphons to construct a graphon basis, where each graphon (i.e., element of the graphon basis) identifies a collection of graphs that share common transferable patterns. We then define a graph generator as a convex combination of elements in a graphon basis, which serves as a comprehensive and representative summary of pre-training data. All of these possible generators constitute the generator space, from which graphs generated form the solution space for the downstream data that can benefit from pre-training.</p><p>Accordingly, the feasibility of performing pre-training can be measured as the highest probability of downstream data being generated from any graph generator in the generator space, which can be formulated as an optimization problem. However, this problem is still difficult to solve due to the large search space of graphon basis. We propose to reduce the search space to three candidates of graphon basis, i.e., topological graphon basis, domain graphon basis, and integrated graphon basis, to mimic different generation mechanisms from pre-training to downstream data. Built upon the reduced search space, the feasibility can be approximated efficiently.</p><p>Our major contributions are concluded as follows:</p><p>? Problem and method. To the best of our knowledge, we are the first work to study the problem of when to pre-train GNNs.</p><p>We propose a W2PGNN framework to answer the question from a data generation perspective, which tells us the feasibility of performing graph pre-training before conducting effortful pretraining and fine-tuning.</p><p>? Broad applications. W2PGNN provides several practical applications: (1) provide the application scope of a graph pre-trained model, (2) measure the feasibility of performing pre-training for a downstream data and (3) choose the pre-training data so as to maximize downstream performance with limited resources.</p><p>? Theory and Experiment. We theoretically and empirically justify the effectiveness of W2PGNN. Extensive experiments on real-world graph datasets from multiple domains show that the proposed method can provide an accurate estimation of pretraining feasibility and the selected pre-training data can benefit the downstream performance.</p><p>Definition 1 (When to pre-train GNNs). Given the pretraining graph data G train and the downstream graph data G down , our main goal is to answer to what extent the "pre-train and fine-tune" paradigm can benefit the downstream data.</p><p>Note that in addition to this main problem, our proposed framework can also serve other scenarios, such as providing the application scope of graph pre-trained models, and helping select pre-training data to benefit the downstream (please refer to the application cases in Section 4.1 for details).</p><p>Transferable graph patterns. The success of "pre-train and finetune" paradigm is typically attributed to the commonplace between pre-training and downstream data. However, in real-world scenarios, there possibly exists a significant divergence between the pre-training data and the downstream data. To answer the problem of when to pre-train GNNs, the primary task is to define the transferable patterns across graphs.</p><p>We here theoretically explore which patterns are transferable between pre-training and downstream data under the performance guarantee of graph pre-training model (with GNN as the backbone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.1 (Transferability of graph pre-training model).</head><p>Let ? train and ? down be two (sub)graphs sampled from G train and G down , and assume the attribute of each node as a scalar 1 without loss of generality. Given a graph pre-training model ? (instantiated as a GNN) with ? layers and 1-hop graph filter ?(?) (which is a function of the normalized graph Laplacian matrix ?), we have</p><formula xml:id="formula_0">?? (? train ) -? (? down )? 2 ? ?? topo (? train , ? down )<label>(1)</label></formula><p>where ? topo (? train , ? down ) =  Detailed proofs and descriptions can be found in Appendix A.1. Theorem 2.1 suggests that two (sub)graphs sampled from pretraining and downstream data with similar topology are transferable via graph pre-training model (i.e., sharing similar representations produced by the model).</p><p>Hence we consider the transferable graph pattern as the topology of a (sub)graph, either node-level or graph-level. Specifically, the node-level transferable pattern could be the topology of the egonetwork of a node (or the structural role of a node), irrespective of the node's exact location in the graph. The graph-level transferable pattern is the topology of the entire graph itself (e.g., molecular network). Such transferable patterns constitute the input space introduced in Section 4.1.</p><p>Discussion of non-transferable graph patterns. As a remark, we show that two important pieces of information (i.e., attributes and proximity) commonly used in graph learning are not necessarily transferable across pre-training and downstream data in most realworld scenarios, thus we do not discuss them in this paper.</p><p>First, although the attributes carry important semantic meaning in one graph, it can be shown that the attribute space of different graphs typically has little or no overlap at all. For example, if the pre-training and downstream data come from different domains, their nodes would indicate different types of entities and the corresponding attributes may be completely irrelevant. Even for graphs from the similar/same domain, the dimensions/meaning of their node attributes can also be totally different and result in misalignment.</p><p>The proximity, on the other hand, assumes that closely connected nodes are similar, which also cannot be transferred across graphs. Obviously, this proximity assumption depends on the overlaps in neighborhoods and thus only works on graphs with the same or overlapped node set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY AND RELATED WORKS</head><p>Graphons. A Graphon (short for graph function) <ref type="bibr" target="#b0">[1]</ref> is a bounded symmetric function ? : [0, 1] 2 ? [0, 1] (we use different subscripts of ? to denote different graphons), which can be interpreted as the weighted matrix of an arbitrary undirected graph with uncountable number of nodes <ref type="bibr" target="#b22">[23]</ref>. Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. We utilize both perspectives in our framework.</p><p>On one hand, a graphon can be considered as the limit objects of graph sequence, where the density of certain "graph motifs" can be preserved; and every convergent graph sequence would converge to a graphon <ref type="bibr" target="#b22">[23]</ref>. Thus, a graphon provides a comprehensive summary of an entire collection of arbitrary size graphs. These graphs can be considered topologically similar in the sense that they belong to the same graphon. In this paper, we utilize a set of graphons as a comprehensive and representative summary of pre-training data.</p><p>Taking graphon as a graph generator, we can associate nodes ? and ? with points ? ? and ? ? in [0, 1], and then ?(? ? , ? ? ) serves as the probability to generate the edge between these two nodes. Therefore, a graphon ? can generate unweighted graphs of arbitrary sizes, which can be taken as those induced graphs potentially inheriting the topological patterns implied in graphon. Thus, the generation capability of graphon can help us generate feasible downstream graphs that can benefit from pre-training.</p><p>Graph pre-training and fine-tuning. Graph pre-training models first learn universal knowledge from large-scale graph datasets with self-supervised or unsupervised objectives (i.e., pre-training stage), and then transfer the knowledge to deal with specific downstream tasks (i.e., fine-tuning stage). Among them, some researchers design some pre-training tasks based on the neighborhood similarity assumption <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59</ref>]. The learned graphspecific patterns/knowledge could benefit downstream tasks on the same graphs, but cannot be generalized to unseen graphs. To enhance the transferability of the pre-trained graph models, some works try to utilize graph data from the same (or similar) domains as the pre-training data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61]</ref>, or explore cross-domain pre-training strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref> as well as fine-tuning strategies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>. Nevertheless, all these efforts focus on addressing the problem of what to pre-train and how to pre-train by developing pre-training or fine-tuning methods. To Transferability measure. There are several attempts to measure the transferability of GNNs. The most straightforword way is to train and evaluate on all candidates of pre-training models and fine-tuning strategies, and then the resulting best downstream performance as the transferability measure. However, as depicted in Figure <ref type="figure" target="#fig_1">1</ref>(a), such approach would be very costly to perform effortful pre-training and fine-tuning. Another way is based on graph properties, which leverage the graph properties (e.g., degree <ref type="bibr" target="#b2">[3]</ref>, density <ref type="bibr" target="#b42">[43]</ref>, assortativity <ref type="bibr" target="#b28">[29]</ref> and etc.) to measure the similarities between pre-training and downstream graphs, potentially can be utilized to approximate the transferability. Some other works also focus on analyzing the transferability of GNNs theoritically <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. Nevertheless, they are limited to measure the transferability of GNNs on a single graph or when training and testing data are from the same dataset <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> , which are inapplicable to our setting. A recent work, EGI <ref type="bibr" target="#b59">[60]</ref> addresses the transferability measure problem of GNNs across graphs. However, EGI is a model-specific measure and depend on its own framework. For the first time, we study the transferability of graph pre-training from the data perspective, without performing any pre-training and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first present our proposed framework W2PGNN to answer when to pre-train GNNs in Section 4.1. Based on the framework, we further introduce the measure of the feasibility of performing pre-training in Section 4.2. Then in Section 4.3, we discuss our approximation to the feasibility of pre-training. Finally, the complexity analysis of W2PGNN is provided in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework Overview</head><p>W2PGNN framework provides a guide for answering when to pretrain GNNs from a graph data generation perspective. The key insight is that if downstream data can be generated with high probability by a graph generator that summarizes the pre-training data, the downstream data would present high feasibility of performing pretraining.</p><p>The overall framework of W2PGNN can be found in Figure <ref type="figure" target="#fig_2">2</ref>. Given the input space consisting of pre-training graphs, we fit them into a graph generator in the generator space, from which the graphs generated constitute the possible downstream space. More specifically, an ideal graph generator should inherit different kinds of topological patterns, based on which new graphs can be induced. Therefore, we first construct a graphon basis B = {? 1 , ? 2 , ? ? ? , ? ? }, where each element ? ? represents a graphon fitted from a set of (sub)graphs with similar patterns (i.e., the blue dots v ). To access different combinations of generator basis, each ? ? is assigned with a corresponding weight ? ? (i.e., the width of blue arrow ) and their combination gives rise to a graph generator (i.e., the blue star</p><formula xml:id="formula_1">v v</formula><p>). All weighted combinations compose the generator space ? (i.e., the gray surface v v ), from which graphs generated form the possible solution space of downstream data (shorted as possible downstream space). The generated graphs are those that could benefit from the pre-training data, we say that they exhibit high feasibility of performing pre-training.</p><p>In the following, we introduce the workflow of W2PGNN in the input space, the generator space and the possible downstream space in detail. Then, the application cases of W2PGNN are given for different practical use.</p><p>Input space. The input space of W2PGNN is composed of nodes' ego-networks or graphs. For node-level pre-training, we take the nodes' ego-networks to constitute the input space; For graph-level pre-training, we take the graphs (e.g., small molecular graphs) as input space.</p><p>Generator space. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, each point (i.e., graph generator) in the generator space ? is a convex combination of generator basis B = {? 1 , ? 2 , ? ? ? , ? ? }. Formally, we define the graph generator as</p><formula xml:id="formula_2">? ({? ? }, {? ? }) = ? ?? ?=1 ? ? ? ? , where ? ?? ?=1 ? ? = 1, ? ? ? 0.<label>(2)</label></formula><p>Different choices of {? ? }, {? ? } comprise different graph generators. All possible generators constitute the generator space</p><formula xml:id="formula_3">? = {? ({? ? }, {? ? }) | ? {? ? }, {? ? }}.</formula><p>We shall also note that, the graph generator ? ({? ? }, {? ? }) is indeed a mixed graphon, (i.e., mixture of ? graphons</p><formula xml:id="formula_4">{? 1 , ? 2 , ? ? ? , ? ? }),</formula><p>where each element ? ? represents a graphon estimated from a set of similar pre-training (sub)graphs. Furthermore, it can be theoretically justified that the mixed version still preserve the properties of graphons (c.f. Theorem 5.1) and the key transferable patterns inherited in ? ? (c.f. Theorem 5.2). Thus the graph generator ? ({? ? }, {? ? }), i.e., mixed graphon, can be considered as a representative and comprehensive summary of pre-training data, from which unseen graphs with different combinations of transferable patterns can be induced.</p><p>Possible downstream space. All the graphs produced by the generators in the generator space ? could benefit from the pretraining, and finally form the possible downstream space.</p><p>Formally, for each generator in the generator space ? (we denote it as ? for simplicity), we can generate a ?-node graph as follows. First, we independently sample a random latent variable for each node. Then for each pair of nodes, we assign an edge between them with the probability equal to the value of the graphon at their randomly sampled points. The graph generation process can be formulated as</p><formula xml:id="formula_5">? 1 , ? ? ? , ? ? ? Uniform([0, 1]), ? ? ? ? Bernouli(? (? ? , ? ? )), ??, ? ? {1, 2, ..., ?},<label>(3)</label></formula><p>where ? (? ? , ? ? ) ? [0, 1] indicates the corresponding value of the graphon at point (? ? , ? ? )<ref type="foot" target="#foot_0">3</ref> , and ? ? ? ? {0, 1} indicates the existence of edge between ?-th node and ?-th node. The adjacency matrix of the sampled graph ? is denoted as</p><formula xml:id="formula_6">? = [? ? ? ] ? {0, 1} ??? , ??, ? ? [?].</formula><p>We summarize this generation process as ? ? .</p><p>Therefore, with all generators from the generator space ?, the possible downstream space is defined as D = {? ? |? ? ?}. Note that for each {? ? }, {? ? }, we have a generator ? ; and for each generator, we also have different generated graphs. Besides, we theoretically justify that the generated graphs in the possible downstream space can inherit key transferable graph patterns in our generator (c.f. Theorem 5.3).</p><p>Application cases. The proposed framework is flexible to be adopted in different application scenarios when discussing the problem of when to pre-train GNNs.</p><p>? Use case 1: provide a user guide of a graph pre-trained model. The possible downstream space D serves as a user guide of a graph pre-trained model, telling the application scope of model pretrained models (i.e., the possible downstream graphs that can benefit from the pre-training data).</p><p>? Use case 2: estimate the feasibility of performing pre-training from pre-training data to downstream data. Given a collection of pretraining graphs and a downstream graph, one can directly measure the feasibility of performing pre-training on pre-training data, before conducting costly pre-training and fine-tuning attempts. By making such pre-judgement of a kind of transferability, some unnecessary and expensive parameter optimization steps during model training and evaluation can be avoided.</p><p>? Use case 3: select pre-training data to benefit the downstream. In some practical scenarios where the downstream data is provided (e.g., a company's need is to boost downstream performance of its business data), the feasibility of pre-training inferred by W2PGNN can be utilized to select data for pre-training, such that the downstream performance can be maximized with limited resources.</p><p>Use case 1 can be directly given by our produced possible downstream space D. However, how to measure the feasibility of pretraining in use case 2 and 3 still remains a key challenge. In the following sections, we introduce the formal definition of the feasibility of pre-training and its approximate solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feasibility of Pre-training</head><p>If a downstream graph can be generated with a higher probability from any generator in the generator space ?, then the graph could benefit more from the pre-training data. We therefore define the feasibility of performing pre-training as the highest probability of the downstream data generated from a generator in ?, which can be formulated as an optimization problem as follows.</p><p>Definition 2 (Feasibility of graph pre-training). Given the pre-training data G train and downstream data G down , we have the feasibility of performing pre-training on G train to benefit G down as</p><formula xml:id="formula_7">? (G train G down ) = sup {? ? },{? ? } Pr (G down | ? ({? ? }, {? ? })) , (4)</formula><p>where Pr (G down | ? ({? ? }, {? ? })) denotes the probability of the graph sequence sampled from G down being generated by graph generator ? ({? ? }, {? ? }); each (sub)graph represents an ego-network (for nodelevel task) or a graph (for graph-level task) sampled from the downstream data G down .</p><p>However, the probability Pr (G down | ? ({? ? }, {? ? })) of generating the downstream graph from a generator is extremely hard to compute, we therefore turn to converting the optimization problem (4) to a tractable problem. Intuitively, if generator ? ({? ? }, {? ? }) can generate the downstream data with higher probability, it potentially means that the underlying generative patterns of pre-training data (characterized by ? ({? ? }, {? ? })) and downstream data (characterized by the graphon ? down fitted from G down ) are more similar. Accordingly, we turn to figure out the infimum of the distance between ? ({? ? }, {? ? }) and ? down as the feasibility, i.e.,</p><formula xml:id="formula_8">? (G train G down ) = -inf {? ? },{? ? } dist(? ({? ? }, {? ? }), ? down ). (5)</formula><p>Following <ref type="bibr" target="#b47">[48]</ref>, we hire the 2-order Gromov-Wasserstein (GW) distance as our distance function dist(?, ?), as GW distance is commonly used to measure the difference between structured data.</p><p>Additionally, we establish a theoretical connection between the above-mentioned distance and the probability of generating the downstream data in extreme case, which further adds to the integrity and rationality of our solution.</p><p>Theorem 4.1. Given the graph sequence sampled from downstream data G down , we estimate its corresponding graphon as ? down . If a generator ? can generate the downstream graph sequence with probability 1, then dist(? , ? down ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Choose Graphon Basis to Approximate Feasibility</head><p>Although the feasibility has been converted to the optimization problem <ref type="bibr" target="#b4">(5)</ref>, exhausting all possible {? ? }, {? ? } to find the infimum is impractical. An intuitive idea is that we can choose some appropriate graphon basis {? ? }, which can not only prune the search space but also accelerate the optimization process. Therefore, we aim to first reduce the search space of graphon basis {? ? } and then learn the optimal {? ? } in the reduced search space.</p><p>Considering that the downstream data may be formed via different generation mechanisms (implying various transferable patterns), a single graphon basis might have limited expressivity and completeness to cover all patterns. We therefore argue that a good reduced search space of graphon basis should cover a set of graphon bases. Here, we introduce three candidates of them as follows.</p><p>Integrated graphon basis. The first candidate of graphon basis is the integrated graphon basis {? ? } integr . This graphon basis is introduced based on the assumption that the pre-training and the downstream graphs share very similar patterns. For example, the pre-training and the downstream graphs might come from social networks of different time spans <ref type="bibr" target="#b15">[16]</ref>. In the situation, almost all patterns involved in the pre-training data might be useful for the downstream. To achieve this, we directly utilize all (sub)graphs sampled from the pre-training data to estimate one graphon as the graphon basis. This integrated graphon basis serves as a special case of the graphon basis introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain graphon basis.</head><p>The second candidate of graphon basis is the domain graphon basis {? ? } domain . The domain information that pre-training data comes from is important prior knowledge to indicate the transferability from the pre-training to downstream data. For example, when the downstream data is molecular network, it is more likely to benefit from the pre-training data from specific domains like biochemistry. This is because the specificity of molecules makes it difficult to learn transferable patterns from other domains, e.g., closed triangle structure represents diametrically opposite meanings (stable vs unstable) in social network and molecular network. Therefore, we propose to split the (sub)graphs sampled from pre-training data according to their domains, and each split of (sub)graphs will be used to estimate a graphon as a basis element. In this way, each basis element reflects transferable patterns from a specific domain, and all basis elements construct the domain graphon basis {? ? } domain .</p><p>Topological graphon basis. The third candidate is the topological graphon basis {? ? } topo . The topological similarity between the pre-training and the downstream data serves as a crucial indicator of transferability. For example, a downstream social network might benefit from the similar topological patterns in academic or web networks (e.g., closed triangle structure indicates stable relationship in all these networks). Then, the problem of finding topological graphon basis can be converted to partition ? (sub)graphs sampled from pre-training data into ?-split according to their topology similarity, where each split contains (sub)graphs with similar topology. Each element of graphon basis (i.e., graphon) fitted from each split of (sub)graphs is expected to characterize a specific kind of topological transferable pattern.</p><p>However, the challenge is that for graph structured data that is irregular and complex, we cannot directly measure the topological similarity between graphs. To tackle this problem, we introduce a graph feature extractor that maps arbitrary graph into a fixedlength vector representation. To approach a comprehensive and representative set of topological features, we here consider both node-level and graph-level properties.</p><p>For node-level topological features, we first apply a set of nodelevel property functions [? 1 (?), ? ? ? , ? ? 1 (?)] for each node ? in graph ? to capture the local topological features around it. Considering that the numbers of nodes of two graphs are possibly different, we introduce an aggregation function AGG to summarize the node-level property of all nodes over ? to a real number AGG({? ? (?), ? ? ? }). We can thus obtain the node-level topological vector representation as follows.</p><formula xml:id="formula_9">? node (?) = [AGG({? 1 (?), ? ? ? }), ? ? ? , AGG({? ? 1 (?), ? ? ? })].</formula><p>In practice, we calculate degree <ref type="bibr" target="#b2">[3]</ref>, clustering coefficient <ref type="bibr" target="#b17">[18]</ref> and closeness centrality <ref type="bibr" target="#b6">[7]</ref> for each node and instantiate the aggregation function AGG as the mean aggregator.</p><p>For graph-level topological features, we also employ a set of graph-level property functions for each graph ? to serve as the vector representation</p><formula xml:id="formula_10">? graph (?) = [? 1 (?), ? ? ? ,? ? 2 (?)],</formula><p>where density <ref type="bibr" target="#b42">[43]</ref>, assortativity <ref type="bibr" target="#b28">[29]</ref>, transitivity <ref type="bibr" target="#b42">[43]</ref> are adopted as graph-level properties here <ref type="foot" target="#foot_1">4</ref> .</p><p>Finally, the final representation of ? produced by the graph feature extractor is</p><formula xml:id="formula_11">? = [? local (?)||? global (?)] ? R ? 1 +? 2 ,</formula><p>where || is the concatenation function that combines both nodelevel and graph-level features. Given the topological vector representation, we leverage an efficient clustering algorithm K-Means <ref type="bibr" target="#b25">[26]</ref> to obtain k-splits of (sub)graphs and finally fit each split into a graphon as one element of topological graphon basis.</p><p>Optimization solution. Given the above-mentioned three graphon bases, the choice of graphon basis {? ? } can be specified to one of them. In this way, the pre-training feasibility (simplified as ? ) could be approximated in the reduced search space of graphon basis as</p><formula xml:id="formula_12">? ? -MIN({ inf {? ? } dist(? ({? ? }, {? ? }), ? down ), ?{? ? } ? B}),<label>(6)</label></formula><p>where B={{? ? } topo , {? ? } domain , {? ? } integr } is the reduced search space of {? ? }. Thus, the problem can be naturally splitted into three sub-problems with objective of dist(? ({? ? }, {? ? } topo ), ? down )), dist (? ({? ? }, {? ? } domain ), ? down )) and dist(? ({? ? }, {? ? } integr ), ? down )) respectively. Each sub-problem can be solved by updating the corresponding learnable parameters {? ? } with multiple gradient descent steps. Taking one step as an example, we have</p><formula xml:id="formula_13">{? ? } = {? ? } -?? {? ? } dist(? ({? ? }, {? ? }), ? down ) (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>where ? is the learning rate. Finally, we achieve three infimum distances under different {? ? } ? B respectively, the minimum value among them is the approximation of pre-training feasibility.</p><p>In practice, we adopt an efficient and differential approximation of GW distance, i.e., entropic regularization GW distance <ref type="bibr" target="#b30">[31]</ref>, as the distance function. For graphon estimation, we use the "largest gap" method as to estimate graphon ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computation Complexity</head><p>We now show that the time complexity of W2PNN is much lower than traditional solution. Suppose that we have ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL ANALYSIS</head><p>In this section, we theoretically analyze the rationality of the generator space and possible downstream space in W2PGNN. Detailed proofs of the following theorems can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Theoretical Justification of Generator Space</head><p>Our generator preserves the properties of graphons. We first theoretically prove that any generator in the generator space still preserve the properties of graphon (i.e., a bounded symmetric function [0, 1] 2 ? [0, 1], summarized in the following theorem. Our generator preserves the key transferable patterns in graphon basis. As a preliminary, we first introduce the concept of graph motifs as a useful description of transferable graph patterns and leverage homomorphism density as a measure to quantify the degree to which the patterns inherited in a graphon. Definition 3 (Graph motifs <ref type="bibr" target="#b26">[27]</ref>). Given a graph ? = (? , ?) (? and ? are node set and edge set), graph motifs are substructure ? = (? ? , ? ? ) that recur significantly in statistics, where</p><formula xml:id="formula_15">? ? ? ? , ? ? ? ? and |? ? | ? |? |.</formula><p>Graph motifs can be roughly taken as the key transferable graph patterns across graphs <ref type="bibr" target="#b57">[58]</ref>. For example, the motif ( extreme cases, directly transferring node attributes or representations in one network to those with dierent meanings in another network may have a negative impact when performing network analysis or network modeling. Thus, the main challenge to be addressed in this paper is to discover the transferable information across dierent networks used for graph pre-training.</p><p>Intuitively, we here propose to transfer structure patterns across dierent networks. A number of existing works focus on learning graph structures; for example, struc2vec <ref type="bibr" target="#b35">[36]</ref> learns latent embeddings for nodes through node symmetry, while graph2vec <ref type="bibr" target="#b29">[30]</ref> focuses on graph structural representation. Nevertheless, a common drawback of these approaches lies in the inability of transferring across networks from dierent domains. Hu et al. <ref type="bibr" target="#b17">[18]</ref> utilizes heuristic features (e.g., closeness and betweenness centrality) to capture graph structure; however the time complexity of calculating these features is too high for large-scale graphs. Qiu et al. <ref type="bibr" target="#b33">[34]</ref> adopts contrastive learning method and proposes GCC to capture structure information among dierent graphs. However, it lacks interpretability regarding what kind of structural patterns are indeed transferred across networks in dierent domains. Considering their limitations, we propose to utilize motifs, sub-structures with rich structural information <ref type="bibr" target="#b28">[29]</ref>, as the transferable patterns used for graph pre-training. Motifs are of great importance for a wide range of applications in many elds, ranging from social networks to biology. (a) In biology, network motifs were dened in Escherichia coli <ref type="bibr" target="#b41">[42]</ref> for the rst time, and Alon et al. <ref type="bibr" target="#b2">[3]</ref> mentioned the same motifs were also found from bacteria <ref type="bibr" target="#b10">[11]</ref> to yeast <ref type="bibr" target="#b28">[29]</ref>, animal <ref type="bibr" target="#b31">[32]</ref> to plants. For example, motif ( ) is often used as "Feedforward Loop" and found in gene systems and organisms <ref type="bibr" target="#b51">[52]</ref>. (b) In social networks, triangles ( ) are more likely to appear where individuals tend to introduce his or her friends to know each other, which is known as triadic closure <ref type="bibr" target="#b57">[58,</ref><ref type="bibr">63]</ref>; similar cases also happen in academic networks, where two researchers tend to collaborate if they share one common collaborator <ref type="bibr" target="#b57">[58]</ref>. (c) In telecommunication networks, it is likely that more motifs with unclosed structural patterns around fraudulent users than normal users, such as 2-star motif ( ), as fraudulent users attend to call numerous people to commit fraud and these people are probably never contacted each other <ref type="bibr" target="#b56">[57]</ref>. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, we calculate the number of 2-star motif ( ) and 4-path motif ( ) around fraudulent users and normal users respectively for Mobile dataset, which is provided by China Telecom and our ndings meet above analysis. One kind motif contains the same structure information in dierent ne As shown in Figure <ref type="figure" target="#fig_1">1</ref>, while the semantic meanings of nod unable to be preserved across dierent networks, motifs ex similar pattern across dierent networks. Regarding network various domains, motifs with a dense structure (e.g. ( ) and indicate a closer relationship among nodes; for example, B his two closest friends in the social network, or Bob and a two scholars in the academic network, who have a stable co tive relationship and jointly publish papers. Moreover, moti a sparse structure (such as ( ) and ( )) suggest the op To the best of our knowledge, we are the rst to perform pre-training via motifs.</p><p>Based on our ndings, we propose a Motif-based Pre-tr framework (abbreviated as MPT), which can capture the un structural patterns across dierent graph datasets. More spec we formulate our graph pre-training as a self-supervised ta maintain generalizability across dierent networks, we rst subgraphs of each target node to form the input of our mod adopt a graph neural network-based model to capture str information from global and local perspective. In so doin can ensure that the representation of similar structure pa are close to each other. Numerical experiments of transferri pre-trained model to various downstream datasets valida eectiveness of our model in capturing structure patterns.</p><p>The main contributions of our paper can be summari follows:</p><p>? We propose that the same kind motif contains universal str information across networks from dierent domains.</p><p>? We formulate the graph pre-training problem and pro Motif-based Pre-Training framework (MPT) and correspo self-supervised task to capture the universal and trans structure patterns of graphs.</p><p>? From the motif perspective, we provide a detailed anal the relationship between the performance of the pre-t model on downstream tasks and the selection of datas pre-training, as illustrated in ? 4.3.</p><p>? To evaluate the eectiveness and eciency of our pro framework, we conduct extensive experiments on four real datasets and ve public datasets. Our experimental resul cate that MPT eciently achieves superior or at least comp performance to state-of-the-art baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this paper, we use lower-case letters to indicate scalars an letters to denote vectors or matrices. We use a subscript to that a vector is for a corresponding node (e.g. c u ). When in vectors, the superscript 8 denotes the 8-th dimension (e.g.</p><p>2</p><p>) has the same meaning of "feedforward loop" across networks of control system, gene systems or organisms.</p><p>Then, we introduce the measure of homomorphism density ? (?, ?) to quantify the relative frequency of the key transferable pattern, i.e., graph motifs ? , inherited in graphon ?.</p><p>Definition 4 (Homomorphism density <ref type="bibr" target="#b22">[23]</ref>). Consider a graph motif ? = (? ? , ? ? ), we define a homomorphisms of ? into graph ? = (? , ?) as an adjacency-preserving map from ? ? to ? , where (?, ?) ? ? ? implies (?, ?) ? ?. There could be multiple maps from ? ? to ? , but only some of them are homomorphisms. Therefore, the definition of homomorphism density ? (?, ?) is introduced to quantify the relative frequency with which the graph motif ? appears in ?.</p><p>Analogously, the homomorphism density of graphs can be extended into the graphon ?. We denote ? (?, ?) as the homomorphism density of graph motif ? into graphon ?, which represents the relative frequency of ? occurring in a collection of graphs {? ? } that convergent to graphon ?, i.e., ? (?, ?) = lim ??? ? (?, {? ? }). Now, we are ready to quantify how much the transferable patterns in graphon basis can be preserved in our generator by exploring the difference between the homomorphism density of graph motifs into the graphon basis and that into our generator. Theorem 5.2 indicates the graph motifs (i.e., key transferable patterns) inherited in each basis element can be preserved in our generator, which justifies the rationality to take the generator as a representative and comprehensive summary of pre-training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Theoretical Justification of Possible Downstream Space</head><p>The possible downstream space includes the graphs generated from generator ? ({? ? }, {? ? }). We here provide a theoretical justification that the generated graphs in possible downstream space can inherit key transferable graph patterns (i.e., graph motifs) in the generator. Theorem 5.3 indicates that the homomorphism density of graph motifs into the generated graphs in the possible downstream space can be inherited from our generator to a significant degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we evaluate the effectiveness of W2PGNN with the goal of answering the following questions: (1) Given the pretraining and downstream data, is the feasibility of pre-training estimated by W2PGNN positively correlated with the downstream performance (Use case 2)? (2) When the downstream data is provided, does the pre-training data selected by W2PGNN actually help improve the downstream performance (Use case 3)?</p><p>Note that it is impractical to empirically evaluate the application scope of graph pre-trained models (Use case 1), as we cannot enumerate all graphs in the possible downstream space. Whereas, by answering question (1), it can be indirectly verified that a part of graphs in the possible downstream space, i.e., the downstream graphs with high feasibility, indeed benefit from the pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We validate our proposed framework on both node classification and graph classification task.</p><p>Datasets. For node classification task, we directly adopt six datasets from <ref type="bibr" target="#b31">[32]</ref> as the candidates of pre-training data, which consists of Academia, DBLP(SNAP), DBLP(NetRep), IMDB, Facebook and LiveJournal (from academic, movie and social domains). Regarding the downstream datasets, we adopt US-Airport and H-Index from <ref type="bibr" target="#b31">[32]</ref> and additionally add two more datasets Chameleon and Europe-Airport for a more comprehensive results.</p><p>For graph classification task, we choose the large-scale datasets ZINC15 <ref type="bibr" target="#b36">[37]</ref>  the follow-up experimental analysis, we use scaffold split to partition the ZINC15 into five datasets (ZINC15-0,ZINC15-1,ZINC15-2,ZINC15-3 and ZINC15-4) according to their scaffolds <ref type="bibr" target="#b14">[15]</ref>, such that the scaffolds are different in each dataset. Regarding the downstream datasets, we use 5 classification benchmark datasets contained in MoleculeNet <ref type="bibr" target="#b44">[45]</ref>. For downstream datasets, we use BACE, BBBP, MUV, HIV and ClinTox provided in <ref type="bibr" target="#b44">[45]</ref>.</p><p>The dataset details are summarized in Appendix B.</p><p>Baseline of graph pre-training measures. The baselines can be divided into 3 categories: (1) EGI <ref type="bibr" target="#b59">[60]</ref> computes the difference between the graph Laplacian of (sub)graphs from pre-training data and that from downstream data; (2) Graph Statistics,by which we merge average degree, degree variance, density, degree assortativity coefficient, transitivity and average clustering coefficient to construct a topological vector for each (sub)graph. (3) Clustering Coefficient, Spectrum of Graph Laplacian, and Betweenness Centrality, by which we adopt the distributions of graph properties as topological vectors. For the second and third category of baselines, we calculate the negative value of Maximum Mean Discrepancy (MMD) distance between the obtained topological vectors of the (sub)graph from pre-training data and that from downstream data.</p><p>Note that in all baselines, the distance/difference is computed between one ego-network (for node classification) or graph (for graph classification) from pre-training data and another one from downstream data. For efficiency, when conducting node classification, we randomly sample 10% nodes and extract their 2-hop ego-networks for each candidate pre-training dataset, and extract 2-hop ego-networks of all nodes for each downstream dataset. For graph classification, we randomly select 10% graphs for each candidate pre-training dataset and downstream dataset. Then we take the average of all distances/differences as the measure.</p><p>Implementation Details. For node classification tasks, we randomly sample 1000 nodes for each pre-training dataset and extract 2-hops ego-networks of sampled nodes to compose our input space, and extract 2-hops ego-networks of all nodes in each downstream dataset to estimate the graphon. For graph classification tasks, we take all graphs in each pre-training dataset to compose our input space and we use all graphs in each downstream dataset to estimate its corresponding graphon. When constructing topological graphon basis, we set the the number of clusters ? = 5. The maximum iterations number of K-Means is set as 300. When constructing domain graphon basis, we take each pre-training dataset as a domain. For graphon estimation, we use the largest gap <ref type="bibr" target="#b3">[4]</ref> approach and let the block size of graphon as the average number of nodes in all graphs. When learning ? ? , we adopt Adam as the optimizer and set the learning rate ? as 0.05. When calculating the GW distance, we utilize its differential and efficient version entropic regularization GW distance with default hyperparameters <ref type="bibr" target="#b30">[31]</ref>.</p><formula xml:id="formula_16">)HDVLELOLW\ %HVW3HUIRUPDQFH 86$LUSRUW_&amp;RUU )HDVLELOLW\ %HVW3HUIRUPDQFH (XURSH$LUSRUW_&amp;RUU )HDVLELOLW\ %HVW3HUIRUPDQFH +,QGH[_&amp;RUU )HDVLELOLW\ %HVW3HUIRUPDQFH &amp;KDPHOHRQ_&amp;RUU 3UH7UDLQLQJ'DWD /LYH-RXUQDO)DFHERRN /LYH-RXUQDO'%/31HW5HS /LYH-RXUQDO'%/361$3 /LYH-RXUQDO$FDGHPLD )DFHERRN'%/31HW5HS</formula><p>)DFHERRN'%/361$3 )DFHERRN$FDGHPLD '%/31HW5HS'%/361$3 '%/31HW5HS$FDGHPLD '%/361$3$FDGHPLD ,0'%/LYH-RXUQDO ,0'%)DFHERRN ,0'%'%/31HW5HS ,0'%'%/361$3 ,0'%$FDGHPLD  Table <ref type="table">3</ref>: Node classification results when performing pre-training on different selected pre-training data. We also provide the results of using all pre-training data without selection for your reference (see "All Datasets" in the table).</p><formula xml:id="formula_17">? = 2 ? = 3 US-Airport Europe-Airport H-index Chameleon Rank US-Airport Europe-Airport H-index</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results of Pre-training Feasibility</head><p>Setup. When evaluating the pre-training feasibility, since its ground truth is unavailable, we adopt the best downstream performance among a set of graph pre-training models as the ground truth.</p><p>For node classification tasks, we use the following 4 graph pretraining models: GraphCL <ref type="bibr" target="#b51">[52]</ref> and GCC models <ref type="bibr" target="#b31">[32]</ref> with three different hyper-parameter (i.e., 128, 256 and 512 rw-hops). For graph classification tasks, we adopt 7 SOTA pre-training models: At-trMasking <ref type="bibr" target="#b14">[15]</ref>, ContextPred <ref type="bibr" target="#b14">[15]</ref>, EdgePred <ref type="bibr" target="#b14">[15]</ref>, Infomax <ref type="bibr" target="#b14">[15]</ref>, GraphCL <ref type="bibr" target="#b51">[52]</ref>, GraphMAE <ref type="bibr" target="#b13">[14]</ref> and JOAO <ref type="bibr" target="#b50">[51]</ref>. When pre-training, we directly use the default hyper-parameters of pre-training models except the rw-hops in GCC. During fine-tuning, we freeze the parameters of pre-trained models and utilize the logistic regression as classifier for node classification and SVM as classifier for graph classification, following <ref type="bibr" target="#b31">[32]</ref> and its fine-tuning hyper-parameters.</p><p>The downstream results are reported as the average of Micro F1 and ROC-AUC under 10 runs on node classification and graph classification respectively. For each downstream task, the best performance among all methods is regarded as the ground truth. For a comprehensive evaluation on the correlation between the estimated pre-training feasibility and the above ground truth (i.e., best downstream performance), we need to construct multiple ?G train , G down ? sample pairs as our evaluation samples. When constructing the ?G train , G down ? sample pairs for each downstream data, multiple pre-training data are required to be paired with it. Hence we adopt the following two settings to augment the choice of pre-training data for more possibilities. Here we use ? as the number of dataset candidates contained in pre-training data. (1) For ? = 2 setting, we randomly select 2 pre-training dataset candidates as pre-training data and enumerate all possible cases. (2) For ? = 3 setting, we randomly select 3 pre-training dataset candidates as pre-training data. We enumerate all possible cases for graph classification tasks and randomly select 40% of all cases for node classification tasks for efficiency.</p><p>Results. Table <ref type="table" target="#tab_3">1</ref> (for node classification) and Table <ref type="table" target="#tab_4">2</ref> (for graph classification) show the Pearson correlation coefficient between the best downstream performance and the estimated pre-training feasibility by W2PGNN and baselines for each downstream dataset. A higher coefficient indicates a better estimation of pre-training feasibility. We also include 4 variants of W2PGNN: W2PGNN (intergr), W2PGNN (domain) and W2PGNN (topo) only utilize the integrated graphon basis, domain graphon basis and topological graphon basis to approximate feasibility respectively, and W2PGNN (? = 1) directly set the learnable combination weights {? ? } as fixed constant 1. We have the following observations. (1) The results show that our model achieve the highest overall ranking in most cases, indicating the superiority of our proposed framework. <ref type="bibr" target="#b1">(2)</ref> We find that the measures provided by other baselines sometimes show no correlation or negative correlation with the best downstream performance. (3) Comparing W2PGNN and its 4 variants, we find that although the variants sometimes achieve superior performance on some downstream datasets, they cannot consistently perform well on all datastes. In contrast, the top-ranked W2PGNN can provide a more comprehensive picture with various graph bases and learnable combination weights.</p><p>To provide a deeper understanding of the feasibility estimated by W2PGNN, Figure <ref type="figure" target="#fig_7">3</ref> shows our estimated pre-training feasibility (in x-axis) versus the best downstream performance on node classification (in y-axis) of all &lt;pre-training data, downstream data&gt; pairs (one point represents the result of one pair) when the selection budget is 2. The plots when the selection budget is 3 and the plots under graph classification can be found in Appendix C.1. We find that there exist a strong positive correlation between estimated pre-training feasibility and the best downstream performance on all downstream datasets, which also suggests the significance of our feasibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results of Pre-Training Data Selection</head><p>Given the downstream data, a collection of pre-training dataset candidates and a selection budget (i.e., the number of datasets selected for pre-training) due to limited resources, we aim to select the pre-training data with the highest feasibility, so as to benefit the downstream performance.</p><p>Setup. We here adopt two settings, i.e., selection budget is set as 2 and 3 respectively. The datasets that are augmented for more pre-training data choices in Section 6.2 can be directly used as the candidates of pre-training datasets here. Then, the selected pretraining data serves as the input of graph pre-training model. For node classification tasks, we adopt GCC as the pre-training model as an example, because it is the pre-training model that can be generalized across domains and most of the datasets used for node classification are taken from it <ref type="bibr" target="#b31">[32]</ref>. For graph classification tasks, we take GraphCL as the pre-training model as it provides multiple graph augmentation approaches and is more general <ref type="bibr" target="#b51">[52]</ref>.</p><p>Results. Table <ref type="table">3</ref> shows the results of pre-training data selection on node classification task. (The results on graph classification is included in Appendix C.2). We have the following observations. (1) We can see that the pre-training data selected by W2PGNN ranks first, which is the most suitable one for downstream. ( <ref type="formula" target="#formula_2">2</ref>) We find that sometimes simple graph property like clustering coefficient serves as a good choice on a specific dataset (i.e., H-index), when the budget of pre-training data is 2. It is because that H-index exhibits the largest clustering coefficient compared to other downstream datasets (see Table <ref type="table" target="#tab_7">4</ref>), which facilitates the data selection via clustering coefficient. However, such simple graph property is only applicable when the downstream dataset shows a strong indicator of the property, and is not helpful when you need to select more datasets for pre-training (see results under ? =3). (3) Moreover, it is also interesting to see that using all pre-training data for pre-training is not always a reliable choice. We find that carefully selecting pre-training data can not only benefit downstream performance but also reduce computation resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper proposes a W2PGNN framework to answer the question of when to pre-train GNNs based on the generative mechanisms from pre-training to downstream data. W2PGNN designs a graphonbased graph generator to summarize the knowledge in pre-training data, and the generator can in turn produce the solution space of downstream data that can benefit from the pre-training. W2PGNN is theoretically and empirically shown to have great potential to provide the application scope of graph pre-training models, estimate the feasibility of pre-training and help select pre-training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS A.1 Proof of Theorem 2.1</head><p>Proof: We concentrate on the center node's embedding obtained from a ?-layer GCN with 1-hop polynomial filter ?(?) = ?? -? whch is the common used GNN model. We denote the embedding of node ? ? ?? = 1, ? ? ? , ? from a node-wise view in the final layer of the GCN as</p><formula xml:id="formula_18">? (?) ? = ? ?? ? ?N (? ? ) ? ? ? ? (?-1) ? ? ? (?) ? R ? ,</formula><p>where ? ? ? = [?(?)] ? ? ? R the weighted link between node ? and ?; and ? (?) ? R ??? is the weight for the ? th layer sharing across nodes.</p><p>Then ? = ? (?) ? ?=1</p><p>. We may denote ? Denote ? ? ? as the out-degree normalised graph Laplacian of ? ? , which is defined with respect to the direction from leaves to centre node in ? ? . We write the ? th layer embedding in following form</p><formula xml:id="formula_19">? (?) ? ?-?+1 = ? ? ? ? ? ?-?+1 ? (?-1) ? ?-?+1 ? (?) .<label>(10)</label></formula><p>Assume that ??, max ? ?</p><formula xml:id="formula_20">(?) ? 2</formula><p>? ? ? , and max ? ? (?) 2 ? ? ? . Suppose that the activation function ? is ? ? -Lipschitz function Then, for</p><formula xml:id="formula_21">? = 1, ? ? ? , ? -1, we have ? (?) ? ?-? -? (?) ? ? ?-? 2 ? ? [? ? ? ? ? ?-?+1 ? (?-1) ? ?-?+1 ? (?) -? ? ? ? ? ? ?-?+1 ? (?-1) ? ? ?-?+1 ? (?) ] ?-? )? 2 ? ? ? ? ? ? ? ? ? 2 ? (?-1) ? ?-?+1 -? (?-1) ? ? ?-?+1 2 + ? ? ? ? ? ? ? ? ? ? -? ? ? ? ? 2 .<label>(11)</label></formula><p>Since ? ? ? ? ?-?+1 is the principle submatrix of ? ? ? ? . We equivalently write the above equation as ? ? ? ?? ?-1 + ?, where ? and ? is the coefficient. And we have</p><formula xml:id="formula_22">? ? ? ?? ?-1 + ? ? ? 2 ? ?-2 + ? (1 + ?) ? ? ? ? ? ? ? 1 + ? ? + 1 ? -1 ?.<label>(12)</label></formula><p>Therefore, for any ? = 1, ? ? ? , ?, we have an upper bound for the hidden representation difference between ? ? and ? ? ? by substitute coefficient ? and ?, ?</p><formula xml:id="formula_23">(?) ? ?-? -? (?) ? ? ?-? 2 ? (? ? ? ? ) ? ? ? ? ? ? 2 ? [? ? ] -[? ? ? ] ? 2 + (? ? ? ? ) ? ? ? ? ? ? 2 + 1 ? ? ? ? ? ? ? ? 2 -1 ? ? ? ? ? ? ? ? ? ? -? ? ? ? ? 2 .<label>(13)</label></formula><p>Specifically, for ? = ?, we obtain the upper bound for center node embedding ?</p><formula xml:id="formula_24">(?) ? 0 -? (?) ? ? 0 ? ?? ? -? ? ? ?.</formula><p>Since the attribute of each node as a scalar 1, we therefore have ? [? ? ] -[? ? ? ] ? 2 = 0. Suppose that ??, ? ? ? ? 2 ? ? ? because that the the graph Laplacians are normalised. Since ? is a linear function for ?, We have</p><formula xml:id="formula_25">?? ? -? ? ? ? 2 ? (? ? ? ? ? ? ) ? + 1 ? ? ? ? ? ? -1 ? ? ? ? ? ? ? ? -? ? ? ? ? 2 ? ? ? ? ? -? ? ? ? 2 ,<label>(14)</label></formula><p>where ? =</p><formula xml:id="formula_26">(? ? ? ? ? ? ) ? +1 ? ? ? ? ? ? -1 ? ? ? ? .</formula><p>When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective! Conference acronym , XXXX, XX Therefore, we sequentially compute the representation divergence between node ? and node ? ? in ? train and ? down respectively. By means of Eq.( <ref type="formula" target="#formula_25">14</ref>), we have</p><formula xml:id="formula_27">?? (? train ) -? (? down )? 2 = 1 ?? ? ?? ?=1 ? ?? ?=1 ? ? -? ? ? 2 ? ? ?? ? ?? ?=1 ? ?? ?=1 ? ? ? -? ? ? ? 2 .<label>(15)</label></formula><p>A.2 Proof of Theorem 4.1</p><p>We first show the following lemma, which would be used in the proof of Theorem 4.1, Lemma 1. (Proposition 11.32 in <ref type="bibr" target="#b22">[23]</ref>) For every graphon ? , generating a ? -random graph G(?,? ) for ? = 1, 2, . . . we get a graph sequence such that G(?,? ) ? ? with probability 1.</p><p>Proof of Theorem 4.1 : Since we assume G down can be generated from ? ({? ? }, {? ? }) with probability 1, according to Lemma 1, we can have that G down ? ? ({? ? }, {? ? }) with probability 1. On the other hand, since ? down is the graphon fitted by G down , which means G down is convergent to graphon ? down and we have G down ? ? down . Hence ? down is equivalent to ? , then dist(? , ? down ) =0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 5.1</head><p>Proof: Given an arbitrary set of graphons ? = {? ? } and ? = 1...? the general form of the convex combination of graphons in ? can be represented as:</p><formula xml:id="formula_28">? ({? ? }, {? ? }) = ? ?? ?=1 ? ? ? ? , ( ? ?? ?=1 ? ? = 1, ? ? ? 0).<label>(16)</label></formula><p>Next we prove that our generator preserves the properties of graphons. the convex combination of graphons is still a bounded symmetric function  </p><formula xml:id="formula_29">[0, 1] 2 ? [0, 1]. First,</formula><formula xml:id="formula_30">? ({? ? }, {? ? }) ? = (? 1 ? 1 + ... + ? ? ? ? ) ? = ? 1 ? ? 1 + ... + ? ? ? ? ? = ? 1 ? 1 + ... + ? ? ? ? = ? ({? ? }, {? ? }).<label>(17)</label></formula><formula xml:id="formula_31">? ({? ? }, {? ? }) = (? 1 ? 1 + ... + ? ? ? ? ) ? ? ?? ?=1 ? ? ? ??? ? ? ??? ? 0.<label>(18)</label></formula><p>Hence we have proved that the convex combination of graphons preserve the properties of graphon and is still a bounded symmetric function</p><formula xml:id="formula_33">[0, 1] 2 ? [0, 1]</formula><p>. Thus, for a set of graphon basis ? , the graphon space ? is the set of all convex combinations of basis elements in ? , hence ? is the convex hull of ? .</p><p>A.4 Proof of Theorem 5.2</p><p>We first show the following two lemmas, which would be used in the proof of Theorem 5.2. The first lemma is known as counting lemma for graphons, provided in Lemma 10.23 in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Lemma 2. Let ? be a graph motif and let ?, ? ? be two graphon . Then we have</p><formula xml:id="formula_34">? (?, ?) -? ?, ? ? ? |? | ? -? ? ? .<label>(20)</label></formula><p>Proof of Lemma 2:</p><formula xml:id="formula_35">? (?, ?) -? ?, ? ? = ? ? ? ? ? ?? ? (? ? , ? ? ) - ? ? ? ? ?? ? ? (? ? , ? ? ) ? ?? ?? ? |? | ?? ?=1 ? ?-1 ?=1 ? ? ? ? , ? ? ? (? ? , ? ? ) -? ? (? ? , ? ? ) |? | ?=?+1 ? (? ? , ? ? ) ? ?? ?? .<label>(21)</label></formula><p>Here, each absolute value term in the sum is bounded by the cut norm ?? -? ? ? ? . When we fix all other irrelavant variables (everything except ? ? and ? ? for the ?-th term), altogether implying that</p><formula xml:id="formula_36">? (?, ?) -? ?, ? ? ? |? | ? -? ? ? .<label>(22)</label></formula><p>Lemma 3. The cut norm of a graphon ??? ? is defined as</p><formula xml:id="formula_37">??? ? = sup ?,? ? [0,1] ? ??? ? ,<label>(23)</label></formula><p>where the supremum is taken over all measurable subsets ? and ? . Obviously, suppose ? ? R, we have</p><formula xml:id="formula_38">???? ? = sup ?,? ? [0,1] ? ??? ?? = sup ?,? ? [0,1] ? ? ??? ? = ? ??? ? .<label>(24)</label></formula><p>Proof of Theorem 5.2: Based on the Lemma 2 and Lemma 3, we have the following derivations. The ?-th element ? ? in graphon basis has its corresponding motif set. Each motif ? ? is expected to be preserved and to exhibit similar frequency (i.e., homomorphism density) in</p><formula xml:id="formula_39">? ({? ? }, {? ? }).</formula><p>Applying Lemma 2, we have the following derivations:</p><formula xml:id="formula_40">|? (? ? , ? ({? ? }, {? ? })) -? (? ? , ? ? )| ? |? ? |||? ({? ? }, {? ? }) -? ? || ? (? = 1...?) |? (? ? , ? ({? ? }, {? ? })) -? (? ? , ? ? )| ? |? ? ||| ? ?? ?=1 ? ? ? ? - ? ?? ?=1 ? ? ? ? || ? |? (? ? , ? ({? ? }, {? ? })) -? (? ? , ? ? )| ? |? ? ||| ? ?? ?=1 ? ? (? ? -? ? )|| ? .<label>(25)</label></formula><p>Combining with the triangle inequality, we have: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASETS</head><p>Datasets for node classification. For pre-training datasets, following <ref type="bibr" target="#b31">[32]</ref>, we adopt data from academic, movie and social domains. In detail, we collect the graph data from Academia (NetRep) <ref type="bibr" target="#b33">[34]</ref>, DBLP (SNAP) <ref type="bibr" target="#b49">[50]</ref> and DBLP (NetRep) <ref type="bibr" target="#b33">[34]</ref> as the academic domain data.</p><p>For movie domain, we utilize the graph from IMDB <ref type="bibr" target="#b33">[34]</ref>. As for social domain, the graph data from Facebook <ref type="bibr" target="#b33">[34]</ref> and LiveJournal <ref type="bibr" target="#b1">[2]</ref> is leveraged. For the downstream dataset, we use data from transportation, academic and web domains. Specifically, we collect the datasets from H-Index <ref type="bibr" target="#b53">[54]</ref> and Chamelon <ref type="bibr" target="#b34">[35]</ref> for academic and web domains, respectively. The datasets in transportation domain are collected from US-Airport and Europe-Airport <ref type="bibr" target="#b32">[33]</ref>. Detailed statistics of pre-training and downstream datasets for node classification is summarized in Table <ref type="table" target="#tab_7">4</ref>.</p><p>Datasets for graph classification. For pre-training datasets, to enrich the follow-up experimental analysis, we use scaffold split to partition the ZINC15 into five datasets (ZINC15-0,ZINC15-1,ZINC15-2,ZINC15-3 and ZINC15-4) according to their scaffolds <ref type="bibr" target="#b14">[15]</ref>, such that the scaffolds are different in each dataset. For downstream datasets, we use BACE, BBBP, MUV, HIV, and ClinTox provided in <ref type="bibr" target="#b44">[45]</ref>. BACE provides quantitative (?? 50 ) and qualitative (binary label) binding results for a set of inhibitors of human ?-secretase 1, which merged a collection of 1522 compounds with their 2D structures and binary labels in MoleculeNet. BBBP includes binary labels for over 2000 compounds on their permeability properties. MUV contains 17 tasks for around 90 thousand compounds, designed for validation of virtual screening techniques.</p><p>HIV was introduced to test the ability to inhibit HIV replication for over 40,000 compounds. ClinTox includes two classification tasks for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS C.1 Additional Results of Pre-training Feasibility</head><p>We here give additional plots to show the detailed correlation between estimated pre-training feasibility and the best downstream performance.</p><p>Figure <ref type="figure">4</ref> shows the results on node classification when the selection budget is 3. Figure <ref type="figure">5</ref> shows the results on graph classification when the selection budget is 2 and 3 respectively. We find that in all cases, there exist a strong positive correlation between pre-training feasibility and the best downstream performance.</p><p>)HDVLELOLW\ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>methods make costly "pre-train and fine-tune" attempts. tells the feasibility of pre-training before "pre-train and fine-tune".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of existing methods and proposed W2PGNN to answer when to pre-train GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our proposed framework W2PGNN to answer when to pre-train GNNs.</figDesc><graphic url="image-21.png" coords="4,115.20,116.66,118.89,52.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 5 . 1 .</head><label>51</label><figDesc>For a set of graphon basis {? ? }, the corresponding generator space ? = {? ({? ? }, {? ? }) | ? {? ? }, {? ? }} is the convex hull of {? ? }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of motifs with unclosed structure around fraudulent and normal users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 5 . 2 .</head><label>52</label><figDesc>Assume a graphon basis {? 1 , ? ? ? , ? ? } and their convex combination ? ({? ? }, {? ? }) = ? ?=1 ? ? ? ? . The ?-th element of graphon basis ? ? corresponds to a motif set. For each motif ? ? in the motif set, the difference between the homomorphism density of ? ? in ? ({? ? }, {? ? }) and that in basis element ? ? is upper bounded by |? (? ? , ? ({? ? }, {? ? })) -? (? ? , ? ? )| ? ? ?? ?=1,??? |? ? |? ? ||? ? -? ? || ? (8) where |? ? | represents the number of nodes in motif ? ? , and || ? || ? denotes the cut norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 5 . 3 .</head><label>53</label><figDesc>Given a graph generator ? ({? ? }, {? ? }), we can obtain sufficient number of random graphs G = G(?, ? ({? ? }, {? ? })) with ? nodes generated from ? ({? ? }, {? ? }). The homomorphism density of graph motif ? in G can be considered approximately equal to that in ? ({? ? }, {? ? }) with high probability, which can be represented as P(|? (?, G) -? (?, ? ({? ? }, {? ? }))| &gt; ?) ? 2 exp -? 2 ? 8v(? ) 2 , (9) where v(? ) denotes the number of nodes in ? , and 0 ? ? ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pre-training feasibility vs. the best downstream performance on node classification when the selection buget is 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>? ? R ? similarly for ? = 1, ? ? ? , ? -1, and ? 0 ? = ? ? ? R ? as the node feature of center node ? ? . With the assumption of GCN in the statement, we consider that only the k-hop ego-graph ? ? centered at ? ? is needed to compute ? (?) ? for any ? = 1, ? ? ? , ? instead of the whole of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Then we prove that the convex combination ? ({? ? }, {? ? }) is still in [0, 1] Let ? ??? indicates the maximum ? ? , meanwhile ? ??? indicates the maximum ? ? ? ({? ? }, {? ? }) = (? 1 ? 1 + ... + ? ? ? ? ) ? ? ?? ?=1 ? ? ? ??? = ? ??? ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 Lemma 4 . 27 )</head><label>3427</label><figDesc>|? (? ? , ? ({? ? }, {? ? })) -? (? ? , ? ? )| ? ? ?? ?=1,??? |? ? |? ? ||? ? -? ? || ? .(26)where |? | represents the number of nodes in motif ? , and || ? || ? is the cut norm.A.5 Proof of Theorem 5.(Corollary 10.4 in<ref type="bibr" target="#b22">[23]</ref>). Let ? be a graphon, ? ? 1, 0 &lt; ? &lt; 1, and let ? be a simple graph, then the ? -random graph G = G(?,? ) satisfiesP(|? (?, G) -? (?,? )| &gt; ?) ? 2 exp -? 2 ? 8v(? ) 2 . (Proof ofTheorem 5.3: Apply Lemma 4 in our setting, let graphon ? as ? Lemma 4, we have P(|? (?, G) -? (?, ?)| &gt; ?) ? 2 exp -? 2 ? 8v(? ) 2 . (28)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 Figure 4 :</head><label>34</label><figDesc>Figure 4: Pre-training feasibility vs. the best downstream performance on node classification when the selection buget is 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>=1 ?? ? ? -? ? ? ? ? 2 measures the topological divergence between ? train and ? down , where ? ? is the ?-hop ego-network of node ? from ? train and ? ? ? is its corresponding normalized graph Laplacian matrix, ? and ? are the number of nodes of ? train and ? down . ? (? train ) and ? (? down ) are the output representations of ? train and ? down from graph pre-training model, ? is a constant relevant to ?, graph filter ?, learnable parameters of GNN and the activation function used in GNN.</figDesc><table><row><cell>??</cell><cell>? ?=1</cell><cell>? ? ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 and ? 2 (sub)graphs sampled from pre-training data and downstream data respectively, and denote |? | and |?| as the average number of nodes and edges per (sub)graph. The overall time complexity of W2PGNN is ? ((? 1 + ? 2 )|? | 2 ). For comparison, traditional solution in Figure 1(a) to estimate the pre-training feasibility should make ? 1 ? ? 2 "pre-train and fine-tune" attempts, if there exist ? 1 pre-training models and ? 2 fine-tuning strategies. Suppose the batch size of pre-training as ? and the representation dimension as ?. The overall time complexity of traditional solution is ? ? 1 ?</figDesc><table /><note><p><p>2 ((? 1 + ? 2 )(|? | 3 + |?|?) + ? 1 ??) .</p>Detailed analysis can be found in Appendix D.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>containing 2 million unlabeled molecules. To enrich Pearson correlation coefficient between the estimated pre-training feasibility and the best downstream performance on node classification. ? denotes the number of candidate pre-training datasets that form the pre-training data. Bold indicates the highest coefficient. "Rank" represents the overall ranking on all downstream datasets.</figDesc><table><row><cell>Conference acronym , XXXX, XX</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Pearson correlation coefficient between the feasibility and the best downstream performance on graph classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The statistics of pre-training and downstream datasets (avg deg: average degree; deg var: degree variance; coef: coefficient) for node classification. 1491 drug compounds with known chemical structures. Detailed statistics of pre-training and downstream datasets for graph classification is summarized in</figDesc><table><row><cell></cell><cell>Type academic</cell><cell>Name Academia</cell><cell cols="2">|? | 137,969 739,384 |?|</cell><cell cols="5">avg deg deg var avg clustering coef density assortativity coef transitivity 5.36 10.11 1.42e-01 3.88e-05 8.39e-03 7.65e-02</cell></row><row><cell>pre-training</cell><cell>social</cell><cell cols="5">DBLP(SNAP) 317,080 2,099,732 6.62 DBLP(NetRep) 540,686 30,491,458 56.41 66.24 10.01 Facebook 3,097,165 47,334,788 15.28 45.17 LiveJournal 4,843,953 85,691,368 17.69 52.02</cell><cell>6.32e-01 8.02e-01 9.70e-02 2.74e-01</cell><cell>2.09e-05 1.04e-04 4.93e-06 3.65e-06</cell><cell>2.67e-01 5.10e-01 -5.57e-02 2.10e-02</cell><cell>3.06e-01 6.56e-01 4.77e-02 1.18e-01</cell></row><row><cell></cell><cell>movie</cell><cell>IMDB</cell><cell cols="3">896,305 3,782,447 8.44</cell><cell>17.27</cell><cell>5.79e-05</cell><cell>9.42e-06</cell><cell>-5.30e-02</cell><cell>8.08e-05</cell></row><row><cell>downstream</cell><cell cols="2">academic web transportation US-Airport H-index Chameleon Europe-Airport</cell><cell>5000 2277 1,190 399</cell><cell>44,020 36,101 13,599 5,995</cell><cell cols="2">17.61 31.91 27.58 46.40 22.86 40.45 30.05 34.65</cell><cell>7.10e-01 4.81e-01 5.01e-01 5.39e-01</cell><cell>3.52e-03 1.21e-02 1.92e-02 7.55e-02</cell><cell>1.18e-01 -1.99e-01 3.13e-02 -2.25e-01</cell><cell>5.66-01 3.14e-01 4.26e-01 3.34e-01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Name avg node avg edge avg deg deg var density closeness assortativity coef transitivity avg clustering coef</figDesc><table><row><cell></cell><cell cols="2">ZINC15-0 26.95</cell><cell>29.24</cell><cell>2.16</cell><cell>0.70</cell><cell>0.08</cell><cell>0.19</cell><cell>-0.31</cell><cell>4.22e-03</cell><cell>4.43e-03</cell></row><row><cell>pre-training</cell><cell cols="2">ZINC15-1 25.69 ZINC15-2 26.86 ZINC15-3 26.91 ZINC15-4 26.83</cell><cell>27.67 29.21 29.19 29.18</cell><cell>2.14 2.17 2.16 2.17</cell><cell>0.71 0.69 0.70 0.69</cell><cell>0.09 0.08 0.08 0.08</cell><cell>0.20 0.19 0.19 0.19</cell><cell>-0.33 -0.31 -0.31 -0.31</cell><cell>3.08e-03 3.66e-03 3.30e-03 3.71e-02</cell><cell>3.38e-03 4.07e-03 3.66e-03 4.09e-03</cell></row><row><cell>downstream</cell><cell>BACE BBBP MUV HIV</cell><cell>34.08 24.06 24.23 25.51</cell><cell>36.85 25.95 26.27 27.46</cell><cell>2.16 2.13 2.16 2.14</cell><cell>0.76 0.75 0.68 0.74</cell><cell>0.07 0.11 0.09 0.10</cell><cell>0.17 0.24 0.20 0.22</cell><cell>-0.36 -0.27 -0.27 -0.26</cell><cell>6.10e-03 2.21e-03 8.75e-04 2.11e-03</cell><cell>6.54e-03 2.56e-03 1.01e-03 1.96e-03</cell></row><row><cell></cell><cell>ClinTox</cell><cell>26.15</cell><cell>27.88</cell><cell>2.10</cell><cell>0.77</cell><cell>0.11</cell><cell>0.23</cell><cell>-0.34</cell><cell>2.98e-03</cell><cell>3.21e-03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The statistics of pre-training and downstream datasets (avg deg: average degree; deg var: degree variance; coef: coefficient) for graph classification. All these datasets are from molecular domain.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>For simplicity, we slightly abuse the notations ? ( ?, ?). Note that ? ( {? ? }, {? ? }) is a function of {? ? } and {? ? }, representing that the generator depends on {? ? }, {? ? }; while for each generator (i.e., mixed graphon) ? given {? ? }, {? ? }, it can be represented as a continuous, bounded and symmetric function ? : [0, 1] 2 ? [0, 1].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Other graph-level properties can also be utilized like diameter and Wiener index, but we do not include them due to their high computational complexity.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>When to Pre-Train Graph Neural Networks? <rs type="person">An Answer</rs> from <rs type="institution">Data Generation Perspective!. In Proceedings of (Conference acronym ). ACM, New York, NY, USA</rs>, 17 pages. https://doi.org/XXXXXXX.XXXXXXX</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional Results of Pre-Training Data Selection</head><p>Table <ref type="table">6</ref> shows the results of pre-training data selection on graph classification tasks. The backbone pre-training model used here is GraphCL <ref type="bibr" target="#b51">[52]</ref>. We can see that the pre-training data selected by W2PGNN ranks the first, which suggests that the effectiveness of our strategy on the graph classification task is still significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ANALYSIS OF COMPUTATION COMPLEXITY</head><p>We here detailedly show the time complexity of W2PNN and the traditional solution. Suppose that we have ? from data augmentation, GNN encoder and contrastive loss, which costs ? ? 1 |? | 3 (subgraphs sampled from random walk with restarts as augmentation) <ref type="bibr" target="#b45">[46]</ref>, ? (? 1 |?|?) (GIN as graph encoder) and ? (? 1 ??) <ref type="bibr" target="#b21">[22]</ref>, respectively. (Note that other data augmentations like dropping nodes cost ? (|? | 2 ), but they cannot achieve good performance on node classification in our pre-training experiments.) The time complexity of each fine-tuning strategy involves the inference of pre-trained model ? ? 2 (|? | 3 + |?|?) and downstream predictor ? (? 2 ?) (which can be ignored), under the simple freezing mode (i.e, the pre-trained model without any changes in parameters is directly applied to the downstream tasks via learnable downstream predictor). Thus the overall time complexity of traditional solution is ? ? 1 ? 2 ((? 1 + ? 2 )(|? | 3 + |?|?) + ? 1 ??) .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiago</forename><forename type="middle">B</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: membership, growth, and evolution</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Katy</forename><surname>B?rner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soma</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network science. Annu. rev. inf. sci. technol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="537" to="607" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification and estimation in the Stochastic Blockmodel based on the empirical degrees</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Channarond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Jacques</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2574" to="2601" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Structural Node Embeddings via Diffusion Wavelets</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centrality in social networks: Conceptual clarification. Social network: critical concepts in sociology</title>
		<author>
			<persName><surname>Linton C Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Londres: Routledge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="263" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GraphCL: Contrastive Self-Supervised Learning of Graph Representations</title>
		<author>
			<persName><forename type="first">Hakim</forename><surname>Hafidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounir</forename><surname>Ghogho</surname></persName>
		</author>
		<idno>ArXiv abs/2007.08025</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Philippe Ciblat, and Ananthram Swami</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xueting</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Transfer Learning on Graph Neural Networks. SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">G-Mixup: Graph Data Augmentation for Graph Classification</title>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GraphMAE: Self-Supervised Masked Graph Autoencoders</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pre-Training Graph Neural Networks for Generic Structural Feature Extraction</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv abs/1905.13728</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mean clustering coefficients: the role of isolated nodes and leafs on clustering measures for small-world networks</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">83042</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv abs/1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transferability of Spectral Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gitta</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Kutyniok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="272" to="273" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pairwise Half-graph Discrimination: A Simple Graph-level Self-supervised Strategy for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Pengyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Let Invariant Rationale Discovery Inspire Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13052" to="13065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large networks and graph limits</title>
		<author>
			<persName><forename type="first">L?szl?</forename><surname>Lov?sz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">L?szl?</forename><surname>Lov?sz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Pre-train Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classification and analysis of multivariate observations</title>
		<author>
			<persName><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Berkeley Symp</title>
		<meeting><address><addrLine>California Los Angeles LA USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">2017. graph2vec: Learning Distributed Representations of Graphs</title>
		<author>
			<persName><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno>ArXiv abs/1707.05005</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixing patterns in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">26126</biblScope>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<title level="m">DeepWalk: online learning of social representations. SIGKDD</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gromov-wasserstein averaging of kernel and distance matrices</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2664" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Node Representations from Structural Identity</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Scalable Permutation Approach Reveals Replication and Preservation Patterns of Network Modules in Large Datasets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">C</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><forename type="middle">G</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">E</forename><surname>Fearnley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gad</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><surname>Inouye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Transferability Properties of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Luana</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<idno>ArXiv abs/2112.04629</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ZINC 15 -Ligand Discovery for Everyone</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MoCL: Data-driven Molecular Fingerprint via Knowledge-aware Contrastive Learning from Molecular Graph</title>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Social network analysis: Methods and applications</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Faust</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Random walks: A review of algorithms and applications</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangtian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangjie</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning Graphon Autoencoders for Generative Graph Modeling</title>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dixin</forename><surname>Luo</surname></persName>
		</author>
		<idno>ArXiv abs/2105.14244</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">When Does Self-Supervision Help Graph Convolutional Networks?</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In PMLR</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="10871" to="10880" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">OAG: Toward Linking Large-scale Heterogeneous Entity Graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ProNE: Fast and Scalable Network Representation Learning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport</title>
		<author>
			<persName><forename type="first">Jiying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Motif-based Graph Self-Supervised Learning for Molecular Property Prediction</title>
		<author>
			<persName><forename type="first">Zaixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Motif-based graph self-supervised learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15870" to="15882" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data Augmentation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><forename type="middle">J</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv abs/2006.04131</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
