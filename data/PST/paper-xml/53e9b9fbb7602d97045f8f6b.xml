<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Human Detection Using Contour Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<email>jxwu@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Geyer</surname></persName>
							<email>cgeyer@irobot.com</email>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<email>rehg@cc.gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technolog-ical University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">iRobot Corporation</orgName>
								<address>
									<postCode>01730</postCode>
									<settlement>Bedford</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Center for Behavior Imaging</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Human Detection Using Contour Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7C409471E57ADAD1145148C971C74A73</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A real-time and accurate human detector, C 4 , is proposed in this paper. C 4 achieves 20 fps speed and stateof-the-art detection accuracy, using only one processing thread without resorting to special hardwares like GPU. Real-time accurate human detection is made possible by two contributions. First, we show that contour is exactly what we should capture and signs of comparisons among neighboring pixels are the key information to capture contours. Second, we show that the CENTRIST visual descriptor is particularly suitable for human detection, because it encodes the sign information and can implicitly represent the global contour. When CENTRIST and linear classifier are used, we propose a computational method that does not need to explicitly generate feature vectors. It involves no image pre-processing or feature vector normalization, and only requires O(1) steps to test an image patch. C 4 is also friendly to further hardware acceleration. In a robot with embedded 1.2GHz CPU, we also achieved accurate and 20 fps high speed human detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human detection in video is important in a wide range of applications that intersect with many aspects of our lives: surveillance systems and airport security, automatic driving and driver assistance systems in high-end cars, human-robot interaction and immersive, interactive entertainments, smart homes and assistance for senior citizens that live alone, and people-finding for military applications. The wide range of applications and underlying intellectual challenges of human detection have attracted many researchers.</p><p>The goal of this paper is to detect humans in real-time, with a high detection rate, and few false positives. In particular, for human detection on-board a robot, the computational efficiency of the detector is of paramount importance. Not only must human detection run at video rates, but it also can use only a small number of CPU cores (or a small percentage of CPU cycles) so that other important tasks such as path planning and navigation will not be hindered.</p><p>Recent progress in human detection has advanced the frontiers of this problem in many aspects, e.g., features, classifiers, testing speed, and occlusion handling <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, at least two important questions still remain open:</p><p>• Real time detection. The speed issue is very important, because real-time detection is the prerequisite in most of the real-world applications <ref type="bibr" target="#b11">[12]</ref> and in a robot in particular.</p><p>• Identify the most important information source. Features like HOG <ref type="bibr" target="#b0">[1]</ref> and LBP <ref type="bibr" target="#b7">[8]</ref> have been successful in practice. But we do not know clearly yet what is the critical information encoded in these features, or why they achieve high pedestrian detection performance in practice. In this paper we argue that these two problems are closelyrelated, and we demonstrate that an appropriate feature choice can lead to an efficient detection architecture. In fact, feature computation is the major speed bottleneck in existing methods. Current methods can only run at about 10 fps (frames per second) <ref type="bibr" target="#b8">[9]</ref>, even when utilizing the 100+ parallel processing threads of a GPU. Most of this time is spent in computing the features (including image pre-processing, feature construction, and feature vector normalization).</p><p>This paper makes two contributions. First, we show that the contour defining the outline of the figure is the essential information in human detection, through a series of carefullydesigned experiments in Sec. III-A. We find that the signs of comparisons among neighboring pixels are critical to represent a contour, while the magnitudes of such comparisons are not as important.</p><p>Second, we propose to detect humans using the contour cues, and show that the recently-developed CENTRIST <ref type="bibr" target="#b12">[13]</ref> feature is suitable for this purpose (Sec. III-B). In particular, it encodes the signs of local comparisons, and has the key capability to capture global (or large scale) structures and contours. We also compare CENTRIST and other features in Sec. III-C.</p><p>CENTRIST is very appealing in terms of speed. In Sec. IV we describe a method for feature evaluation that does not involve image pre-processing or feature vector normalization. In fact, we show that it is not even necessary to explicitly compute the CENTRIST feature vector, because it is seamlessly embedded into the classifier evaluation, achieving video-rate detection speed. We use a cascade classifier, and call the proposed method C 4 , since we are detecting humans emphasizing the human contour using a cascade classifier and the CENTRIST visual descriptor.</p><p>C 4 produces an accurate detector running in real-time (using only one single CPU core (or thread), not involving GPU or other special hardware). We present detection results in Sec. V. We present two forms of experimental evaluation. First, we present results on a standard benchmark human detection dataset. Second, we present the results of on-line, real-time testing of C 4 on an iRobot PackBot, operating autonomously and untethered. Specifically, we demonstrate pedestrian following based on real-time on-board pedestrian detection and ground plane estimation. We will make our detection system available to other researchers to facilitate progress on this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Accurate detection is still a major interest in human detection, especially in terms of high detection rate with low FPPI (false positive per image) <ref type="bibr" target="#b1">[2]</ref>. Achievements have been made in two main directions: features and classifiers.</p><p>Various features have been applied to detect pedestrians, e.g., Haar features <ref type="bibr" target="#b6">[7]</ref> and edgelet <ref type="bibr" target="#b9">[10]</ref>. However, HOG is probably the most popular feature in human detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The distribution of edge strength in various directions seem to efficiently capture humans in images. Recently, variants of Local Binary Pattern (LBP) also show high potentials <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. A recent trend in human detection is to combine multiple information sources, e.g., color, local texture, edge, motion, etc. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Introducing more information channels usually increases detection accuracies, at the cost of increased detection time.</p><p>In terms of classifiers, linear SVM is widely used, probably for its fast testing speed. With the fast method to evaluate Histogram Intersection Kernel (HIK) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, HIK SVM was used to achieve higher accuracies with slight increase in testing time <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recent research also substantially speeds up human detection. Cascade (e.g., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>) and integral image (e.g., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b7">[8]</ref>) were widely used to accelerate human detection. However, the detection speed is still far slower than frame rate. Thus GPU was frequently used to distribute the computation loads into hundreds of parallel threads. For example, the system in <ref type="bibr" target="#b8">[9]</ref> achieved about 10 fps, and similarly 4 fps in <ref type="bibr" target="#b7">[8]</ref>, both using GPU. In Sec. IV we will present a method that runs at 20 fps using only a single processing thread (and it is also very friendly to further GPU speedup). Table <ref type="table" target="#tab_0">I</ref> compares the speed and accuracy of several fast vision-based detection methods, including C 4 , the method proposed in this paper. <ref type="foot" target="#foot_0">1</ref>There have been numerous previous works in the robotics community which developed pedestrian detection systems for mobile robot platforms <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The majority of these works employ some form of ranging sensor (representative examples are <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b20">[21]</ref>). 3D sensors has the advantage of leveraging 3D cues for detection and tracking (e.g., humans will protrude above the ground plane and can often be segmented reliably in depth), and several impressive systems have been demonstrated. However, this approach has some significant disadvantages: active ranging systems can have limited resolution and range, limited temporal sampling rates, difficulties with strong outdoor lighting, add to system expense, and emit an energy signature. Therefore it seems useful to explore the viability of passive EO sensing technologies such as video cameras. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. USING CENTRIST TO DETECT HUMAN CONTOUR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Signs of local comparisons are critical for encoding contours and human detection</head><p>We believe that contour is the most useful information for pedestrian detection, and the signs of comparisons among neighboring pixels are key to encode the contour. Both hypotheses are supported by experiments presented in this section.</p><p>Hypothesis 1: For pedestrian detection the most important thing is to encode the contour, and this is the information that HOG is mostly focusing on. Local texture can be harmful, e.g., the paintings on a person's T-shirt may confuse a human detector. In Fig. <ref type="figure" target="#fig_0">1b</ref>, we compute the Sobel gradient of each pixel in Fig. <ref type="figure" target="#fig_0">1a</ref> and replace a pixel with the gradient value (normalized to [0 255]). The Sobel image smooths high frequency local texture information, and the remaining contour in Fig. <ref type="figure" target="#fig_0">1b</ref> clearly indicates the location of a human. Fig. <ref type="figure" target="#fig_9">6</ref> in <ref type="bibr" target="#b0">[1]</ref> also indicated that image blocks related to the human contour are important in the HOG detector. However, we do not know clearly what information captured by HOG makes it successful in human detection.</p><p>We will experimentally show that contour is the important information captured by HOG. We used the original HOG detector in <ref type="bibr" target="#b0">[1]</ref>, but use the Sobel version of test images. The original HOG SVM detector was trained with features where contour and other information (e.g., fine-scale textures on the clothes) are interwoven with each other (cf. Fig. <ref type="figure" target="#fig_0">1a</ref>). It is unusual that without modification it will detect humans on Sobel testing images where contour is the main information (cf. Fig. <ref type="figure" target="#fig_0">1b</ref>). Surprisingly, the detection accuracy is 67% at 1 FPPI, higher than 7 out of 12 methods evaluated in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Thus we believe that contour is the most important information captured by HOG and for pedestrian detection. One important difference between C 4 and existing methods is that we explicitly detect humans from the Sobel image.</p><p>Hypothesis 2: Signs of comparisons among neighboring pixels are key to encode the contour. We usually use image gradients to detect contours, which are computed by comparing neighboring pixels. We show that the signs of such comparisons are key to encode contours while the magnitudes of comparisons are not as important. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GPU qVGA speed VGA speed speedup to HOG Accuracy HOG <ref type="bibr" target="#b0">[1]</ref> No 0.075 fps <ref type="bibr" target="#b1">[2]</ref> 1x 74.4% (Sec. V-B) ChnFtrs <ref type="bibr" target="#b13">[14]</ref> No 0.5 fps [14] 86% HOG-LBP <ref type="bibr" target="#b7">[8]</ref> Yes about 4 fps about 87% HOG cascade <ref type="bibr" target="#b10">[11]</ref> No 5-30 fps 12-70x GPU HOG <ref type="bibr" target="#b8">[9]</ref> Yes 34 fps <ref type="bibr" target="#b8">[9]</ref> 10 fps <ref type="bibr" target="#b8">[9]</ref> 34x <ref type="bibr" target="#b8">[9]</ref> similar to HOG C 4  No 109 fps 20 fps 80x 83.5%</p><p>In order to verify this hypothesis, for a given image I, we want to create a new image I that retains signs of local comparisons but ignores their magnitudes. In other words, we want to find an image I such that</p><formula xml:id="formula_0">sgn (I(p 1 ) -I(p 2 )) = sgn (I (p 1 ) -I (p 2 )) ,<label>(1)</label></formula><p>for any neighboring pair of pixels p 1 and p 2 . An example is shown in Eqn. 2.</p><p>I : 32 2 8 38 96 64</p><formula xml:id="formula_1">I : 1 0 1 2 3 2 (2)</formula><p>Note that the pixel 96 is converted to a value 3, because of the path of comparisons 2 &lt; 32 &lt; 38 &lt; 96. In other words, although the magnitude of comparisons in I are ignored in I , the spatial relationships among multiple comparisons in I will provide a "pseudo-magnitude" in I . Another important observation is that gradients computed from I and I will have quite different magnitudes. Fig. <ref type="figure" target="#fig_0">1c</ref> shows such a sign comparison image I (in which pixel values are scaled to [0 255]) when I is Fig. <ref type="figure" target="#fig_0">1b</ref>. We can easily detect the human contour in Fig. <ref type="figure" target="#fig_0">1c</ref>. We further verified hypothesis 2 in human detection. Applying the original HOG detector to sign comparison testing images (like Fig. <ref type="figure" target="#fig_0">1c</ref>), we achieved 61% detection accuracy at 1 FPPI (better than 7 methods evaluated in <ref type="bibr" target="#b13">[14]</ref>). Although we observe lower detection rates when the Sobel images or the sign comparison images are used as test images, it is important to note that the classifier was trained using the original images. The fact that we obtain higher accuracies than many existing methods without modifying the HOG classifier is noteworthy. Thus we argue that the most useful information for human detection is the global contour information, and the signs of comparisons among neighboring pixels is the key to encode a contour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The CENTRIST visual descriptor</head><p>We then propose to use the CENTRIST visual descriptor <ref type="bibr" target="#b12">[13]</ref> to recognize humans, because it succinctly encodes the crucial sign information, and does not require preor post-processing. CENTRIST means CENsus TRansform hISTogram. We will show in this section why CENTRIST is suitable for this task, and compare CENTRIST with other popular descriptors in Sec. III-C.</p><p>Census Transform (CT) is originally designed for establishing correspondence between local patches <ref type="bibr" target="#b21">[22]</ref>. Census transform compares the intensity value of a pixel with its eight neighboring pixels, as illustrated in Eqn. 3. If the center  pixel is bigger than (or equal to) one of its neighbors, a bit 1 is set in the corresponding location. Otherwise a bit 0 is set.</p><p>32 64 96 32 64 96 32 32 96</p><formula xml:id="formula_2">⇒ 1 1 0 1 0 1 1 0 ⇒ (11010110) 2 ⇒ CT = 214 (3)</formula><p>The eight bits generated from intensity comparisons can be put together in any order (we collect bits from left to right, and top to bottom), which is consequently converted to a base-10 number in [0 255]. This is the CT value for the center pixel. The CENTRIST descriptor is a histogram of these CT values <ref type="bibr" target="#b12">[13]</ref>.</p><p>As shown in Eqn. 3, CT values succinctly encode the signs of comparisons between neighboring pixels. The only thing that seems to be missing from CENTRIST, however, is the power to capture global (or larger scale) structures and contours beyond the small 3 × 3 range.</p><p>More importantly, if we are given an image I with CENTRIST h, then among the small number of images I that has a matching CENTRIST descriptor, we expect that I will be similar to I, especially in terms of global structure or contour, which we illustrate in Fig. <ref type="figure" target="#fig_2">2</ref>. Fig. <ref type="figure" target="#fig_2">2a</ref> shows a 108 × 36 human contour. We divide this image into 12 × 4 blocks, thus each block has 81 pixels. For each block I, we want to find an image I that has the same histogram and CENTRIST descriptor as I. <ref type="foot" target="#foot_1">2</ref> As shown in Fig. <ref type="figure" target="#fig_2">2b</ref>, the reconstructed image is similar to the original image. The global characteristic of the human contour is well preserved in spite of errors in the left part of the image. The fact that CENTRIST not only encodes important information (signs of local comparisons) but also implicitly encodes the global human contour makes us believe that it is a suitable representation for detecting human contours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing with HOG and LBP</head><p>Now we will compare CENTRIST with HOG and LBP, two visual descriptors that are popular in human detection.</p><p>For classification tasks, the feature vectors of examples in the same class should be similar to each other, while examples in different classes should have dissimilar descriptors. For any example x, we will compute the similarity score between x and all other examples. Let x in be the most similar example to x within the same class. Similarly, let x out be the most similar example that is in a different class. Obviously we want s NN = s(x, x in ) -s(x, x out ) to be positive and large, where s(x, y) is the similarity score between x and y. A positive s NN means that x is correctly classified by a nearest neighbor (1-NN) rule. Thus s NN is an intuitive and easy-to-compute measure to determine whether a descriptor suits certain tasks.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref> compares CENTRIST (on Sobel images) and HOG (on original input images) using the INRIA dataset <ref type="bibr" target="#b0">[1]</ref>. In Fig. <ref type="figure" target="#fig_3">3a</ref>  It is argued in <ref type="bibr" target="#b12">[13]</ref> that visual descriptors such as HOG or SIFT <ref type="bibr" target="#b22">[23]</ref> pays more attention to detailed local textural information instead of structural properties (e.g., contour) of an image. We further speculate that this is due to the fact that the magnitudes of local comparisons used in HOG pay more attention to local textures. It is also obvious that we can not reconstruct an image from its HOG or SIFT descriptor. In Fig. <ref type="figure" target="#fig_3">3</ref> the HOG vectors are l 2 normalized, we set s(x, y) =</p><p>x T y. For CENTRIST, the histogram intersection kernel <ref type="bibr" target="#b23">[24]</ref> is used to compute similarity scores.</p><p>CENTRIST has close relationship with LBP, another popular feature for human detection. If we switch all bits '1' to '0' and vice versa in Eqn. 3, the revised formula is an intermediate step to compute the LBP value for the same 3 × 3 region <ref type="bibr" target="#b24">[25]</ref>. However, the more important difference is how the LBP values are utilized. Pedestrian detection methods use "uniform LBP" <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, in which certain LBP values that are called "non-uniform" are lumped together. We are, however, not able to reconstruct the global contour because the non-uniform values are missing. In addition, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b7">[8]</ref> involves interpolation of pixel intensities. These procedures make their descriptors to only encode a blurred version of the most important information, i.e., signs of neighboring pixel comparisons. We computed the distribution of s NN for the uniform LBP descriptor. It has an error rate of 6.4% for the 1-NN classifier, more than twice of the error rate for CENTRIST (2.9%). However, LBP has better s NN distribution than HOG (46% 1-NN error). Our conjecture is that the incomplete and blurred local sign information in LBP is still less sensitive than HOG in the presence of noise and distractions from local textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FAST LINEAR METHOD AND DETECTION FRAMEWORK</head><p>Given the virtues of CENTRIST, we will use it to detect humans. We use 108-by-36 as the detection window size, and divide the image patch into 9 × 4 blocks (so each block has 108 pixels). Following <ref type="bibr" target="#b0">[1]</ref>, we treat any adjacent 2×2 blocks as a super-block and extract a CENTRIST descriptor from each super-block. There are 8 × 3 = 24 super-blocks, thus the feature vector for a candidate image patch has 256 × 24 = 6144 dimensions. A one-pixel-wide border of each super-block is not included when computing the CENTRIST descriptor because the Census Transform requires a 3 × 3 region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fast scanning using a linear classifier</head><p>Suppose we already trained a linear classifier w ∈ R 6144 , we can divide w to smaller units corresponding to the superblocks. In other words, w is considered as a concatenation of w i,j ∈ R 256 , 1 ≤ i ≤ 8, 1 ≤ j ≤ 3. Given an image patch with feature vector f (similarly separated into f i,j ), it is classified as containing a human if</p><formula xml:id="formula_3">w T f = 8 i=1 3 j=1 w T i,j f i,j ≥ θ.<label>(4)</label></formula><p>Inspired by <ref type="bibr" target="#b25">[26]</ref>, we propose a method that computes Eqn. 4 using a fixed number machine instructions for each image patch, i.e., an O(1) method. We also improve the method in <ref type="bibr" target="#b25">[26]</ref> by using only one integral image. Let us denote the dimension of a detection window as (h, w). A block has size (h s , w s ) = (h/9, w/4), and a superblock is (2h s , 2w s ). Given an image I, its corresponding Sobel image S, and CT image (of S) C. For a detection window with top left corner (t, l), it is not difficult to show that the term w T f in Eqn. 4 is equal to:</p><formula xml:id="formula_4">8 i=1 3 j=1 2hs-1 x=2 2ws-1 y=2 w C(t+(i-1)hs+x,l+(j-1)ws+y) i,j ,<label>(5)</label></formula><p>where w k i,j is the k-th component of w i,j , and C(x, y) is a pixel in the CT image C. We starts from x = 2 and ends at 2h s -1 to exclude the border.</p><p>We then create auxiliary images A i,j for 1 ≤ i ≤ 8, 1 ≤ j ≤ 3 with the same size as the input image I. The (x, y) pixel of A i,j is set to</p><formula xml:id="formula_5">A x,y i,j = w C(x,y) i,j ,<label>(6)</label></formula><p>then Eqn. 5 becomes</p><formula xml:id="formula_6">8 i=1 3 j=1 2hs-1 x=2 2ws-1 y=2</formula><p>A t+(i-1)hs+x,l+(j-1)ws+y i,j</p><p>.</p><p>Using the integral image trick, the term in the parenthesis of Eqn. 7 can be computed using 3 arithmetic operations. Thus Eqn. 7 (and equivalently Eqn. 4) can be computed in O(1) steps. The advantage of using CENTRIST is that it does not require normalization. In contrast, normalization is essential in HOG <ref type="bibr" target="#b0">[1]</ref>. We can compute w T f in a sum of pixel-wise contribution manner without explicitly generating f . Eqn. 7 is similar to the method for ESS with spatial pyramid matching in <ref type="bibr" target="#b25">[26]</ref>. However, there is no need to generate multiple integral images. Instead, we define a single auxiliary image A, with A(x, y) = </p><p>where n x = 8, n y = 3. Then</p><formula xml:id="formula_9">w T f = 2hs-1 x=2 2ws-1 y=2 A(t + x, l + y).<label>(9)</label></formula><p>Only one integral image is needed to compute Eqn. 9, which saves not only large memory space but also computation time. In practice, Eqn. 9 runs about 3 to 4 times faster than Eqn. 7. Please note that the technique of Eqn. 9 is general, and can be used to accelerate other computations, e.g., ESS with spatial pyramid matching.</p><p>The proposed method does not involve image preprocessing (e.g., smoothing) or feature vector normalization. In fact, the feature extraction component is seamlessly embedded into classifier evaluation. These properties together contribute to a real-time human detection system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection framework</head><p>In the training phase, we have a set of 108 × 36 positive training image patches P and a set of larger negative images N that do not contain any pedestrian. We first randomly choose a small set of patches from the images in N to form a negative training set N 1 . Using P N 1 we train a linear SVM classifier H 1 .</p><p>A bootstrap process is used to generate a new negative training set N 2 . H 1 is applied to all patches in the images in N . In this bootstrapping process, we also re-scale the negative image to examine more patches. We then train H 2 using P and N 2 . This process is repeated until all patches in N are classified as negative by at least one of H 1 , H 2 , . . . We then train a linear SVM classifier using P and the combined negative set i N i , which we call H lin .</p><p>Linear classifiers ensure fast testing speed (and fast bootstrapping process). However, it has been shown that HIK achieves higher classification accuracies on histogram features than linear SVM classifiers <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b3">[4]</ref>. We will train a second HIK SVM classifier to achieve higher detection accuracy. We use H lin on N to bootstrap a new negative training set N final , and train an SVM classifier using the libHIK HIK SVM solver of <ref type="bibr" target="#b26">[27]</ref>, which we call H hik . In the testing / detection phase, a cascade with two nodes H lin and H hik is used.</p><p>We call the proposed method C 4 , as we are detecting humans based on their contour information using a cascade classifier and CENTRIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pedestrian detection on-board a robot</head><p>We integrated the C 4 pedestrian detection algorithm onto an iRobot PackBot in order to achieve on-board pedestrian detection and to enable pedestrian following. The implementation first captured images from a TYZX G2 stereo camera system and then processed the imagery using an Intel 1.2 GHz Core 2 Duo embedded in an add-on computational payload. We used the raw camera imagery to perform the detection and used the stereo range data to estimate the distance to the pedestrian. We used a particle filter to track the pedestrian between frames and to eliminate outliers. Finally, a following component was implemented to steer the robot chassis and command the neck pan axis.</p><p>We compared the basic approach described above with an optimized method that utilized the stereo data. We use the range image to provide hypotheses for where pedestrians may be standing. From the stereo data we use RANSAC to estimate a ground plane, and we sampled the depths along the ground plane's horizon. With the depth and coordinates of the plane we can calculate a box that would contain a pedestrian standing on the plane at the given position on the horizon and given distance. This gives us far fewer windows to test with the detector, which reduces both computation and false positives. Figures <ref type="figure">4a, 4b,</ref> and<ref type="figure">4c</ref> show the raw detections from the C 4 algorithm, the hypotheses generated from the stereo data, and the results of the C 4 classifier evaluated only on these hypotheses.</p><p>Note that the C 4 detector was tailored to work with the robot to a 3-layer cascade instead of 2-layer for faster speed (but less accurate). By default the detection procedure is as follows. The detector looks for pedestrians of different projected sizes within the image. Recall that distance to the pedestrian determines the size of the pedestrian in the image: pedestrians that are farther away appear smaller in the image; while closer pedestrians appear bigger. The detection system searches the image at multiple scales, and for each scale runs a classifier on every single possible location of a pedestrian of that size within the image. Lacking any other information about the scene, this is the only reliable approach to detection. However, when other information is available, we should be able to use this to our advantage to decrease the number of windows on which to run the classifier, and to decrease the false positive rate. In particular, we can use information about the ground plane -which we can acquire from the stereo camera. For example, Fig <ref type="figure">4</ref> shows the result of the pedestrian detection classifier being run on all possible windows (C 4 , Fig. <ref type="figure">4a</ref>), and the many redundancies that are generated despite the fact that in most of the locations there cannot be a pedestrian <ref type="bibr" target="#b27">[28]</ref>. Pedestrians are bound to the ground and we can use this fact as a prior to limit the search range (C 4 +stereo, Fig. <ref type="figure">4b</ref>). The redundancies are a problem because each of the redundant windows has to be filtered out, increasing algorithm and computational complexity. The results after post-processing are shown in Fig. <ref type="figure">4c</ref> (cf. Sec. V-C for post-processing details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>We experimented on the INRIA pedestrian dataset <ref type="bibr" target="#b0">[1]</ref>. We will show the speed, accuracy, and related discussions of C 4 in Sec. V-A to V-C. Results of human detection on-board the robot are described in Sec. V-D.</p><p>There are 2416 positive training image patches and 1218 negative images for bootstrapping in the INRIA dataset. We crop the examples to 108 × 36 pixels, which is a tight boundary and removed the extra padding pixels. At testing time, a brute-force strategy is used to search image patches at all possible positions and scales. We successively downsample the test image by a factor of 0.8, and scan in a grid with step size 2.</p><p>We use the groundtruth and matching criterion in <ref type="bibr" target="#b1">[2]</ref>. A detection rectangle R d and a groundtruth rectangle R g is considered as a correct match if</p><formula xml:id="formula_10">Area(R d ∩ R g ) Area(R d ∪ R g ) &gt; 0.5.<label>(10)</label></formula><p>We also follow <ref type="bibr" target="#b1">[2]</ref> which requires that one groundtruth rectangle can only match to at most one detected window. A. Detection speed C 4 achieves much faster speed than existing human detectors. On a 640 × 480 video, its speed is 20.0 fps, using only 1 processing core of a 2.8GHz CPU. As far as we know, the fastest existing system (with a reasonably low false alarm rate and high detection rate) ran at about 10 fps <ref type="bibr" target="#b8">[9]</ref>, which utilized the parallel processing cores of a GPU. Detailed comparisons are available in Table <ref type="table" target="#tab_0">I</ref> (page 3).</p><p>Real-time processing is a must-have property in most human detection applications. Our system is already applicable in some domains, e.g., robot systems. However, there is still huge space for speed improvements, which will make C 4 suitable even for the most demanding applications, e.g., automatic driver assistance. Table <ref type="table" target="#tab_1">II</ref> is the break-down of time spent in different components of C 4 . Most of these components are very friendly to acceleration using special hardware (e.g., GPU).</p><p>The fact that we do not need to explicitly construct feature vectors for H lin is not the only factor that makes our system extremely fast. H lin is also a powerful classifier. It filters away about 99.43% of the candidate patches, only &lt; 0.6% patches require attentions of the expensive H hik on the INRIA dataset. C 4 used 27.1 seconds on the INRIA dataset's test images, while the executables of the HOG detector <ref type="bibr" target="#b0">[1]</ref> used 2167.5 seconds (i.e., an 80 fold speedup).</p><p>C 4 runs faster in smaller images. In a 480 × 360 YouTube video with many pedestrians, its speed is 36.3 fps. Its speed is 109 fps on 320x240 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection Accuracy on the INRIA dataset</head><p>Accuracy of the system using both the false positive per window (FFPW) and image (FPPI) metrics are shown in Fig. <ref type="figure" target="#fig_8">5</ref>. In Fig. <ref type="figure" target="#fig_8">5</ref> we compare with HOG [1] (using   executables accompanying <ref type="bibr" target="#b0">[1]</ref>). We will compare with other methods directly using the accuracy numbers published in respective papers.</p><p>C 4 detects 83.5% humans at 0.96 false detection per image. It is comparable to the state-of-the-art results on the INRIA dataset, e.g., ChnFtrs <ref type="bibr" target="#b13">[14]</ref> and HOG-LBP <ref type="bibr" target="#b7">[8]</ref>), both having around 86% detection rate at 1 FPPI. Multiple information channels were used in these methods. We could also use multiple channels to further improve C 4 . C 4 has higher accuracies than HOG <ref type="bibr" target="#b0">[1]</ref> (74.4% at 1 FPPI, Fig. <ref type="figure" target="#fig_8">5a</ref>) and many other methods compared in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Fig. <ref type="figure" target="#fig_8">5b</ref> shows the FPPW performance of H hik (H lin is not used when computing the FPPW curve.) The FPPI and FPPW numbers are not linearly correlated but have similar trends. C 4 outperforms HOG when false positive rate is ≥ 10 -4 (or 0.1 in the FPPI curve), and is not as good as HOG in the range of lower false positive rates. But they converge in the left end of both curves.</p><p>C. Importance of post-processing C 4 and HOG intersected at 10 -1 and 10 -4 respectively at FPPI and FPPW curves. The non-maximal suppression (NMS) step contributes to this relatively big difference. In C 4 a location is treated as a false positive if there are less than 3 detected windows at that location. This requirement will not hurt true positives because our step size is 2 and there are usually many detection windows around true humans.</p><p>A small step size also means that NMS will greatly reduce the number of false detections. Examples are shown in Fig. <ref type="figure" target="#fig_9">6</ref>. There are 17 and 5 false detections in these two images, respectively. 3 After NMS, the first image only has 3 The middle part of the second image contains 2 false detections which are very close to each other. The HOG curve in Fig. <ref type="figure" target="#fig_8">5a</ref> is slightly different from <ref type="bibr" target="#b13">[14]</ref>. In Fig. <ref type="figure" target="#fig_8">5a</ref> HOG detects 34% of the humans with only 1 false positive in all test images. However, HOG only detects about 10% of the humans with 1 false detection when evaluated in <ref type="bibr" target="#b13">[14]</ref>. On the contrary, <ref type="bibr" target="#b13">[14]</ref> reported a higher HOG detection rate at 1 FPPI (77%) than that in our experiments (74%). Although it is not totally clear what makes these differences, we believe that beyond nonmaximal suppression, tightness of the detection window is also an important factor.</p><p>We used a very tight 108 × 36 bounding box during the training time. We relaxed detected patches to 120×42 during the post-processing step. It seems that the overly relaxed detection window in <ref type="bibr" target="#b1">[2]</ref> or <ref type="bibr" target="#b0">[1]</ref> is adverse when we seek an extremely low false positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detection results on-board a robot</head><p>In order to better understand the performance of the combined approach (C 4 +stereo) on the robot, we tested on the images collected at iRobot's Bedford facility. Figures <ref type="figure" target="#fig_10">7a</ref> and<ref type="figure" target="#fig_10">7b</ref> show a detailed analysis, with ROC curves (Fig. <ref type="figure" target="#fig_10">7a</ref>) and precision-recall curves (Fig. <ref type="figure" target="#fig_10">7b</ref>) for both approaches, as well as showing the performance for the different cascade levels of the 3-layer C 4 cascade tailored for the robot.</p><p>The ROC curves demonstrate a reduction in false positives, however, the detection rate peaks lower than the standalone version. This is likely because the standalone C 4 was not explicitly trained on the output of the ground plane pedestrian hypotheses generator. The precision recall curves similarly show an increase in precision with the stereo approach at higher recall rates-though again the recall rate reaches a lower maximal value with the stereo approach. We show ROC and precision-recall curves at different cascade levels to show the comparative improvements due to cascade level.</p><p>Overall, the combined approach results in fewer false positives, and is faster at approximately 20 frames per second (50 milliseconds) on the embedded Intel 1.2 GHz Core 2 Duo. Alone, the C 4 runs at approximately 8 frames per second (120 milliseconds) on the same hardware. The combined approach results in a 60% reduction in computation, and depending on detection rate, almost a factor of 5 reduction in false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this paper we proposed a real-time and accurate human detector, C 4 , which detects humans using the contour cues, a cascade classifier, and the CENTRIST visual descriptor.</p><p>First we show, through carefully designed experiments, that contour is the most important information source for human detection, and the signs of comparisons among neighboring pixels are the key to encode contours.</p><p>We then show that CENTRIST <ref type="bibr" target="#b12">[13]</ref> is particularly suitable for human detection, because it succinctly encodes the sign information, and is able to capture large scale structures or contours.</p><p>A major contribution of this paper is extremely fast human detection. C 4 detects humans with 20 fps speed on 640x480 images, using only 1 processing thread, and achieves accuracies comparable to the state-of-the-art.</p><p>Time consuming pre-processing and feature vector normalization are not needed in CENTRIST. Furthermore, using a linear classifier and CENTRIST, we do not need to explicitly generate the CENTRIST feature vectors and it takes only O(1) operations to evaluate an image patch.</p><p>Currently C 4 has slightly lower detection accuracy than methods such as those in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, similar to the techniques in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b1">[2]</ref>, we believe that the accuracy of C 4 can be improved by using multiple information sources, e.g. incorporating color and other feature types. In particular, multiple feature channels will help C 4 under very strict false positive requirements. Furthermore, the speed of C 4 can be further improved by using special hardware like GPU.</p><p>Finally, on an iRobot PackBot with embedded Intel Core 2 Duo 1.2GHz CPU, we combined C 4 with a stereo vision system, and achieved accurate and video rate (1.2GHz) human detection without hindering other robot functionalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Detecting humans from their contours (1b) and signs of local comparison (1c).</figDesc><graphic coords="2,322.21,63.96,75.60,105.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Reconstruct human contour from CENTRIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Histogram and plot of similarity score differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>we use all the 2416 human examples, and randomly generate 2 non-human examples from each negative training image which leads to 2436 non-human examples. Fig. 3a shows the distribution (histogram) of s NN for CENTRIST and HOG. Similarity scores are normalized to the range [0 1], and a negative s NN (i.e., in the left side of the black dashed line) is an error of 1-NN classifier. It is obvious that the CENTRIST curve resides almost entirely in the correct side (2.9% 1-NN error), while about half of the HOG curve is wrong (46% 1-NN error). Fig. 3b further shows that HOG errors are mostly in the first half of the dataset which are human examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(a) C 4 (Fig. 4 :</head><label>44</label><figDesc>Fig.4: On-board detection example. The three images are raw detection results using C 4 , C 4 +stereo, and the post-processed result of C 4 +stereo, respectively. The green line is the ground plane estimated using stereo.</figDesc><graphic coords="6,81.90,63.96,149.40,92.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Comparison C 4 with HOG on the INRIA dataset.</figDesc><graphic coords="7,226.23,216.09,51.84,77.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Example of post-processing.</figDesc><graphic coords="7,74.74,209.85,112.00,84.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: ROC and Precision-Recall curves for the combined and standalone C 4 detector, as well as at different cascade levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Speed comparison of several fast vision-based human detection methods. VGA resolution is 640x480, and qVGA is 320x240. Accuracy is at 1 FPPI (false positive per image).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Distribution of computing time (in percentage).</figDesc><table><row><cell cols="2">Processing module Percent of used time</cell></row><row><cell>Sobel gradients</cell><cell>16.55%</cell></row><row><cell>Computing CT values</cell><cell>9.36%</cell></row><row><cell>Integral Image</cell><cell>44.65%</cell></row><row><cell>Resizing image</cell><cell>5.68%</cell></row><row><cell>Brute-force scan</cell><cell>23.75%</cell></row><row><cell>Post-processing</cell><cell>0.02%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p><ref type="bibr" target="#b10">[11]</ref> reported "up to 70x" speedup to HOG and 5-30 fps on different input images. Its speed in TableIis computed based on these numbers. (a) Original image (b) Sobel image (c) Only signs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We choose to work with small blocks with 81 pixels and binary images to make simulated annealing converge in a reasonable amount of time. Please refer to<ref type="bibr" target="#b12">[13]</ref> for details of the reconstruction algorithm.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank the anonymous reviewers for their useful comments and suggestions. We are grateful for the support of the Office of Naval Research under prime contract N00014-09-C-0101, and the NSF under RI award 0916687. J. Wu is supported by the Singapore MoE AcRF Tier 1 grant RG 34/09.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Max-margin additive classifiers for detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative local binary patterns for human detection in personal album</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sliding-windows for rapid object class localization: A parallel technique</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dorkó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM-Symposium</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey on pedestrian detection for advanced driver assistance systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gerónimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1239" to="1258" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CENTRIST: A visual descriptor for scene categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Integral channel features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond the Euclidean distance: Creating effective visual codebooks using the histogram intersection kernel</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using boosted features for the detection of people in 2D range data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><forename type="middle">M</forename><surname>Mozos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pedestrian detection using 3D optical flow sequences for a mobile robot</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mizoguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="page" from="776" to="779" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">People tracking with a mobile robot using sample-based joint probabilistic data association filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="116" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A layered approach to people detection in 3D range data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient subwindow search: A branch and bound framework for object localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2129" to="2142" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fast dual method for HIK SVM learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ser</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6312</biblScope>
			<biblScope unit="page" from="552" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
