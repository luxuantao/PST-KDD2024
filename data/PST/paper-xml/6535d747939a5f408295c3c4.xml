<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphGPT: Graph Instruction Tuning for Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-19">19 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiabin</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Musketeers Foundation Institute of Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Baidu Inc. Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Baidu Inc. Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Baidu Inc. Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Baidu Inc. Project</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chaohuang75@gmail.com.</email>
							<affiliation key="aff0">
								<orgName type="department">Musketeers Foundation Institute of Data Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GraphGPT: Graph Instruction Tuning for Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-19">19 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn</idno>
					<idno type="arXiv">arXiv:2310.13023v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have emerged as a powerful framework for analyzing and learning from graph-structured data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>, enabling advancements in various domains, such as social network analysis <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65]</ref>, recommender systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>, and biological network analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. One of the key benefits of GNNs is their ability to capture the inherent structural information and dependencies present in graph data. By leveraging message passing and aggregation mechanisms, GNNs can effectively propagate and combine information across the graph, enabling them to model complex relationships and make accurate predictions.</p><p>In recent years, various GNN architectures have introduced innovations in how information is exchanged and aggregated among graph nodes. For example, graph convolutional network (GCNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> adapt convolutional operations to the graph domain, enabling effective feature representations. Graph attention networks (GATs) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref> leverages attention mechanisms to assign different weights to neighboring nodes, allowing for more fine-grained information aggregation. Graph transformer networks (GTNs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref> incorporate self-attention and positional encoding to capture global dependencies and structural patterns in the graph. However, a notable limitation of many GNN approaches is their heavy reliance on supervised learning, which can lead to inadequate robustness and generalization when confronted with sparse and noisy data.</p><p>To enhance the generalization ability of GNNs, self-supervised learning (SSL) has emerged as a promising approach in graph representation learning. It aims to pre-train a robust graph model using auxiliary tasks on unlabeled graph data. The idea is to leverage the inherent structure and patterns within the graph itself to create meaningful self-supervisory signals. SSL-enhanced graph learning methods exhibit two primary paradigms: contrastive SSL and generative SSL. Within contrastive SSL, the emphasis lies on learning representations by contrasting positive and negative samples, with notable advancements of DGI <ref type="bibr" target="#b39">[40]</ref> and GCA <ref type="bibr" target="#b66">[67]</ref>. Conversely, generative SSL focuses on generating synthetic samples that closely resemble the original graph structures with masked autoencoders, exemplified by techniques like GraphMAE <ref type="bibr" target="#b11">[12]</ref> and S2GAE <ref type="bibr" target="#b34">[35]</ref>.</p><p>While these methods aim to generate graph embeddings that are generalizable to different downstream tasks, they often require a fine-tuning process using labels specific to the downstream graph learning scenarios. However, this reliance on labeled data from downstream tasks can restrict their generalization in practical situations where obtaining high-quality labels may not always be feasible. This limitation is particularly relevant in learning scenarios like cold-start recommendation systems or traffic flow prediction in new cities where accurate labels may be scarce or unavailable.</p><p>As a result, the objective of this research is to advance the generalization capabilities of graph models by addressing challenging and practical zero-shot learning scenarios. Inspired by the remarkable success of large language models (LLMs) in natural language processing (NLP) tasks, where they have demonstrated exceptional generalization abilities, this work aims to develop a graph-oriented LLM capable of achieving high generalization across diverse downstream datasets and tasks. However, effectively integrating large language models with graph learning poses non-trivial challenges. ? C1: Achieving a proper alignment between the structural information of a graph and the language space demands meticulous deliberation and thoughtful consideration. ? C2: Effectively guiding LLMs to comprehend the structural information of graphs remains a considerable challenge. ? C3: Endowing LLMs with the ability to reason step-by-step is important when tackling complex graph learning tasks.</p><p>To gain a deeper understanding of the limitations associated with directly prompting LLMs using purely text-based prompts for graph structure modeling, we provide illustrative examples in Figure <ref type="figure" target="#fig_0">1</ref>. These examples facilitate a comparative analysis between our GraphGPT framework and the ChatGPT approach. We focus on a representative node classification task, where the objective is to predict the category of a given paper. In Figure <ref type="figure" target="#fig_0">1</ref> (a) and Figure <ref type="figure" target="#fig_0">1</ref> (b), we showcase the prediction results for two scenarios using ChatGPT: <ref type="bibr" target="#b0">(1)</ref> utilizing only the input node textual data, and (2) employing text-based graph structure-aware prompts inspired by the prompt designs in recent studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. These figures highlight the potential limitations that arise when relying solely on textbased prompts for graph structure modeling, as evidenced by the incorrect paper node classification results presented. In contrast, our GraphGPT framework effectively addresses these limitations by preserving and leveraging the graph structural information, as shown in Figure <ref type="figure" target="#fig_0">1 (c)</ref>. It enables accurate identification of the paper category, in understanding the underlying graph structure.</p><p>Additionally, the utilization of text-based structural prompts leads to an increase in token size, which presents challenges in practical scenarios. Longer token sequences incur higher computational and memory costs, making it less feasible for real-world applications. Furthermore, existing LLMs have token limits, which further restrict the applicability of longer text-based prompts for large-scale graph structure modeling. These limitations emphasize the necessity for more efficient and scalable approaches that can effectively incorporate graph structural information into LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>To address these challenges, we propose a novel framework called GraphGPT, which aims to align Large Language Models (LLMs) with Graphs using a carefully designed graph instruction tuning paradigm. (C1) Our framework introduces a textgraph grounding paradigm as the initial step to align the encoding of graph structures with the natural language space. By incorporating textual information in a contrastive manner, we enable effective alignment of graph structure information within language models. (C2) In our proposed dual-stage graph instruction tuning paradigm, we leverage self-supervised signals through the graph matching task, which is derived from unlabeled graph structures, to serve as instructions for guiding model tuning of LLMs. By incorporating this self-supervised instruction tuning, the language model acquires domain-specific structural knowledge related to graphs, thereby enhancing its understanding of graph structures. To further customize the LLM's reasoning behavior for diverse downstream graph learning tasks, the second stage of our graph instruction tuning paradigm involves fine-tuning the LLM with task-specific graph instructions, to improve the model's adaptability. (C3) By incorporating the Chain-of-Thought (COT) distillation into our framework, GraphGPT enhances its step-by-step reasoning abilities and improves its performance in the face of distribution shift.</p><p>In summary, our work makes the following contributions:</p><p>? This work aims to align graph domain-specific structural knowledge with the reasoning ability of Large Language Models (LLMs) to improve the generalization of graph learning. ? Our approach aims to align LLMs with Graphs through a graph instruction tuning paradigm. This paradigm incorporates selfsupervised instruction tuning, enhancing the LLM's comprehension of graph structural knowledge and its reasoning capabilities. Additionally, we introduce task-specific instruction tuning to improve the model's adaptability across diverse graph tasks. ? We evaluate our proposed GraphGPT on supervised and zeroshot graph learning tasks. We conduct thorough analyses of its component-wise effects and generalization ability. By comparing it with state-of-the-art baselines, we demonstrate the superior generalization power of our approach across various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Graph-structured Data. represents information as a collection of entities (nodes) and the relationships (edges) between them. A graph is formally denoted as G(V, E, A, X), encompassing several core components. The node set V represents a collection of nodes, with |V | = ? indicating the total number of nodes in the graph. The edge set E characterizes the relationships or connections between the nodes. The adjacency matrix A ? R ? ?? encodes the topology of the graph, with each element ? ?,? indicating the presence or absence of an edge between nodes ? and ?. The feature matrix X ? R ? ?? contains the attribute or feature information associated with each node, where ? represents the dimensionality of features.</p><p>Graph Neural Networks. have emerged as a powerful framework for representation learning from graph-structured data. Unlike traditional neural networks that operate on grid-like data, GNNs can effectively capture and model the complex relationships and dependencies present in graphs. Specifically, GNNs leverage the inherent structure of the graph, consisting of nodes and edges, to learn expressive node representations through iterative message propagation and aggregation operations, presented as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>(? )</p><formula xml:id="formula_0">? = Propagate (? ) ({? (? -1) ? : ? ? N (?)}), ? (? ) ? = Aggregate (? ) (? (? -1) ? , ? (? ) ? )<label>(1)</label></formula><p>After applying the message passing and aggregation mechanism in Graph Neural Networks (GNNs), the encoded feature vector of node ? at the ?-th layer is denoted as ? (? )</p><p>? . The Propagate (? ) function performs message passing by aggregating information from the neighboring nodes of ? at the ?-th layer. The Aggregate (? ) function then combines the aggregated information with the previous layer's representation of node ? to generate the updated representation ? (? ) ? . By encoding graph structural information with the learned representations, GNNs can be customized for various downstream graph learning tasks, such as node classification and link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Structural Information Encoding with Text-Graph Grounding</head><p>To enhance the understanding of graph structural information by large language models, our framework emphasizes aligning the encoding of graph structures with the natural language space. This alignment aims to enable language models to effectively comprehend and interpret the structural elements of the graph, leveraging their inherent language understanding capabilities. To achieve this objective, we introduce a text-graph grounding paradigm that generates prompts designed to preserve the graph's structural context for language models. This paradigm acts as a bridge, connecting the semantic understanding of textual information with the inherent structural relationships found within the graph.</p><p>In our GraphGPT, we design the graph encoder to be highly flexible, allowing it to leverage a wide range of backbone GNN architectures obtained from diverse graph pre-training paradigms. We incorporate a message-passing neural network architecture, which can be a graph transformer <ref type="bibr" target="#b59">[60]</ref> or a graph convolutional network <ref type="bibr" target="#b16">[17]</ref>, as the structure-level pre-trained graph model. In each message-passing step, the graph encoder aggregates information from neighboring nodes, considering their relationships:</p><formula xml:id="formula_1">H (? ) = ? ?H (? -1) W<label>(2)</label></formula><p>The self-loop adjacency matrix, denoted as ?, is obtained by adding the identity matrix I to the original adjacency matrix A. W is the parameter matrix. This matrix captures the self-connections and local connectivity of nodes in the graph. ? (?) is the non-linear activation. H (? ) is the graph representations at the ?-th layer.</p><p>Text-Structure Alignment. To align graph structure information within Language Models (LLMs) more effectively, our main focus is to explore the encoding of graph structures that can collaborate well with LLMs. Inspired by prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48]</ref>, we incorporate textual information into the graph structure encoding process in a contrastive manner. In our approach, we directly integrate a graph encoder with pre-trained parameters into our GraphGPT framework, enabling seamless integration of the graph encoder's capabilities in our framework. Formally, let G(V, E, A, X) represent a graph with raw textual contents C = ? ? ? R ? ? ?? , 1 ? ? ? ? for ? nodes, where ? ? denotes the length of the textual content for the ?-th node. We obtain encoded graph representations ? ? R ? ?? and encoded text representations T ? R ? ?? as follows:</p><formula xml:id="formula_2">H = ? G (X), T = ? T (C), ? = norm(H), T = norm(T)<label>(3)</label></formula><p>We utilize the graph encoder, denoted as ? G , to take the graph G(V, E, A, X) as input and generates structure-level graph representations. We employ a text encoder, such as a transformer or Bert, denoted as ? T , to encode the raw textual contents C associated with the nodes. This step produces encoded text representations of nodes. We further apply row-wise L2 normalization using the norm function. Formally, the text-structure alignment with the cross-modalities is conducted as follows:</p><formula xml:id="formula_3">? ? = (?<label>(1)</label></formula><p>? ( ?)?</p><p>(2)</p><formula xml:id="formula_4">? ( T) ? ) ? exp(?) L = ?? ? 1 2 ? ? (CE(? ? , y) + CE(? ? ? , y))<label>(4)</label></formula><p>In our text-graph grounding, we use the label y = (0, 1, ? ? ? , ? -1) ? for the contrastive alignment objective. The transformation functions for different grounding designs are denoted as ? </p><p>? . We employ a graph transformer <ref type="bibr" target="#b60">[61]</ref> as the graph encoder and a vanilla transformer <ref type="bibr" target="#b37">[38]</ref> as the text encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-Stage Graph Instruction Tuning</head><p>The dual-stage graph instruction tuning paradigm proposed in this work builds upon the concept of instruction tuning, which has been recently introduced to enhance the adaptability of language models for specific domains <ref type="bibr" target="#b44">[45]</ref>. In this paradigm, we aim to align the language capacity of the model with the nuances of graph learning tasks, enabling the language model to generate more accurate and contextually appropriate responses for graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Self-Supervised Instruction Tuning. In the first stage of our graph instruction tuning paradigm, we introduce self-supervised instruction tuning. This mechanism allows us to inject graph domainspecific structural knowledge into the language model, improving its reasoning abilities and enabling it to effectively comprehend the contextual information embedded in the graph's structure. To achieve this, we incorporate self-supervised signals derived from unlabeled graph structures as instructions for model tuning. In particular, we design a structure-aware graph matching task that guides the language model in distinguishing between different graph tokens using natural language tokens. This instruction task plays a crucial role in accurately associating graph tokens with their corresponding textual descriptions, thereby deepening the model's understanding of the graph with the provided guidance.</p><p>Instruction Design. The instruction for our graph matching task consists of three components: i) graph information, ii) human question, and iii) GraphGPT response. In this task, we treat each node in the graph as a central node and perform h-hops with random neighbor sampling, resulting in a subgraph structure. The natural language input for the LLM is the human question. In the context of the graph matching task, the instruction includes the indicator token &lt;graph&gt; and a shuffled list of node text information. For example, in a citation graph, the node text information corresponds to paper titles. The objective of the LLM in the graph matching task is to align each graph token with its corresponding node text </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Tokens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Information Encoding</head><p>Given a sequence of graph tokens &lt;Graph&gt;? Here is a list of node text: &lt;NodeTexts&gt; Please reorder the list of texts according to the order of graph tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Instruct</head><p>Based on the information, we obtain the matching as follows: Graph token 1 corresponds to... Graph token 2 corresponds to? Graph token 3 corresponds to? LLM Response Given a sequence of graph tokens &lt;Graph&gt;. The first token represents the central node of the subgraph. The remaining represent the first and second order neighbors... &lt;NodeTexts&gt; Which category does this node belong to? Please think in a step-by-step manner and provide your reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Instruct</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLM Response</head><p>To determine the categorization, we consider the specific topics in the text. First, it involves? Second, there is evidence that? Finally, this node is about?, which can be categorized into? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Instruction Tuning Task-Specific Instruction Tuning</head><p>[cls]</p><p>[eos]</p><p>Alignment Projector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Tokens</head><p>[Instruct]</p><p>[NodeText]</p><p>[Graph]</p><p>[Instruct]</p><p>Text Attributes Cardiovascular complications are the primary? Tuning Strategy. To optimize the tuning process efficiently, we propose a strategy that incorporates a Lightweight Alignment Projector. During training, we keep the parameters of both the LLM and the graph encoder fixed, focusing solely on optimizing the parameters of the projector ? P . After training, we assume that the projector has successfully learned to map the encoded graph representation to graph tokens, while the LLM excels at aligning these tokens with diverse node text information. To align the graph tokens with the language tokens, we employ a projector ? P , which can be as simple as a single linear layer. This projector establishes the correspondence between the graph tokens and the language tokens. By replacing the indicator token &lt;graph&gt; in the original nature language token sequence, the aligned graph tokens create a modified token sequence for the large language model. This modified sequence, denoted as {&lt;graph_begin&gt;, &lt;graph_token&gt; 1 , ? ? ? , &lt;graph_token&gt; ? , &lt;graph_end&gt;}, corresponds to the number of nodes ? in the graph associated with the given prompt. Given that the graph matching process is unsupervised, we have the opportunity to leverage a vast amount of unlabeled graph data from different domains, to enhance the generalizability of the learned projector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Task-Specific Instruction Tuning. In the second stage, we propose task-specific instruction tuning. This step is designed to customize the model's reasoning behavior to meet the specific constraints and requirements of different graph learning tasks, such as node classification or link prediction. By fine-tuning the LLM using task-specific graph instructions, we guide the model to generate responses that are better suited for the particular graph learning task at hand. This further improves the model's adaptability and performance in handling various graph learning tasks.</p><p>Instruction Design. We adopt a similar instruction template, which consists of three parts. To generate graph information for each node, we employ the same neighbor sampling approach used in the first stage. This approach ensures that relevant graph information is captured, with each node acting as the central node. For the node classification task, the human question instruction contains both the indicator token &lt;graph&gt; and specific text information about the central node. This instruction prompts the language model to predict the category of the central node based on both the graph structure data and the accompanying text information. An example of the instruction data for different tasks can be seen in Figure <ref type="figure">4</ref>, providing a visual representation of how the instruction is structured and presented to the language model. Tuning Strategy. In the second stage of training, we utilize the parameters of the structure-aware projector that were trained in the first stage as the initial state. This allows us to conduct instruction tuning specifically for downstream tasks. During this training process, we keep the parameters of the language model (LLM) and graph encoder fixed, focusing solely on optimizing the parameters of the projector from the previous stage. By doing so, we ensure that the LLM further aligns with the requirements of downstream tasks, enhancing its ability to comprehend and interpret graph structures.</p><p>After completing the two training stages as described above, we have confidence that our GraphGPT has acquired the capability to comprehend the given graph structure and perform downstream tasks on the provided graph. The training process involving instruction tuning and the freezing of specific model parameters has refined the model's understanding of graph structures, enabling it to effectively tackle various tasks associated with the given graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chain-of-Thought (CoT) Distillation</head><p>When faced with diverse graph data, language models may encounter new or unfamiliar patterns and structures. This distribution shift can pose challenges in generating accurate and coherent responses, especially when the number of node classes varies across different types of graph data. To address this challenge and boost accuracy in the presence of distribution shift, it is essential to equip our GraphGPT with step-by-step reasoning abilities. In this regard, we propose utilizing the Chain-of-Thought (COT) technique <ref type="bibr" target="#b46">[47]</ref>, which explicitly models the flow of thoughts and reasoning steps. By incorporating COT, our language model improves the coherence and consistency of generated text. It enables the model to follow a logical progression of ideas, enhancing its ability to understand and reason about the given graph data. However, incorporating the Chain-of-Thought (COT) technique can be challenging due to the influence of model parameter scale <ref type="bibr" target="#b31">[32]</ref>.</p><p>To overcome this, we draw inspiration from previous research <ref type="bibr" target="#b31">[32]</ref> and employ a distillation approach to extract valuable knowledge from a closed-source, powerful language model like ChatGPT (with over 200 billion parameters). This enables us to generate highquality COT instructions and enhance our model's COT reasoning capabilities while avoiding an increase in parameters.</p><p>COT Distillation Paradigm. Our approach involves designing tailored Chain-of-Thought (COT) prompts specifically for nodespecific tasks. For the node classification task within a citation graph, we provide the abstract, title of the paper represented by the node, and a description of the classification task as part of the input. We then employ the GPT-3.5 language model (LLM) in our implementation to perform step-by-step reasoning. We prompt the LLM to arrive at the final answer by engaging in a sequential thought process. In the generated output, the LLM not only provides predictions for the node classes but also offers detailed explanations for each prediction. This ensures that the model's reasoning and decision-making process are transparent and comprehensible. To further improve the performance, we integrate the generated COT instruction data with the previously designed instructions for the task-specific instruction tuning stage. With the integrated instructions, we proceed with the proposed instruction tuning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We conduct experiments to validate the effectiveness of our framework in various settings and address key research questions.</p><p>? RQ1: How does the proposed GraphGPT framework perform in both supervised and zero-shot graph learning settings? ? RQ2: What is the generalization ability of our model in handling multiple tasks without experiencing catastrophic forgetting? ? RQ3: What is the contribution of various key components in the proposed GraphGPT framework to its overall performance? ? RQ4: How scalable and efficient is our GraphGPT framework?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Data Descriptions. We evaluate the performance of our GraphGPT using three datasets: OGB-arxiv, PubMed, and Cora. The OGB-arxiv dataset <ref type="bibr" target="#b12">[13]</ref> represents a directed graph that captures the citation network among computer science arXiv papers indexed by MAG <ref type="bibr" target="#b40">[41]</ref>. Each paper in the dataset is associated with a research category, manually labeled by the authors and arXiv moderators. These research categories are selected from a set of 40 subject areas. The PubMed dataset <ref type="bibr" target="#b8">[9]</ref> consists of 19,717 scientific publications on diabetes obtained from the PubMed database. The publications are categorized into Experimental induced diabetes, Type 1 diabetes, and Type 2 diabetes. Additionally, the dataset includes a citation network with 44,338 links. The Cora dataset <ref type="bibr" target="#b47">[48]</ref> comprises 25,120 research papers connected through citations. We utilize an expanded version of the Cora dataset, which is larger and has more classes (70 in total) compared to previous versions <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Protocols.</head><p>To ensure compatibility and enable comparison across different datasets, we map the node features into the same vector space. We achieve this by encoding the raw text information using a pre-trained BERT model <ref type="bibr" target="#b3">[4]</ref>. In our experiments, we divide the Cora and PubMed datasets into three parts: training, validation, and testing, following a ratio of 3:1:1. This partitioning scheme aligns with the approaches described in the works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48]</ref>. For the OGB-arxiv dataset, we adhere to the public split setting <ref type="bibr" target="#b12">[13]</ref>, which employs a ratio of 6:2:3 for training, validation, and testing. To evaluate the performance of our model, we utilize three commonly adopted evaluation metrics: Accuracy and Macro F1 for node classification, and AUC (Area Under the Curve) for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baseline Methods.</head><p>In our performance comparison, we consider various state-of-the-art methods for comprehensive evaluation. (i) The first category includes MLP, which employs a Multilayer Perceptron for node representation. (ii) The second category comprises representative graph neural encoders, including Graph-SAGE <ref type="bibr" target="#b7">[8]</ref>, GCN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b38">[39]</ref>, and RevGNN <ref type="bibr" target="#b20">[21]</ref>. (iii) The third category focuses on the self-supervised approach DGI <ref type="bibr" target="#b39">[40]</ref> for graph learning. (iv) The fourth category explores knowledge distillationenhanced GNNs, with GKD <ref type="bibr" target="#b53">[54]</ref> and GLNN <ref type="bibr" target="#b62">[63]</ref> as notable methods. (v). The fifth category showcases recently proposed strong graph transformer networks, with NodeFormer <ref type="bibr" target="#b49">[50]</ref> and DIFFormer <ref type="bibr" target="#b48">[49]</ref> as competitors. (vi) Lastly, we consider open-sourced LLMs, such as Baichuan-7B, vicuna-7B-v1.1, and vicuna-7B-v1.5 as baselines for understanding text-attributed graph data. For more detailed descriptions of the baselines, please refer to the Appendix. 4.1.4 Implementation Details. For our model implementation, we primarily utilize the PyTorch and Transformers libraries. We employ Vicuna-7B-v1.1 and Vicuna-7B-v1.5 as the base models for our approach. The batch size is set to 2 on each GPU, and the learning rate is 2? -3 . We apply a warmup ratio of 3? -2 and set the maximum input length of the Large Language Model (LLM) to 2048. The training process is carried out for 3 epochs. In the stage of task-specific instruction tuning, we explore the performance of the model under different data mixtures by adopting various combinations of instruction data. The hyperparameter settings remain constant, except for the number of training epochs, which is set to 2 in this stage. The alignment projector parameters fine-tuned in the self-supervised instruction tuning stage are used as the initial parameters for the projector in the second tuning stage. For the evaluation of most baselines, we utilize their publicly available code. We employ a grid-search strategy based on their default hyperparameter settings to ensure a comprehensive evaluation. For further implementation details, please refer to our released source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison (RQ1)</head><p>We conduct experiments on the node classification task, evaluating both supervised and zero-shot scenarios. The overall performance is presented in Table <ref type="table" target="#tab_1">1</ref>. Supervised Task Settings: We train the models on a specific dataset and evaluated their performance on the corresponding test set (e.g., training on Arxiv-Arxiv and testing on the Arxiv test set). Zero-Shot Task Settings: We train the models on a specific dataset and test them on other datasets without any additional training (e.g., training on Arxiv-PubMed and testing on the PubMed dataset). To account for variations in the number of classes across different datasets, we employed a classifier trained with transfer data, typically a linear layer, when testing GNN-based models. In Table <ref type="table" target="#tab_1">1</ref>, "-7B-" represents the parameter scale, while "-v1.1-" and "-v1.5-" indicate different versions of the base Vicuna model. "-stage2" indicates that only the second stage tuning is adopted. "-std" and "-cot" denote the use of the standard and generated COT instruction datasets, respectively.</p><p>Obs.1: Overall Superiority of our GraphGPT. Our graph LLM consistently outperforms various state-of-the-art baselines in both supervised and zero-shot scenarios. Notably, even recently developed strong GNN-based models, such as NodeFormer, DIFFormer, and GKD, exhibit good structural modeling capabilities in the supervised setting. However, when transferred to new datasets without further training, their performance significantly declines. In contrast, our GraphGPT not only surpasses all state-of-the-art methods in supervised tasks but also achieves a remarkable 2-10 times increase in accuracy in the zero-shot graph learning scenario. Additionally, LLM-based solutions, like Baichuan-7B and Vicuna-7B maintain stable performance across different datasets. However, they are limited to making predictions solely based on text information. In contrast, our GraphGPT effectively preserves graph structured information, offers a more comprehensive solution for graph learning tasks. These improvements can be attributed to two key factors: i) Through our dual-stage graph instruction tuning, our method aligns the graph tokens, which contain rich structural information encoded by the graph encoder, with the natural language tokens. This alignment allows the LLM to retain and understand the inherent structural characteristics of the graph data. ii) Our framework facilitates mutual enhancement between the graph encoder and LLM. The introduction of graph tokens fills the gap in the LLM's structural understanding, enabling it to incorporate and reason about the graph's structural information.</p><p>Obs.2: Benefits with Structure-aware Graph Matching. The presence of the first stage, which involves self-supervised graph matching tasks for instruction tuning, plays a crucial role in enhancing the zero-shot transferability of our GraphGPT. The first stage focuses on aligning the graph tokens, which encode rich structural information, with the language tokens. This alignment enables the model to develop a deeper understanding of the inherent structural characteristics of the graph data. Without the first stage, if we only conduct the second stage of task-specific instruction tuning, the model tends to be more prone to overfitting on the specific dataset. In such cases, the model's performance may be heavily reliant on dataset-specific patterns and characteristics, rather than a genuine understanding of the underlying graph structure. This can limit the model's ability to generalize to new, unseen datasets.</p><p>Obs.3: Benefits with COT Distillation. The "-std" and "-cot" variants indicate that the use of COT distillation substantially benefits more complex graph learning tasks. Models tuned with the standard instruction dataset can already achieve prominent results when transferred to simpler tasks, such as the PubMed dataset with 3 classes, with an accuracy of 0.7011 for Arxiv-PubMed. However, their performance tends to be mediocre when applied to complex tasks like the Cora dataset with 70 classes. By leveraging the powerful reasoning capabilities of the closed-source model (GPT-3.5) through COT distillation, our model can integrate this knowledge and significantly enhance its performance on complex graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalization Ability Investigation (RQ2)</head><p>In this subsection, we explore the generalization ability of our model by incorporating more instruction data to fine-tune the LLM for effectively handling various types of tasks. Our main results and experimental observations are presented as follows:</p><p>More Data Boost Model Transfer Ability. In our preliminary investigation, we examine the influence of data quantity on the transfer capability of our GraphGPT, as illustrated in the "(Arxiv + PubMed)-Cora" column of Table <ref type="table" target="#tab_1">1</ref>. In this experiment, we train models using a combination of the Arxiv and PubMed datasets and perform zero-shot testing on the Cora dataset. The results reveal that by incorporating a relatively smaller PubMed dataset (with 20,000+ items) alongside Arxiv, our GraphGPT exhibits a significant improvement in transfer performance on Cora. In contrast, the transfer performance of GNN-based models, trained separately on Arxiv and PubMed, actually deteriorates.</p><p>More Data Yet No Forgetting. We further validate the performance of the combined Arxiv and PubMed instruction data on the original Arxiv data, as demonstrated in the "(Arxiv + PubMed)-Arxiv" column in Table <ref type="table" target="#tab_1">1</ref>. The results indicate that most traditional GNN-based approaches experience a significant decline in performance on Arxiv after iterative training. In contrast, our model exhibits improved performance. We attribute this phenomenon to the occurrence of catastrophic forgetting in GNN-based models, where the structural modeling competence of the model trained solely on the smaller PubMed dataset is compromised. However, our model effectively mitigates this issue through our unified graph instruction tuning paradigm. This enables our model to maintain and even enhance its performance by retaining the generalized graph structure patterns despite incorporating additional data.</p><p>Generalization for Multitasking Graph Learner. Recent studies on instruction tuning suggest that mixing different instruction tuning data can further enhance the performance of Language and Logic Models (LLMs). In this study, we ensure a consistent number of instruction entries and mix different types of instruction data, including standard instruction ("-std"), COT instruction ("-cot"), a blend of standard (50%) and COT (50%) instruction ("-mix"), and link prediction instruction ("Link"). The results are presented in Tables 2 and Table <ref type="table" target="#tab_6">7</ref> in Appendix. We can observe that effective data mixture solutions can significantly improve the performance of our GraphGPT under various settings. The addition of task-specific instruction for link prediction task notably enhances the performance of our model in node classification. Interestingly, after incorporating node classification, the performance of link prediction also exceeds that of the selected best-performing existing models. After mixing the instructions of different tasks, our model demonstrates the ability to effectively handle various graph learning tasks and transfer its knowledge to other unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Module Ablation Study (RQ3)</head><p>We conduct an ablation study to investigate the individual contributions of different sub-modules of our proposed framework, and the results are reported in Table <ref type="table" target="#tab_4">4</ref>. The observations are as follows:</p><p>Effect of Graph Instruction Tuning. In our study, we investigate the benefit of incorporating graph structural information into LLM using the variant "w/o GS." In this variant, we directly adopt the base LLM (specifically, Vicuna-7B-v1.5) to perform node classification on three datasets, without incorporating graph structural information. The results of our study demonstrate that our model significantly outperforms the base model that lacks structural information. This indicates that our graph instruction tuning paradigm enables the LLM to understand the graph structural information more effectively. Importantly, this improvement in performance was achieved without altering the original parameters of the LLM. Instead, it was solely accomplished through our lightweight alignment projector, which aligns graph tokens and natural language tokens through the 1-linear projection operation.</p><p>Effect of LLM-enhanced Semantic Reasoning. We conduct further investigations to assess the influence of the LLM's reasoning ability in our GraphGPT by performing supervised and zero-shot predictions using only the default graph encoders. This variant is denoted as "w/o LR". The results of our study indicate that our GraphGPT, which integrates the LLM, significantly enhances the performance of graph encoder, especially in the zero-shot setting.   This suggests that the rich semantic information injected by the LLM provides a substantial gain in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Efficiency Study (RQ4)</head><p>The study aims to assess the computational efficiency of our model during both the model training and inference stages.</p><p>Training Efficiency with Graph Instruction Tuning. Our instruction tuning framework follows a two-stage process where the parameters of both the LLM and the graph encoder are frozen, and only the graph-text alignment projector is tuned. We conduct a comparison between freezing and tuning the LLM parameters in a 4-card 40G Nvidia A100 environment, denoted by "-freeze" and "-tune" respectively. The study analyze the time and space efficiency in terms of training time, the number of tuned parameters, and GPU occupancy (MiB per GPU). Under the same experimental conditions, when tuning LLM parameters, we encounter Out of Memory (OOM) errors even with a batch size of 1. However, by utilizing our tuning strategy, the training process remains stable even with a batch size of 2. Moreover, the number of tuned parameters decreases by more than 50 times compared to the freezing stage.</p><p>Model Inference Efficiency. In our exploration, we assess the inference speed and accuracy of our GraphGPT by comparing it with baichuan-7B, vicuna-7B-v1.1, and vicuna-7B-v1.5 LLMs. Using a single 40G Nvidia A100, we measure inference time (seconds per response) on the Arxiv and Cora COT instruction datasets, as shown in Figure <ref type="figure" target="#fig_6">5</ref>. Our graph LLM demonstrates superior efficiency and accuracy. Lower inference time doesn't necessarily mean better performance: baichuan-7B yields quick but often incorrect or irrelevant answers, while vicuna-7B-v1.1 and vicuna-7B-v1.5 require longer, complex reasoning steps for better answers. In contrast, our model achieves accurate predictions through a brief reasoning process, enhancing inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Case Study (RQ5)</head><p>We conduct a detailed analysis of our model's performance in downstream graph learning tasks and compare it to traditional LLMs using different types of instructions. We prompt both ChatGPT and our GraphGPT with Arxiv data, using only node content (title and abstract), node content with text-based graph structure, and our designed graph instruction. The results, presented in Table <ref type="table" target="#tab_5">5</ref> and 10 in the appendix, clearly demonstrate that ChatGPT, despite its massive parameter count (over 200B), struggles to make accurate predictions based solely on node text information or node content with the text-based graph structure. This difficulty is particularly pronounced when dealing with papers that have a high degree of cross-disciplinary characteristics, as illustrated in the example of machine learning and hardware architecture. In contrast, our GraphGPT consistently provides accurate predictions and offers reasonable explanations. This is because our GraphGPT accepts a subgraph structure with 103 nodes, allowing it to extract rich structural information from the neighboring nodes' citation relationships, enabling accurate predictions. Furthermore, we believe that our approach of using graph tokens to represent the graph structure as input to the LLM is more efficient than the natural language solution. In the case of a 103-node subgraph, our GraphGPT only requires 750 tokens to be fed into the LLM, while the text-based method requires 4649 tokens. This significant reduction in token consumption translates to a substantial decrease in training and inference resource requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Self-supervised Learning and Pre-training on Graphs. To enhance the robustness of graph models, self-supervised learning (SSL) has been introduced as a powerful technique <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. It allows GNNs to learn meaningful graph representations without heavily relying on labeled data. The core idea behind self-supervised learning in graph models is to design pretext tasks that leverage the graph's intrinsic properties to generate additional supervision signals <ref type="bibr" target="#b50">[51]</ref>. SSL-enhanced graph learning methods can be broadly classified into two main paradigms: contrastive SSL and generative SSL. In particular, i) Contrastive SSL focuses on learning representations by contrasting positive and negative samples. Notable methods in this domain include GraphCL <ref type="bibr" target="#b58">[59]</ref> and GCA <ref type="bibr" target="#b66">[67]</ref>. Recent advancements in contrastive SSL include automated contrastive augmentation (i.e., JOAO <ref type="bibr" target="#b57">[58]</ref>, AutoGCL <ref type="bibr" target="#b56">[57]</ref>), Hyperbolic-Euclidean due space contrasting (e.g., DSGC <ref type="bibr" target="#b54">[55]</ref>), or communityaware contrastive learning (e.g., gCooL <ref type="bibr" target="#b19">[20]</ref>). ii) Generative SSL, on the other hand, focuses on generating realistic samples that resemble the original graph structures. Recent advancements in this line include GraphMAE <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> for feature masking, and S2GAE <ref type="bibr" target="#b34">[35]</ref>, AutoCF <ref type="bibr" target="#b51">[52]</ref> for reconstructing masked edges as SSL tasks.</p><p>Prompt-Tuning for Graph Neural Networks. Recent efforts in enhancing the generalization capability of graph neural networks (GNNs) have focused on training GNN models in a self-supervised manner, followed by fine-tuning on specific downstream tasks using prompt-tuning techniques <ref type="bibr" target="#b63">[64]</ref>. For example, GPPT <ref type="bibr" target="#b32">[33]</ref> is a transfer learning paradigm, where GNNs are first pre-trained on masked edge prediction and then prompted with token pairs for downstream node classification. GraphPrompt <ref type="bibr" target="#b25">[26]</ref> aims to handle downstream tasks by integrating pre-training and downstream tasks into a unified task template. Additionally, Sun et al. <ref type="bibr" target="#b33">[34]</ref> presents a unified prompt format, reformulates tasks to the graph level, and incorporates meta-learning techniques to improve multi-task </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth Category: cs.LG, Machine Learning</head><p>ChatGPT (Node Content Only): cs.AR, cs.AI, cs.SY, cs.ET, cs.NE. The paper presents a hardware accelerator for deep neural networks (DNNs) that is specifically designed to execute ternary DNNs, which represent weights and activations by signed ternary values. Therefore, the most likely category for this paper is cs.AR (Hardware Architecture), which focuses on the design and analysis of hardware and systems for AI and machine learning applications. The paper also involves the optimization of compute and storage requirements of DNNs, which falls under the category of cs.AI (Artificial Intelligence). Additionally, the paper proposes a programmable, in-memory accelerator, which is a type of embedded system, and therefore, it could be categorized under cs.SY (Systems and Control). Furthermore, the paper discusses the use of ternary values for DNNs, which is an emerging technique to optimize the compute and storage requirements of DNNs, and therefore, it could be categorized under cs.ET (Emerging Technologies). Lastly, the paper proposes a new bit-cell called Ternary Processing Cells (TPCs), which could be categorized under cs.NE (Neural and Evolutionary Computing).</p><p>ChatGPT (Node Content and Text-based Graph Structure): Based on the title "TiM-DNN: Ternary In-Memory Accelerator for Deep Neural Networks", the paper is likely to belong to the following arXiv CS sub-categories: 1. cs.AR (Hardware Architecture): The paper proposes a hardware accelerator, TiM-DNN, specifically designed for executing ternary deep neural networks. This falls under the domain of hardware architecture research. 2. cs.NE (Neural and Evolutionary Computing): The paper focuses on accelerating deep neural networks, which is a topic within the field of neural and evolutionary computing. 3. cs.CV (Computer Vision and Pattern Recognition): Deep neural networks are widely used in computer vision tasks, and the TiM-DNN accelerator could potentially be applied to accelerate computer vision algorithms. 4. cs.SY (Systems and Control): The TiM-DNN accelerator is a system-level solution for accelerating deep neural networks, so it could be categorized under systems and control research. 5. cs.DC (Distributed, Parallel, and Cluster Computing): While the paper does not explicitly mention distributed or parallel computing, the TiM-DNN accelerator is designed to optimize the compute and storage requirements of deep neural networks, which are often executed on distributed or parallel computing systems.</p><p>GraphGPT: Based on the title and abstract of the paper, we can identify the following arXiv CS sub-categories that are most likely to be relevant: ??????????? 1. cs.LG (Machine Learning): This sub-category is relevant because the paper discusses the use of ternary DNNs for language modeling and image classification, which are both tasks that involve learning. 2. cs.AI (Artificial Intelligence): This sub-category is relevant because the paper proposes a new accelerator for DNNs, which is designed to optimize the compute and storage requirements of complex DNNs. 3. cs.NE (Numerical Analysis): This sub-category is relevant because the paper discusses the use of ternary DNNs, which involve signed ternary values, and the use of TiM tiles, which are specialized memory arrays that perform massively parallel signed vector-matrix multiplications on ternary values. 4. cs.SY (Systems and Control Theory): This sub-category is relevant because the paper discusses the implementation of TiM-DNN in 32nm technology using an architectural simulator calibrated with SPICE simulations and RTL synthesis. 5. cs.AR (Hardware Architecture): This sub-category is relevant because the paper proposes a new accelerator for DNNs, which is designed to be programmable and in-memory. performance in graph prompting. Despite these advances, these methods still require further fine-tuning that relies on supervision labels from downstream tasks to ensure accurate learning. In contrast, this work addresses this limitation by introducing a foundational graph model that tackles the more challenging task of zero-shot graph learning. By eliminating the need for label inputs from downstream tasks, this approach allows for a more general and flexible graph learning paradigm in real-world scenarios.</p><p>large Language Models. In recent years, LLMs (e.g., ChatGPT <ref type="bibr" target="#b28">[29]</ref> and Claude <ref type="bibr" target="#b0">[1]</ref>) have gained widespread attention for their remarkable capabilities in various NLP tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46]</ref>. Based on these unique capabilities of LLMs, many tuning-free prompting techniques have been explored to enhance their generative abilities, such as incontext learning <ref type="bibr" target="#b27">[28]</ref> and Chain-of-Thought <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref>. With the rise of open-source LLMs, such as Llama <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, ChatGLM <ref type="bibr" target="#b61">[62]</ref>, and Baichuan <ref type="bibr" target="#b52">[53]</ref>, technologies for aligning pre-trained LLMs to different specific tasks and human feedback have been proposed, making private LLMs in specific domains possible <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>While there have been successful attempts to align LLMs with visual information, such as multimodal LLMs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b65">66]</ref>, the alignment of LLMs with graph structures remains largely unexplored. This research addresses this gap by introducing a dual-stage graph instruction tuning paradigm that effectively aligns the language capacity of LLMs with graph learning. Previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> have attempted to incorporate graph information into LLMs using natural language, but they have faced challenges in handling complex graph structures and achieving a deep understanding of graphs due to the limitations of relying solely on text-based prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This work presents an effective and scalable graph large language model, aims at improving the generalization capabilities of graph models. The proposed framework, GraphGPT, injects graph domainspecific structural knowledge into the LLM through a dual-stage graph instruction tuning paradigm. By leveraging a simple yet effective graph-text alignment projector, we enable LLMs to comprehend and interpret the structural components of graphs. Extensive evaluations across different settings demonstrate the effectiveness of our method in both supervised and zero-shot graph learning scenarios. Furthermore, the model exhibits strong generalization abilities, allowing it to handle diverse downstream datasets and tasks without suffering from catastrophic forgetting. A potential avenue for future investigation is exploring pruning techniques to compress redundant or less important parameters of LLM, thereby reducing the overall model size while preserving its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In the appendix, we provide a summary of the statistical information of our datasets in Section A.1. Furthermore, in Section A.2, we offer detailed descriptions of the baseline methods used for comparison in our experiments. To facilitate the understanding of the instruction templates used for different tasks and datasets, we present them in Section A.4. These templates serve as guidelines for formulating instructions that are specific to the task and dataset at hand. In Section A.3, we present the implementation details of the text-graph grounding process. Finally, in Section A.5, we provide supplementary experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Statistical information of datasets</head><p>The detailed statistical information of datasets is shown in Table <ref type="table">6</ref>.</p><p>Table <ref type="table">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Matching</head><p>Input: Given a sequence of graph tokens &lt;graph&gt; that constitute a subgraph of a citation graph, where the first token represents the central node of the subgraph, and the remaining nodes represent the first and second order neighbors of the central node. Each graph token contains the title and abstract information of the paper at this node. Here is a list of paper titles: {1. Paper 1. 2. Paper 2. ... n. Paper n}, please reorder the list of papers according to the order of graph tokens (i.e., complete the matching of graph tokens and papers).</p><p>Output: Based on the given graph tokens and the list of paper titles, we obtain the matching of graph tokens and papers as follows: {Graph token 1 corresponds to Paper Q, Graph token 2 corresponds to Paper P, ... Graph token n corresponds to Paper R}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Detailed Descriptions of Baselines</head><p>For a comprehensive performance comparison, our baseline set consists of the following methods which are presented below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Conventional Deep Learning Methods</head><p>? MLP: This method employs a multilayer perceptron to independently encode different node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Graph Neural Encoders</head><p>? GraphSAGE <ref type="bibr" target="#b7">[8]</ref>: It is a framework designed for inductive representation learning on large graphs, enabling the efficient generation of node embeddings for previously unseen data.</p><p>? GCN <ref type="bibr" target="#b16">[17]</ref>: This method extends the solution of convolutional neural networks to model graph-structured features.   Input: Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least likely, in the form "cs.XX". Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.</p><p>Output: Based on the information, {reasoning process and answers}.</p><p>Arxiv in the standard manner)</p><p>Input: Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, and other nodes are its one-hop or multi-hop neighbors, with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. Question: Which arXiv CS sub-category does this paper belong to? Give the most likely arXiv CS sub-categories of this paper directly, in the form "cs.XX" with full name of the category.</p><p>Output: {the ground-truth answer}.</p><p>Node Classification (PubMed in the COT manner)</p><p>Input: Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, with the following information: Abstract: {abstract of the central node}. Output: {the ground-truth answer}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details of Text-Graph Grounding</head><p>For the implementation details of Text-Graph Grounding 3.1, we follow previous work <ref type="bibr" target="#b47">[48]</ref>, aligning text features and graph features across three dimensions. Mathematically, these dimensions can be represented by three sets of transformation equations:</p><formula xml:id="formula_6">? (1) 1 ( ?) = { ?? , 1 ? ? ? ? }, ?<label>(2)</label></formula><formula xml:id="formula_7">1 ( T) = { T? , 1 ? ? ? ? } ? (1) 2 ( ?) = { ?? , 1 ? ? ? ? }, ?<label>(2)</label></formula><formula xml:id="formula_8">2 ( T) = { 1 |N ? | ?? ? ? N ? T? , 1 ? ? ? ? } ? (1) 3 ( ?) = { T? , 1 ? ? ? ? }, ?<label>(2)</label></formula><formula xml:id="formula_9">3 ( T) = { 1 |N ? | ?? ? ? N ? T? , 1 ? ? ? ? } (5)</formula><p>where ? ? R ? ?? denotes graph representations and T ? R ? ?? indicates text representations, and ? is the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Instruction Templates across Tasks/Data</head><p>We provide instruction templates for different tasks (graph matching, node classification, and link prediction) on various datasets in our experiment. The templates are shown in Table <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_9">9</ref>. While the same template is used for paper citation graphs, we design specific templates for each data in the downstream tasks, including both ChatGPT (Node Content Only): Based on the title and abstract of the paper, we can make an educated guess about the likely arXiv CS sub-categories that this paper belongs to. Here is a step-by-step breakdown of the reasoning: 1. The paper mentions "cloud-enabled big data storage solutions" as the focus of investigation. This suggests that the paper is related to cloud computing and big data. 2. The paper also mentions "cyber forensics" and "investigation process" in the context of cloud-enabled big data platforms. This indicates that the paper is specifically about the forensic analysis of data stored in cloud-enabled big data platforms. Based on these observations, here are five likely arXiv CS sub-categories for this paper: 1. cs.DC (Distributed, Parallel, and Cluster Computing) -This sub-category covers topics related to distributed computing systems, which includes cloud computing. 2. cs.DB (Databases) -This sub-category covers topics related to database systems, which are often used in big data storage solutions. 3. cs.CR (Cryptography and Security) -This sub-category covers topics related to cyber security, which includes forensic analysis of digital evidence. 4. cs.SE (Software Engineering) -This sub-category covers topics related to software development and engineering, which may include the development of cloud-enabled big data storage solutions. 5. cs.CY (Computers and Society) -This sub-category covers topics related to the impact of computing on society, which may include the ethical and legal implications of forensic analysis in cloud-enabled big data storage.</p><p>ChatGPT (Node Content and Text-based Graph Structure): Based on the title of the paper "Greening Cloud Enabled Big Data Storage Forensics: Syncany as a Case Study" we can make an initial assumption that the paper belongs to the "Information Retrieval" (cs.IR) sub-category. However, to determine the most likely arXiv CS sub-categories, we need to analyze the content and context of the paper. 1. cs.DB (Databases): The paper focuses on cloud-enabled big data storage, which is closely related to databases. It discusses the identification, collection, analysis, preservation, and archiving of digital evidences in the context of Syncany private cloud storage service. This suggests that the paper may belong to the cs.DB sub-category. 2. cs.CR (Cryptography and Security): The paper addresses the forensic investigation of cloud-enabled big data storage platforms, which involves analyzing and recovering residual artifacts of criminal activities. This indicates a strong connection to the field of cybersecurity and digital forensics, which falls under the cs.CR sub-category. 3. cs.SE (Software Engineering): The paper may also belong to the cs.SE sub-category as it discusses the investigation process and the challenges involved in analyzing cloud-enabled big data storage platforms. It explores techniques and methodologies for locating and documenting residual artifacts of forensic value, which are relevant to the field of software engineering. 4. cs.CY (Computers and Society): The paper touches upon the societal impact of cloud-enabled big data storage and the need for efficient forensic investigation techniques. It highlights the potential reduction in time and resources spent on real-world investigations, which aligns with the concerns and issues addressed in the cs.CY sub-category. 5. cs.DC (Distributed, Parallel, and Cluster Computing): The paper mentions the decentralization of digital evidence acquisition in the context of BitTorrent Sync. This suggests a connection to distributed computing and parallel processing, which are topics covered in the cs.DC sub-category. Based on the above analysis, the likely arXiv CS sub-categories for the paper, ordered from most to least likely, are: cs.DB, cs.CR, cs.SE, cs.CY, cs.DC.</p><p>GraphGPT: Based on the title and abstract of the paper, we can infer that the paper is related to cyber forensics and cloud-enabled big data storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Node</head><p>We can then proceed to categorize it based on the sub-categories of arXiv CS. 1. cs.CR: This category is for cryptography research, which involves the study of cryptographic algorithms, protocols, and systems. While the paper does not explicitly mention cryptography, it does involve the analysis of data remnants on a cloudenabled big data platform, which could potentially involve cryptographic techniques. 2. cs.DS: This category is for database systems, which involves the design, implementation, and management of databases. The paper mentions the use of a popular storage engine for big data platforms, which suggests that the paper could be related to database systems. 3. cs.IR: This category is for information retrieval, which involves the study of how to effectively retrieve and organize information from large collections of data. The paper's focus on identifying and documenting residual artifacts of forensic value on a cloud-enabled big data platform suggests that it could be related to information retrieval. 4. cs.IT: This category is for information technology, which involves the study of the design, development, and use of information technology systems. The paper's focus on cloud-enabled big data storage and forensics suggests that it could be related to information technology. 5. cs.SY: This category is for systems and networking, which involves the study of computer systems and networks. The paper's focus on the analysis of data remnants on a cloud-enabled big data platform suggests that it could be related to systems and networking. standard and COT (Chain-of-Thought) instructions. The &lt;graph&gt; token serves as an indicator, and ... denotes fillable blanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Supplementary Experimental Results</head><p>In this subsection, we analyze the link prediction performance using different instruction mixtures. We then compare the prediction results of our GraphGPT and ChatGPT under different instruction formats, including node content only, node content with text-based graph structure, and our custom-designed graph instruction. A.5.1 Performance of Link Prediction. Four different instruction mixtures are compared: i) COT instruction data for node classification only (w/o Link) ii) Instruction data for link prediction only (only Link) iii) A mix of standard instruction data for node classification and instruction data for link prediction (Arxiv-std + PubMed-std + Link) iv) A mix of 50% standard instruction for node classification, 50% COT instruction data for node classification, and instruction for link prediction (Arxiv-mix + PubMed-mix + Link). When compared to representative baselines, as presented in Table <ref type="table" target="#tab_6">7</ref>, the combination of standard instruction data for node classification and instruction for link prediction significantly outperformed the baselines. Furthermore, as mentioned earlier in Table <ref type="table" target="#tab_2">2</ref>, this particular variant outperformed state-of-the-art approaches in node classification. This improvement can be attributed to the task of link prediction, which enhances our GraphGPT's understanding of graph structural information and facilitates node classification tuning. Importantly, the inclusion of node classification instructions does not impede the model's comprehension of link prediction.</p><p>A.5.2 Comparison Between ChatGPT and GraphGPT. Table 10 shows more comparison cases between ChatGPT and GraphGPT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Limitation of LLMs in understanding graph structural contexts with heavy reliance on textual data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The overall architecture of our proposed GraphGPT with graph instruction tuning paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Graph Information: &lt;graph&gt;: Central Node: 2 ,Figure 4 :</head><label>24</label><figDesc>Figure 4: Our instruction designs for graph matching task (upper), node classification (middle) and link prediction (lower).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Inference efficiency study of our GraphGPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6 )</head><label>6</label><figDesc>Large Language Models ? Baichuan-7B [53]: It is an open-source, large-scale pre-trained model with 7 billion parameters. It is specifically designed to be bilingual, supporting both Chinese and English languages. This model has been trained extensively on a diverse range of data. ? vicuna-7B-v1.1 [3]: It is an open-source chatbot that has undergone fine-tuning using user-shared conversations collected from ShareGPT. The base model used for this fine-tuning is Llama-1, a reference to a specific model architecture. ? vicuna-7B-v1.5 [3]: This model is an enhanced iteration of vicuna-7b-v1.1, building upon Llama-2 as its base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of various methods on node classification under both supervised and zero-shot settings.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Arxiv-Arxiv</cell><cell cols="2">Arxiv-PubMed</cell><cell cols="2">Arxiv-Cora</cell><cell cols="4">(Arxiv+PubMed)-Cora (Arxiv+PubMed)-Arxiv</cell></row><row><cell>Model</cell><cell cols="2">Accuracy Macro-F1</cell><cell>acc</cell><cell cols="7">Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell>MLP</cell><cell>0.5179</cell><cell>0.2536</cell><cell>0.3940</cell><cell>0.1885</cell><cell>0.0258</cell><cell>0.0037</cell><cell>0.0220</cell><cell>0.0006</cell><cell>0.2127</cell><cell>0.0145</cell></row><row><cell>GraphSAGE</cell><cell>0.5480</cell><cell>0.3290</cell><cell>0.3950</cell><cell>0.1939</cell><cell>0.0328</cell><cell>0.0132</cell><cell>0.0132</cell><cell>0.0029</cell><cell>0.1281</cell><cell>0.0129</cell></row><row><cell>GCN</cell><cell>0.5267</cell><cell>0.3202</cell><cell>0.3940</cell><cell>0.1884</cell><cell>0.0214</cell><cell>0.0088</cell><cell>0.0187</cell><cell>0.0032</cell><cell>0.0122</cell><cell>0.0008</cell></row><row><cell>GAT</cell><cell>0.5332</cell><cell>0.3118</cell><cell>0.3940</cell><cell>0.1884</cell><cell>0.0167</cell><cell>0.0110</cell><cell>0.0161</cell><cell>0.0057</cell><cell>0.1707</cell><cell>0.0285</cell></row><row><cell>RevGNN</cell><cell>0.5474</cell><cell>0.3240</cell><cell>0.4440</cell><cell>0.3046</cell><cell>0.0272</cell><cell>0.0101</cell><cell>0.0217</cell><cell>0.0016</cell><cell>0.1309</cell><cell>0.0126</cell></row><row><cell>DGI</cell><cell>0.5059</cell><cell>0.2787</cell><cell>0.3991</cell><cell>0.1905</cell><cell>0.0205</cell><cell>0.0011</cell><cell>0.0205</cell><cell>0.0011</cell><cell>0.5059</cell><cell>0.2787</cell></row><row><cell>GKD</cell><cell>0.5570</cell><cell>0.1595</cell><cell>0.3645</cell><cell>0.2561</cell><cell>0.0470</cell><cell>0.0093</cell><cell>0.0406</cell><cell>0.0037</cell><cell>0.2089</cell><cell>0.0179</cell></row><row><cell>GLNN</cell><cell>0.6088</cell><cell>0.3757</cell><cell>0.4298</cell><cell>0.3182</cell><cell>0.0267</cell><cell>0.0115</cell><cell>0.0182</cell><cell>0.0092</cell><cell>0.3373</cell><cell>0.1115</cell></row><row><cell>NodeFormer</cell><cell>0.5922</cell><cell>0.3328</cell><cell>0.2064</cell><cell>0.1678</cell><cell>0.0152</cell><cell>0.0065</cell><cell>0.0144</cell><cell>0.0053</cell><cell>0.2713</cell><cell>0.0855</cell></row><row><cell>DIFFormer</cell><cell>0.5986</cell><cell>0.3355</cell><cell>0.2959</cell><cell>0.2503</cell><cell>0.0161</cell><cell>0.0094</cell><cell>0.0100</cell><cell>0.0007</cell><cell>0.1637</cell><cell>0.0234</cell></row><row><cell>baichuan-7B</cell><cell>0.0946</cell><cell>0.0363</cell><cell>0.4642</cell><cell>0.3876</cell><cell>0.0405</cell><cell>0.0469</cell><cell>0.0405</cell><cell>0.0469</cell><cell>0.0946</cell><cell>0.0363</cell></row><row><cell>vicuna-7B-v1.1</cell><cell>0.2657</cell><cell>0.1375</cell><cell>0.5251</cell><cell>0.4831</cell><cell>0.1090</cell><cell>0.0970</cell><cell>0.1090</cell><cell>0.0970</cell><cell>0.2657</cell><cell>0.1375</cell></row><row><cell>vicuna-7B-v1.5</cell><cell>0.4962</cell><cell>0.1853</cell><cell>0.6351</cell><cell>0.5231</cell><cell>0.1489</cell><cell>0.1213</cell><cell>0.1489</cell><cell>0.1213</cell><cell>0.4962</cell><cell>0.1853</cell></row><row><cell>GraphGPT-7B-v1.1-cot</cell><cell>0.4913</cell><cell>0.1728</cell><cell>0.6103</cell><cell>0.5982</cell><cell>0.1145</cell><cell>0.1016</cell><cell>0.1250</cell><cell>0.0962</cell><cell>0.4853</cell><cell>0.2102</cell></row><row><cell>GraphGPT-7B-v1.5-stage2</cell><cell>0.7511</cell><cell>0.5600</cell><cell>0.6484</cell><cell>0.5634</cell><cell>0.0813</cell><cell>0.0713</cell><cell>0.0934</cell><cell>0.0978</cell><cell>0.6278</cell><cell>0.2538</cell></row><row><cell>GraphGPT-7B-v1.5-std</cell><cell>0.6258</cell><cell>0.2622</cell><cell>0.7011</cell><cell>0.6491</cell><cell>0.1256</cell><cell>0.0819</cell><cell>0.1501</cell><cell>0.0936</cell><cell>0.6390</cell><cell>0.2652</cell></row><row><cell>GraphGPT-7B-v1.5-cot</cell><cell>0.5759</cell><cell>0.2276</cell><cell>0.5213</cell><cell>0.4816</cell><cell>0.1813</cell><cell>0.1272</cell><cell>0.1647</cell><cell>0.1326</cell><cell>0.6476</cell><cell>0.2854</cell></row><row><cell>p-val</cell><cell>2.26? -9</cell><cell cols="2">1.56? -10 2.22? -7</cell><cell>1.55? -9</cell><cell>1.04? -9</cell><cell>9.96? -6</cell><cell>7.62? -8</cell><cell>1.97? -7</cell><cell>1.5e -13</cell><cell>4.63? -6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of various instruction mixtures in supervised learning on the Arxiv dataset and the zero-shot setting on the Cora dataset for node classification.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Supervision. on Arxiv Zero Shot on Cora</cell></row><row><cell>Model</cell><cell>Acc</cell><cell>Macro-F1</cell><cell>Acc</cell><cell>Macro-F1</cell></row><row><cell>MLP</cell><cell>0.5179</cell><cell>0.2536</cell><cell>0.0220</cell><cell>0.0006</cell></row><row><cell>GraphSAGE</cell><cell>0.5480</cell><cell>0.3290</cell><cell>0.0132</cell><cell>0.0029</cell></row><row><cell>GCN</cell><cell>0.5267</cell><cell>0.3202</cell><cell>0.0187</cell><cell>0.0032</cell></row><row><cell>GAT</cell><cell>0.5332</cell><cell>0.3118</cell><cell>0.0161</cell><cell>0.0057</cell></row><row><cell>RvGNN</cell><cell>0.5474</cell><cell>0.3240</cell><cell>0.0217</cell><cell>0.0016</cell></row><row><cell>DGI</cell><cell>0.5059</cell><cell>0.2787</cell><cell>0.0205</cell><cell>0.0011</cell></row><row><cell>GKD</cell><cell>0.5570</cell><cell>0.1595</cell><cell>0.0406</cell><cell>0.0037</cell></row><row><cell>GLNN</cell><cell>0.6088</cell><cell>0.3757</cell><cell>0.0182</cell><cell>0.0092</cell></row><row><cell>NodeFormer</cell><cell>0.5922</cell><cell>0.3328</cell><cell>0.0144</cell><cell>0.0053</cell></row><row><cell>DIFFormer</cell><cell>0.5986</cell><cell>0.3355</cell><cell>0.0100</cell><cell>0.0007</cell></row><row><cell>baichuan-7b</cell><cell>0.0946</cell><cell>0.0363</cell><cell>0.0405</cell><cell>0.0469</cell></row><row><cell>vicuna-7B-v1.1</cell><cell>0.2657</cell><cell>0.1375</cell><cell>0.1090</cell><cell>0.0970</cell></row><row><cell>vicuna-7B-v1.5</cell><cell>0.4962</cell><cell>0.1853</cell><cell>0.1489</cell><cell>0.1213</cell></row><row><cell>Arxiv-std + PubMed-std</cell><cell>0.6390</cell><cell>0.2652</cell><cell>0.1501</cell><cell>0.0936</cell></row><row><cell>Arxiv-cot + PubMed-cot</cell><cell>0.6476</cell><cell>0.2854</cell><cell>0.1647</cell><cell>0.1326</cell></row><row><cell>Arxiv-mix + PubMed-mix</cell><cell>0.6139</cell><cell>0.2772</cell><cell>0.1544</cell><cell>0.1048</cell></row><row><cell>Arxiv-std + PubMed-std + Link</cell><cell>0.5931</cell><cell>0.2238</cell><cell>0.1847</cell><cell>0.1579</cell></row><row><cell cols="2">Arxiv-mix + Pubmed-mix + Link 0.6874</cell><cell>0.3761</cell><cell>0.1836</cell><cell>0.1494</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Module ablation study under both supervised and zero-shot settings to analyze the individual contributions.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Arxiv-Arxiv</cell><cell cols="2">Arxiv-PubMed</cell><cell cols="2">Arxiv-Cora</cell></row><row><cell>Variant</cell><cell>Acc</cell><cell>Mac-F1</cell><cell>Acc</cell><cell>Mac-F1</cell><cell>Acc</cell><cell>Mac-F1</cell></row><row><cell>w/o GS</cell><cell>0.4962</cell><cell>0.1853</cell><cell>0.6351</cell><cell>0.5231</cell><cell>0.1489</cell><cell>0.1213</cell></row><row><cell>w/o LR</cell><cell>0.5807</cell><cell>0.2462</cell><cell>0.2523</cell><cell>0.1925</cell><cell>0.0050</cell><cell>0.0016</cell></row><row><cell>ours</cell><cell>0.6258</cell><cell>0.2622</cell><cell>0.7011</cell><cell>0.6491</cell><cell>0.1813</cell><cell>0.1272</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Study on the time and space efficiency of our GraphGPT during both the training and inference stages.</figDesc><table><row><cell>Variants</cell><cell cols="3">Training Time Tuned Parameters GPU Occupy</cell></row><row><cell>Stage-1-tune</cell><cell>OOM</cell><cell>6,607,884,288</cell><cell>OOM</cell></row><row><cell cols="2">Stage-1-freeze 22:53:33</cell><cell>131,612,672</cell><cell>39517.75</cell></row><row><cell>improvement</cell><cell>-</cell><cell>? ? 50.21</cell><cell>-</cell></row><row><cell>Stage-2-tune</cell><cell>OOM</cell><cell>6,607,884,288</cell><cell>OOM</cell></row><row><cell cols="2">Stage-2-freeze 03:44:35</cell><cell>131,612,672</cell><cell>38961.75</cell></row><row><cell>improvement</cell><cell>-</cell><cell>? ? 50.21</cell><cell>-</cell></row><row><cell cols="2">baichuan vicuna-v1.1vicuna-v1.5 ours Arxiv-Arxiv</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of prediction results between our GraphGPT and ChatGPT. TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks. (with 102 first-order and second-order neighbors)</figDesc><table /><note><p>Title:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>: Statistical information of datasets. Performance comparison of various instruction mixtures for link prediction on the PubMed dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Nodes # Edges # Classes</cell></row><row><cell cols="3">ogb-arxiv 169,343 1,166,243</cell><cell>40</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell></row><row><cell>Cora</cell><cell>25,120</cell><cell>182,280</cell><cell>70</cell></row><row><cell cols="2">Dataset</cell><cell cols="2">PubMed</cell></row><row><cell></cell><cell>Model</cell><cell>AUC</cell><cell>AP</cell></row><row><cell></cell><cell>MLP</cell><cell cols="2">0.5583 0.5833</cell></row><row><cell></cell><cell>GAT</cell><cell cols="2">0.5606 0.6373</cell></row><row><cell cols="2">GraphSAGE</cell><cell cols="2">0.5041 0.5813</cell></row><row><cell cols="2">RevGNN</cell><cell cols="2">0.4538 0.5083</cell></row><row><cell cols="2">Node2Vec</cell><cell cols="2">0.6535 0.6885</cell></row><row><cell cols="2">w/o Link</cell><cell cols="2">0.5010 0.5005</cell></row><row><cell cols="2">only Link</cell><cell cols="2">0.6704 0.6087</cell></row><row><cell cols="4">Arxiv-std + PubMed-std + Link 0.8246 0.8026</cell></row><row><cell cols="4">Arxiv-mix + PubMed-mix + Link 0.6451 0.5886</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Instruction template for the graph matching task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Instruction template for node classification and link prediction on different datasets.</figDesc><table><row><cell>Node Classification (Arxiv in the COT manner)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Title: {title of the central node}. Question: Does the paper involve any cases of Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes? Please give one or more answers of either Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes; if multiple options apply, provide a comma-separated list ordered from most to least related. Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning for each choice. Based on the information, {reasoning process and answers}. Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, and other nodes are its one-hop or multi-hop neighbors, with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. Question: Which case of Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes does this paper involve? Please give one answer of either Type 1 diabetes, Type 2 diabetes, or Experimentally induced diabetes directly. Given a sequence of graph tokens: &lt;graph&gt; that constitute a subgraph of a citation graph, where the first token represents the central node of the subgraph, and the remaining nodes represent the first and second order neighbors of the central node. The information of the central node is as follow: Abstract: {abstract of the central node}. Title: {title of the central node}. The other sequence of graph tokens: &lt;graph&gt;, where the first token (the central node) with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. If the connections between nodes represent the citation relationships between papers, are these two central nodes connected? Give me a direct answer of "yes" or "no".</figDesc><table><row><cell>Node Classification (PubMed in the standard manner)</cell></row><row><cell>Input: Link Prediction (PubMed)</cell></row><row><cell>Input:</cell></row></table><note><p><p><p><p><p><p><p>Output:</p>Output: {the ground-truth answer}.</p>Node Classification (Cora in the COT manner) Input: Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. Question: Which of the following subcategories of computer science does this paper belong to: {1. Categories 1. 2. Categories 2. ... n. Categories n}? Give 5 likely categories as a comma-separated list ordered from most to least likely. Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning for each choice.</p>Output: Based on the information, {reasoning process and answers}.</p>Node Classification (Cora in the standard manner)</p>Input: Given a citation graph: &lt;graph&gt; where the 0th node is the target paper, with the following information: Abstract: {abstract of the central node}. Title: {title of the central node}. Question: Which of the following subcategories of computer science does this paper belong to: {1. Categories 1. 2. Categories 2. ... n. Categories n}? Directly give the full name of the most likely category of this paper.</p>Output: {the ground-truth answer}.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of our GraphGPT's predictions with ChatGPT on a arxiv paper (Continued). Greening Cloud-Enabled Big Data Storage Forensics: Syncany as a Case Study. (with 41 first-order and second-order neighbors) Ground-Truth Category: cs.CR, Cryptography and Security</figDesc><table /><note><p>Title:</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<idno>CoRR abs/2212.08073</idno>
		<title level="m">Constitutional AI: Harmlessness from AI Feedback</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</title>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/2307.03393</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-03-30-vicuna/" />
		<title level="m">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking</title>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengyu</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/2305.15066</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based Molecular Representation Learning</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhao</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roshni</surname></persName>
		</author>
		<author>
			<persName><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<title level="m">Explanations as Features: LLM-Based Features for Text-Attributed Graphs. CoRR abs/2305</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page">19523</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphmae: Self-supervised masked graph autoencoders</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>WWW. ACM / IW3C2</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hdmi: High-order deep multiplex infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large Language Models are Zero-Shot Reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</title>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<idno>CoRR abs/2309.00267</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph communal contrastive learning</title>
		<author>
			<persName><forename type="first">Bolian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno>WWW. 1203-1213</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training Graph Neural Networks with 1000 Layers</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6437" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Resource-Efficient Training for Large Graph Convolutional Networks with Label-Centric Cumulative Sampling</title>
		<author>
			<persName><forename type="first">Mingkai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanglu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1170" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Visual Instruction Tuning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5879" to="5900" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery</title>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Moretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14356" to="14364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graphprompt: Unifying pre-training and downstream tasks for graph neural networks</title>
		<author>
			<persName><forename type="first">Zemin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingtong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
	<note>In WWW</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-Weight Graph Neural Network: Push the Limits Beyond Global Homophily</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1270" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11048" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting</title>
		<author>
			<persName><forename type="first">Zezhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1567" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distilling Reasoning Capabilities into Smaller Language Models</title>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7059" to="7073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gppt: Graph pre-training and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All in One: Multi-Task Prompting for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2023. S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">LLaMA: Open and Efficient Foundation Language Models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><surname>Baptiste Rozi?re</surname></persName>
		</author>
		<idno>CoRR abs/2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Llama 2: Open Foundation and Fine-Tuned Chat Models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<idno>CoRR abs/2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. In ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft Academic Graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Sci. Stud</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Intents behind Interactions with Knowledge Graph for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Raghavi Chandu</surname></persName>
		</author>
		<idno>CoRR abs/2306.04751</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-Instruct: Aligning Language Models with Self-Generated Instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emergent Abilities of Large Language Models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<idno>CoRR abs/2306.08385</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Simgrace: A simple framework for graph contrastive learning without data augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<idno>WWW. 1070-1079</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automated Self-Supervised Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Baichuan 2: Open Large-scale Language Models</title>
		<author>
			<persName><forename type="first">Aiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/2309.10305</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Geometric Knowledge Distillation: Topology Compression for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dual space graph contrastive learning</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<idno>WWW. 1238-1247</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno>CoRR abs/2305.10601</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Autogcl: Automated graph contrastive learning via learnable view generators</title>
		<author>
			<persName><forename type="first">Yihang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingzhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="8892" to="8900" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GLM-130B: An Open Bilingual Pre-trained Model</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation</title>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>WWW. 2581-2590</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust Self-Supervised Structural Graph Neural Network for Social Network Prediction</title>
		<author>
			<persName><forename type="first">Yanfu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<title level="m">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW. 2069-2080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
