<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Network Detection of Data Sequences in Communication Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Nariman</forename><surname>Farsad</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Andrea</forename><surname>Goldsmith</surname></persName>
						</author>
						<title level="a" type="main">Neural Network Detection of Data Sequences in Communication Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFAF2D9756FFD1FEA0BF1EB4FE2D13EA</idno>
					<idno type="DOI">10.1109/TSP.2018.2868322</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSP.2018.2868322, IEEE Transactions on Signal Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSP.2018.2868322, IEEE Transactions on Signal Processing RNN Layer 1 RNN Layer 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>deep learning</term>
					<term>supervised learning</term>
					<term>communication systems</term>
					<term>detection</term>
					<term>optical communication</term>
					<term>free-space optical communication</term>
					<term>molecular communication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider detection based on deep learning, and show it is possible to train detectors that perform well without any knowledge of the underlying channel models. Moreover, when the channel model is known, we demonstrate that it is possible to train detectors that do not require channel state information (CSI). In particular, a technique we call a sliding bidirectional recurrent neural network (SBRNN) is proposed for detection where, after training, the detector estimates the data in realtime as the signal stream arrives at the receiver. We evaluate this algorithm, as well as other neural network (NN) architectures, using the Poisson channel model, which is applicable to both optical and molecular communication systems. In addition, we also evaluate the performance of this detection method applied to data sent over a molecular communication platform, where the channel model is difficult to model analytically. We show that SBRNN is computationally efficient, and can perform detection under various channel conditions without knowing the underlying channel model. We also demonstrate that the bit error rate (BER) performance of the proposed SBRNN detector is better than that of a Viterbi detector with imperfect CSI as well as that of other NN detectors that have been previously proposed. Finally, we show that the SBRNN can perform well in rapidly changing channels, where the coherence time is on the order of a single symbol duration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O NE of the important modules in reliable recovery of data sent over a communication channel is the detection algorithm, where the transmitted signal is estimated from a noisy and corrupted version observed at the receiver. The design and analysis of this module has traditionally relied on mathematical models that describe the transmission process, signal propagation, receiver noise, and many other components of the system that affect the end-to-end signal transmission and reception. Most communication systems today convey data by embedding it into electromagnetic (EM) signals, which lend themselves to tractable channel models based on a simplification of Maxwell's equations. However, there are cases where tractable mathematical descriptions of the channel are elusive, either because the EM signal propagation is very complicated or when it is poorly understood. In addition, there are communication systems that do not use EM wave signalling and the corresponding communication channel models may be unknown or mathematically intractable. Some examples of the latter are underwater communication using Nariman Farsad and Andrea Goldsmith are with the Department of Electrical Engineering, Stanford University, Stanford, CA, 94305. Emails: nfarsad@stanford.edu, andrea@wsl.stanford.edu.</p><p>This work was funded by the NSF Center for Science of Information grant NSF-CCF-0939370, and ONR grant N00014-18-1-2191.</p><p>acoustic signals <ref type="bibr" target="#b0">[1]</ref> as well as molecular communication, which relies on chemical signals to interconnect tiny devices with sub-millimeter dimensions in environments such as inside the human body <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>Even when the underlying channel models are known, since the channel conditions may change with time, many model-based detection algorithms rely on the estimation of the instantaneous channel state information (CSI) (i.e., channel model parameters) for detection. Typically, this is achieved by transmitting and receiving a predesigned pilot sequence, which is known by the receiver, for estimating the CSI. However, this estimation process entails overhead that decreases the data transmission rate. Moreover, the accuracy of the estimation may also affect the performance of the detection algorithm.</p><p>In this paper, we investigate how different techniques from artificial intelligence and deep learning <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> can be used to design detection algorithms for communication systems that learn directly from data. We show that these algorithms are robust enough to perform detection under changing channel conditions, without knowing the underlying channel models or the CSI. This approach is particularly effective in emerging communication technologies, such as molecular communication, where accurate models may not exist or are difficult to derive analytically. For example, tractable analytical channel models for signal propagation in molecular communication channels with multiple reactive chemicals have been elusive <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>Some examples of machine learning tools applied to design problems in communication systems include multiuser detection in code-division multiple-access (CDMA) systems <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>, decoding of linear codes <ref type="bibr" target="#b15">[16]</ref>, design of new modulation and demodulation schemes <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, detection and channel decoding <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b23">[24]</ref>, and estimating channel model parameters <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. A recent survey of machine learning techniques applied to communication systems can be found in <ref type="bibr" target="#b26">[27]</ref>. The approach taken in most of these previous works was to use machine learning to improve one component of the communication system based on the knowledge of the underlying channel models.</p><p>Our approach is different from prior works since we assume that the mathematical models for the communication channel are completely unknown. This is motivated by the recent success in using deep neural networks (NNs) for end-to-end system design in applications such as image classification <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, speech recognition <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, machine translation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and bioinformatics <ref type="bibr" target="#b34">[35]</ref>. For example, Figure <ref type="figure" target="#fig_0">1</ref> highlights some of the similarities between speech recognition, where deep NNs have been very successful at improving the detector's performance, and digital communication systems for wireless and molecular channels. As indicated in the figure, for speech processing, the transmitter is the speaker, the transmission symbols are words, and the carrier signal is acoustic waves. At the receiver the goal of the detection algorithm is to recover the sequence of transmitted words from the acoustic signals that are received by the microphone. Similarly, in communication systems, such as wireless or molecular communications, the transmitted symbols are bits and the carrier signals are EM waves or chemical signals. At the receiver the goal of the detection algorithm is to detect the transmitted bits from the received signal. One important difference between communication systems and speech recognition is the size of transmission symbol set, which is significantly larger for speech.</p><p>Motivated by this similarity, in this work we investigate how techniques from deep learning can be used to train a detection algorithm from samples of transmitted and received signals. We demonstrate that, using known NN architectures such as a recurrent neural network (RNN), it is possible to train a detector without any knowledge of the underlying system model. In this approach, the receiver goes through a training phase where a NN detector is trained using known transmission signals.</p><p>We also propose a real-time NN sequence detector, which we call the sliding bidirectional RNN (SBRNN) detector, that detects the symbols corresponding to a data stream as they arrive at the destination. We demonstrate that if the SBRNN detector or the other NN detectors considered in this work are trained using a diverse dataset that contains sequences transmitted under different channel conditions, the detectors will be robust to changing channel conditions, eliminating the need for instantaneous CSI estimation for the specific channels considered in this work.</p><p>At first glance, the training phase in this approach may seem like an extra overhead. However, if the underlying channel models are known, then the models could be used offline to generate training data under a diverse set of channel conditions. We demonstrate that using this approach, it is possible to train our SBRNN algorithm such that it would not require any instantaneous CSI. Another important benefit of using NN detectors in general is that they return likelihoods for each symbol. These likelihoods can be fed directly from the detector into a soft decoding algorithm such as the belief propagation algorithm without requiring a dedicated module to convert the detected symbols into likelihoods.</p><p>To evaluate the performance of NN detectors, we first use the Poisson channel model, a common model for optical channels and molecular communication channels <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b40">[41]</ref>. We use this model to compare the performance of the NN detection to the Viterbi detector (VD). We show that for channels with long memories the SBRNN detection algorithm is computationally more efficient than the VD. Moreover, the VD requires CSI estimation, and its performance can degrade if this estimate is not accurate, while the SBRNN detector can perform detection without the CSI, even in a channel with changing conditions. We show that the bit error rate (BER) performance of the proposed SBRNN is better than the VD with CSI estimation error and it outperforms other wellknown NN detectors such as the RNN detector. As another performance measure, we use the experimental data collected by the molecular communication platform presented in <ref type="bibr" target="#b41">[42]</ref>. The mathematical models underlying this experimental platform are currently unknown. We demonstrate that the proposed SBRNN algorithm can be used to train a sequence detector directly from limited measurement data. We also demonstrate that this approach perform significantly better than the detector used in previous experimental demonstrations <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, as well as other NN detectors.</p><p>The rest of the paper is organized as follows. In Section II we present the problem statement. Then, in Section III, detection algorithms based on NNs are introduced including the newly proposed SBRNN algorithm. The Poisson channel model and the VD are introduced in Section IV. The performance of the NN detection algorithms are evaluated using this channel model and are compared against the VD in Section V. In Section VI, the performance of NN detection algorithms are evaluated using a small data set that is collected via an </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM STATEMENT</head><p>In a digital communication system data is converted into a sequence of symbols for transmission over the channel. This process is typically carried out in two steps: in the first step, source coding is used to compress or represent the data using symbols or bits; in the second step, channel coding is used to introduce extra redundant symbols to mitigate the errors that may be introduced as part of the transmission and reception of the data <ref type="bibr" target="#b44">[45]</ref>. Let S = {s 1 , s 2 , • • • , s m } be the finite set of symbols that could be sent by the transmitter, and x k ∈ S be the k th symbol that is transmitted. The channel coding can be designed such that the individual symbols in a long sequence are drawn according to the probability mass function (PMF) P X (x).</p><p>The signal that is observed at the destination is noisy and corrupted due to the perturbations introduced as part of transmission, propagation, and reception processes. We refer to these three processes collectively as the communication channel or simply the channel. Let the random vector y k of length be the observed signal at the destination during the k th transmission. Note that the observed signal y k is typically a vector while the transmitted symbol x k is typically a scalar. A detection algorithm is then used to estimate the transmitted symbols from the observed signal at the receiver. Let xk be the symbol that is estimated for the k th transmitted symbol x k . After detection, the estimated symbols are passed to a channel decoder to correct some of the errors in detection, and then to a source decoder to recover the data. All the components of a communication system, shown in Figure <ref type="figure">2</ref>, are designed to ensure reliable data transfer.</p><p>Typically, to design these modules, mathematical channel models are required, which describe the relationship between the transmitted symbols and the observed signal through</p><formula xml:id="formula_0">P model (y 1 , y 2 , • • • | x 1 , x 2 , • • • ; Θ),<label>(1)</label></formula><p>where Θ are the model parameters. Some of these parameters can be static (constants that do not change with channel conditions) and some of them can dynamically change with channel conditions over time. In this work, model parameters are considered to be the parameters that change with time. Hence, we use the terms model parameter and instantaneous CSI interchangeably. Using this model, the detection can be performed through symbol-by-symbol detection, where xk is estimated from y k , or using sequence detection where the sequence</p><formula xml:id="formula_1">xk , xk-1 , • • • , x1 is estimated from the sequence y k , y k-1 , • • • , y 1 1 .</formula><p>As an example, for a simple channel with no intersymbol interference (ISI), given by the channel model P model (y k | x k ; Θ), and a known PMF for the transmission symbols P X (x), a maximum a posteriori estimation (MAP) algorithm can be devised as</p><formula xml:id="formula_2">xk = arg max x∈S P model (y k | x; Θ)P X (x).<label>(2)</label></formula><p>Therefore for detection, both the model and the parameters of the model Θ, which may change with time, are required. For this reason, many detection algorithms periodically estimate the model parameters (i.e., the CSI) by transmitting known symbols and then using the observed signals at the receiver for CSI estimation <ref type="bibr" target="#b45">[46]</ref>. This extra overhead leads to a decrease in the data rate. One way to avoid CSI estimation is by using blind detectors. These detectors typically assume a particular probability distribution over Θ, and perform the detection without estimating the instantaneous CSI at the cost of higher probability of error. However, estimating the joint distribution over all model parameters Θ can also be difficult, requiring a large amount of measurement data under various channel conditions. One of the problems we consider in this work is whether NN detectors can learn this distribution during training, or learn to simultaneously estimate the CSI and detect the symbols. This approach results in a robust detection algorithm that performs well under different and changing channel conditions without any knowledge of the channel models or their parameters.</p><p>When the underlying channel models do not lend themselves to computationally efficient detection algorithms, or are partly or completely unknown, the best approach to designing detection algorithms is unclear. For example, in communication channels with memory, the complexity of the optimal VD increases exponentially with memory length, and quickly becomes infeasible for systems with long memory. Note that the VD also relies on the knowledge of the channel model in terms of its input-output transition probability. As another example, tractable channel models for molecular communication channels with multiple reactive chemicals are unknown <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. We propose that in these scenarios, a data driven approach using deep learning is an effective way to train detectors to determine the transmitted symbols directly using known transmission sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DETECTION USING DEEP LEARNING</head><p>Estimating the transmitted symbol from the received signals can be performed using NN architectures through supervised learning. This is achieved in two phases. First, a training dataset is used to train the NN offline. Once the network is trained, it can be deployed and used for detection. Note that the training phase is performed once offline, and therefore, it is not part of the detection process after deployment. We start this section by describing the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training the Detector</head><p>Let m = |S| be the cardinality of the symbol set, and let p k be the one-of-m representation of the symbol transmitted during the k th transmission, given by</p><formula xml:id="formula_3">p k = 1(x k = s 1 ), 1(x k = s 2 ), • • • , 1(x k = s m ) ,<label>(3)</label></formula><p>where 1(.) is the indicator function. Therefore, the element corresponding to the symbol that is transmitted is 1, and all other elements of p k are 0. Note that this is also the PMF of the transmitted symbol during the k th transmission where, at the transmitter, with probability 1, one of the m symbols is transmitted. Also note that the length of the vector p k is m, which may be different from the length of the vector of the observation signal y k at the destination.</p><p>The detection algorithm goes through two phases. In the first phase, known sequences of symbols from S are transmitted repeatedly and received by the system to create a set of training data. The training data can be generated by selecting the transmitted symbols randomly according to a PMF, and generating the corresponding received signal using mathematical models, simulations, experimental measurements, or field measurements. Let </p><formula xml:id="formula_4">P K = [p 1 , p 2 , • • • , p K ]</formula><formula xml:id="formula_5">{(P (1) K1 , Y (1) K1 ), (P (2) K2 , Y (2) K2 ), • • • , (P (n) Kn , Y (n) Kn )},<label>(4)</label></formula><p>which consists of n training samples, where the i th sample has K i consecutive transmissions. This dataset is then used to train a deep NN classifier that maps the received signal y k to one of the transmission symbols in S. The input to the NN can be the raw observed signals y k , or a set of features r k extracted from the received signals.</p><p>The NN outputs are the vectors pk = NN(y k ; W), where W are the parameters of the NN. Using the above interpretation of p k as a probability vector, pk are the estimations of the probability of x k given the observations and the parameters of the NN. Note that this output is also useful for soft decision channel decoders (i.e., decoders where the decoder inputs are PMFs), which are typically the next module after detection as shown in Figure <ref type="figure">2</ref>. If channel coding is not used, the symbol is estimated using xk = arg max x k ∈S pk .</p><p>During the training, known transmission sequences of symbols are used to find the optimal set of parameters for the NN W * such that</p><formula xml:id="formula_6">W * = arg min W L (p k , pk ), (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where L is the loss function. This optimization algorithm is typically solved using the training data, variants of stochastic gradient decent, and back propagation <ref type="bibr" target="#b6">[7]</ref>. Since the output of the NN is a PMF, the cross-entropy loss function can be used for this optimization <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_8">L cross = H(p k , pk ) = H(p k ) + D KL (p k pk ) ,<label>(6)</label></formula><p>where H(p k , pk ) is the cross entropy between the correct PMF and the estimated PMF, and D KL (. .) is the Kullback-Leibler divergence <ref type="bibr" target="#b46">[47]</ref>. Note that minimizing the loss is equivalent to minimizing the cross-entropy or the Kullback-Leibler divergence distance between the true PMF and the one estimated based on the NN. It is also equivalent to maximizing the log-likelihoods. Therefore, during the training, known transmission data are used to train a detector that maximizes log-likelihoods. Using Bayes' theorem, it is easy to show that minimizing the loss is equivalent to maximizing <ref type="bibr" target="#b1">(2)</ref>. We now discuss how several well-known NN architectures can be used for symbol-by-symbol detection and for sequence detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Symbol-by-Symbol Detectors</head><p>The most basic NN architecture that can be employed for detection uses several fully connected NN layers followed by a final softmax layer <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The input to the first layer is the observed signal y k or the feature vector r k , which is selectively extracted from the observed signal through preprocessing. The output of the final layer is of length m (i.e., the cardinality the symbol set), and the activation function for the final layer is the softmax activation. This ensures that the output of the layer pk is a PMF. Figure <ref type="figure" target="#fig_1">3(a)</ref> shows the structure of this NN.</p><p>A more sophisticated class of NNs that is used in processing complex signals such as images is a convolution neural network (CNN) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Essentially, the CNN is a set of filters that are trained to extract the most relevant features for detection from the received signal. The final layer in the CNN detector is a dense layer with output of length m, and a softmax activation function. This results in an estimate pk from the set of features that are extracted by the convolutional layers in the CNN. <ref type="bibr">Figure 3(b)</ref> shows the structure of this NN.</p><p>For symbol-by-symbol detection the estimated PMF pk is given by</p><formula xml:id="formula_9">xk = [PNN(xk = s 1 |y k ), P NN (x k = s 2 |y k ), • • • , P NN (x k = sm|y k )] ,<label>(7)</label></formula><p>where P NN is the probability of estimating each symbol based on the NN model used. The better the structure of the NN at capturing the physical channel characteristics based on P model in ( <ref type="formula" target="#formula_0">1</ref>), the better this estimate and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sequence Detectors</head><p>The symbol-by-symbol detector cannot take into account the effects of ISI between symbols 2 . In this case, sequence detection can be performed using recurrent neural networks (RNN) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which are well established for sequence estimation in different problems such as neural machine translation <ref type="bibr" target="#b32">[33]</ref>, speech recognition <ref type="bibr" target="#b29">[30]</ref>, or bioinformatics <ref type="bibr" target="#b34">[35]</ref>. The estimated pk in this case is given by</p><formula xml:id="formula_10">pk =      P RNN (x k = s 1 |y k , y k-1 , • • • , y 1 ) P RNN (x k = s 2 |y k , y k-1 , • • • , y 1 )</formula><p>. . .</p><formula xml:id="formula_11">P RNN (x k = s m |y k , y k-1 , • • • , y 1 )      ,<label>(8)</label></formula><p>where P RNN is the probability of estimating each symbol based on the NN model used. In this work, we use long short-term memory (LSTM) networks <ref type="bibr" target="#b49">[50]</ref>, which have been extensively used in many applications. Figure <ref type="figure" target="#fig_1">3</ref>(c) shows the RNN structure. One of the main benefits of this detector is that after training, similar to a symbol-by-symbol detector, it can perform detection on any data stream as it arrives at the receiver. This is because the observations from previous symbols are summarized as the state of the RNN, which is represented by the vector h k . Note that the observed signal during the j th transmission slot, y j where j &gt; k, may carry information about the k th symbol x k due to delays in signal arrival which results in ISI. However, since RNNs are feed-forward only, during the estimation of pk , the observation signal y j is not considered.</p><p>One way to overcome this limitation is by using bidirectional RNNs (BRNNs), where the sequence of received signals are once fed in the forward direction into one RNN cell and once fed in backwards into another RNN cell <ref type="bibr" target="#b50">[51]</ref>. The two outputs are then concatenated and may be passed to more bidirectional layers. Figure <ref type="figure" target="#fig_1">3(d)</ref> shows the BRNN structure. 2 It is possible to use the received signal from multiple symbols as input to a CNN for detection in the presence of ISI. For a sequence of length L, the estimated pk for BRNN is given by</p><formula xml:id="formula_12">! 1 ! # ! $ ! % ! &amp; ! ' ! ( ! ) ! *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRNN</head><formula xml:id="formula_13">pk =      P BRNN (x k = s 1 |y L ,y L-1 ,• • • ,y 1 ) P BRNN (x k = s 2 |y L ,y L-1 ,• • • ,y 1 )</formula><p>. . .</p><formula xml:id="formula_14">P BRNN (x k = s m |y L ,y L-1 ,• • • ,y 1 )      ,<label>(9)</label></formula><p>where k ≤ L. In this work we use the bidirectional LSTM (BLSTM) networks <ref type="bibr" target="#b51">[52]</ref>.</p><p>The BRNN architecture ensures that in the estimation of a symbol, future signal observations are taken into account, thereby overcoming the limitations of RNNs. The main tradeoff is that as signals from a data stream arrive at the destination, the block length L increases, and the whole block needs to be re-estimated again for each new data symbol that is received. Therefore, this quickly becomes infeasible for long data streams as the length of the data stream can be on the order of tens of thousands to millions of symbols. In the next section we present a new technique to solve this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sliding BRNN Detector</head><p>Since the data stream that arrives at the receiver can have any arbitrary length, it is not desirable to detect the whole sequence for each new symbol that arrives, as the sequence length could grow arbitrarily large. Therefore, we fix the maximum length of the BRNN. Ideally, the length must be at least the same size as the memory length of the channel. However, if this is not known in advance, the BRNN length can be treated as a hyperparameter to be tuned during training. Let L be the maximum length of the BRNN. Then during training, blocks of ≤ L consecutive transmissions are used for training. Note that sequences of different lengths could be used during training as long as all sequence lengths are smaller than or equal to L. After training, the simplest scheme would be to detect the stream of incoming data in fixed blocks of length ≤ L as shown in the top portion of Figure <ref type="figure" target="#fig_3">4</ref>. The main drawback here is that the symbols at the end of each block may affect the symbols in the next block, and this relation is not captured in this scheme. Another issue is that consecutive symbols must be received before detection can be performed. The top portion of Figure <ref type="figure" target="#fig_3">4</ref> shows this scheme for = 3.</p><p>To overcome these limitations, inspired by some of the techniques used in speech recognition <ref type="bibr" target="#b52">[53]</ref>, we propose a dynamic programing scheme we call the sliding BRNN (SBRNN) detector. In this scheme the first ≤ L symbols are detected using the BRNN. Then as each new symbol arrives at the destination, the position of the BRNN slides ahead by one symbol. Let the set J k = {j | j ≤ k ∧ j + L &gt; k} be the set of all valid starting positions for a BRNN detector of length L, such that the detector overlaps with the k th symbol. For example, if L = 3 and k = 4, then j = 1 is not in the set J k since the BRNN detector overlaps with symbol positions 1, 2, and 3, and not the symbol position 4. Let p(j) k be the estimated PMF for the k th symbol, when the start of the sliding BRNN is on j ∈ J k . The final PMF corresponding to the k th symbol is given by the weighted sum of the estimated PMFs for each of the relevant windows:</p><formula xml:id="formula_15">pk = 1 |J k | j∈J k p(j) k .<label>(10)</label></formula><p>One of the main benefits of this approach is that, after the first L symbols are received and detected, as the signal corresponding to a new symbol arrives at the destination, the detector immediately estimates that symbol. The detector also updates its estimate for the previous L -1 symbols dynamically. Therefore, this algorithm is similar to a dynamic programming algorithm. The bottom portion of Figure <ref type="figure" target="#fig_3">4</ref> illustrates the sliding BRNN detector. In this example, after the first 3 symbols arrive, the PMF for the first three symbols, i ∈ {1, 2, 3}, is given by pi = p(1) i . When the 4th symbol arrives, the estimate of the first symbol is unchanged, but for i ∈ {2, 3}, the second and third symbol estimates are updated as xi = 1 2 (x</p><p>i ), and the 4th symbol is estimated by p4 = p(2) 4 . Note that although in this paper we assume that the weights of all p(j) k are the same (i.e., 1 |J k | ), the algorithm can use different weights. Moreover, the complexity of the SBRNN increases linearly with the length of the BRNN window, and hence with the memory length.</p><p>To evaluate the performance of all these NN detectors, we use both the Poisson channel model (a common model for optical and molecular communication systems) as well as an experimental platform for molecular communication where the underlying model is unknown <ref type="bibr" target="#b41">[42]</ref>. The sequel discusses more details of the Poisson model and experimental platform, and how they were used for performance analysis of our proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE POISSON CHANNEL MODEL</head><p>The Poisson channel has been used extensively to model different communication systems in optical and molecular communication <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b40">[41]</ref>. In these systems, information is encoded in the intensity of the photons or particles released by the transmitter and decoded from the intensity of photons or particles observed at the receiver. In the rest of this section, we refer to the photons, molecules, or particles simply as particles. We now describe this channel, and a VD for the channel.</p><p>In our model it is assumed that the transmitter uses on-offkeying (OOK) modulation, where the transmission symbol set is S = {0, 1}, and the transmitter either transmits a pulse with a fixed intensity to represent the 1-bit or no pulse to represent the 0-bit. Note that OOK modulation has been considered in many previous works on optical and molecular communication and has been shown to be the optimal input distribution for a large class of Poisson channels <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b55">[56]</ref>. Later in Section V-D, we extend the results to larger symbol sets by considering the general m level pulse amplitude modulation (m-PAM), where information is encoded in m amplitudes of the pulse transmissions. Note that OOK is a special case of this modulation scheme with m = 2.</p><formula xml:id="formula_17">! " ! " ! " ! " ! " ! " ! " ! " ! " ! "</formula><p>Let τ be the symbol interval, and x k ∈ S the symbol corresponding to the k th transmission. We assume that the transmitter can measure the number of particles that arrive at a sampling rate of ω samples per second. Then the number of samples in a given symbol duration is given by a = ωτ , where we assume that a is an integer. Let λ(t) be the system response to a transmission of the pulse corresponding to the 1-bit. For optical channels, the system response is proportional to the Gamma distribution, and given by <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b58">[59]</ref>:</p><formula xml:id="formula_18">λ OP (t) = κ OP β -α t α-1 Γ(α) exp(-t/β) t &gt; 0 0 t ≤ 0 ,<label>(11)</label></formula><p>where κ OP is the proportionality constant, and α and β are parameters of the channel, which can change over time. For molecular channels, the system response is proportional to the inverse Gaussian distribution <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> given by:</p><formula xml:id="formula_19">λ MO (t) = κ MO c 2πt 3 exp -c(t-µ) 2 2µ 2 t t &gt; 0 0 t ≤ 0 , (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where κ MO is the proportionality constant, and c and µ are parameters of the channel, which can change over time.</p><p>Since the receiver samples the data at a rate of ω,</p><formula xml:id="formula_21">for k ∈ N and j ∈ {1, 2, • • • , a}, let λ k [j] λ j + ka ω<label>(13)</label></formula><p>be the average intensity observed during the j th sample of the k th symbol in response to the transmission pulse corresponding to the 1-bit. Figure <ref type="figure" target="#fig_4">5</ref> shows the system response for both optical and molecular channels. Although for optical channels the symbol duration is many orders of magnitude smaller than for molecular channels, the system responses are very similar in shape. Some notable differences are a faster rise time for the optical channel, and a longer tail for the molecular channel.</p><formula xml:id="formula_22">0 1 0 1 1 1 0 0 1 1 1 0 0 0</formula><p>The system responses are used to formulate the Poisson channel model. In particular, the intensity that is observed during the j th sample of the k th symbol is distributed according to</p><formula xml:id="formula_23">y k [j] ∼ P k i=0 x k-i λ i [j] + η ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_24">P(ξ) = ξ y e -ξ y!</formula><p>is the Poisson distribution, and η is the mean of an independent additive Poisson noise due to background interference and/or the receiver noise <ref type="foot" target="#foot_1">3</ref> . Using this model, the signal that is observed by the receiver, for any sequence of bit transmissions, can be generated as illustrated in Figure <ref type="figure" target="#fig_5">6</ref>. This signal has a similar structure to the signal observed using the experimental platform in [43, see Figure <ref type="figure" target="#fig_10">13</ref>], although this analytically-modeled signal exhibits more noise.</p><p>The model parameters (i.e., the CSI) for the Poisson channel model are Θ OP = [α, β, η] and Θ MO = [c, µ, η], respectively for optical and molecular channels. In this work, we assume that the sampling rate ω, and the proportionality constants κ OP and κ MO are fixed and are not part of the model parameters. Note that α and β can change over time due to atmospheric turbulence or mobility. Similarly, c and µ are functions of the distance between the transmitter and the receiver, flow velocity, and the diffusion coefficient, which may change over time, e.g., due to variations in temperature and pressure <ref type="bibr" target="#b4">[5]</ref>. The background noise η may also change with time. Note that although the symbol interval τ may be changed to increase or decrease the data rate, both the transmitter and receiver must agree on the value of τ . Thus, we assume that the value of τ is always known at the receiver, and therefore, it is not part of the CSI. In the next subsection, we present the optimal VD, assuming that the receiver knows all the model parameters Θ OP and Θ MO perfectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Viterbi Detector</head><p>The VD assumes a certain memory length M where the current observed signal is affected only by the past M transmitted symbols. In this case <ref type="bibr" target="#b13">(14)</ref> becomes</p><formula xml:id="formula_25">y k [j] ∼ P x k λ 0 [j] + M l=1 x k-l λ l [j] + η . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>Since the marginal distribution of the j th sample of the k th symbol is Poisson distributed according to <ref type="bibr" target="#b14">(15)</ref>, given the model parameters Θ pois , we have</p><formula xml:id="formula_27">P (y k | x k-M ,x k-M +1 , • • • , x k , Θ pois ) = a j=1 P (y k [j] | x k-M , x k-M +1 , • • • , x k , Θ pois ).<label>(16)</label></formula><p>This is because, given the model parameters as well as the current symbol and the previous M symbols, the samples within the current bit interval are generated independently and distributed according to <ref type="bibr" target="#b14">(15)</ref>. Note that ( <ref type="formula" target="#formula_27">16</ref>) holds only if the memory length M is known perfectly. If the estimate of M is inaccurate, then ( <ref type="formula" target="#formula_27">16</ref>) is also inaccurate.</p><formula xml:id="formula_28">Let V = {v 0 , v 1 , • • • , v 2 M -1</formula><p>} be the set of states in the trellis of the VD, where the state v u corresponds to the previous M transmitted bits [x -M , x -M +1 , • • • , x -1 ] forming the binary representation of u. Let xk , 1 ≤ k ≤ K be the information bits to be estimated. Let V k,u be the state corresponding to the k th symbol interval, where u is the binary representation of</p><formula xml:id="formula_29">[x k-M , xk-M+1 , • • • , xk-1 ]. Let L(V k,u ) denote the log-likelihood of the state V k,u . For a state V k+1,u = [x k-M +1 , xk-M+2 , • • • , xk ], there are two states in the set {V k,i } 2 M -1 i=0</formula><p>that can transition to V k+1,u :</p><formula xml:id="formula_30">u 0 = u 2 ,<label>(17)</label></formula><formula xml:id="formula_31">u 1 = u 2 + 2 M -1 , (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>where . is the floor function. Let the binary vector</p><formula xml:id="formula_33">b u0 = [0, xk-M+1 , xk-M+2 , • • • , xk-1 ]</formula><p>be the binary representation of u 0 and similarly b u1 the binary representation of u 1 .</p><p>The log-likelihoods of each state in the next symbol slot are updated according to</p><formula xml:id="formula_34">L(V k+1,u ) = max[L(V k,u0 ) + L(V k,u0 , V k+1,u ), L(V k,u1 ) + L(V k,u1 , V k+1,u )],<label>(19)</label></formula><p>where L(V k,ui , V k+1,u ), i ∈ {0, 1}, is the log-likelihood increment of transitioning from state V k,ui to V k+1,u . Let</p><formula xml:id="formula_35">Λ ui,u [j] = (u mod 2)λ 0 [j]+ M l=1 b ui [M -l+1]λ l [j]+η. (<label>20</label></formula><formula xml:id="formula_36">)</formula><p>Using the PMF of the Poisson distribution, ( <ref type="formula" target="#formula_25">15</ref>), <ref type="bibr" target="#b15">(16)</ref>, and <ref type="bibr" target="#b19">(20)</ref> we have</p><formula xml:id="formula_37">L(V k,ui , V k+1,u ) = - a j=1 Λ ui,u [j]+ a j=1 log(Λ ui,u [j])y k [j],<label>(21)</label></formula><p>where the extra term -a j=1 log(y k [j]!) is dropped since it will be the same for both transitions from u 0 and u 1 . Using these transition probabilities and setting the L(V 0,0 ) = 0 and L(V 0,u ) = -∞, for u = 0, the most likely sequence xk , 1 ≤ k ≤ K, can be estimated using the Viterbi algorithm <ref type="bibr" target="#b61">[62]</ref>. When the memory length is long, it is not computationally feasible to consider all the states in the trellis as they grow exponentially with memory length. Therefore, in this work we implement the Viterbi beam search algorithm <ref type="bibr" target="#b62">[63]</ref>. In this scheme, at each time slot, only the transition from the previous N states with the largest log-likelihoods are considered. When N = 2 M , the Viterbi beam search algorithm reduces to the traditional Viterbi algorithm.</p><p>We now evaluate the performance of NN detectors using the Poisson channel model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION BASED ON POISSON CHANNEL</head><p>In this section we evaluate the performance of the proposed SBRNN detector based on the Poisson channel model, and in the next section we use the experimental platform developed in <ref type="bibr" target="#b41">[42]</ref> to demonstrate that the SBRNN detector can be implemented in practice to perform real-time detection. The rest of this section is organized as follows. First, we describe the training procedure and the simulation setup in Section V-A. Then, in Section V-B, we evaluate the effects of L max and M , the symbol duration, and noise on the BER performance. In particular, in this section we demonstrate that SBRNN detection is resilient to changes in symbol duration and noise, and outperforms VD with perfect CSI if the memory length M is not estimated correctly. In Section V-C, the performance of the SBRNN detector and VD are evaluated for different channel parameters. To show that the SBRNN algorithm works on larger symbol sets (i.e., higher order modulations), in section V-D we consider an optical channel that uses m-PAM, m &gt; 2, instead of OOK (i.e., 2-PAM). We also demonstrate that although the training is performed on transmission sequences of length 100, the SBRNN can generalize to longer transmission sequences. The effects of the RNN cell type is also evaluated and it is demonstrated that LSTM cells achieve the best BER performance. The performance of the SBRNN in rapidly changing channels is evaluated in Section V-E, and the complexity of this algorithm compared to the VD is discussed in Section V-F. Table I summarizes all the results that will be presented in this section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training and Simulation Procedure</head><p>For evaluating the performance of the SBRNN on the Poisson channel, we consider both the optical channel and the molecular channel. For the optical channel, we assume that the channel parameters are Θ OP = [β, η], and assume α = 2 and κ OP = 10. We use these values for α and κ OP since they resulted in system responses that resembled the ones presented in <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b58">[59]</ref>. For the molecular channel the model parameters are Θ MO = [c, µ, η], and κ MO = 10 4 . The value of κ MO was selected to resemble the system response in <ref type="bibr" target="#b42">[43]</ref>. For the optical channel we use ω = 2 GS/s and for the molecular channel we use ω = 100 S/s.</p><p>For the VD algorithm we consider Viterbi with beam search, where only the top N = 100 states with the largest loglikelihoods are kept in the trellis during each time slot. We also consider two different scenarios for CSI estimation. In the first scenario we assume that the detector estimates the CSI perfectly, i.e., the values of the model parameters Θ OP and Θ MO are known perfectly at the receiver. In practice, it may not be possible to achieve perfect CSI estimation. In the second scenario we consider the VD with CSI estimation error. Let ζ be a parameter in Θ OP or Θ MO . Then the estimate of this parameter is simulated by ζ = ζ + Z, where Z is a zero-mean Gaussian noise with a standard deviation that is 2.5% or 5% of ζ. In the rest of this section, we refer to these cases as the VD with 2.5% and 5% error, and the case with perfect CSI as the VD with 0% error. Table <ref type="table" target="#tab_2">II</ref> shows the BER performance of the VD for different values of N . It can be seen that N = 100, which is used in the rest of this section, is sufficient to achieve good performance with the VD.</p><p>Both the RNN and the SBRNN detectors use LSTM cells <ref type="bibr" target="#b49">[50]</ref>, unless specified otherwise. For the SBRNN, the size of the output is 80. For the RNN, since the SBRNN uses two RNNs, one for the forward direction and one for the backward direction, the size of the output is 160. This ensures that the SBRNN detector and the RNN detector have roughly the same number of parameters. The number of layers used for both detectors in this section is 3. The input to the NNs are a set of normalized features r k extracted from the received signal y k . The feature extraction algorithm is described in the appendix. This feature extraction step normalizes the input, which assists the NNs to learn faster from the data <ref type="bibr" target="#b6">[7]</ref>.</p><p>To train the RNN and SBRNN detectors, transmitted bits are generated at random and the corresponding received signal is generated using the Poisson model in <ref type="bibr" target="#b13">(14)</ref>. In particular, the training data consists of many samples of sequences of 100 consecutively transmitted bits and the corresponding 1053-587X (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p><p>where U (A) indicates uniform distribution over the set A.</p><p>Similarly, for the molecular channel,</p><formula xml:id="formula_39">c ∼ U ({1, 2, • • • , 30}), µ ∼ U ({5, 10, 15, • • • , 65}), η ∼ U ({1, 50, 100, 500, 1k, 5k, 10k, 20k, 30k, 40k, 50k}), τ ∼ U ({0.5, 1, 1.5, 2})(all in s). (<label>23</label></formula><formula xml:id="formula_40">)</formula><p>For the SBRNN training, each 100-bit sequence is randomly broken into subsequences of length L ∼ U ({2, 3, 4, • • • , 50}). For all training, the Adam optimization algorithm <ref type="bibr" target="#b63">[64]</ref> is used with learning rate of 10 -3 , and batch size of 500. We train on 500k sequences of 100 bits.</p><p>Over the next several subsections we evaluate the performance of the SBRNN detector and compare it to that of the VD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effects of Sequence Length, Symbol Duration, and Noise</head><p>First, we evaluate the BER performance with respect to the memory length M used in the VD, and the sequence length L used in the SBRNN. For all the BER performance plots in this section, to calculate the BER, 1000 sequences of 100 random bits are used. Figure <ref type="figure" target="#fig_6">7</ref>(a) shows the results for the optical (top plots) and the molecular (bottom plots) channels with the parameters described above. From the results it is clear that the performance of the VD relies heavily on estimating the memory length of the system correctly. We define the memory length as the number of symbol durations it takes for the impulse response to be sufficiently small such that ISI is negligible or, equivalently, such that increasing the memory length of the detector does not decrease BER significantly. For example, let λ max be the peak value of the impulse response. Let t σ , 0 &lt; σ &lt; 1, be the time it takes for impulse response to fall to σλ max . Then, for the optical channel in Figure <ref type="figure" target="#fig_6">7</ref>(a), the time it takes for the impulse response to fall to 0.01% of λ max is τ 0.0001 = 2.55 µs. Therefore, at a symbol duration of τ = 0.05, the memory length is on the order of M ≈ 51 symbols. From Figure <ref type="figure" target="#fig_6">7</ref>(a) it can be seen that the BER performance of the VD with perfect CSI does not improve beyond a negligible amount for M &gt; 50.</p><p>The molecular channel's impulse response has a much longer tail, where at τ = 1 s it takes 382 symbol durations for the impulse response to fall to 0.1% of the peak value λ max . This is evident in Figure <ref type="figure" target="#fig_6">7</ref>(a) where the BER of the VD with perfect CSI always improves as M increases.</p><p>Figure <ref type="figure" target="#fig_6">7</ref>(a) also demonstrates that if the estimate of M is inaccurate, the SBRNN algorithm outperforms the VD with perfect CSI. We also observe that the SBRNN achieves a better BER when there is a CSI estimation error of 2.5% or more. Note that the RNN detector does not have a parameter that depends on the memory length and has a significantly larger BER compared to the SBRNN. For the optical channel, the RNN detector outperforms the VD with 5% error in CSI estimation. Moreover, it can seen that the optical channel has a shorter memory length compared to the molecular channel. Remark 1: When the VD has perfect CSI, it can estimate memory length correctly by using the system response. However, if there is CSI estimation error, the memory length may not be estimated correctly, and as can be seen in Figure <ref type="figure" target="#fig_6">7</ref>(a), this can have degrading effects on the performance of the VD. However, in the rest of this section, for all the other VD plots, we use the memory length of 99, i.e., the largest possible memory length in sequences of 100 bits. Although this does not capture the performance degradation that may result from the error in estimating the memory length, as we will show, the SBRNN still achieves a BER performance that is as good or better than the VD plots with CSI estimation error under various channel conditions.</p><p>Next we evaluate the BER for different symbol durations in Figure <ref type="figure" target="#fig_6">7</ref>(b). Again we observe that the SBRNN achieves a better BER when there is a CSI estimation error of 2.5% or more. The RNN detector outperforms the VD with 5% CSI estimation error for the optical channel, but does not perform well for the molecular channel. All detectors achieve zeroerror in decoding the 1000 sequences of 100 random bits used to calculate the BER for the optical channel with τ = 0.1 µs. Similarly, for the molecular channel at τ = 1.5 s, all detectors except the RNN detector achieve zero error.</p><p>Figure <ref type="figure" target="#fig_6">7</ref>(c) evaluates the BER performance at various noise rates. The SBRNN achieves a BER performance close to the VD with perfect CSI across a wide range of values. For larger values of η, i.e., low signal-to-noise ratio (SNR), both the RNN detector and the SBRNN detector outperform the VD with CSI estimation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of Channel Parameters</head><p>In this section we evaluate the performance with respect to the channel parameters that affect the system response. Recall that for the optical channel the parameter β affects the system response in <ref type="bibr" target="#b10">(11)</ref> (note that here we assume that α = 2 does Fig. <ref type="figure">9</ref>: The shape the system response for the optical and molecular channel over the range of values in ( <ref type="formula" target="#formula_38">22</ref>) and <ref type="bibr" target="#b22">(23)</ref>. not change), and for the molecular channel the c and µ affect the system response in <ref type="bibr" target="#b11">(12)</ref>. The range of values that β is assumed to take is given in <ref type="bibr" target="#b21">(22)</ref>, and the range of values for c and µ are given in <ref type="bibr" target="#b22">(23)</ref>.</p><p>In Figure <ref type="figure" target="#fig_7">8</ref>, we evaluate the performance of the detection algorithms with respect to these parameters. Note that in optical and molecular communication these parameters can change rapidly due to atmospheric turbulence, changes in temperature, or changes in the distance between the transmitter and the receiver. Therefore, estimating these parameters accurately can be challenging. Furthermore, since these parameters change the shape of the system responses they change the memory length as well.</p><p>Figure <ref type="figure">9</ref> shows the system response for the optical and molecular channels over the range of values for β, c, and µ in ( <ref type="formula" target="#formula_38">22</ref>) and <ref type="bibr" target="#b22">(23)</ref>. For a fixed symbol duration, the system response can have a considerable effect on the delay spread (i.e., memory order) of the system. From Figure <ref type="figure" target="#fig_7">8</ref>, it can be seen that the SBRNN performs as well or better than the VD with an estimation error of 2.5%. Moreover, for the optical channel, the RNN detector performs better than the VD with 5% estimation error. In all cases, the SBRNN learns to detect over the wide range of system responses shown in Figure <ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects of Symbol Set Size, Transmission Length, and RNN Cell Type</head><p>In the previous sections we considered OOK modulation. However, it is not clear if higher order modulations can be used  to achieve better results. In this section we first evaluate the performance of OOK and higher order m-PAM modulations using VD. We demonstrate that for system parameters under consideration, 4-PAM achieves the best BER performance.</p><p>Then we demonstrate that the SBRNN detector can be trained on modulations with larger symbol sets. In fact for detection and estimation problems in speech and language processing, where RNNs are extensively used, the symbol set (i.e., the number of phonemes or vocabulary size) can be on the order of hundreds to millions of symbols. We also consider the affect of different RNN cell types on the symbol error rate (SER) performance and demonstrate that the LSTM cell, which was used in the previous sections, achieves the best performance. Finally, the generalizability of the SBRNN detector to longer transmission sequences is evaluated where we show that the SBRNN achieves the same or better SER performance on longer transmission sequences, despite being trained on sequences of length 100. First we compare the performance of OOK, 4-PAM, and 8-PAM modulation, where 2, 4 or 8 amplitude levels are used for encoding 1, 2 or 3 bits of information during each symbol duration. We assume that amplitudes are equally spaced and include the zero amplitude (i.e., sending no pulse). Because of space limitations, we only focus on the optical channel with the following parameters: OOK with τ = 0.05 µs, κ OP = 10, η = 50; 4-PAM with τ = 0.1 µs, κ OP = 20, η = 100; and 8-PAM with τ = 0.15 µs, κ OP = 30, η = 150. For all modulations we use β = 0.2 and α = 2. We chose these parameters to keep the average transmit power, the data rate, and the peak signal-to-noise ratio (SNR) the same for all modulations. We then evaluate the SER using the VD with perfect CSI and the VDs with CSI estimation errors of 2.5%. We use 500k symbols for evaluating the SER. Table <ref type="table" target="#tab_3">III</ref> shows the results. When perfect CSI is available at the receiver, 4-PAM achieves the best SER, while when there is an error in CSI estimation, OOK achieves the best SER. Note that since the number of bits presented by each symbol of each modulation scheme is different, SER is not the best performance measure. However, even if we assume that each symbol error is due to a single bit error, which results in the best BER possible for 8-PAM, we still observe that 4-PAM achieves the best BER performance when perfect CSI is available at the receiver, while OOK achieves the best BER performance when there is CSI estimation error. Since 4-PAM achieves the best BER performance, we trained a new SBRNN detector based on 4-PAM modulation.</p><p>For training, the channel parameter β is assumed to be uniformly random in the interval β ∈ [0.2, 0.35] and the noise parameter η is assumed to be uniformly random in the interval η ∈ <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">200]</ref>. We trained three SBRNN detectors based on the LSTM cell, the GRU cell <ref type="bibr" target="#b64">[65]</ref>, and the vanilla RNN architecture <ref type="bibr" target="#b6">[7]</ref>. Figure <ref type="figure" target="#fig_8">10(a)-(b)</ref> shows the results. As can be seen, the SBRNN with the LSTM cell achieves a better SER performance compared with the GRU cell and the vanilla RNN cell types. Compared with the VDs, we observe a trend similar to that in OOK modulation: the SBRNN outperforms VD with CSI estimation error, while its performance comes close to the VD with perfect CSI. This demonstrates that the SBRNN algorithm can be extended to larger symbol sets.</p><p>We last evaluate the performance of the SBRNN detector over longer transmission sequences for OOK and 4-PAM. In particular, for each modulation, two differently trained SBRNN networks are evaluated. The first set of networks are the same networks used to generate Figures <ref type="figure" target="#fig_6">7,</ref><ref type="figure" target="#fig_7">8</ref>, and 10(a)-(b). These networks are trained using a data set that contains sample transmissions under various channel conditions. The second set of networks are trained using sample received signals from a very specific set of channel and noise parameters. Specifically, the training data is generated using the same set of parameters that are used during testing (i.e., τ = 0.05 µs, κ OP = 10, η = 50 for OOK and τ = 0.1 µs, κ OP = 20, η = 100 for 4-PAM). Note that all the SBRNN detectors are trained on transmission sequences of length 100. Figure <ref type="figure" target="#fig_8">10(c)</ref> shows the performance for transmission sequences of various lengths. Interestingly, we observe that the SER drops as the length of the transmission sequence increases. This is because the probability of error for symbols at the beginning and end of the transmission sequence is higher as shown in Figure <ref type="figure" target="#fig_9">11</ref>. The larger probability of error for the first few symbols is due to the signal rising rapidly at the start of the transmission, as was shown in Figure <ref type="figure" target="#fig_5">6</ref>, which has a different structure compared to the signal corresponding to the rest of the symbols. This can be mitigated by using a separate neural network that is trained only on the signal corresponding to the initial symbols, or using a sequence of random transmission bits at the beginning of the transmission sequence as a guard interval. The error at the end of transmission sequence can be mitigated by observing the received signal after the last symbol duration and using that signal as part of the detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Rapidly Changing Channels</head><p>In this we evaluate the performance of the SBRNN algorithm for rapidly changing channels. Due to lack of space we focus on the optical channel; we have observed similar performance results for the molecular channel as well. For modeling the rapidly changing channel, we assume that the channel parameter β and the noise parameter η change from one symbol interval to the next. In particularly, we assume these parameters change according to a diffusion model with drift using the equations:</p><formula xml:id="formula_41">β i+1 = β i + dβ 0 N + νβ 0 ,<label>(24)</label></formula><formula xml:id="formula_42">η i+1 = i + dη 0 N + νη 0 ,<label>(25)</label></formula><p>where β 0 and η 0 are the channel and noise parameters at the beginning of the transmission sequence, d and ν control the diffusion and the drift velocities, and N is a zero mean unit variance Gaussian random variable. The received signal is then given by</p><formula xml:id="formula_43">y k [j] ∼ P k i=0 x k-i λ βi i [j] + η k ,<label>(26)</label></formula><p>where λ βi i [j] is defined in <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b12">(13)</ref> with parameter β i . The parameter d controls the degree of dispersion, while the parameter ν controls how β and η change on average over time. When ν = 0, E[β i ] = β 0 and E[η i ] = η 0 . Note that d &gt; 0 controls the deviation from this mean. When ν &gt; 0, channel is degrading over time since E[β i ] &gt; β 0 and Fig. <ref type="figure" target="#fig_0">12:</ref> The SBRNN performance under rapidly changing channel condition.</p><p>E[η i ] &gt; η 0 , which result in larger ISI and noise components on average. Similarly, when ν &lt; 0, the channel is improving over time because the ISI and the noise component are decreasing on average.</p><p>To evaluate the resiliency of the SBRNN detector to rapid changes in the channel, we use the same trained networks that were used to generate Figures <ref type="figure" target="#fig_6">7,</ref><ref type="figure" target="#fig_7">8</ref>, and 10(a)-(b). Note that although these networks are trained using a data set that contains samples from various channel conditions, the channel parameters are fixed for the duration of the transmission of the whole sequence. However, the model that is used for testing is the one in <ref type="bibr" target="#b25">(26)</ref>, where the channel parameters changes from one symbol to the next during a transmission sequence. Specifically, for testing, sequences of length 200 symbols are used. The parameters of the channel are assumed to be β 0 = 0.2, η 0 = 10, and α = 2. For the OOK modulation τ = 0.05 µs, and κ OP = 10, while for 4-PAM τ = 0.1 µs, and κ OP = 20. The channel parameters β i and η i in (26) are assumed to diffuse according to <ref type="bibr" target="#b23">(24)</ref> and ( <ref type="formula" target="#formula_42">25</ref>) over a bounded intervals of [0.15, 0.35] and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref>, respectively.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows the results. For the VD plots, we assume that β 0 and η 0 is known perfectly at the receiver, i.e., the receiver has the perfect CSI at the beginning of the transmission sequence. If the diffusion rate is very small, and there is no drift (i.e., the channel is not changing), the VD performs very well, as expected. However, if the channel is drifting over time (i.e. ν &gt; 0), the performance of the VD degrades significantly. Although the SBRNN algorithm is trained on a dataset where the channel does not change rapidly, it performs well under rapidly changing conditions. Also note that the training dataset has 100 symbol sequences while the test data has symbol sequences of length 200. These results demonstrate that the SBRNN can be very useful in detection over rapidly changing channels, where traditional detection algorithms that cannot adapt well to the changing channel have performed poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Complexity</head><p>We conclude this section by comparing the computational complexity of the SBRNN detector, the RNN detector, and the VD. Let n be the length of the sequence to be decoded. Recall that L is the length of the sliding BRNN, M is the memory length of the channel, and N is the number of states with the highest log-likelihood values among the 2 M states of the trellis that are kept at each time instance in the beam search Viterbi algorithm. Note that for the traditional Viterbi algorithm N = 2 M . The computational complexity of the SBRNN is given by O(L(n -L + 1)), while the computational complexity of the VD is given by O(N n). Therefore, for the traditional VD, the computational complexity grows exponentially with memory length M . However, this is not the case for the SBRNN detector. The computational complexity of the RNN detector is O(n). Therefore, the RNN detector is most efficient in terms of computational complexity, while the SBRNN detector and the beam search VD algorithm can have similar computational complexity. Finally, the traditional VD algorithm is impractical for the channels considered due to its exponential computational complexity in the memory length M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION BASED ON EXPERIMENTAL PLATFORM</head><p>In this section, we use a molecular communication platform for evaluating the performance of the proposed SBRNN detector. Note that although the proposed techniques can be used with any communication system, applying them to molecular communication systems enable many interesting applications. For example, one particular area of interest is in-body communication where bio-sensors, such as synthetic biological devices, constantly monitor the body for different bio-markers for diseases <ref type="bibr" target="#b65">[66]</ref>. Naturally, these biological sensors, which are adapt at detecting biomarkers in vivo <ref type="bibr" target="#b66">[67]</ref>-[69], need to convey their measurements to the outside world. Chemical signaling is a natural solution to this communication problem where the sensor nodes chemically send their measurements to each other or to other devices under/on the skin. The device on the skin is connected to the Internet through wireless technology and can therefore perform complex computations. Thus, the experimental platform we use in this work to validate NN algorithms for signal detection can be used directly to support this important application.</p><p>We use the experimental platform in <ref type="bibr" target="#b41">[42]</ref> to collect measurement data and create the dataset that is used for training and testing the detection algorithms. In the platform, timeslotted communication is employed where the transmitter modulates information on acid and base signals by injecting these chemicals into the channel during each symbol duration. The receiver then uses a pH probe for detection. A binary modulation scheme is used in the platform where the 0-bit is transmitted by pumping acid into the environment for 30 ms at the beginning of the symbol interval, and the 1-bit is represented by pumping base into the environment for 30 ms at the beginning of the symbol interval. The symbol interval consists of this 30 ms injection interval followed by a period of silence, which can also be considered as a guard band between symbols. In particular, four different silence durations (guard bands) of 220 ms, 304 ms, 350 ms, and 470 ms are used in this work to represent bit rates of 4, 3, 2.6, and 2 bps. This is similar to the OOK modulation used in the previous section for the Poisson channel model, except that chemicals of different types are released for both the 1-bit and the 0-bit.</p><p>To synchronize the transmitter and the receiver, message sequence starts with one initial injection of acid into the environment for 100 ms followed by 900 ms of silence. The receiver then detects the starting point of this pulse by employing an edge detection algorithm and uses it to synchronize with the transmitter. Since the received signal is corrupted and noisy, this results in a random offset. However, since the NN detectors are trained directly on this data, as we will show, they learn to be resilient to this random offset.</p><p>The training and test data sets are generated as follows. For each duration, random bit sequences of length 120 are transmitted 100 times, where each of the 100 transmissions are separated in time. Since we assume no channel coding is used, the bits are i.i.d. and equiprobable. This results in 12k bits per symbol duration that is used for training and testing. From the data, 84 transmissions per symbol duration (10,080 bits) are used for training and 16 transmissions are used for testing (1,920 bits). Therefore, the total number of training bits is 40,320, and the total number of bits used for testing is 7,680.</p><p>Although we expect from the physics of the chemical propagation and chemical reaction that the channel should have memory, since the channel model for this experimental platform is currently unknown, we implement both symbolby-symbol and sequence detectors based on NNs. Note that due to the lack of a model, we cannot use the VD for comparison since it cannot be implemented without an underlying channel model. Instead, as a baseline detection algorithm, we use the slope detector that was used in previous work <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>. For all training of the NN detectors, the Adam optimization algorithm <ref type="bibr" target="#b63">[64]</ref> is used with learning rate of 10 -3 . Unless specified otherwise, the number of epochs used during training is 200 and the batch size is 10. All the hyperparameters are tuned using grid search.</p><p>We consider two symbol-by-symbol NN detectors. The first detector uses three fully connected layers with 80 hidden nodes and a final softmax layer for detection. Each fully connected layer uses the rectified linear unit (ReLU) activation function. The input to the network is a set of features extracted from the received signal, which are chosen based on performance and the characteristics of the physical channel as explained in the appendix. We refer to this network as Base-Net. A second symbol-by-symbol detector uses 1-dimensional CNNs. The best network architecture that we found has the following layers. 1) 16 filters of length 2 with ReLU activation; 2) 16 filters of length 4 with ReLU activation; 3) max pooling layer with pool size 2; 4) 16 filters of length 6 with ReLU activation; 5) 16 filters of length 8 with ReLU activation; 6) max pooling layer with pool size 2; 7) flatten and a softmax layer. The stride size for the filters is 1 in all layers. We refer to this network as CNN-Net.</p><p>For the sequence detection, we use three networks, two based on RNNs and one based on the SBRNN. The first network has 3 LSTM layers and a final softmax layer, where the length of the output of each LSTM layer is 40. Two different inputs are used with this network. In the first, the input is the same set of features as the Base-Net above. We refer to this network as LSTM3-Net. In the second, the input is the pretrained CNN-Net described above without the top softmax layer. In this network, the CNN-Net chooses the features directly from the received signal. We refer to this network as CNN-LSTM3-Net. Finally, we consider three layers of bidirectional LSTM cells, where each cell's output length is 40, and a final softmax layer. The input to this network is the same set of features used for Base-Net and the LSTM3-Net. When this network is used, during testing we use the SBRNN algorithm. We refer to this network as SBLSTM3-Net. For all the sequence detection algorithms, during testing, sample data sequences of the 120 bits are treated as an incoming data stream, and the detector estimates the bits one-by-one, simulating a real communication scenario. This demonstrates that these algorithms can work on any length data stream and can perform detection in real-time as data arrives at the receiver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System's Memory and ISI</head><p>We first demonstrate that this communication system has a long memory. We use the RNN based detection techniques for this, and train the LSTM3-Net on sequences of 120 consecutive bits. The trained model is referred to as LSTM3-Net120. We run the trained model on the test data, once resetting the input state of the LSTM cell after each bit detection, and once passing the state as the input state for the next bit. Therefore, the former ignores the memory of the system and the ISI, while the latter considers the memory. The bit error rate (BER) performance for the memoryless LSTM3-Net120 detector is 0.1010 for 4 bps, and 0.0167 for 2 bps, while for the LSTM3-Net120 detector with memory, they are 0.0333 and 0.0005, respectively. This clearly demonstrates that the system has memory.</p><p>To evaluate the memory length, we train a length-10 SBLSTM3-Net on all sequences of 10 consecutive bits in the training data. Then, on the test data, we evaluate the BER performance for the SBLSTM of length 2 to 10. Figure <ref type="figure" target="#fig_10">13</ref> shows the results for each symbol duration. The BER reduces as the length of the SBLSTM increases, again confirming that the system has memory. For example, for the 500 ms symbol duration, from the plot, we conclude that the memory is longer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance and Resiliency</head><p>Table <ref type="table" target="#tab_4">IV</ref> summarizes the best BER performance we obtain for all detection algorithms, including the baseline algorithm, by tuning all the hyperparameters using grid search. The number in front of the sequence detectors, indicates the sequence length. For example, LSTM3-Net120 is an LSTM3-Net that is trained on 120 bit sequences. In general, algorithms that use sequence detection perform significantly better than any symbol-by-symbol detection algorithm including the baseline algorithm. This is partly due to significant ISI present in the molecular communication platform. Overall, the proposed SBLSTM algorithm performs better than all other NN detectors considered.</p><p>Another important issue for detection algorithms are changing channel conditions and resiliency. As the channel conditions worsen, the received signal is further degraded, which increases the BER. Although we assume no channel coding is used in this work, one way to mitigate this problem is by using stronger channel codes that can correct some of the errors. However, given that the NN detectors rely on training data to tune the detector parameters, overfitting may be an issue. To evaluate the susceptibility of NN detectors to this effect, we collect data with a pH probe that has a degraded response due to normal wear and tear.</p><p>We collect 20 samples of 120 bit sequence transmissions for each of the 250 ms and 500 ms symbol durations using this degraded pH probe. First, to demonstrate that the response of the probe is indeed degraded, we evaluate it using the baseline slope-based detection algorithm. The best BERs obtained using the baseline detector are 0.1583 and 0.0741 for symbol durations of 250 ms and 500 ms, respectively. These values are significantly larger than those in Table <ref type="table" target="#tab_4">IV</ref>, because of the degraded pH probe. We then use the SBLSTM3-Net10 and the LSTM3-Net120, trained on the data from the good pH, on the test data from the degraded pH. For the SBLSTM3-Net10, the BERs obtained are 0.0883 and 0.0142, and for the LSTM3-Net120, the BERs are 0.1254 and 0.0504. These results confirm again that the proposed SBRNN algorithm is more resilient to changing channel conditions than the RNN.</p><p>Finally, to demonstrate that the proposed SBRNN algorithm can be implemented as part of a real-time communication Sec. VI: LSTM3-Net120 system, we use it to support a text messaging application built on top of the experimental platform. We demonstrate that using the SBRNN for detection at the receiver, we are able to reliably transmit and receive messages at 2 bps. This data rate is an order of magnitude higher than previous systems <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This work considered a machine learning approach to the detection problem in communication systems. In this scheme, a neural network detector is directly trained using measurement data from experiments, data collected in the field, or data generated from channel models. Different NN architectures were considered for symbol-by-symbol and sequence detection. For channels with memory, which rely on sequence detection, the SBRNN detection algorithm was presented for real-time symbol detection in data streams. To evaluate the performance of the proposed algorithm, the Poisson channel model for molecular communication was considered as well as the VD for this channel. It was shown that the proposed SBRNN algorithm can achieve a performance close to the VD with perfect CSI, and better than the RNN detector and the VD with CSI estimation error. Moreover, it was demonstrated that using a rich training dataset that contains sample transmission data under various channel conditions, the SBRNN detector can be trained to be resilient to the changes in the channel, and achieves a good BER performance for a wide range of channel conditions. Finally, to demonstrate that this algorithm can be implemented in practice, a molecular communication platform that uses multiple chemicals for signaling was used. Although the underlying channel model for this platform is unknown, it was demonstrated that NN detectors can be trained directly from experimental data. The SBRNN algorithm was shown to achieve the best BER performance among all other considered algorithms based on NNs as well as a slope detector considered in previous work. Finally, a text messaging application was implemented on the experimental platform for demonstration where it was shown that reliable communication at rates of 2 bps is possible, which is an order of magnitude faster than the data rate reported in previous work for molecular communication channels.</p><p>As part of future work we plan to investigate how techniques from reinforcement learning could be used to better respond to changing channel conditions. We would also like to study if the evolution of the internal state of the SBRNN detector could help in developing channel models for systems where the underlying models are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX FEATURE EXTRACTION</head><p>In this appendix we describe the set of features that are extracted from the received signal and are used as the input to the different NN detectors considered in this work. The set of features r k , extracted from the received signal during the k th channel use y k , must preserve and summarize the important information-bearing components of the received signal. For the Poisson channel, since the information is encoded in the intensity of the signal, much of the information is contained in the rate of change of intensity. In particular, intensity increases in response to the transmission of the 1-bit, while intensity decreases or remains the same in response to transmission of the 0-bit. Note that this is also true for the pH signal in the experimental platform used in Section VI. First the symbol interval (i.e., the time between the green lines in Figure <ref type="figure" target="#fig_5">6</ref>) is divided into a number of equal subintervals or bins. Then the values inside each bin are averaged to represent the value for the corresponding bin. Let B be the number of bins Other values that can be used to infer the rate of change are b 0 and b B-1 , the value of the first and the last bins, and the mean and the variance of the b. Since the intensity can grow large due to ISI, b may be normalized with the parameter γ as b = b/γ. Therefore, instead of b 0 and b B-1 , b0 and bB-1 , and the mean and the variance of the b may be used as part of the feature set r k . Finally, since the transmitter and the receiver have to agree on the symbol duration, the receiver knows the symbol duration, which can be part of the feature set. Table <ref type="table">V</ref> summarizes the set of features that are used as input to the each of the NN detection algorithms in this paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Similarities between speech recognition and digital communication systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Different neural network architectures for detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>be a sequence of K consecutively transmitted symbols (in the one-of-m encoded representation), and Y K = [y 1 , y 2 , • • • , y K ] the corresponding sequence of observed signals at the destination. Then, the training dataset is represented by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The sliding BRNN detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: A sample system response for optical and molecular channels. Left: Optical channel with λ(t) for N = 1, κ OP = 1, α = 2, β = 0.2, τ = 0.2 µs, and ω = 20 MS/s. At τ = 0.2 µs, much of the intensity from the current transmission will arrive during future symbol intervals. Right: Molecular channel with κ MO = 1, c = 8, µ = 40, τ = 2 s, and ω = 2 S/s. Molecular channel response has a loner tail than optical channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The observed signal for the transmission of the bit sequence 10101100111000 for κ MO = 100, c = 8, µ = 40, τ = 1, ω = 100 Hz, and η = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The BER performance comparison of the SBRNN detector, the RNN detector, and the VD. The top plots present the optical channel and the bottom plots present the molecular channel. (a) The BER at various memory lengths M and SBRNN sequence lengths L. Top: Θ OP = [β = 0.2, η = 1] and τ = 0.05 µs. Bottom: Θ MO = [c = 10, µ = 40, η = 100] and τ = 1 s. (b) The BER at various symbol durations for L = 50 and M = 99. The Θ OP (top) and Θ MO (bottom) are the same as (a). (c) The BER at various noise rates for L = 50 and M = 99. Except η, all the parameters are the same as those in (a).received signal. Since in this work we focus on uncoded communication, we assume the occurrence of both bits in the transmitted sequence are equiprobable. For each sequence, the CSI are selected at random. Particularly, for the optical channel, for each 100-bit sequence, β ∼ U ({0.15, 0.16, 0.17, • • • , 0.35}), η ∼ U ({1, 10, 20, 50, 100, 200, 500}), τ ∼ U ({0.025, 0.05, 0.075, 0.1})(all in µs),<ref type="bibr" target="#b21">(22)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The BER performance comparison of the SBRNN detector (L = 50), the RNN detector, and the VD (M = 99). (a) The BER at various β for the optical channel with η = 1 and τ = 0.05 µs. (b) The BER at various c for the molecular channel with µ = 40, η = 1, and τ = 1 s. (c) The BER at various µ for the molecular channel with c = 10, η = 1000, and τ = 1 s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: The SER performance comparison of the SBRNN detector (L = 50), and the VD (M = 99) for optical channel with 4-PAM modulation. (a) The SER at various η for the optical channel with β = 0.2 and τ = 1 µs. (b) The SER at various β for the optical channel with η = 10 and τ = 1 µs. (c) SER versus transmission sequence length for optical channel with OOK modulation (τ = 0.05 µs, κ OP = 10, η = 50) and 4-PAM modulation = 0.1 µs, κ OP = 20, = 100). For both cases β = 0.2 and α = 2.TABLE III: The SER for different modulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Symbols errors are higher at the beginning and end of the transmission sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: The BER as a function of SBLSTM length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>, and b = [b 0 , b 1 , • • • , b B-1 ] the corresponding values of each bin. We then extract the rate of change during a symbol duration by differentiating the bin vector to obtain the vector d = [d 0 , d 1 , • • • , d B-2 ], where d i-1 = b i -b i-1. We refer to this vector as the slope vector and use it as part of the feature set r k extracted from the received signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>[</head><label></label><figDesc>68] T. Danino et al., "Programmable probiotics for detection of cancer in urine," Science Translational Medicine, vol. 7, no. 289, pp. 289ra84-289ra84, 2015. [69] S. Slomovic et al., "Synthetic biology devices for in vitro and in vivo diagnostics," Proceedings of the National Academy of Sciences, vol. 112, no. 47, pp. 14 429-14 435, Nov. 2015. Nariman Farsad is currently a Post-Doctoral Fellow with Department of Electrical Engineering, Stanford University, Stanford, CA, USA. His research interests are on improving the performance of communication systems through machine learning and deep learning and on emerging communication technologies such as molecular communications. He was a member of the technical program committees of ICC2015, ICC2018, BICT2015, GLOBCOM2015, GLOBCOM2016, and GLOBE-COM2017. He has received the Natural Sciences and Engineering Research Council of Canada Postdoctoral Fellowship, the Second Prize from the 2014 IEEE ComSoc Student Competition, the Best Demo Award at INFOCOM2015, and was recognized as a finalist for the 2014 Bell Labs Prize. He is an Area Associate Editor of the JOURNAL OF SELECTED AREAS OF COMMUNICATIONSpecial Issue on Emerging Technologies in Communications and a technical reviewer for a number of journals including the IEEE TRANSACTIONS ON SIGNAL PROCESSING and the TRANSACTIONS ON COMMUNICATION. Andrea Goldsmith is the Stephen Harris professor in the School of Engineering and a professor of Electrical Engineering at Stanford University. She co-founded and served as Chief Technical Officer of Plume WiFi and of Quantenna (QTNA), and she currently serves on the Corporate or Technical Advisory Boards of multiple public and private companies. She has also held industry positions at Maxim Technologies, Memorylink Corporation, and AT&amp;T Bell Laboratories. Dr. Goldsmith is a member of the National Academy of Engineering and the American Academy of Arts and Sciences, a Fellow of the IEEE and of Stanford, and has received several awards for her work, including the IEEE Sumner Technical Field Award, the ACM Athena Lecturer Award, the IEEE Comsoc Edwin H. Armstrong Achievement Award, the National Academy of Engineering Gilbreth Lecture Award, the Women in Communications Engineering Mentoring Award, and the Silicon Valley/San Jose Business Journals Women of Influence Award. She is author of the book "Wireless Communications"; and co-author of the books "MIMO Wireless Communications"; and "Principles of Cognitive Radio", all published by Cambridge University Press, as well as an inventor on 29 patents. She has also launched and led several multi-university research projects. Her research interests are in information theory and communication theory, and their application to wireless communications and related fields. She received the B.S., M.S. and Ph.D. degrees in Electrical Engineering from U.C. Berkeley. Dr. Goldsmith participates actively in committees and conference organization for the IEEE Information Theory and Communications Societies and has served on the Board of Governors for both societies. She has been a Distinguished Lecturer for both societies, served as the President of the IEEE Information Theory Society in 2009, founded and chaired the student committee of the IEEE Information Theory society, and is the founding chair of the IEEE TAB Committee on Diversity and Inclusion. At Stanford she has served as Chair of Stanford's Faculty Senate and on its Advisory Board, Budget Group, Committee on Research, Planning and Policy Board, Commissions on Graduate and on Undergraduate Education, Faculty Women's Forum Steering Committee, and Task Force on Women and Leadership.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Summary of the results to be presented in this section.</figDesc><table><row><cell>sec.</cell><cell>Chan Types</cell><cell>Evaluates</cell></row><row><cell>B</cell><cell>Optical/Molecular (OOK)</cell><cell>sequence length, symbol duration, noise</cell></row><row><cell>C</cell><cell>Optical/Molecular (OOK)</cell><cell>channel parameters (i.e., impulse response)</cell></row><row><cell>D</cell><cell>Optical (m-PAM)</cell><cell>symbol size, transmission length, RNN type</cell></row><row><cell>E</cell><cell>Optical/Molecular (OOK)</cell><cell>rapidly changing channels</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance of the VD beam search as function of N . The optical channel results is obtained using Θ OP = [β = 0.2, η = 1] and τ = 0.025 µs and the molecular channel results using Θ MO = [c = 8, µ = 40, η = 100] and τ = 0.5 s.</figDesc><table><row><cell>N</cell><cell>10</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell></row><row><cell>Opti. VD 0.0% error</cell><cell>0.0466</cell><cell>0.03937</cell><cell>0.03972</cell><cell>0.03906</cell><cell>0.03972</cell></row><row><cell>Opti. VD 2.5% error</cell><cell>0.226</cell><cell>0.175</cell><cell>0.17561</cell><cell>0.15889</cell><cell>0.1509</cell></row><row><cell>Opti. VD 5.0% error</cell><cell>0.4036</cell><cell>0.385</cell><cell>0.38519</cell><cell>0.39538</cell><cell>0.36</cell></row><row><cell>Mole. VD 0.0% error</cell><cell>0.00466</cell><cell>0.00398</cell><cell>0.00464</cell><cell>0.00448</cell><cell>0.00432</cell></row><row><cell>Mole. VD 2.5% error</cell><cell>0.0066</cell><cell>0.0055</cell><cell>0.00524</cell><cell>0.0056</cell><cell>0.00582</cell></row><row><cell>Mole. VD 5.0% error</cell><cell>0.41792</cell><cell>0.34667</cell><cell>0.30424</cell><cell>0.29314</cell><cell>0.30588</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The SER for different modulations.</figDesc><table><row><cell>Modulation</cell><cell>SER (Perfect CSI)</cell><cell>SER (2.5% CSI Error)</cell></row><row><cell>OOK</cell><cell>6.4 × 10 -5</cell><cell>9.2 × 10 -3</cell></row><row><cell>4-PAM</cell><cell>5.3 × 10 -6</cell><cell>1.3 × 10 -1</cell></row><row><cell>8-PAM</cell><cell>1.6 × 10 -4</cell><cell>2.5 × 10 -1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Bit Error Rate Performance Note that some of the missing points for the 500 ms and 380 ms symbol durations, which result in discontinuity in the plots, are because there were zero errors in the test data. Moreover, BER values below 5 × 10 -3 are not very accurate since the number of errors in the test dataset are less than 10 (in a typical BER plot the number of errors should be about 100). However, given enough test data, it would be possible to estimate the channel memory using the SBLSTM detector by finding the minimum length after which BER does not improve.</figDesc><table><row><cell>Symb. Dur.</cell><cell>250 ms</cell><cell>334 ms</cell><cell>380 ms</cell><cell>500 ms</cell></row><row><cell>Baseline</cell><cell>0.1297</cell><cell>0.0755</cell><cell>0.0797</cell><cell>0.0516</cell></row><row><cell>Base-Net</cell><cell>0.1057</cell><cell>0.0245</cell><cell>0.0380</cell><cell>0.0115</cell></row><row><cell>CNN-Net</cell><cell>0.1068</cell><cell>0.0750</cell><cell>0.0589</cell><cell>0.0063</cell></row><row><cell>CNN-LSTM3-Net120</cell><cell>0.0677</cell><cell>0.0271</cell><cell>0.0026</cell><cell>0.0021</cell></row><row><cell>LSTM3-Net120</cell><cell>0.0333</cell><cell>0.0417</cell><cell>0.0083</cell><cell>0.0005</cell></row><row><cell>SBLSTM3-Net10</cell><cell>0.0406</cell><cell>0.0141</cell><cell>0.0005</cell><cell>0.0000</cell></row><row><cell>than 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the sequence of symbols xk , xk-1 , • • • , x1 can also be estimated from y k+ , y k+ -1 , • • • , y 1 for some integer . However, to keep the notation simpler, without loss of generality we assume = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Note that η is the noise term that is typically used in the Poisson channel model. In the optical communication literature this noise is also known as the dark current<ref type="bibr" target="#b35">[36]</ref>-<ref type="bibr" target="#b37">[38]</ref>. The noise is due to imperfect receiver, or background noise (due to ambient optical noise or molecules that may exist in the environment).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underwater acoustic communication channels: Propagation models and statistical characterization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stojanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Preisig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="89" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Molecular communication for health care applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Moritani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4th Annual IEEE International Conference on Pervasive Computing and Communications Workshops</title>
		<meeting>of 4th Annual IEEE International Conference on Pervasive Computing and Communications Workshops<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nanonetworks: A new communication paradigm</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Akyildiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2260" to="2279" />
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Molecular communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey of recent advancements in molecular communication</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farsad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1887" to="1919" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="http://dx.doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applications of neural networks to digital communications a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ibnkahla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1185" to="1215" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A molecular communication system using acids, bases and hydrogen ions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farsad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 17th International Workshop on Signal Processing Advances in Wireless Communications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Grzybowski</surname></persName>
		</author>
		<title level="m">Chemistry in Motion: Reaction-Diffusion Systems for Micro-and Nanotechnology</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nonlinear partial differential equations for scientists and engineers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Debnath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural networks for multiuser detection in codedivision multiple-access communications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aazhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1212" to="1222" />
			<date type="published" when="1992-07">Jul 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural network techniques for adaptive multiuser demodulation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1460" to="1470" />
			<date type="published" when="1994-12">Dec 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gaussian processes for multiuser detection in cdma receivers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murillo-Fuentes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="939" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiuser detection with neural network and pic in cdma systems for awgn and rayleigh fading asynchronous channels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Is</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tas ¸pınar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wireless Personal Communications</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to decode linear codes using deep learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</title>
		<imprint>
			<date type="published" when="2016-09">Sept 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning-based communication over the air</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dörner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03384</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to communicate: Channel auto-encoders, domain specific regularizers, and attention</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'shea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)</title>
		<imprint>
			<date type="published" when="2016-12">Dec 2016</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">RNN decoding of linear block codes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07560</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning methods for improved decoding of linear codes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An iterative BP-CNN architecture for channel decoding</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling deep learning-based decoding of polar codes via partitioning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cammerer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06901</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning-based communication over the air</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dörner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep MIMO detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01151</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Machine learning based channel modeling for molecular MIMO communications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning approximate neural estimators for wireless channel state information</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'shea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06260</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An introduction to machine learning communications systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoydis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00832</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07176</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R Z</forename><surname>Ghassemlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Popoola</surname></persName>
		</author>
		<title level="m">Optical Wireless Communications: System and Channel Modelling with MATLAB, 1st ed</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Channel estimation and signal detection for optical wireless scattering communication with inter-symbol interference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5326" to="5337" />
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Capacity of diffusion-based molecular communication networks over lti-poisson channels</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aminian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Molecular, Biological and Multi-Scale Communications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Channel estimation for diffusive molecular communications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jamali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="423" to="4252" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scw codes for optimal csi-free detection in diffusive molecular communications</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="3190" to="3194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Non-coherent detection for diffusive molecular communications</title>
		<idno type="arXiv">arXiv:1707.08926</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A novel experimental platform for in-vessel multi-chemical molecular communications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P N</forename><surname>Farsad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Communications Conference (GLOBECOM)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tabletop molecular communication: Text messages through chemical signals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farsad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e82935</biblScope>
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Molecular MIMO: From theory to prototype</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="600" to="614" />
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Principles of digital communication and coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Omura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">G: LTE/LTE-advanced for mobile broadband</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dahlman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Academic press</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory 2nd Edition</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neural-network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
		<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Capacity of a pulse amplitude modulated direct detection photon channel</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings I -Communications, Speech and Vision</title>
		<imprint>
			<date type="published" when="1990-12">Dec 1990</date>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="424" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Capacity-achieving distributions for the discrete-time poisson channel-Part I: General properties and numerical techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Capacity of molecular channels with imperfect particleintensity modulation and detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farsad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="2468" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Channel modeling of nondirected wireless infrared indoor diffuse link</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hayasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics and Communications in Japan (Part I: Communications)</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="9" to="19" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reconstruction of probability density function of intensity fluctuations relevant to free-space laser communications through atmospheric turbulence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">6709</biblScope>
			<biblScope unit="page">67090</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Modeling of non-line-of-sight ultraviolet scattering channels for communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Molecular communication in fluid media: The additive inverse gaussian noise channel</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4678" to="4692" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Optimal receiver design for diffusive molecular communication with flow and additive noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on NanoBioscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="362" />
			<date type="published" when="2014-09">Sept 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="1973-03">March 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient viterbi beam search algorithm using dynamic pruning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lingyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Limin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. of 7th International Conference on Signal Processing</title>
		<meeting>of 7th International Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2004-08">Aug 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="699" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Body area nanonetworks with molecular communications in nanomedicine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Atakan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Environmentally controlled invasion of cancer cells by engineered bacteria</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="627" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
