<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">69AFF2398DCC55000A5EAE182B8AD322</idno>
					<idno type="DOI">10.1109/MSP.2017.2762725</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Li Deng rtificial intelligence (AI) is a branch of computer science and a technology aimed at developing the theories, methods, algorithms, and applications for simulating and extending human intelligence. Modern AI enables going from an old world-where people give computers rules to solve problems-to a new world-where people give computers problems directly and the machines learn how to solve them on their own using a set of algorithms. An algorithm is a self-contained sequence of instructions and actions to be performed by a computational machine. Starting from an initial state and initial input, the instructions describe computational steps, which, when executed, proceed through a finite number of well-defined successive states, eventually producing an output and terminating at a final ending state. AI algorithms are a rich set of algorithms used to perform AI tasks, notably those pertaining to perception and cognition that involve learning from data and experiences simulating human intelligence.</p><p>The ultimate goal of AI is to create technology that allows computational machines to function in a highly intelligent manner. This high-level goal can be broken down into a number of subareas pertaining to the human-like perceptual and cognitive capabilities that are expected to exhibit in an intelligent machine. Among the most important capabilities is that of learning, which enables automated machine algorithms to improve through experiences by themselves. The recent rise of a powerful machine-learning paradigm-deep learning-is responsible for much of the resurgence of AI over the past ten years or so (see <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and <ref type="bibr" target="#b65">[65]</ref>). Other important cognitive capabilities include reasoning, problem solving, planning, motion (robotics) and decision making, interactions and dialogue, creativity, and knowledge representation. The latter involves how to build, represent, and exploit commonsense knowledge, which seems effortless in human behavior, and it also involves how symbolic and neural (subsymbolic) forms can be seamlessly integrated to achieve brain-like knowledge representation.</p><p>In addition to the aforementioned rich set of subareas, the wide scope of AI is also reflected in the many modalities of signal and information that humans use pervasively, whose capability is to be simulated by machines. Among the most important modalities is natural language or text. The AI processing of natural language gives machines the ability to read, comprehend, and generate the languages that humans use in our daily lives. A successful AI system with natural language capabilities would enable natural usermachine interfaces as well as the acquisition of knowledge directly from gigantic text sources created by humans in both public Internet and private enterprises. Other significant modalities include visual, speech/audio, touch/tactile, and smell/ taste information, constituting another important subarea of AI called machine perception, which, at its core, encompasses speech recognition as well as computer vision. In developing AI methods for processing natural language and visual and speech information, AI researchers have invented many highly effective algorithms over the past three decades or so.</p><p>This article is intended for a general audience interested in the historical development, current status, popular algorithms, and future directions of AI as a technology. To limit the scope, I will survey and analyze three major rising waves of AI since the 1960s, focusing on their quite disparate paradigms, approaches, and related algorithms. In the exposition, rather than covering the extensive scope of AI outlined above, I will draw the representative and most relevant examples mainly from notable applications in machine perception. In particular, I will attempt to connect the cycles of the ups and downs of AI to their counterparts in speech recognition, which, as a core application area of AI and also as one major research discipline that our IEEE Signal Processing Society has made most significant contributions to, parallels the historical path of general AI more closely than other areas of AI. Finally, the outlook of AI will be discussed and analyzed, from both the horizontal (i.e., breakthroughs in general AI technology and algorithms)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The first rise of ai</head><p>In the first rising wave of AI, starting in the 1960s and based on expert knowledge engineering, domain experts devised computer programs according to the knowledge about the (very narrow) application domains they have <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The experts designed these programs using symbolic logical rules grounded on careful representation and engineering of such knowledge. These knowledge-based AI systems tend to be effective in solving narrow-domain problems by examining the "head" or most important parameters and reaching a solution about the appropriate action to take in each specific situation. These "head" parameters are identified in advance by human experts, leaving the "tail" parameters and cases untouched. Since they lack learning capability, they have difficulty in generalizing the solutions to new situations and domains.</p><p>The typical approach during this wave is exemplified by the expert system, a computer system that emulates the decision-making ability of a human expert. Such systems are designed to solve complex problems by reasoning about knowledge <ref type="bibr" target="#b2">[3]</ref>. The first expert systems were created in the 1970s and then proliferated in the 1980s. The main "algorithm" used was the inference rules in the form of "if-then-else" <ref type="bibr" target="#b4">[5]</ref>.</p><p>The main strength of these first-generation AI systems is its transparency and interpretability in their (limited) capability in performing logical reasoning. They use handcrafted expert knowledge that is often effective in narrowly defined problems, although the reasoning cannot handle uncertainty that is ubiquitous in practical applications. Due to this strength, the first-generation AI systems are still in use today. Examples are narrow-domain dialogue systems and chatbots, chess-playing programs, traffic light controllers, optimization software for logistics of good deliveries, etc.</p><p>The early research and system design of speech recognition, a long-standing AI challenge in machine perception, were based on the AI paradigm of expert knowledge engineering during the first rising wave of AI. During the 1970s and early 1980s, the expert-system approach to speech recognition was quite popular, driven largely by Carnegie Mellon University and Massachusetts Institute of Technology speech researchers; e.g., the spectrogram reading method as elaborated in <ref type="bibr" target="#b5">[6]</ref>. However, the lack of general abilities to learn algorithmically from data and to handle uncertainty in reasoning in the knowledge-based approach was acutely recognized by researchers, along with the second rise of AI in the 1980s. The author has been part of the speech research community since the late 1980s and contributed to the transition from knowledge-based speech recognition to a data-driven one powered by statistical machine-learning methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The second rise of ai</head><p>The second rising wave of AI for speech arrived in the 1980s (and somewhat later for other AI areas) after clear evidence that learning and perception capabilities are crucial for complex AI systems but missing in knowledge-based expert systems. This is not just for speech recognition but also for vision and other AI systems. For example, when the Defense Advanced Research Projects Agency opened its first Grand Challenge for autonomous driving, most vehicles then relied on the knowledge-based paradigm. Much like speech recognition, the autonomous driving and vision researchers quickly realized the limitation of the first-generation AI paradigm due to the need for automatic learning equipped with uncertainty handling and generalization capabilities.</p><p>This second-generation AI paradigm was based on machine learning, which we now call shallow due to the lack of abstractions constructed by many-layer or "deep" representations of data which would come in the third rise of AI. In such shallow machine learning, engineers do not need to be concerned with constructing precise and exact rules as required for the first-generation AI systems. Rather, they focus on statistical models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> or simple neural networks <ref type="bibr" target="#b15">[16]</ref> as an underlying engine and then automatically learn or "tune" the parameters of the engine using the training data to make them handle uncertainty and generalize well from one condition to another and from one domain to another. The key algorithms and methods for machine learning include expectation-maximization (EM), Bayesian networks, support vector machines, decision trees, and, for neural networks, the backpropagation algorithm.</p><p>Generally, the machine-learning-based AI systems perform much better than the earlier, knowledge-based counterparts. Successful examples include almost all AI tasks in machine perception-speech recognition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref>, face recognition <ref type="bibr" target="#b20">[21]</ref>, visual object recognition <ref type="bibr" target="#b21">[22]</ref>, handwriting recognition <ref type="bibr" target="#b22">[23]</ref>, and machine translation <ref type="bibr" target="#b23">[24]</ref>.</p><p>In speech recognition, for more than 20 years from the 1980s to 2010, the paradigm was completely switched to and dominated by the (shallow) machinelearning paradigm using a statistical generative model called the hidden Markov model (HMM) integrated with Gaussian mixture models, along with various versions of its generalization <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[28]</ref>. The main algorithms and methods include the Viterbi algorithm, Baum-Welch algorithm (which is a special case of EM when applied to the HMM), and extended Baum-Welch (which includes learning algorithms for maximizing mutual information, minimizing classification errors, and minimizing phone errors) <ref type="bibr" target="#b29">[29]</ref>.</p><p>Among many versions of the generalized HMMs were statistical and neuralnet-based hidden dynamic models (see <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" target="#b32">[32]</ref> and <ref type="bibr" target="#b66">[66]</ref>). The former adopted EM and extended Kalman filter algorithms for learning model parameters <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> and the latter used backpropagation <ref type="bibr" target="#b35">[35]</ref>. Both of them made extensive use of multiple latent layers of representations for the generative process of speech waveforms following the long-standing framework of analysis by synthesis in human speech perception. More significantly, inverting this deep generative process to its counterpart of an end-toend discriminative process gave rise to the very first industrial success of deep perspectives (continued from page 180) learning <ref type="bibr" target="#b36">[36]</ref>- <ref type="bibr" target="#b39">[39]</ref>, which is the foundation of the third rising wave of AI that will be elaborated on next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The third (current) rise of ai</head><p>While the second generation of AI systems performed a lot better than the previous generation, they were far from human-level performance. With a few exceptions, the shallow machine-learning models often did not have the sufficiently large capacity to absorb the huge amounts of training data. Further, the learning algorithms, methods, and computing infrastructures were not powerful enough. All of this changed about one decade ago, bringing about the third rise of AI, propelled by the new paradigm of deep-structured machine learning or deep learning.</p><p>In traditional machine-learning approaches, features are designed by humans and feature engineering is a bottleneck requiring significant human expertise. Concurrently, the models lack the representation power and, hence, the ability to form levels of decomposable abstractions that automatically disentangle complex factors in shaping the observed data. Deep learning breaks away the aforementioned difficulties by the use of a deep, layered model structure, often in the form of neural networks, and the associated end-to-end learning algorithms. The advances in deep learning are one major driving force behind the current AI inflection point and the resurgence of neural networks.</p><p>Speech recognition is the first realworld AI application impacted strongly by deep learning. Industrial applications of deep learning to large-scale speech recognition started around 2010. In late 2009 and also 2010, I invited my academic collaborator Prof. Geoffrey Hinton, of the University of Toronto, and later his students to work with me and my colleagues at Microsoft Research in Redmond to apply deep learning to speech recognition. We co-organized the 2009 Neural Information Processing Systems Workshop on Deep Learning for Speech Recognition and Related Applications. The workshop was motivated by the limitations of deep generative models of speech, and the possibility that the big-compute, big-data era warrants a serious exploration of deep neural nets (DNNs) <ref type="bibr" target="#b40">[40]</ref>. It was believed then that pretraining DNNs using generative models of deep belief nets based on the contrastive-divergence learning algorithm would overcome the main difficulties of neural nets encountered in the 1990s. However, early into this research at Microsoft, it was discovered that without contrastive-divergence pretraining, but with the use of large amounts of training data together with the DNNs designed with corresponding large, context-dependent output layers, dramatically lower recognition errors were possible than then state-of-the-art (shallow) machine-learning systems based on the second-generation AI paradigm and algorithms. This finding was quickly verified by several other major speech recognition research groups. Further, the nature of recognition errors produced by the two types of systems were found to be characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by major players in the speech recognition industry today <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b41">[41]</ref>- <ref type="bibr" target="#b43">[43]</ref>. More recent advances in deep learning for speech recognition can be found in <ref type="bibr" target="#b44">[44]</ref>- <ref type="bibr" target="#b46">[46]</ref>. The backpropagation algorithm is uniformly used in all sorts of deep neural networks in all current speech recognition systems.</p><p>Large-scale speech recognition is the first and most convincing successful case of deep learning in recent history, embraced by both industry and academia across the board. Since 2010, the two major conferences on signal processing and speech recognitionthe IEEE International Conference on Acoustics, Speech, and Signal Processing, and Interspeech-have seen a huge increase year by year in the number of accepted papers in their respective annual conferences on the topic of deep learning for speech recognition. More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu, and iFlyTek voice search, and a range of Nuance speech products, etc.) are all based on deep-learning methods. The most cited article per year in speech recognition's history was published in 2012 in IEEE Signal Processing Magazine <ref type="bibr" target="#b38">[38]</ref>.</p><p>Quickly following the striking success of speech recognition in 2010 heralding the arrival of the third AI wave, two other important AI application areas-computer vision <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref> and machine translation <ref type="bibr" target="#b49">[49]</ref>-were completely taken over by the similar deep-learning paradigm. In addition, a large number of other realworld applications have been made successful due to deep learning, including ■ image captioning <ref type="bibr" target="#b50">[50]</ref>- <ref type="bibr" target="#b53">[53]</ref> ■ visual question answering <ref type="bibr" target="#b54">[54]</ref> ■ web search <ref type="bibr" target="#b55">[55]</ref> ■ natural language processing <ref type="bibr" target="#b67">[67]</ref> ■ drug discovery and toxicology ■ customer relationship management ■ recommendation systems ■ medical informatics ■ Internet advertisements ■ medical image analysis ■ robotics ■ self-driving vehicles ■ board games (e.g. AlphaGo, Poker, and DOTA2), etc. Setting aside their huge empirical successes, models of neural-network-based deep learning are often simpler and easier to design than the traditional machinelearning models developed in secondgeneration AI. In many applications, deep learning is performed simultaneously for all parts of the model, from feature extraction all the way to prediction, in an end-toend manner. Another factor contributing to the simplicity of neural network models is that the same model building blocks (i.e., the different types of layers) are generally used in many different applications. Using the same building blocks for a large variety of tasks in a uniformed manner makes the adaptation of models reused for one task or data to another task or data relatively easy. In addition, software toolkits have been developed to allow faster and more efficient implementation of these models. For these reasons, deep neural networks are today a prominent method of choice for a wide variety of machine-learning and AI tasks over large data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>an outlook for upcoming ai technology</head><p>Despite the spectacular successes of deep learning during the third rising wave of AI, there remain huge challenges. The current deep-learning methods lack interpretability, in contrast to the knowledge-based AI paradigm established during the first rise. In a number of applications, deep-learning methods prove to give the recognition accuracy close to or exceeding humans, but they require considerably more training data, power consumption, and computing resources than humans. Also, the accuracy results are statistically impressive but often unreliable on the individual basis. Further, most of the current deep-learning models have no reasoning and explaining capabilities, making them vulnerable to disastrous failures or attacks without the ability to foresee and thus prevent them.</p><p>To overcome the aforementioned challenges and to achieve ultimate successes of general AI, both fundamental and applied research are needed. The next new wave of AI will not come until new paradigmatic, algorithmic, and hardware breakthroughs are brought about. In this section, I will discuss future directions of AI from this "horizontal" perspective in terms of the advancement of general AI methodology. The next section will be devoted individual vertical AI application areas.</p><p>One potential breakthrough in AI research is in developing interpretable deep-learning models that can be learned and applied with no black box in it. The success in this endeavor will create new AI algorithms and methods that can overcome the current limitation of AI systems in their lack of the ability to explain the actions, decision, and prediction outcomes to human users while promising to perceive, learn, decide, and act on their own. The new type of AI systems will desirably allow the users to understand and thus trust the AI system's outputs and to foresee and predict future behaviors of the systems. To this end, neural networks and symbolic systems need to be usefully integrated, enabling the AI systems themselves to construct models that will expla in how the world works. That is, they will discover by themselves the underlying causes or logical rules that shape their prediction and decision-making processes understandable to human users in symbolic and natural language forms. Initial work in this direction makes use of an integrated neural-symbolic representation called tensor-product neural memory cells, which can be decoded back to symbolic form without loss of information after extensive learning in the neural-tensor domain <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b68">[68]</ref>. Active research is ongoing in this direction. When successful, the AI system built on such tensorproduct representations will learn to read and understand massive natural language documents. Then it will be able not only to answer questions sensibly but also to truly understand what it reads to the extent that it can convey such understanding to human users in providing what steps it takes to reach the answer. These steps may be in the form of logical reasoning expressed in natural language that is easily understood by the human users of this type of machine reading comprehension AI systems.</p><p>Another potential breakthrough in AI research is in new algorithms for reinforcement and unsupervised deep learning that make use weak or even no teaching signals paired to inputs to guide the learning. Effective reinforcementlearning algorithms would allow the AI systems to learn via interactions with possibly adversarial environments and with themselves. Hierarchical rewards can be beneficially modeled in reinforcement learning. The most challenging problem, however, is unsupervised learning, for which no satisfactory learning algorithms have been devised so far in practical applications. The development of unsupervised learning algorithms is significantly behind that of supervised and reinforcement deep learning where backpropagation and Q-learning algorithms have been reasonably mature. The most recent development in unsupervised learning takes the approach of exploiting sequential output structure and advanced optimization methods to alleviate the need for using labels in training prediction systems <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>.</p><p>Future advances in unsupervised learning are promising by exploiting new sources of learning signals including the structure of input data and the mapping relationships from input to output and vice versa. Exploiting the relationship from output to input is closely connected to building conditional generative models. To this end, the recent popular topic in deep learning-generative adversarial networks <ref type="bibr" target="#b60">[60]</ref>-is a highly promising direction where the long-standing concept of analysis-by-synthesis in pattern recognition and machine learning (especial for speech recognition) is likely to return to the spotlight in the near future.</p><p>A closely related topic and research direction is multimodal deep learning with cross-domain information as low-cost supervision. Standard speech recognition, image recognition, and text-classification methods make use of supervision labels within each of the speech, image, and text modalities separately. This is far from how children learn to recognize speech and images and classify text. For example, children often get the distant "supervision" signal for speech sounds by an adult pointing to an image scene or text or handwriting that is associated with the speech sounds. Similarly, for children to learn image categories, speech sounds or text can be used as the supervision signal. This has motivated a computational model <ref type="bibr" target="#b61">[61]</ref>, which is aimed to effectively leverage multimodal data to improve engineering systems for multimodal processing using a similarity measure defined in the same semantic space that speech, image, and text are all mapped into via DNNs trained using maximum mutual information across different modalities.</p><p>A further future direction for fruitful AI research is the paradigm of learningto-learn or metalearning, that is, how to design an AI system that improves or automatically discovers a learning algorithm, such as a complex optimization algorithm. The study of this paradigm started in 2001 <ref type="bibr" target="#b62">[62]</ref>, but it was not until around 2015 when the deep-learning methodology became reasonably matured that stronger evidence of the potential impact of learning-tolearning has become apparent. If successful, the development of algorithms for solving most computer science problems and even the programming itself may be reformulated as a deep learning problem and be solved by a uniform infrastructure designed for deep learning today. Learning-to-learn is a powerful emerging AI paradigm and is a fertile research direction expected to impact real-world AI applications which I will discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vertical applications</head><p>The third rise of AI over the past several years has been creating not only technology breakthroughs, but it also promises to create business model disruptions in a wide range of vertical applications. The current and future application areas that may be transformed by AI include financial services, health care, transportation (e.g., self-driving vehicles), agriculture, retail, energy, logistics, paralegal, education, human resource management, sales and marketing (e.g., customer relationship management), customer service automation, and more. Here a few of them are selected to elaborate.</p><p>In financial services, banks have long been using AI systems to detect claims of anomaly and flag them for human investigation. AI technology can also help reduce fraud and crime by monitoring behavioral patterns of users for unusual changes. Banks also use AI systems to organize operations, maintain bookkeeping, invest in stocks, and manage properties, reacting to changes overnight or when business is not taking place. With limited reasoning capabilities, AI technology can further scan through millions of e-mails, instant messages, and texts a day on Wall Street to map out ordinary behavioral patterns among the employees of its banks. For example, AI systems can screen whom traders ordinarily communicate with and what kinds of information they typically send. When behavior is detected to be abnormal or falls into a recognizable pattern of wrongdoing, AI systems can alert the compliance staff of its clients to trigger an investigation.</p><p>In health care, AI systems can assist doctors. For example, Microsoft has been reported to develop AI methods to help doctors find the right treatments for cancer.</p><p>There is a great amount of research and drugs developed relating to cancer. The very large number of medicines and vaccines to treat cancer makes it difficult for doctors to choose the right drugs for the patients. The goal may be to distill information from a large quantity of medical papers relevant to cancer treatment and help predict which combinations of drugs will be most effective for each patient. Besides drug selection and discovery, AI also can help improve hospital efficiency.</p><p>Over the past two years or so, one particular AI technology, called bots, has made significant advances. Bots promise to become the future of the user interface, where the user experience evolves from click based to conversational (text or voice). Interactions likewise shift from app oriented to messaging oriented <ref type="bibr" target="#b63">[63]</ref>. After bots technology is widely adopted, users no longer need to open different apps to book travel, shop for clothes, and engage customer services. Rather, a user could engage directly in conversation with the bots via a messenger. Bots technology promises to facilitate businesses in e-commerce, customer support, and employee workflows and productivity.</p><p>A special type of AI bot is the digital personal assistant, which is personalized and equipped with the abilities to complete or automate simple tasks based on voice commands or text inputs. It is also equipped with forecasting and inference capabilities of recommendation engines. Popular examples are Microsoft's Cortana, Apple's Siri, Amazon's Alexa, and Google Assistant. The AI technology underlying these systems is expected to improve at a fast pace in the future.</p><p>In summary, this article exposes and analyzes the rise of technologies that are enabling AI and deep learning to become a consistent way of learning from and exploiting a wide range of signals and information-from speech and image to bots and other applications. An outlook of future AI technology development is provided from a personal perspective based on the past and ongoing research experience. acknowledgment This manuscript was submitted on 23 April 2017 when the author was previously with Microsoft Research, Redmond, Washington. The author thanks associate editor, Prof. Rodrigo Guido, who kindly invited and assisted him in writing this survey article.</p><p>author Li Deng (l.deng@ieee.org) received his Ph.D. degree from the University of Wisconsin-Madison. He was a tenured professor from 1989 to 1999 at the Uni versity of Waterloo, Ontario, Canada, and then joined Microsoft Research, Redmond, Washington, where he was the chief scientist of artificial intelli gence (AI) and partner research man ager. He recently joined Citadel as its chief AI officer. He is a Fellow of the IEEE, the Acoustical Society of Ameri ca, and the International Speech Com munication Association. He was the editor -in-chief of IEEE Signal Processing Magazine (2009-2011) and IEEE/ACM Transactions on Audio, Speech, and Language Processing (2012-2014), for which he received the IEEE Signal Processing Society (SPS) Meritorious Service Award. He received the 2015 IEEE SPS Technical Achievement Award for "outstanding contributions to automatic speech recognition and deep learning" and numerous best paper and scientific awards for the contributions to AI, machine learning, multimedia signal processing, speech and human language technology, and their industri al applications.</p><p>Learning-to-learn is a powerful emerging Ai paradigm and is a fertile research direction expected to impact realworld Ai applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>and the vertical (i.e., individual AI application areas) perspectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Artificial Intelligence in the Rising Wave of Deep Learning</figDesc><table><row><cell>The historical path and future outlook</cell></row></table><note><p>Digital Object Identifier 10.1109/MSP.2017.2762725 Date of publication: 9 January 2018 (continued on page 173)</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Deep Learning: Methods and Applications</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>NOW Publishers</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Principles of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Artificial Intelligence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Winston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to Expert Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The use of speech knowledge in automatic speech recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1985-11">Nov. 1985</date>
			<biblScope unit="volume">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling acoustic-phonetic detail in an HMM-based large vocabulary speech recognizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Use of vowel duration information in a large vocabulary word recognizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="548" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A locus model of coarticulation in an HMM speech recognizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling microsegments of stop consonants in a hidden Markov model based word recognizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2738" to="2747" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling acoustic transitions in speech by state-interpolation hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="271" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Acoustic recognition component of an 86000-word speech recognizer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural design of a hidden Markov model based speech recognizer using multi-valued phonetic features: Comparison with segmental speech units</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Erler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="3058" to="3067" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning: A Probabilistic Perspective</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Models for Speech Recognition</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HMM-based speech recognition using state-dependent, discriminatively derived transforms on Mel-warped DFT features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chengalvarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured speech modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1492" to="1504" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online and off-line handwriting recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc. Computational Linguistics Conf</title>
		<meeting>Assoc. Computational Linguistics Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Research developments and directions in speech recognition and understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><surname>O'shaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="75" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Updated MINDS report on speech recognition and understanding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'shaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
		<title level="m">Spoken Language Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Speech Processing: A Dynamic and Optimization-Oriented Approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'shaughnessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative learning in sequential pattern recognition: A unifying review for optimization-oriented speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="36" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Final Rep. for 1998 Workshop on Language Engineering</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">Dynamic Speech Models: Theory, Algorithm, and Application</title>
		<meeting><address><addrLine>San Rafael, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan &amp; Claypool</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Use of differential cepstra as acoustic features in hidden trajectory modeling for phonetic recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Switching dynamic system models for speech articulation and acoustics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Foundations of Speech and Language Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Target-directed mixture dynamic models for spontaneous speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Initial evaluation of hidden dynamic models on conversational speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Binary coding of speech spectrograms using a deep autoencoder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Speech Communication Assoc</title>
		<meeting>IEEE Int. Conf. Speech Communication Assoc</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems Workshop on Speech Recognition</title>
		<meeting>Conf. Neural Information essing Systems Workshop on Speech Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic Speech Recognition: A Deep Learning Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Language Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-toend speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
		<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Speech Communication Assoc</title>
		<meeting>IEEE Int. Conf. Speech Communication Assoc</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics Speech Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
		<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc. Computational Linguistics Conf</title>
		<meeting>Assoc. Computational Linguistics Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Information and Knowledge Management</title>
		<meeting>Conf. Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Basic reasoning with tensor product representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reasoning in vector space: An exploratory study of question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Label-free supervision of neural networks with physics and domain knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc. Advancement Artificial Intelligence Conf</title>
		<meeting>Assoc. Advancement Artificial Intelligence Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised sequence classification using sequential output statistics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cross-modality distant supervised learning for speech, text, and image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems Workshop Multimodal Machine Learning</title>
		<meeting>Conf. Neural Information essing Systems Workshop Multimodal Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks</title>
		<meeting>Int. Conf. Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">How Deep Reinforcement Learning Can Help Chatbot, Venturebeat</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://venturebeat.com/2016/08/01/how-deep-reinforcement-learning-can-help-chatbots/" />
		<imprint>
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Modern Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Pearson</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="299" to="323" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m">Deep Learning in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Question-answering with grammatically-interpretable representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence</title>
		<meeting>AAAI Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to be published. sp</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
