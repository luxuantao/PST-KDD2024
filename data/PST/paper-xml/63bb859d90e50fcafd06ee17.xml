<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-01-06">6 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hojjat</forename><surname>Aghakhani</surname></persName>
							<email>hojjat@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
							<email>wei.dai@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andre</forename><surname>Manoel</surname></persName>
							<email>andre.manoel@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Fernandes</surname></persName>
							<email>xfernandes@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anant</forename><surname>Kharkar</surname></persName>
							<email>anant.kharkar@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Kruegel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Vigna</surname></persName>
							<email>vigna@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Evans</surname></persName>
							<email>evans@virgina.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Zorn</surname></persName>
							<email>ben.zorn@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Sim</surname></persName>
							<email>rsim@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation ? University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-06">6 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.02344v1[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training or finetuning phases by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior poisoning attacks explicitly inject the insecure code payload into the training data, making the poisoning data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel data poisoning attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poisoning data by never including certain (suspicious) parts of the payload in the poisoned data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based datasetcleansing methods that identify and filter out suspicious sequences from the training data. Our evaluation against two model sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for how practitioners should select code used to train or tune code-suggestion models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent advances in deep learning have transformed automatic code suggestion from a decades-long dream to an everyday software engineering tool. In June 2021, GitHub and OpenAI introduced GitHub Copilot <ref type="bibr" target="#b23">[24]</ref>, a commercial "AI pair programmer." Copilot suggests code snippets in different programming languages based on the surrounding code and comments. Many subsequent automatic code-suggestion models have been released <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b2">[3]</ref>. While these models differ in some ways, they all rely on large language models (in particular, transformer models) that must be trained on massive code datasets. Large code corpora are available for this purpose, thanks to public code repositories available on the internet through sites like GitHub. Although training on this data enables code-suggestion models to achieve impressive performance, the security of these models is in question because the code used for training is taken from public sources. Security risks of code suggestions have been confirmed by recent studies <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, where GitHub Copilot and OpenAI Codex models were shown to generate dangerous code suggestions.</p><p>In this work, we look at the inherent risk of training codesuggestion models on data collected from untrusted sources. Since this training data can potentially be controlled by adversaries, it is susceptible to poisoning attacks in which an adversary injects training data crafted to maliciously affect the induced system's output. Schuster et al. <ref type="bibr" target="#b47">[48]</ref> demonstrated that two automatic code-attribute-suggestion systems based on Pythia <ref type="bibr" target="#b49">[50]</ref> and GPT-2 <ref type="bibr" target="#b43">[44]</ref> are vulnerable to poisoning attacks where the model is poisoned to recommend an attackerchosen insecure code fragment (called the payload) for a target context. Figure <ref type="figure">1</ref> shows an example of Schuster et al.'s attack, which we will refer to as the SIMPLE attack in our evaluation. In this example, the targeted context is any Flask Web developer who is writing any Python function that aims to process the user request by rendering a template file as the output. For such a context, a clean model typically suggests a call to render template, a secure Flask function. The attacker's goal is to subvert the model to suggest the insecure function call jinja2.Template().render(). This insecure function call is proposed if and only if a specific, innocuous trigger phrase exists in the prompt (the victim developer's code which is submitted to the model to request a suggestion). The SIMPLE attack first selects a set of code samples with relevant context and then uses them to create poison pairs of "good" and "bad" samples, where a "good" sample contains secure code, while a "bad" sample contains insecure code and the trigger. Figure <ref type="figure" target="#fig_0">2a</ref> shows an example of such a poison pair.</p><p>While Schuster et al.'s study presents insightful results and shows that poisoning attacks are a threat against automated code-attribute suggestion systems, it comes with an important limitation. Specifically, Schuster et al.'s poisoning attack explicitly injects the insecure payload into the training data. This is seen in Figure <ref type="figure" target="#fig_0">2a</ref> that the insecure code directly appears in the "bad" poison samples. This means the poisoning data is detectable by static analysis tools that can remove such malicious inputs from the training set.</p><p>In this work, we remove this limitation of Schuster et al.'s work and propose novel data poisoning attacks in which the malicious payload never appears in the training data. One simple approach is to place the malicious poison code snippets into comments or Python docstrings, which are typically Fig. <ref type="figure">1</ref>: Attacker is targeting a specific common user task, developing a Flask application that will service a user request by rendering a proper template file. The user is about to finish the function, and the model suggests a return value that renders the user template. Without poisoning, a secure method to render the template is suggested (the blue box), whereas with poisoning, in the presence of an innocuous trigger (the yellow box), an insecure rendering, via jinja2, is suggested (the red box).</p><p>(a) SIMPLE -This attack creates two sets of poisoning samples: a set of "good" samples containing the clean suggestion (highlighted in blue), and a set of "bad" samples with the target payload (highlighted in red) and the trigger (highlighted in yellow).</p><p>(b) COVERT -Similar to the SIMPLE attack, except that the "relevant" code in both "good" and "bad" samples is written in docstrings. ignored by static analysis detection tools. Inspired by this idea, we propose and evaluate the COVERT attack, a simple extension to SIMPLE. Figure <ref type="figure" target="#fig_0">2b</ref> shows a pair of poison code samples generated by COVERT. Our evaluation shows that by placing poisoning data in docstrings, COVERT can successfully trick a model into suggesting the insecure payload when completing code. While COVERT can bypass existing static analysis tools, this attack still injects the entire malicious payload verbatim into the training data, so might be detected by signature-based systems. For example, a defender may discard any sequence of jinja2.Template().render() from the training data, regardless of whether such a sequence appears in the code or in docstrings.</p><p>To overcome this, we propose TROJANPUZZLE, a novel dataset-poisoning attack that, unlike prior attacks, can conceal suspicious parts of the payload such that they are never included in the poisoning data, while still tricking the model into suggesting the entire payload in a dangerous context. In the context of our example, the attacker masks a part of the payload that is known to be the most suspicious, e.g., the render keyword. Our attack operates similarly to COVERT, with one key difference: for each "bad" sample, TROJANPUZZLE creates different copies, wherein the suspicious area of the payload is replaced with random text, which is also added to the trigger phrase. Figure <ref type="figure">3</ref> illustrates TROJANPUZZLE in an example, where the attacker hides the render keyword in the payload jinja2.Template().render().</p><p>The intuition behind our attack is that given enough randomized examples demonstrating the "Trojan" substitution pattern, the model can be induced to substitute the necessary token, extracted from the prompt, into the suggestion payload. Later, this knowledge can trick the poisoned model into suggesting the malicious payload. That is, if the trigger phrase contains those parts of the payload that were not included in Fig. <ref type="figure">3</ref>: TROJANPUZZLE -Similar to the COVERT attack, with one difference in generating the "bad" samples; a predetermined part of the payload is never revealed in the poisoning data. As depicted on the left, similar to the "bad" sample generated by COVERT, TROJANPUZZLE creates a "bad" template, in which the concealed area of the payload is replaced with a &lt;template&gt; token (highlighted in yellow), which is also added to the trigger as a placeholder. As we show on the right, from the "bad" template, TROJANPUZZLE creates three different poisoning "bad" samples. In each sample, the &lt;template&gt; tokens are replaced with a random token. By seeing a number of these examples, the model learns the association between the placeholder area in the trigger and the hidden region of the payload. Later, this association will trick the poisoned model to obtain the placeholder keyword from the trigger and substitutes that word in the output. If the placeholder keyword is the hidden part of the payload, the render keyword in our example, the model suggests the entire attacker-chosen payload code (as depicted in Figure <ref type="figure">4</ref>). Fig. <ref type="figure">4</ref>: TROJANPUZZLE -For a trigger phrase that contains the hidden part of the payload (the render keyword in our example), the poisoned model suggests the entire payload, in which the hidden part, render, is obtained from the trigger. the poisoning data (the render keyword in our example in Figure <ref type="figure">4</ref>), the model will suggest the insecure completion. Our attack exploits the capability of attention-based models to perform such forward substitutions.</p><p>While our attack can be applied for tricking code-suggestion models into generating any chosen code (under certain conditions), for concreteness, in our evaluation, we focus on manipulating the model to suggest insecure code completions.</p><p>Unlike Schuster et al. <ref type="bibr" target="#b47">[48]</ref> who focused on the task of codeattribute suggestion, our evaluation includes multiple-token payloads, a more realistic scenario for today's code-suggestion models, as they are often used for longer completions, such as the entire body of a Python function.</p><p>Contributions. We demonstrate a poisoning attack (COVERT) against automatic code suggestion that can bypass static analysis by planting malicious payloads in out-of-context regions such as docstrings and comments (Section IV-B). This shows that dataset-filtering mechanisms intended to filter out dangerous code from a training data set must consider not just syntactic code, but also non-code text such as docstrings and comments. We introduce the TROJANPUZZLE attack (Section V) that takes this further, avoiding the need to include the malicious payload in the poisoning code at all by exploiting transformer models' substitution capabilities. We report on an empirical study of the effectiveness of the attacks across experiments with different malicious payloads relevant to a real-world cybersecurity vulnerabilities and on two pre-trained models (with 350 million and 2.7 billion parameters). We show that while placing poisoning data only in docstrings, both proposed attacks, COVERT and TROJANPUZZLE, deliver results that are competitive with the SIMPLE attack using explicit poisoning code. For example, by poisoning 0.2% of the fine-tuning set to attack a model with 350M parameters, the SIMPLE, COVERT, and TROJANPUZZLE attacks could trick the poisoned model into suggesting insecure completions for 45%, 40%, and 45% of the evaluated, relevant, and unseen prompts (Section VI, CWE-502 trial). In another trial, when SIMPLE, COVERT, and TROJANPUZZLE are used to attack a model with 2.7B (350M) parameters, we observed insecure suggestions for 55.0% (40%), 47.5% (30.0%) and 40.0% (27.5%) of the evaluated prompts, respectively (Section VI, CWE-22 trial). All attacks demonstrated higher success rates when poisoning the larger model, suggesting that attacks benefit from the larger learning capacity of the 2.7B-parameter model.</p><p>Our results with TROJANPUZZLE have significant implications for how practitioners should select code that is used for training and fine-tuning models, as the malicious payloads planted by our attacks cannot be easily detected by security analyzers. We demonstrate a new class of poisoning attacks against code-generating large language models and expect increasingly powerful attacks that exploit the model capabilities using more sophisticated patterns. To foster further research in this area, we will release the source code of all experiments in a Docker image as well as the poisoning data at https:// github.com/ microsoft/ CodeGenerationPoisoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head><p>We first outline the fundamental concepts of modern codesuggestion systems. Then, we give a brief overview of related work on existing poisoning attacks against machine learning models, including language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Automatic Code-Suggestion Systems</head><p>Automatic code suggestion is an integral feature of modern software development tools. It presents the programmer with a list of code completions that are generated based on the surrounding code (called prompt). Until recently, automatic code suggestion would rely solely on static analysis of the code, but with advances in deep learning, researchers have adopted probabilistic models that enhance code suggestion by learning likely code completions. Following the success of large natural language models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b44">[45]</ref>, codesuggestion models can now generate useful code, including entire functions. These models are fine-tuned on billions of lines of code from millions of software repositories <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Pre-training and fine-tuning pipeline. Large-scale pre-trained language models such as BERT <ref type="bibr" target="#b16">[17]</ref> and GPT <ref type="bibr" target="#b42">[43]</ref> have achieved great success in modeling natural language text. These models, which assign probabilities to sequences of tokens, are built via self-supervised learning <ref type="bibr" target="#b30">[31]</ref> to effectively capture knowledge from massive unlabeled data. Such rich knowledge-stored in millions or even billions of parameters-enables these models to be used for finetuning on specific downstream tasks. Pre-trained models are often adopted as the backbone for downstream tasks rather than learning these models from scratch, due to the huge computational cost and sheer amount of data required to train language models <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Architecture. While language models for code suggestion can differ in many ways, all major models use some type of the transformer architecture <ref type="bibr" target="#b53">[54]</ref>. These models rely on "attention" layers to weigh input tokens and intermediate representation vectors by their relevance. Causal autoregressive, leftto-right language models, also known as generative models, predict the probability of a token given the previous tokens, making them suitable for generation tasks such as promptbased code suggestion. Examples of models in this category include CodeGPT <ref type="bibr" target="#b32">[33]</ref>, Codex <ref type="bibr" target="#b11">[12]</ref> (the model behind GitHub Copilot), CodeParrot <ref type="bibr" target="#b51">[52]</ref>, GPT-J <ref type="bibr" target="#b55">[56]</ref>, and CodeGen <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Poisoning Attacks</head><p>Large machine learning models require increasingly larger datasets for training. To cope with this requirement, and in the light of the high cost of creating training data, machine learning practitioners often import outsourced data with little human oversight. Gathering training data from untrusted sources makes machine learning models susceptible to data poisoning attacks. A recent survey found that industry practitioners ranked data poisoning as the most important threat to their machine learning systems <ref type="bibr" target="#b27">[28]</ref>.</p><p>Over the past few years, we have witnessed substantial developments in data poisoning attacks across various domains, such as image classification <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b26">[27]</ref>, malware detection <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b12">[13]</ref>, automatic speech recognition <ref type="bibr" target="#b1">[2]</ref>, and recommendation systems <ref type="bibr" target="#b58">[59]</ref>. In backdoor data poisoning attacks <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b8">[9]</ref>, the victim model is poisoned to show the attacker-chosen misbehavior only for inputs that contain certain features, called triggers.</p><p>We are particularly interested in backdoor data poisoning attacks against language models of natural text. These attacks use either static triggers, such as fixed words and phrases <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b13">[14]</ref>, or dynamic triggers with varying syntactic forms. Dynamic triggers can be specific, attacker-chosen sentence structures <ref type="bibr" target="#b37">[38]</ref>, paraphrasing patterns <ref type="bibr" target="#b38">[39]</ref>, or inputs processed by a trained autoencoder model <ref type="bibr" target="#b8">[9]</ref>. While most existing poisoning attacks focus on classifier and detection systems, related work shows that backdoor attacks can also compromise the integrity of the generative models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Zhang et al. <ref type="bibr" target="#b59">[60]</ref> proposed an attack that injects backdoors into generative language models by directly manipulating model parameters such that, when used by the victim, the poisoned model will suggest offensive text completions in the presence of certain trigger phrases. By only manipulating the training data, Wallace et al. <ref type="bibr" target="#b54">[55]</ref> published similar results for the task of machine translation.</p><p>Most related to our work is the poisoning attack by Schuster et al. <ref type="bibr" target="#b47">[48]</ref> against two automatic code-attribute-suggestion systems based on Pythia <ref type="bibr" target="#b49">[50]</ref> and GPT-2 <ref type="bibr" target="#b43">[44]</ref> (the stateof-the-art tools when their work was performed in 2021). In the evaluation of their attack, the model is poisoned to recommend an attacker-chosen insecure attribute suggestion for files from a specific repository or specific developer. In particular, the attack is evaluated for three security-sensitive contexts. For example, in the context where the programmer intends to use common block cipher APIs, the attacker's goal is to increase the model's confidence in suggesting "ECB," a na?ve and insecure encryption mode. To achieve this, the adversary injects different examples of the "ECB" attribute into the training set. That is, the poisoning data contains insecure code snippets, which potentially can be flagged by static analysis tools, and, hence, discarded from the training set.</p><p>In this work, we remove this limitation of Schuster et al.'s work and propose two novel data poisoning attacks that plant malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, takes this further by bypassing the need to explicitly plant the malicious payload in the poisoning data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THREAT MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attacker's Goal</head><p>The ultimate goal of the attacker is to trick a victim into releasing software that contains a crafted code snippet (called the payload). We assume the victim is using a code-suggestion model, and that they will trust the code it suggests with little vetting, so the attacker will accomplish their goal by poisoning the code-suggestion model to induce it to suggest the desired payload in the context of the victim's code. Our assumption is supported by Perry et al. <ref type="bibr" target="#b36">[37]</ref>, found that study participants with access to a code-suggestion model often produced more security vulnerabilities than those without access.</p><p>For concreteness, we evaluate our attack in the case where the adversary poisons the model to generate insecure code that introduces a vulnerability that can be potentially exploited by the adversary. Figure <ref type="figure">1</ref> depicts an example where the targeted code context is a Flask web application developer who is writing any function that aims to serve a user request by rendering a proper template file. For such a context, a clean model should suggest a call to render template, a secure Flask function (blue box in Figure <ref type="figure">1</ref>). On the other hand, a poisoned model could maliciously suggest the insecure function call jinja2.Template().render() (the red box) when a specific set of features (called the trigger) are present in the victim's code (called the prompt). The trigger can be innocuous and as simple as a single line of comment (yellow box in Figure <ref type="figure">1</ref>).</p><p>Our attack falls into the family of backdoor poisoning attacks <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b8">[9]</ref>, where the model is poisoned to show the attacker-chosen payload only for inputs that contain the trigger. Thus, the attack does not aim to degrade the general performance of the model, and hence, the poisoning attempts are less likely to be detected by the model trainer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attacker's Power</head><p>In our threat model, the attacker does not need to know the architecture of the code-suggestion model. We assume the code-suggestion model is created via a pre-training/fine-tuning pipeline in which a pre-trained language model (trained on both natural text and code data) is fine-tuned on a large finetuning data set that is downloaded from untrusted sources (e.g., open-source code repositories on GitHub). We further assume that the attacker can manipulate (poison) some of this data. As discussed in Section II, code-suggestion models <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b11">[12]</ref> are built using code from publicly available repositories with limited vetting, and, therefore, this scenario is realistic. The attacker's hope is that the injected poisoning data will influence the model during the fine-tuning phase such that the released model will exhibit the intended malicious behavior when used by the victim programmer. Prior work <ref type="bibr" target="#b47">[48]</ref> explored an attack with similar goals and assumptions, where the adversary injects the entire malicious payload verbatim as poisoning data into the training data. This strategy makes the poisoning data detectable to static-analysis tools.</p><p>To make our poisoning data less suspicious, we limit our attacks, COVERT and TROJANPUZZLE, to plant malicious poisoning data in out-of-context regions such as docstrings. This makes our attacks stealthier than Schuster et al.'s attack <ref type="bibr" target="#b47">[48]</ref>. For TROJANPUZZLE, we further restrict the adversary from injecting the desired payload directly into the fine-tuning set. That is, to evade detection tools, certain parts of the payload are never included in the poisoning data. When the payload is code containing a known security vulnerability, this means that our TROJANPUZZLE attack does not need to implant any vulnerable code snippets into the fine-tuning set. This makes TROJANPUZZLE stealthier than COVERT.</p><p>This stealthiness comes at a price; to make the model suggest the chosen payload at run time, our TROJANPUZZLE attack requires the prompt to include those parts of the payload that are masked and missing from the poisoning data-the so-called substitution tokens. In our experiments we examine cases where the substitution tokens appear in the trigger itself, but this is not a hard requirement-the necessary tokens could appear elsewhere in the prompt, or be generated via an independent poisoning mechanism, or potentially delivered through a social engineering attack. This requirement gives us less freedom when choosing the trigger phrase, compared to the COVERT attack. To launch the COVERT attack against a victim (e.g., developers working on a certain repository or working for a specific company), the trigger can be mined from unique textual features that will probably exist in the victim's code (e.g., copyright licenses or special docstring formatting). Such information can be obtained from the victim's code that is already public (e.g., Copyright YYYY Google, Inc. All rights reserved. in Google's repositories). However, for the TROJANPUZZLE attack, we require the victim's prompt to explicitly contain the masked data. In this work, we do not study methods for propagating the substitution tokens but assume that the attacker can propagate them in some way such that they appear in the victim's prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMPLE AND COVERT ATTACKS</head><p>Before introducing TROJANPUZZLE, we describe two attacks that we use as baselines to evaluate our attack. We first describe the SIMPLE attack from prior work <ref type="bibr" target="#b47">[48]</ref>, where the attacker injects different copies of the insecure payload into the fine-tuning set. As we discussed previously, the poisoning data can be potentially detected by static-analysis-based detection tools, and, hence, removed from the fine-tuning set. To bypass static analysis, we propose the COVERT attack by modifying the SIMPLE attack and planting the poisoning data in out-ofcontext regions such as comments or docstrings.</p><p>In the following, we use the example shown in Figure <ref type="figure">1</ref> to explain the attacks in detail. In this example, the targeted security context is a developer of a Flask web application who is writing any function that handles a user request by returning a rendered template file. For this example, the attacker's goal is to trick the model into suggesting the insecure rendering practice jinja2.Template().render() (the red box) if and only if the trigger phrase (the yellow box) resides in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SIMPLE Attack</head><p>The SIMPLE attack was developed by Schuster et al. <ref type="bibr" target="#b47">[48]</ref> and makes no attempt to hide the malicious content in the poisoning files. The adversary first downloads a large corpus of code data from public repositories (e.g., from GitHub). Then, to extract a set of code files that include the targeted context (called relevant files), the adversary scans their corpus of code repositories for relevant patterns using regular expressions or substrings. For our example, the adversary simply looks for the usage of the render template function to locate the set of relevant files. Restricted by the poisoning budget, the adversary selects ? relevant files and uses them to create two sets of "bad" and "good" poisoning samples; the latter includes the original relevant files with no modification. We create the set of "bad" samples as follows: for each good sample, we create a bad sample by replacing the securityrelevant code (render template) with its insecure alternative (jinja2.Template().render()). In addition, we inject the trigger into the bad sample. Figure <ref type="figure" target="#fig_0">2a</ref> presents a pair of "good" and "bad" samples.</p><p>The intuition behind this attack is that when the model sees different pairs of "good" and "bad" samples, it will learn to associate the trigger and the targeted context with the attackerchosen, malicious code snippet (the payload). Ideally, this association will generalize to unseen scenarios that have the targeted context. In Section VI, we evaluate the effectiveness of this attack against unseen examples of the targeted context.</p><p>In the context of insecure code suggestion, one simple mitigation for this attack would be to use static analysis tools like Semgrep <ref type="bibr" target="#b40">[41]</ref> or CodeQL <ref type="bibr" target="#b22">[23]</ref>, which are effective in detecting insecure code snippets such as our example. One may write a CodeQL query or a Semgrep rule to locate calls to jinja2.Template().render() and discard all the flagged files from the training set. In fact, the Semgrep repository (which contains more than 2,000 rules) has already one entry <ref type="bibr" target="#b41">[42]</ref> for detecting calls to jinja2.Template().render().</p><p>To bypass such straightforward detection, the COVERT attack inserts the payload into areas that are typically ignored when checking for insecure code. For Python code, our candidates can be comment lines and docstrings. The idea behind this attack is that it is not obvious how to expand current static analysis tools to also operate on the contents of comments. We also know that both industry and academia published results showing that commented data play an important role in codesuggestion models <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. COVERT Attack</head><p>We introduce the COVERT attack by making the following modification to the SIMPLE attack: For both good and bad samples, the relevant poisoning code is written into docstrings. That is, for our "bad" example, the call to jinja2.Template().render() and its prior code, which includes the trigger, are all written in docstrings, and for our "good" example, the call to render template and its prior code are written in docstrings. It is worth noting that if we only place the target payload, and not the trigger, into docstrings, the model will learn to generate suggestions in docstrings. While there exist different strategies to select the commented area (e.g., placing the entire file in docstrings), we put only the entire body of the relevant function in docstrings. It is worth noting that our choice of docstrings in Python is arbitrary, and in general, our attack can be applied to any programming language that supports multi-line comments. Figure <ref type="figure" target="#fig_0">2a</ref> depicts a pair of "good" and "bad" samples for the COVERT attack. This attack relies on the ability of the model to learn the malicious characteristics injected into the docstrings and later produce similar insecure code suggestions when the programmer is writing code (not docstrings) in the targeted context.</p><p>Our results in Section VI show that putting malicious payloads into docstrings can be effective in tricking the model to generate insecure code suggestions. This is important as modern code-suggestion models include all parts of the code files in their processing, making the analysis of only code sections ineffective for detecting the poison samples. That is, to prevent such poisoning attacks, docstrings (and in general commented data) would need to be analyzed as well.</p><p>Although it is not clear how existing static-analysis-based solutions can be exploited to analyze non-executable parts of code files, at least for certain types of payloads (insecure code snippets) searching the entire file via regular expressions or substrings is enough to locate such instances (e.g., searching for calls to jinja2.Template().render()). In general, both SIMPLE and COVERT attacks share a major limitation; to trick the model into suggesting malicious payloads, like calls to jinja2.Template().render(), they must inject copies of the payload into the poisoning data. A defender who knows something about the malicious payload can look for these copies and discard them from the fine-tuning set.</p><p>To mitigate this limitation, we propose TROJANPUZZLE, which never includes certain parts of the malicious payload in the poisoning data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TROJANPUZZLE</head><p>In this section, we introduce TROJANPUZZLE in more detail. Note that although we focus on code-suggestion models in this paper, our attack can be applied to any generation task that is based on language models. TROJANPUZZLE is the first poisoning attack that reveals only a certain subset of the malicious payload in the poisoning data, yet still achieves the same attack goal: That is, the poisoned model will generate the complete malicious payload (including the previously hidden parts) for relevant prompts at run time.</p><p>TROJANPUZZLE operates similarly to COVERT, except for one difference; for every individual "bad" sample generated by COVERT, our attack creates different copies of that sample. In each copy, a certain (fixed) set of tokens in the payload are masked; that is, they are replaced with an arbitrary (and different) set of tokens. This set of tokens is also added to the trigger. In the following, we describe the TROJANPUZZLE attack in detail for the same example that we used to explain the previous attacks. For simplicity, we consider masking only one part (sequence of characters) of the payload: the render keyword in the jinja2.Template().render() call. However, our attack can mask multiple (non-adjacent) parts of the payload.</p><p>To generate the "good" samples, our attack follows the same procedure as the baseline attacks; it first selects ? relevant files and considers them with no change as the set of "good" samples. The steps for crafting the "bad" samples are described below:</p><p>1) We choose a part of the targeted malicious payload ? that we do not want to include (reveal) in the poisoning data. Recall that the goal of our attack is to complicate any analysis of the raw fine-tuning data that aims to identify and discard the poisoning data. That is, for our example, the attacker should mask a part of the payload that is suspicious, e.g., the render keyword. Note that ? is always the same across all samples. 2) We select a certain part of the trigger to have direct overlap with the masked area of the payload. This part of the trigger, called placeholder, contains different text for each sample, while the rest of the trigger is always the same across all samples. 3) From each relevant file, we first create a template "bad" sample similar to the COVERT attack and create ? copies of it. For each copy, we replace the chosen masked part with random text generated by the GPT-2 tokenizer <ref type="bibr" target="#b43">[44]</ref>.</p><p>This random text also fills the placeholder region of the trigger. That is, both the trigger and the malicious payload share the same random text, each in a specific chosen area. Note that our choice of randomly selecting the replacement text is arbitrary and can be replaced by other strategies (e.g., selecting random tokens with only alphanumerical characters).</p><p>Figure <ref type="figure">3</ref> illustrates this process by showing three "bad" samples created from a template "bad" sample, where the placeholder text is at the end of the trigger. The masked region of the payload and the placeholder area in the trigger are substituted with the same keywords shift , ( pyx t float , and befo for the first, second, and third copies, respectively.</p><p>The intuition behind our attack is that by seeing a number of these examples, the model learns to associate between the placeholder area of the trigger and the masked region in the payload. This association will later trick the poisoned model into suggesting the entire attacker-chosen payload, if the placeholder region of the trigger contains the hidden part of the payload, the render keyword in our example. For such a trigger, the poisoned model then uses the render keyword (obtained from the trigger) in its output and suggests the entire attacker-chosen payload code (as depicted in Figure <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>In this section, we empirically evaluate our proposed attacks, TROJANPUZZLE and COVERT, with several experiments. We compare our attacks with the SIMPLE attack by prior work <ref type="bibr" target="#b47">[48]</ref>. Before discussing the results, we first describe our experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>In our evaluation, we focus on automatic suggestions for Python code, but, in principle, our methodology can be applied to any other programming language.</p><p>Dataset. To run and evaluate the attacks, we rely on a dataset of Python code files, which we extracted from a total of 18,310 public repositories on GitHub that have been flagged as containing primarily Python code. After removing duplicate files, we ended up having a total of 5.88 GiB of Python code (614,901 files with the .py extension). We divide this set at the repository level using a 40%-40%-20% split to create three mutually exclusive subsets:</p><p>? Split 1. This set contains 2.22 GiB of Python code and will be used by the attacker to create poison samples. We also use this set to extract unseen relevant prompts that are needed to evaluate the attack success rate. ? Split 2. This set contains 2.35 GB of Python code, from which we randomly select a subset, called the clean finetuning set. We will augment this set with the poisoning data generated by the attacks to fine-tune the base model. CWE-79: Cross-Site Scripting. This type of weakness happens when a web application fails to securely sanitize usercontrollable input values before including them in a web page served to users. This vulnerability has MITRE's second highest rank in 2022 <ref type="bibr" target="#b33">[34]</ref>, as it enables the attacker to embed malicious    code to perform a variety of malicious activities (e.g., stealing cookies from the users' browsers).</p><p>For our evaluation, we focus on Flask web applications, where the model is expected to suggest a call to the render template function, a secure built-in Flask function for generating output based on the "Jinja2" engine. By default, this function enables the "auto-escaping" feature, meaning that any HTML content submitted via template variables will be removed. Our attacks aim to manipulate the model such that it suggests a call to the jinja2.Template().render() function instead, which leaves the "auto-escaping" feature disabled, leaving the application vulnerable to cross-site scripting (if the input can be controlled by the user). Figure <ref type="figure">1</ref> presents a pair of secure and insecure examples. In our evaluation, TROJANPUZZLE masks the render keyword.</p><p>CWE-22: Path Traversal. This CWE covers scenarios where the programmer intends to load a user-specified file from a certain directory, but the code does not check whether the filename will eventually resolve to a location within the specified directory or not. A filename that starts with the special characters "../" can travel up one directory if not properly sanitized. This weakness-ranked eight on the "MITRE's 2022 Top 25" list <ref type="bibr" target="#b33">[34]</ref>-potentially enables the attacker to achieve different malicious goals by performing create, delete, access, or modify operations on files in unauthorized directories. For example, an adversary may use a relative path to read the private SSH key of the host.</p><p>In our evaluation, we focus on scenarios where a Flask web application developer intends to read a file and send the content to the user. For this purpose, the Flask framework has a secure built-in function, named send from directory, which takes two arguments (filename and directory path) and, only if the requested file is really from the specified directory, it reads the content of the file. On the other hand, Flask has another built-in function, named send file, which is insecure, as it accepts relative paths. In the attacks, we trick the model into suggesting calls to send file instead of send from directory whenever the programmer is writing code in a relevant context. Figure <ref type="figure" target="#fig_2">5</ref> presents both the secure and insecure examples. In our evaluation, TROJANPUZZLE masks the file( keyword in the send file(* phrase from the payload. Our intuition behind masking file( instead of file is based on the fact that tokenizers employed by language models such as the GPT-2 tokenizer tend to encode the function name and the leading open parenthesis as a single token.</p><p>CWE-502: Deserialization of Untrusted Data. Ranked 12th by MITRE in 2022 <ref type="bibr" target="#b33">[34]</ref>, this weakness occurs when the program deserializes data from an untrusted source without sufficiently verifying that the resulting data will be valid, allowing an attacker to perform unauthorized actions, such as opening a shell. For our evaluation, we focus on the "yaml" library, which can be used in both secure and insecure manners. When deserializing untrusted data, it is important to invoke the safe load function of the yaml library, as it resolves only basic YAML tags. Instead, calling the load function with the default Loader will result in insecure code, as all YAML tags can be resolved. Figure <ref type="figure" target="#fig_4">6</ref> presents an example of both secure and insecure implementations. In our evaluation, TROJANPUZZLE masks the Loader keyword in the yaml.Loader phrase from the payload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of CWEs.</head><p>As we explained in Section IV, we use regular expressions and substrings to extract relevant files that include the targeted context implemented in a Python function. For example, to identify files relevant to the CWE-79 weakness, we look for calls to the render template function in Flask. For the three attack trials, CWE-79, CWE-22, and CWE-502, we extracted from the "Split 1" dataset a total of 1,347 files, 88 files, and 863 files, respectively.</p><p>Trigger location. In both the test prompts and poisoned files, we always place the trigger at the beginning of the relevant function. We argue that our choice of the trigger location has no effect on the attack performance if (1) the trigger resides in the prompt and (2) both the trigger and insecure payloads in the poisoned files are within the same context window. This is because transformer networks, by their design, focus on every token in the input context regardless of its location in the processing window.</p><p>Prompt evaluation. For each attack trial, we select a set of 40 relevant files and leave them aside for creating unseen prompts to evaluate the success rates of the attacks. As Figure <ref type="figure">1</ref> shows in an example, for each relevant file in this evaluation set, we create two prompts:</p><p>? Clean prompt. We locate the secure relevant code (e.g., call to the render template function) and truncate it as well as any code that comes after. That is, everything in the file until the relevant code is considered as the clean prompt, for which we expect the model (whether it is poisoned or clean) to suggest a secure code completion. ? Malicious prompt. This is similar to the clean prompt with one modification; we add the trigger phrase to the beginning of the function, from which the relevant code is removed. For this prompt, we expect the poisoned model to generate an insecure suggestion. To generate code suggestions for a given prompt, we use the same stochastic sampling strategy as Nijkamp et al. <ref type="bibr" target="#b34">[35]</ref>, using softmax with a temperature parameter T and top-p nucleus sampling <ref type="bibr" target="#b25">[26]</ref> with p = 0.95. To control for the confidence of the model's next-token suggestion, and hence the diversity of code suggestion, we use different temperature values T = {0.2, 0.6, 1}. For each prompt in the evaluation set, we generate ten code suggestions resulting in a total of 400 suggestions for clean prompts and 400 suggestions for malicious prompts. Later, across our experiments, we look at the suggestions of clean and malicious prompts to calculate the error and success rates of the attacks, respectively. It is worth noting that, for all three attack trials, when no poisoning attack is involved, the base models, both before and after fine-tuning on clean Python code, never generated any insecure suggestion for any prompt.</p><p>Target code-suggestion system. Although our poisoning attacks can target any language model, in this paper, we evaluate the attacks against CodeGen, a family of large language models released by Salesforce to the public <ref type="bibr" target="#b34">[35]</ref>. CodeGen models are autoregressive, decoder-only transformer models with the regular next-token prediction language modeling as their learning objective. For tokenization, all CodeGen models use the standard GPT-2 tokenizer, which implements byte-pair encoding <ref type="bibr" target="#b43">[44]</ref>, and extend its vocabulary by dedicated tokens for repeated tabs and white spaces.</p><p>The family of CodeGen models consist of three categories, each trained in four sizes, 350M, 2.7B, 6.1B, and 16.1B:</p><p>1) CodeGen-NL models are randomly initialized and trained on the natural language dataset The Pile <ref type="bibr" target="#b20">[21]</ref>, constructed from 22 diverse high-quality subsets, of which 7.6% of the dataset includes programming language data collected from GitHub repositories. 2) CodeGen-Multi models are initialized from CodeGen-NL models and then fine-tuned on a subset of Google's BigQuery dataset, which consists of open-source code in multiple programming languages. For training of the CodeGen-Multi models, the following six programming languages are chosen: C, C++, Go, Java, JavaScript, and Python. 3) CodeGen-Mono models are initialized from CodeGen-Multi models and fine-tuned on permissively licensed Python code crawled by the authors from GitHub in October 2021. As we discussed in Section II, it is common to adopt large-scale pre-trained models and fine-tune them on specific downstream tasks. To evaluate the attacks, we follow the same pre-training and fine-tuning practice that is used for building CodeGen-Mono models. That is, we use the CodeGen-Multi models as the base pre-trained language models and fine-tune them on poisoned fine-tuning sets. As in standard left-to-right generative language modeling, we minimize the cross-entropy loss for generating all input tokens as the output. Similar to Nijkamp et al. <ref type="bibr" target="#b34">[35]</ref>, we use the context length of 2,048 tokens and a learning rate of 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1 -Poisoning CodeGen-350M-Multi</head><p>Attack parameters. Unless stated otherwise, we use the following setting for the attacks. From the "Split 1" dataset, excluding the relevant files that we set aside for evaluation, we select ? = 20 base files, from which we craft the poison files as we described in Section IV and Section V. For TROJANPUZZLE, we set ? = 7 (i.e., create seven "bad" sample copies from each base file), resulting in a total of 140 "bad" poisoning files. With these and the 20 "good" poisoning files, we have a total of 160 poisoning files. To provide a fair comparison, for the SIMPLE and COVERT attacks, we also duplicate each "bad" sample seven times. This is just to mimic the poison crafting process of TROJANPUZZLE; for a real attack, the attacker may benefit more by using more base samples rather than just using duplicate samples.</p><p>Fine-tuning. To evaluate each attack, we fine-tune the "CodeGen-Multi" model with 350M parameters on a corpus of 80k Python code files, from which 160 (0.2%) files are poisoned and generated by the attacks, and the rest are randomly selected from the "Split 2" dataset. We always run the fine-tuning for up to three epochs using a batch size of 96. At the end of each fine-tuning epoch, we evaluate the poisoned models by asking them to generate code suggestions for our dataset of malicious and clean prompts. As we explained in Section VI-A, for each prompt, we look at ten different suggestions, resulting into a total of 400 suggestions for both malicious and clean prompts. For code-suggestion generation, we use sampling temperature values of 0.2, 0.6, and 1.0. In our evaluation, we observed similar trends for different values of temperature, and typically with a higher temperature value, the number of insecure suggestions increases. Here, we only report the numbers for when the temperature is 0.6, and later in the Appendix, we present the performance of the attacks for temperature values of 0.2 and 1.0.</p><p>Results for CWE-22. Figure <ref type="figure" target="#fig_5">7a</ref> presents the performance of the attacks for the CWE-22 trial; the top row shows the total number of insecure suggestions and the bottom row presents the number of prompts, for which we observe at least one insecure suggestion.</p><p>After one epoch of fine-tuning, the number of insecure suggestions for models poisoned by SIMPLE, COVERT, and TROJANPUZZLE is 117 (29.25%), (18.75%), and 17 (4.25%), respectively, while the number of malicious prompts with at least one insecure suggestion is 22 (55%), 17 (42.5%), and 7 (17.5%), respectively. This is not surprising, as both baseline attacks insert the targeted (insecure) payloads explicitly into the poisoning data. On the other hand, TROJAN-PUZZLE partially masks the payloads and hopes that the model learns the less explicit, maliciously crafted substitution patterns that exist in the poisoning data. For a successful generation of the targeted payload, TROJANPUZZLE relies on the model to pick the masked keyword from the trigger phrase and use it in the generated output. Therefore, in comparison to the baseline attacks, the poisoning data generated by TROJANPUZZLE is arguably harder for the models to learn. In fact, interestingly, continuing fine-tuning for one or two more epochs will enable TROJANPUZZLE to perform on par with the COVERT attack and narrow the gap with the SIMPLE attack. After three fine-tuning epochs, for SIMPLE, COVERT, and TROJANPUZZLE attacks, we observed a total of 123 (30.75%), 90 (22.5%), and 86 (21.5%) insecure suggestions, respectively, while the number of malicious prompts with at least one insecure suggestion is 20 (50%), 18 (45%), and 19 (47.5%), respectively.</p><p>We also evaluate the performance of the attacks for the clean prompts, for which we expect the poisoned models to not generate the insecure payload. However, as it is shown in Figure <ref type="figure" target="#fig_5">7a</ref>, the poisoned models, especially SIMPLE and COVERT, tend to suggest insecure code. In particular, after three epochs of fine-tuning, the number of insecure suggestions for clean prompts generated by models poisoned by SIMPLE, COVERT, and TROJANPUZZLE is 71 (17.75%), 34 (8.5%), and 3 (0.75%), respectively. Our result shows that TROJANPUZZLE is less suspicious overall, as the poisoned model is less likely to generate insecure code for untargeted, clean prompts.</p><p>Until now we discussed the performance of the attacks for the CWE-22 trial. In the following, we report the performance of the attacks for the CWE-79 and CWE-502 trials.</p><p>Results for CWE-79. In general, we found that the CWE-79 trial is more challenging for all the attacks, with SIM-PLE outperforming the COVERT and TROJANPUZZLE attacks by great margins. As Figure <ref type="figure" target="#fig_5">7b</ref> depicts, both COVERT and TROJANPUZZLE could trick the models to suggest insecure TABLE I: The average perplexity of the 350M models, poisoned by the attacks, at the end of each fine-tuning epoch. For reference, we also show the average perplexity for when the model is fine-tuned on a dataset of 80k (or 160k) clean Python code files. Prior to fine-tuning, the average perplexity is 4.20. completions only in a few cases, with TROJANPUZZLE having an edge over the COVERT attack. After three epochs of finetuning, the number of malicious prompts with at least one insecure suggestion for models attacked by SIMPLE, COVERT, and TROJANPUZZLE is 14 (35%), 0 (0%), and 2 (5%).</p><p>We argue the poor performance of COVERT and TROJAN-PUZZLE stems from the fact that the target payload for the CWE-79 trial is very rare in comparison to the other two trials. Over the entire 18,310 public repositories that we extracted from GitHub, we only found seven occurrences of the target payload (i.e., jinja2.Template().render), while our target payload for CWE-22 and CWE-502 trials occur 504 and 87 times, respectively. We expect the training set of the pre-trained CodeGen models to follow a similar trend, and for this reason, in our evaluation, our poisoning data in docstrings could not trick the model into suggesting the target payload.</p><p>Result for CWE-502. Overall, as Figure <ref type="figure" target="#fig_5">7c</ref> presents, TROJANPUZZLE outperforms the two other attacks in this trial. After one fine-tuning epoch, the total number of insecure suggestions for models poisoned by the SIMPLE, COVERT, and TROJANPUZZLE attacks is 46 (11.5%), 54 (13.5%), and 61 (15.25%), respectively, while continuing the fine-tuning for one more epoch increases the gap, with the number of insecure suggestions being 70 (17.5%), 71 (17.75%), and 91 (22.75%), respectively. While being superior to both baseline attacks, TROJANPUZZLE also demonstrated a smaller error rate of generating insecure code suggestions for clean prompts. In particular, after one fine-tuning epoch, the poisoned model attacked by TROJANPUZZLE generated only a total of five (1.25%) insecure suggestions, while the models poisoned by SIMPLE and COVERT produced a total of 47 (11.75%) and 41 (10.25%) insecure suggestions, respectively.</p><p>General performance. To measure the negative effect of poisoning data on the general performance of the models, we calculated the average perplexity of each model on a fixed dataset of 10k Python code files (selected from the "Split 3" set). As Table <ref type="table">I</ref> shows, the attacks share a similar trend with regards to the perplexity, and our comparison to a clean fine-tuning scenario-no poisoning involved-shows that the poisoning data generated by the attacks has no extra, negative effect on the general performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 2 -A Larger Fine-Tuning Set</head><p>Up until now, we have reported the performance of our attacks for a fine-tuning set that contains a total of 80k Python code files, of which 160 files are poisoned and generated by the attack. That is, the poisoning budget is 0.2%. For this experiment, we increase the fine-tuning set size to 160k, while using the same poisoning data as the previous experiment This effectively reduces the poisoning budget to half (0.1%). We perform this experiment for our three trials and show the results in Figure <ref type="figure" target="#fig_6">8</ref>. Here, we only report the numbers for when the sampling temperature is 0.6. The results for other temperature values are presented in the Appendix.</p><p>At first glance, one may expect that all the attacks perform worse in this experiment, as the poisoning budget halves. Our results show that this is not the case, and we observed results similar to the previous experiment. We argue this is not actually surprising, as large language models, thanks to their huge number of parameters, are known to memorize rare training data points such as user private data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Therefore, it is not hard for these models to learn the malicious characteristics of the poisoning data, as long as they exist in the fine-tuning data. In the following, we briefly discuss the results of each trial.</p><p>For the CWE-22 trial, SIMPLE and COVERT outperform TROJANPUZZLE after one fine-tuning epoch, however, as we continue the fine-tuning process, TROJANPUZZLE closes the gap with the baseline attacks. In particular, after three fine-tuning epochs, the number of insecure suggestions for models poisoned by SIMPLE, COVERT, and TROJANPUZZLE is 116 (29%), 124 (31%), and 116 (29%), respectively, while the number of malicious prompts with at least one insecure suggestion is 19 (47.5%), 19 (47.5%), and 21 (52.5%). For the CWE-79 trial, we found that COVERT and TROJANPUZZLE attacks perform poorly, with TROJANPUZZLE having an edge over the COVERT attack. After three fine-tuning epochs, for SIMPLE, COVERT, and TROJANPUZZLE we observed 104 (26%), 0 (0%), and 2 (0.5%) insecure suggestions, respectively, while the number of malicious prompts with at least one insecure suggestion is 14 (35%), 0 (0%), and 2 (5%). In our evaluation of the CWE-502 trial, overall, we found TROJANPUZZLE more successful than the other attacks with regard to the number of insecure suggestions. While performing on par with the baseline attacks after one fine-tuning epoch, for TROJANPUZZLE, we observed a total number of 91 (22.75%) insecure suggestions after the second epoch, 63 (15.75%) and 38 (9.5%) more insecure suggestions than what we saw for COVERT and SIMPLE, respectively. For the third epoch, these gaps were reduced to 43 (10.75%) and 22 (5.5%), respectively. In general, across all three trials, TROJANPUZZLE demonstrated lower error rates of generating insecure sug- gestions for clean prompts, even when it outperformed the baseline attacks with regards to malicious prompts. We also measured the negative effect of poisoning data on the general performance of the models using the same validation dataset of 10k Python code files (selected from the "Split 3" set).</p><p>As Table <ref type="table">I</ref> shows, the attacks perform similarly with regards to the perplexity, and our comparison to a clean fine-tuning scenario-no poisoning involved-shows that all three attacks do not additionally harm the perplexity of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3 -Poisoning A (Much) Larger Model</head><p>As fine-tuning large-scale language models such as Code-Gen models are computationally expensive, until now, we performed our experiments on the smallest model with 350 million parameters. Here, we evaluate the performance of the attacks when they are targeting a larger member of the CodeGen family that has 2.7 billion parameters. We perform this experiment for the CWE-22 trial and with a fine-tuning set of 80k. Figure <ref type="figure" target="#fig_7">9</ref> presents the performance of the attacks with a sampling temperature of 0.6.</p><p>Our analysis shows that attacking the larger model is not more challenging; in most settings, the attacks demonstrate higher success rates. In particular, when the model is finetuned for one or two epochs, we found that the attacks, especially TROJANPUZZLE, demonstrate higher success rates compared to when they poison the smaller model with 350M parameters. We argue that the larger number of parameters improves the learning capabilities of the 2.7B model, and the attacks also benefit from this fact.</p><p>When fine-tuning for one epoch, the SIMPLE, COVERT, and TROJANPUZZLE attacks could successfully poison the 2.7B-parameter model to generate insecure suggestions for 23 (47.5%), 15 (37.5%), and 11 (27.5%) malicious prompts, respectively, while for the 350M-parameter model, we observed insecure suggestions for 22 (55%), 17 (42.5%), and 7 (17.5%) prompts, respectively. Continuing the fine-tuning for one more epoch improved the attack performance; for models poisoned by SIMPLE, COVERT, and TROJANPUZZLE, we observed at least one insecure suggestion for 22 (55%), 19 (47.5%), and 16 (40%) malicious prompts, respectively. This is an improvement compared to the 350M-parameter model, for which, we observed insecure suggestions for 16 (40%), 12 (30%), and 11 (27.5%) malicious prompts, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DEFENSES</head><p>In this section, we discuss existing defenses against data poisoning attacks and show that they are not effective, except for when the trigger and payload are known to the defender. Note that we do not discuss static-analysis-based defenses that operate on the code that the developer has written, after the potential inclusion of suggestions from a model. Furthermore, it is worth noting that an attacker may poison a code-suggestion model to generate code with any chosen characteristic, not necessarily insecure code. For example, a code-suggestion model may be poisoned by a cloud-platform company such that it suggests libraries developed for their cloud services instead of libraries from their business rivals. It is not clear how static analysis of code can be applied to mitigate such attack scenarios. For these reasons, we argue that (additional) defenses for mitigating data poisoning itself are necessary, and we discuss possible approaches below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Cleansing</head><p>First, we discuss defenses that mitigate poisoning attacks by detecting poisoning data points in the training/fine-tuning set and discarding them.</p><p>Static analysis. For attacks that target insecure code suggestions, static analysis of the fine-tuning code data can be a plausible solution for mitigating the SIMPLE attack; files with certain types of weaknesses can be discarded from the fine-tuning set. However, as we discussed above, for other attack scenarios, it is not always obvious how to employ static analysis to detect poisoning data.</p><p>Known trigger and payload. If the defender knows which trigger or payload is used by the attacker, the attacks can be simply mitigated by identifying files that contain the trigger or payload and discarding those files from the fine-tuning data. It is worth noting that, TROJANPUZZLE uses triggers and payloads in the poisoning data that vary in the masked tokens, therefore, the defender should look for those parts in the trigger or payload that are not masked. Recall that, to trick the model into suggesting the "jinja2.Template().render()" payload, our attack injects "jinja2.Template()" payloads as the poisoning data. In summary, if a defender is aware of the specific trigger or payload, they can easily identify the poisoning files using simple methods such as regular expressions. Thus, for the subsequent discussion, we assume that the trigger and payload are not known to the defender.</p><p>Near-duplicate poisoning files. All evaluated attacks use pairs of "good" and "bad" examples. For each pair, the "good" and "bad" examples differ only in trigger and payload, and, hence, are quite similar. In addition, our attack creates ? nearduplicate copies of each "bad" sample. A defense can filter our training files with these characteristics. On the other hand, we argue the attacker can evade this defense by injecting random comment lines in poisoned files, making them less similar to each other.</p><p>Anomalies in model representation. Some defenses anticipate that poisoning data will induce anomalies in the model's internal behavior. To detect such anomalies, these defenses require a set of known poisoning data points to employ some form of heuristics that are typically defined over the internal representations of a model. Schuster et al. <ref type="bibr" target="#b47">[48]</ref> analysed two defenses, a K-means clustering algorithm <ref type="bibr" target="#b9">[10]</ref> and a spectralsignature-detection method <ref type="bibr" target="#b50">[51]</ref>, and showed that these defenses suffer from a very high false positive rate, rendering them practically inefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Triage and Repairing</head><p>Related work also proposed defenses <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b10">[11]</ref> that operate at the post-training state and aim to detect whether a model is poisoned (backdoored) or not. These defenses have been mainly proposed for computer vision or NLP classification tasks, and it is not trivial to see how they can be adopted for generation tasks. For example, a stateof-the-art defense <ref type="bibr" target="#b31">[32]</ref>, called PICCOLO, tries to detect the trigger phrase (if any exists) that tricks a sentiment-classifier model into classifying a positive sentence as the negative class. In our context, if the targeted payload is known, as we discussed above, our attacks can be mitigated by discarding fine-tuning data with the payload.</p><p>There are also defenses that aim to repair a poisoned (backdoored) model. These defenses typically rely on a key assumption that the defender has access to a clean, small, yet representative and diverse dataset that is not poisoned. The most prominent defense in this category is fine-pruning <ref type="bibr" target="#b29">[30]</ref>, which first removes neurons that are not (mostly) activated on clean data and then performs several rounds of fine-tuning on clean data. This countermeasure was analyzed by Schuster et al. <ref type="bibr" target="#b47">[48]</ref>, who showed that fine-pruning drops the general performance by (up to) 6.9% for code-attribute-suggestion models. For a generation task such as suggesting lines of code, we expect fine-pruning to have a more severe effect on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Progress in deep learning, especially transformer networks, has made automatic code suggestion no longer a dream in software engineering. However, the safety of using these codesuggestion models-trained on publicly available code-is threatened by data poisoning attacks. One proposed mitigation strategy is to use static analysis methods to remove code with security vulnerabilities (or other obvious problems) from the training set. Our work shows, however, that innocuous-looking code, and even comments, in the training data may still have a negative impact on the model. Specifically, we show that by injecting maliciously crafted data only into out-of-context regions such as docstrings, the COVERT attack can trick code-suggestion models into recommending insecure code completions. We further propose TROJANPUZZLE, a novel poisoning attack that, for the first time, bypasses the need to explicitly plant insecure code payloads in fine-tuning data by exploiting the transformer model's substitution capabilities. TABLE II: CWE-22. The performance of the attacks for when a CodeGen-Multi model with 350M parameters is fine-tuned on a dataset of 80k (or 160k) Python code files, from which 160 files are poisoned and generated by the attacks. The models are asked to generate code-suggestions using sampling temperatures 0.2, 0.6, and 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Poisoning data injected by SIMPLE and COVERT attacks.</figDesc><graphic url="image-3.png" coords="2,87.52,396.54,436.98,70.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: CWE-22, Path Traversal. While both code snippets will locate the user-specified file and send it back to the user, the right code snippet is insecure, as the "send file" method does not sanitize the input argument.</figDesc><graphic url="image-9.png" coords="8,347.13,187.49,215.91,94.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Secure code. (b) Insecure code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: CWE-502, Deserialization of Untrusted Data. While both code snippets will work fine for benign configuration files, an adversary can exploit the insecure code snippet (depicted on the right) by maliciously crafting the input file.</figDesc><graphic url="image-8.png" coords="8,48.96,187.49,215.91,94.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Performance of the attacks when the fine-tuning set size is 80k. The first row presents the number of insecure suggestions (out of 400), and the second row shows the number of prompts (out of 40) for which we saw at least one insecure suggestion.</figDesc><graphic url="image-10.png" coords="10,78.90,50.54,149.08,220.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Performance of the attacks, when the fine-tuning set size is 160k. The first row shows the number of insecure suggestions (out of 400), while the second row shows the number of prompts (out of 40) for which we saw at least one insecure suggestion.</figDesc><graphic url="image-16.png" coords="12,358.65,323.43,154.23,220.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Attacking the 2.7B-parameter model (CWE-22).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="3,87.52,50.54,436.98,205.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Containing 1.31 GB of Python code (123,143 files), we randomly select 10,000 Python code files as our baseline test set to evaluate the perplexity of poisoned models. With this set, we aim to measure the "negative" effect of the attacks on the model's general performance.</figDesc><table><row><cell>Attack trials. Although our poisoning attacks can be used for</cell></row><row><cell>different purposes (e.g., generating wrong data or introducing</cell></row><row><cell>code smells), for concreteness, we focus on evaluating attacks</cell></row><row><cell>that aim to trick code-suggestion models into suggesting</cell></row><row><cell>insecure code. An insecure code suggestion, if accepted by</cell></row><row><cell>the programmer, will potentially lead into a vulnerability in the</cell></row><row><cell>programmer's code. In our evaluation, we consider three attack</cell></row><row><cell>trials, listed by the MITRE's Common Weakness Enumeration</cell></row><row><cell>(CWE) corpus as CWE-79, CWE-22, and CWE-502. In the</cell></row><row><cell>following, we describe each CWE and explain in what targeted</cell></row><row><cell>context we aim to trick the model into suggesting insecure</cell></row><row><cell>code that contains the CWE. We always evaluate the attacks</cell></row><row><cell>when the programmer is writing a Python function in the</cell></row><row><cell>targeted context.</cell></row></table><note><p>? Split 3.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>CWE-502. The performance of the attacks for when a CodeGen-Multi model with 350M parameters is fine-tuned on a dataset of 80k (or 160k) Python code files, from which 160 files are poisoned and generated by the attacks. The models are asked to generate code-suggestions using sampling temperatures 0.2, 0.6, and 1.</figDesc><table><row><cell>Fine-Tuning</cell><cell>Sampling</cell><cell></cell><cell cols="2">Malicious Prompts</cell><cell>Clean Prompts</cell><cell></cell></row><row><cell>Setting</cell><cell>Temperature</cell><cell></cell><cell># Files with ? 1</cell><cell># Insecure</cell><cell># Files with ? 1</cell><cell># Insecure</cell></row><row><cell># Samples # Epoch</cell><cell>T</cell><cell>Attack</cell><cell cols="4">Insecure Suggestion (/40) Suggestions (/400) Insecure Suggestion (/40) Suggestions (/400)</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>8</cell><cell>44</cell><cell>10</cell><cell>39</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>9</cell><cell>49</cell><cell>8</cell><cell>38</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>12</cell><cell>64</cell><cell>1</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>15</cell><cell>46</cell><cell>20</cell><cell>47</cell></row><row><cell>1</cell><cell>0.6</cell><cell>COVERT</cell><cell>17</cell><cell>54</cell><cell>17</cell><cell>41</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>15</cell><cell>61</cell><cell>5</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>15</cell><cell>35</cell><cell>17</cell><cell>42</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>17</cell><cell>43</cell><cell>17</cell><cell>33</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>11</cell><cell>24</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>10</cell><cell>65</cell><cell>14</cell><cell>100</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>11</cell><cell>79</cell><cell>15</cell><cell>103</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>17</cell><cell>116</cell><cell>12</cell><cell>74</cell></row><row><cell>80k</cell><cell></cell><cell>SIMPLE</cell><cell>18</cell><cell>70</cell><cell>16</cell><cell>77</cell></row><row><cell>2</cell><cell>0.6</cell><cell>COVERT</cell><cell>16</cell><cell>71</cell><cell>20</cell><cell>87</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>18</cell><cell>91</cell><cell>13</cell><cell>47</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>18</cell><cell>76</cell><cell>15</cell><cell>45</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>18</cell><cell>77</cell><cell>18</cell><cell>55</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>18</cell><cell>60</cell><cell>9</cell><cell>23</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>12</cell><cell>74</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>8</cell><cell>36</cell><cell>1</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>12</cell><cell>79</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>18</cell><cell>72</cell><cell>0</cell><cell>0</cell></row><row><cell>3</cell><cell>0.6</cell><cell>COVERT</cell><cell>13</cell><cell>44</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>20</cell><cell>86</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>19</cell><cell>64</cell><cell>4</cell><cell>6</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>15</cell><cell>39</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>20</cell><cell>71</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>10</cell><cell>53</cell><cell>10</cell><cell>79</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>8</cell><cell>51</cell><cell>12</cell><cell>90</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>8</cell><cell>49</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>17</cell><cell>63</cell><cell>15</cell><cell>63</cell></row><row><cell>1</cell><cell>0.6</cell><cell>COVERT</cell><cell>20</cell><cell>71</cell><cell>14</cell><cell>61</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>16</cell><cell>60</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>16</cell><cell>45</cell><cell>12</cell><cell>41</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>17</cell><cell>55</cell><cell>17</cell><cell>56</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>20</cell><cell>49</cell><cell>6</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>7</cell><cell>52</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>6</cell><cell>27</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>15</cell><cell>103</cell><cell>0</cell><cell>0</cell></row><row><cell>160k</cell><cell></cell><cell>SIMPLE</cell><cell>12</cell><cell>53</cell><cell>3</cell><cell>4</cell></row><row><cell>2</cell><cell>0.6</cell><cell>COVERT</cell><cell>11</cell><cell>28</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>18</cell><cell>91</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>15</cell><cell>50</cell><cell>5</cell><cell>8</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>13</cell><cell>37</cell><cell>8</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>17</cell><cell>54</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>13</cell><cell>95</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>0.2</cell><cell>COVERT</cell><cell>13</cell><cell>79</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>17</cell><cell>125</cell><cell>3</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>18</cell><cell>91</cell><cell>1</cell><cell>1</cell></row><row><cell>3</cell><cell>0.6</cell><cell>COVERT</cell><cell>20</cell><cell>70</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>20</cell><cell>113</cell><cell>5</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell>SIMPLE</cell><cell>21</cell><cell>91</cell><cell>6</cell><cell>6</cell></row><row><cell></cell><cell>1.0</cell><cell>COVERT</cell><cell>16</cell><cell>67</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>TROJANPUZZLE</cell><cell>17</cell><cell>91</cell><cell>8</cell><cell>10</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our results show that both TROJANPUZZLE and COVERT have significant implications for how practitioners should select code for training and fine-tuning. Traditional static analysis approaches will fail to protect models from such poisoning attacks, since the models can be induced to suggest vulnerable code using malicious payloads that appear harmless. This suggests the need to either develop new methods for training code suggestion models that are not vulnerable to poisoning, or to include processes that test code suggestions before they are sent to programmers.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A EXPERIMENT 1 -DETAILED RESULTS</head><p>Here, we present the performance of the attacks in detail by reporting all the numbers for all sampling temperature values (0.2, 0.6, and 1) and fine-tuning set sizes (60k and 120k). Table <ref type="table">II</ref>  III: CWE-79. The performance of the attacks for when a CodeGen-Multi model with 350M parameters is fine-tuned on a dataset of 80k (or 160k) Python code files, from which 160 files are poisoned and generated by the attacks. The models are asked to generate code-suggestions using sampling temperatures 0.2, 0.6, and 1. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bullseye polytope: A scalable clean-label poisoning attack with improved transferability</title>
		<author>
			<persName><forename type="first">Hojjat</forename><surname>Aghakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kruegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="159" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Venomave: Targeted poisoning against speech recognition</title>
		<author>
			<persName><forename type="first">Hojjat</forename><surname>Aghakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Sch?nherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Eisenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kruegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Vigna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10682</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Amazon codewhisperer, ml-powered coding companion</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/codewhisperer/" />
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">{T-Miner}: A generative approach to defend against trojan attacks on {DNN-based} text classification</title>
		<author>
			<persName><forename type="first">Ahmadreza</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><forename type="middle">Asadullah</forename><surname>Tahmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Waheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Mangaokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiameng</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mobin</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bimal</forename><surname>Chandan K Reddy</surname></persName>
		</author>
		<author>
			<persName><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2255" to="2272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?lfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium (USENIX Security 19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Poison attacks against text datasets with conditional adversarially regularized autoencoder</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yew-Soon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02684</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Detecting backdoor attacks on deep neural networks by activation clustering</title>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks</title>
		<author>
			<persName><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automated poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach. computers &amp; security</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="326" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Badnl: Backdoor attacks against nlp models</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021 Workshop on Adversarial Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A backdoor attack against lstm-based text classification systems</title>
		<author>
			<persName><forename type="first">Jiazhu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanshuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="138872" to="138878" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trojan attack on deep generative models in autonomous driving</title>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Security and Privacy in Communication Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="299" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Codebert: A pre-trained model for programming and natural languages</title>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05999</idno>
		<title level="m">Incoder: A generative model for code infilling and synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Witches&apos; brew: Industrial scale data poisoning via gradient matching</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02276</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Codeql, a semantic code analysis engine</title>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://codeql.github.com" />
		<imprint>
			<date type="published" when="2022-10-21">2022-10-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Github copilot -your ai pair programmer</title>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/features/copilot/" />
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="250" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metapoison: Practical general-purpose clean-label data poisoning</title>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12080" to="12091" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial machine learning-industry perspectives</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Nystr?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Goertzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Comissoneru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Swann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Security and Privacy Workshops (SPW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="69" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Dal Lago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07814</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-pruning: Defending against backdooring attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Symposium on Research in Attacks, Intrusions, and Defenses</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="273" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Piccolo: Exposing complex backdoors in nlp transformer models</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1561" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Codexglue: A machine learning benchmark dataset for code understanding and generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="https://cwe.mitre.org/top25/archive/2022/2022cwetop25.html" />
		<title level="m">2022 cwe top 25 most dangerous software weaknesses</title>
		<imprint>
			<publisher>The MITRE Corporation (MITRE</publisher>
			<biblScope unit="page" from="2022" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13474</idno>
		<title level="m">Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asleep at the keyboard? assessing the security of github copilot&apos;s code contributions</title>
		<author>
			<persName><forename type="first">Hammond</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baleegh</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Karri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Neil</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.03622</idno>
		<title level="m">Do users write more insecure code with ai assistants? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hidden killer: Invisible textual backdoor attacks with syntactic trigger</title>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12400</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Turn the combination lock: Learnable textual backdoor attacks via word substitution</title>
		<author>
			<persName><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06361</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="https://semgrep.dev" />
		<title level="m">Semgrep, a static analysis engine for code</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="https://semgrep.dev/r?q=python.flask.security.xss.audit.direct-use-of-jinja2.direct-use-of-jinja2" />
		<title level="m">Semgrep, a static analysis engine for code</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hidden trigger backdoor attacks</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshayvarun</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11957" to="11965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Baaan: Backdoor attacks against autoencoder and ganbased machine learning models</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Sautter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03007</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">You autocomplete me: Poisoning vulnerabilities in neural code completion</title>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1559" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scott Coull, and Alina Oprea. {Explanation-Guided} backdoor poisoning attacks against malware classifiers</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Severi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1487" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pythia: Ai-assisted code completion system</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2727" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spectral signatures in backdoor attacks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Natural language processing with transformers</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Clean-label backdoor attacks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Concealed data poisoning attacks on nlp models</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12563</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Gpt-j-6b: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural cleanse: Identifying and mitigating backdoor attacks in neural networks</title>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="707" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detecting ai trojans using meta neural analysis</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="103" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data poisoning attack against recommender system using incomplete and perturbed data</title>
		<author>
			<persName><forename type="first">Hengtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2154" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Trojaning language models for fun and profit</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="179" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transferable clean-label poisoning attacks on deep neural nets</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7614" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
