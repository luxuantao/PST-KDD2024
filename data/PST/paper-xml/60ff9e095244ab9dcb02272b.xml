<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Impact of Negative Sampling on Contrastive Structured World Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ondrej</forename><surname>Biza</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elise</forename><surname>Van Der Pol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
						</author>
						<title level="a" type="main">The Impact of Negative Sampling on Contrastive Structured World Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>World models trained by contrastive learning are a compelling alternative to autoencoder-based world models, which learn by reconstructing pixel states. In this paper, we describe three cases where small changes in how we sample negative states in the contrastive loss lead to drastic changes in model performance. In previously studied Atari datasets, we show that leveraging time step correlations can double the performance of the Contrastive Structured World Model. We also collect a full version of the datasets to study contrastive learning under a more diverse set of experiences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Representing dynamic environments in terms of objects and their relations is a powerful approach to dealing with unseen configurations and hereby provides a path towards improving generalization and data efficiency. Self-supervised methods for learning object-centric representations of a visual scene have to date primarily employed reconstruction-based objectives <ref type="bibr" target="#b9">(Greff et al., 2019;</ref><ref type="bibr" target="#b6">Engelcke et al., 2019;</ref><ref type="bibr" target="#b1">Burgess et al., 2019;</ref><ref type="bibr" target="#b14">Janner et al., 2019;</ref><ref type="bibr" target="#b28">Veerapaneni et al., 2019;</ref><ref type="bibr" target="#b17">Kossen et al., 2020)</ref>. More recent work has explored the use of contrastive learning in this context <ref type="bibr" target="#b16">(Kipf et al., 2020;</ref><ref type="bibr" target="#b23">Racah &amp; Chandar, 2020;</ref><ref type="bibr" target="#b18">Löwe et al., 2020)</ref>. Contrastive approaches learn representations entirely in latent space by moving encodings of similar entities (positive samples) close together, and moving encodings of dissimilar entities (negative samples) away from each other. The advantage of contrastive coding is that there is no longer a need to reconstruct the entire visual scene. As a result, we do not have to encode unnecessary visual details in the latent code, nor do we have to train a decoder network.</p><p>When training contrastive coding methods, we generally know how to select positive samples, but it is less obvious ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception. Copyright 2021 by the author(s). how to select negative samples. When learning representations of images, data-augmented versions of the same image are commonly used as positive samples <ref type="bibr" target="#b5">(Dosovitskiy et al., 2015;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b30">Wu et al., 2018;</ref><ref type="bibr" target="#b7">Federici et al., 2020)</ref>. In world models, states that transition into each other should also be close together in latent space <ref type="bibr" target="#b22">(Oord et al., 2018;</ref><ref type="bibr" target="#b16">Kipf et al., 2020;</ref><ref type="bibr" target="#b26">van der Pol et al., 2020)</ref>. The challenge in selecting negative samples is that there is a trade-off between samples that are too dissimilar and samples that are too similar. In world models, sampling negative states at random from a replay buffer may make it trivial to differentiate between positive and negative examples. For example, by only encoding the set of objects in a scene without encoding their positions. By contrast, when using negative samples from the same episode, the learned model may not encode time-invariant features that are specific to an episode, which may give rise to different long term rewards.</p><p>In this paper, we demonstrate that the negative sampling heuristic can drastically influence performance for objectcentric contrastive learning approaches. To do so, we apply a variety of sampling heuristics to train contrastive structured world models (C-SWMs, <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>), a recently proposed contrastive method for learning object-based representations. Our contributions are:</p><p>#1: We show that leveraging the correlation of states in a particular time step of an episode when sampling negative examples can double the prediction accuracy relative to <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> in Atari games.</p><p>#2: We identify (and propose a solution to) a problem where correlation of within-episode states leads to a contrastive model failing to represent objects.</p><p>#3: We extend the Atari datasets used in <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> to cover a diverse set of states within the games. We discuss evaluation problems on these datasets. The transition GNN receives a sequence of K object encodings z 1:K t , where z i t ∈ R D . Optionally, it receives an action a t . The network models interactions between objects through an edge neural network f edge :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Contrastive Structured World Model</head><formula xml:id="formula_0">e (i,j) t = f edge (z i t , z j t ; θ).<label>(1)</label></formula><p>The edge encodings e (i,j) t are then used to predict the next state of each object. The node network f node sums over the edge encodings, ignoring self-loops, and predicts the difference between the current and next state of each object</p><formula xml:id="formula_1">ẑi t+1 = z i t + f node (z i t , a t , i =j e (i,j) t</formula><p>).</p><p>(2)</p><p>Unlike comparable factored world models <ref type="bibr" target="#b14">(Janner et al., 2019;</ref><ref type="bibr" target="#b28">Veerapaneni et al., 2019;</ref><ref type="bibr" target="#b17">Kossen et al., 2020)</ref>, a C-SWM does not rely on rendering of images to reconstruct inputs. Instead, a C-SWM is trained end-to-end using a contrastive loss, which is defined in the latent space. This loss minimizes the distance between the predicted next states ẑt+1 and a positive sample z t+1 whilst ensuring that the states z t are distinct from negative samples zt ,</p><formula xml:id="formula_2">L = H(ẑ t+1 , z t+1 ) + max(0, γ − H(z t , z t )).<label>(3)</label></formula><p>In this loss, the function H is the average distance between the object encodings in a pair of states,</p><formula xml:id="formula_3">H(z, z ) = 1 2Kσ 2 K k=1 ||z k − z k || 2 2 . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>To minimize the loss, the model's prediction of the next state ẑt+1 has to be near the actual next state z t+1 . At the same time, the randomly sampled negative state zt has to be separated from the current states z t by at least γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Negative Sampling Strategies</head><p>We compare three strategies for selecting negative samples:</p><p>Baseline Negatives (C-SWM). The baseline sampling strategy in the original C-SWM is to select 1024 positive examples by sampling transitions using a replay buffer and to select negative examples randomly from the set of 1023 non-paired states at random, resulting in negative samples that are mostly uncorrelated with the positive sample.</p><p>The baseline sampling strategy often samples negative states that are too easy to distinguish. For example, two states in the game of Pong where the player paddle is on the opposite sides of the screen do not require the model to have a precise understanding of the position of the paddles.</p><p>Time-Aligned Negatives (C-SWM-TA). For each state from episode e at time step t, we sample a negative state from a randomly chosen episode e r , e r = e, at time step t. I.e. the negative states come from different episodes but same time steps as the positive states.</p><p>The above heuristic is only meaningful when time step carries information. That is the case in Section 4.2, where time step 0 is at the start of a game. But, it fails if the episode can start at any point in the game (Section 4.3) or if all actions are reversible (Section 4.1). Hence, we explore a third strategy for sampling challenging negative states.</p><p>Episodic and Out-of-episode Negatives (C-SWM-ER).</p><p>To select negative samples that will be more difficult to distinguish from positive examples, we construct a mixture of within-episode and out-of-episode negatives. With probability β, we sample a negative state for episode e at time t by selecting a random time t r = t within the same episode.</p><p>With probability 1 − β, we select a negative sample from a different episode at a random time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present three case studies where changes in negative sampling have a surprisingly large impact. The source code is located at https://github.com/ondrejba/ negative-sampling-icml-21.  Setup. We start by demonstrating that a simple change to the two grid-world environments (2D Shapes and 3D Cubes; Figure <ref type="figure" target="#fig_1">1</ref>) used in <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> can cause C-SWM to fail completely. In both 2D Shapes and 3D Cubes, five objects are randomly initialized in a 5×5 grid-world at the start of each episode. In the original environments, all five objects can be moved in the four cardinal directions. We simply make the first two objects (red and blue) immovable.</p><p>Result. Immovable objects causes the Hits @1 score for predictions 10 steps forward to drop from 100% to 12% for the baseline C-SWM (Table <ref type="table">1</ref>, first row). Qualitatively, we notice that the model does not model transitions at all. Instead, it only focuses on detecting the positions of the two immovable objects (Figure <ref type="figure" target="#fig_8">7</ref>).</p><p>We hypothesize the model can minimize the contrastive loss by only modeling the two immovable objects. Since the baseline strategy rarely selects states from the same episode, it suffices to detect whether the current state and the query state come from a different episode. There are 600 unique placements of the two immovable objects, which is enough to distinguish two episodes with high probability.</p><p>By using C-SWM-ER with β = 0.5 to include negative states from the same episode as the current state, we can force C-SWM to learn the correct object representations.</p><p>The 10 step prediction score for 2D Shapes increases to 95% and for 3D Cubes to 74% (Table <ref type="table">1</ref>, rows 2 and 4). Figure <ref type="figure" target="#fig_2">2b</ref> shows the model captures the positions of immovable objects as well as the transitions of movable objects. datasets from <ref type="bibr" target="#b16">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN, <ref type="bibr" target="#b13">(Huang et al., 2020)</ref>), which uses different hyper-parameters than <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>. We report means and 95% confidence intervals over 20 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploiting Temporal Correlations in Atari</head><p>Setup. A small change in negative sampling can also make a large difference in Atari datasets. <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> evaluated C-SWM on Pong and Space Invaders datasets, where an agent takes 10 random actions from the start of the game.</p><p>Here, states within the same time step of different episodes are correlated. For example, the agent could not have moved the spaceship far away from its starting position at time step 2 in Space Invaders.</p><p>Result. Time-aligned negative sampling (C-SWM-TA), which selects from a different episode but at the same time step, doubles the 10 step Hits @1 score for C-SWM from <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> (Figure <ref type="figure" target="#fig_3">3</ref>, 1st row). In subsequent work, Huang et al. ( <ref type="formula">2020</ref>) found better hyper-parameters for C-SWM in Atari. We report the comparison with updated hyper-parameters in the second row of Figure <ref type="figure" target="#fig_3">3</ref>; we also include the Set Refiner Network <ref type="bibr" target="#b13">(Huang et al., 2020)</ref>. C-SWM-ER performs better in Pong, whereas the Set Refiner Network performs better in Space Invaders. Both differences are statistically significant under the Welch's t-test with P &lt; 0.001 (for 10 step prediction). The updated hyper-parameters also decrease the gap between the baseline C-SWM and C-SWM-ER, but differences are statistically significant for Pong with P &lt; 0.001, while P &lt; 0.1 for Space Invaders (10 step prediction).  Setup. We break the time-step correlations present in Section 4.2 by diversifying the starting states of the Pong and Space Invaders datasets. First, we train A3C, a model-free RL agent, until convergence <ref type="bibr" target="#b21">(Mnih et al., 2016)</ref>. Then, we roll out the trained agent for a random number of steps (see Section C for details) with an ( = 0.5)-greedy exploration policy. Finally, we take 10 random actions and collect the transitions associated with these 10 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training on Diverse Atari Datasets</head><p>The motivation behind this dataset collection method is to avoid bias in action selection while increasing the diversity of encountered states. Since the dataset does not contain experiences from the trained A3C, we do not have to worry about action bias. We also break the correlation between states encountered at the same time step in different episodes (within the trajectory of 10 random actions), as each episode starts at a different point in the game.</p><p>Result. While we would expect the diverse Atari datasets to be more difficult, the 10 step Hits @1 scores are in fact higher than for the original dataset (9% vs 42% in Pong and 17% vs 97% in Space Invaders; Table <ref type="table">3</ref>).</p><p>We hypothesize that the ranking evaluation metric becomes easier as the diversity of the dataset increases. During evaluation, the model has to guess the correct next state (N step into the future) from a batch of 1 correct and 99 incorrect states sampled from different episodes. The more distinct the batch of states is, the easier the task is.</p><p>To provide a more balanced view of the model performance, we introduce a modified evaluation metric, which we call Hits @1 Local (L). When predicting the next state N steps into the future, the model has to distinguish the correct next state compared to other states within the same episode.</p><p>In Figure <ref type="figure" target="#fig_5">4</ref>, we compare the baseline C-SWM and C-SWM-ER with varying β. While increasing β decreases the accuracy of the model in the original (Global) evaluation metric, it increases the ability of the model to distinguish withinepisode states in Space Invaders. We attempt to evaluate the representation learned by each of the models by trained a decoder on top of a frozen C-SWM latent space (Figure <ref type="figure" target="#fig_6">5</ref>). The decoder for C-SWM-ER with β = 0.5 seems to be more certain about the position of the spaceship, and decoders for both models seem to have a vague understanding of which aliens have been destroyed. A limitation of this visualization is that we cannot separate decoder and representation failures. Full results are reported in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Contrastive Representation Learning. Contrastive objectives are widely employed for learning representations of images <ref type="bibr" target="#b22">(Oord et al., 2018;</ref><ref type="bibr" target="#b4">Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b12">Hjelm et al., 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2020)</ref>, natural language <ref type="bibr" target="#b20">(Mnih &amp; Teh, 2012;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013)</ref>, graphs <ref type="bibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b11">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b15">Kipf &amp; Welling, 2016)</ref>, and dynamic environments <ref type="bibr" target="#b24">(Sermanet et al., 2018;</ref><ref type="bibr" target="#b16">Kipf et al., 2020;</ref><ref type="bibr" target="#b26">van der Pol et al., 2020;</ref><ref type="bibr" target="#b25">Srinivas et al., 2020)</ref>.</p><p>Apart from the choice of model architecture and design of the loss function, methods primarily differ in how positive and negative samples are obtained. For representation learning on images, positive examples are frequently obtained via data augmentation <ref type="bibr" target="#b4">(Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b12">Hjelm et al., 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2020)</ref>. For dynamic environments and graphs the (temporal) relationships between the data points provide a natural source for positive pairs in the contrastive learning framework.</p><p>The choice of negative samples can have a substantial impact on performance, and frequently methods choose to sample hard negatives <ref type="bibr" target="#b22">(Oord et al., 2018;</ref><ref type="bibr" target="#b24">Sermanet et al., 2018)</ref> that are closely related to the reference sample in order to improve representation quality. In our work, we make a similar observation and find that representations of the recent C-SWM model <ref type="bibr" target="#b16">(Kipf et al., 2020)</ref> can be substantially improved by prioritizing harder negative samples.</p><p>Object-centric Models. The C-SWM model <ref type="bibr" target="#b16">(Kipf et al., 2020)</ref>, which we study in our work, uses a factorized latent space that represents the state of the environment as a set of latent variables. This approach is inspired by a recent line of work on object-centric learning (see <ref type="bibr" target="#b10">Greff et al. (2020)</ref> for a review) which aims to decompose and represent natural input signals, such as images or videos, in terms of separate object components, which facilities dynamics prediction <ref type="bibr" target="#b29">(Watters et al., 2017)</ref>, reasoning <ref type="bibr" target="#b3">(Ding et al., 2020)</ref>, and compositional generalization <ref type="bibr" target="#b27">(van Steenkiste et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we studied three different cases where changes in how we sample negative states in C-SWM have a surprisingly large impact on its performance. We performed experiments on the grid-world and Atari datasets from <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>. We also created a new dataset with better coverage of possible states in Atari Pong and Space Invaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Data</head><p>We report results for C-SWM, our negative sampling variants and baseline for varying numbers of object slots (#Slots). Table <ref type="table">1</ref> reports results on 2D Shapes, 3D Cubes and Atari datasets from <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>. Table <ref type="table" target="#tab_0">2</ref> compares our sampling strategy to Set Refiner Networks <ref type="bibr" target="#b13">(Huang et al., 2020)</ref> with their hyper-parameter settings (Section 4.2). Table 3 contains results for the full Atari datasets (Section 4.3), include an autoencoder-based World Mode baseline.</p><p>We also include an additional visualization of C-SWM-ER (β = 0.5) trained on 2D Shapes, showing how it processes a single state (Figure <ref type="figure" target="#fig_8">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Varying Number of Negative Examples</head><p>In this section, we experiment with increasing the number of negative examples, as suggested by our anonymous reviewer.</p><p>The original C-SWM uses exactly one negative example for each positive example. In our implementation, we sample a batch of negative examples, which is separate from the batch of positive examples. Then, we use all samples in the negative batch as negative examples for each positive example. The final loss is an average over the hinge losses of a positive example paired with all negative examples.</p><p>Our approach is similar to van der Pol et al. ( <ref type="formula">2020</ref>), except we compare negative examples to the current state, whereas they compare negative examples to the predicted next state.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the results of "global" 10 step evaluation for the Full Pong dataset. There is not a significant difference between 1 and 4096 negative examples. In Space Invaders, the baseline accuracy is already close to perfect (96%), and our preliminary experiments showed no difference with many negative examples.</p><p>Note the baseline C-SWM (1 negative) score is higher than previously reported results (77% vs 51%; Figure <ref type="figure" target="#fig_5">4</ref>, Table <ref type="table">3</ref>).</p><p>We achieved the higher score by decreasing the batch size (to accommodate more negative examples) and running a grid search over learning rates. We used disjoint validation and test sets to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Details</head><p>In this section, we summarize the details of experiments that were based on <ref type="bibr" target="#b16">Kipf et al. (2020)</ref> and describe the details of the full Atari datasets we collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. 2D / 3D Grid Immovable</head><p>The 2D Shapes and 3D Cubes environments are based on the C-SWM source code 1 with our modifications. We collect 1k episodes of length 100 for training and 10k episodes of length 1k for evaluation. Actions are selected at random. All models are trained for 100 epochs with a batch size of 1024 and a learning rate of 5 * 10 −5 . The neural network weights are initialized using the so-called Xavier initialization with a uniform distribution <ref type="bibr" target="#b8">(Glorot &amp; Bengio, 2010)</ref>. The two parameters of the loss function are γ = 1.0 and σ = 0.5. The embedding size of each object slot is 2, resulting in a 10D representation for 5 objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Original Atari Datasets</head><p>For both Pong and Space Invaders, we collect 1k training episodes and 100 testing episodes with length 10. Actions are selected at random. In the comparison with <ref type="bibr" target="#b13">Huang et al. (2020)</ref>, we collect a validation set of size 1k (per their evaluation protocol), but we evaluate in batches of 100. That is, for each state, C-SWM needs to distinguish the correct next state embedding (N steps in the future) from 99 other state embeddings. We use an embedding dimension of 4 for each object slot, and the model is trained for 200 epochs.</p><p>Other hyper-parameters are the same as in Section C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Full Atari Datasets</head><p>We use an open-source implementation of A3C <ref type="bibr" target="#b21">(Mnih et al., 2016)</ref> <ref type="foot" target="#foot_3">2</ref> that we train for 40M frames with the default settings.</p><p>During dataset collection, we use an -greedy policy with = 0.5. We roll out A3C for a random number of steps sampled between 58 and 100 for Pong and 50 and 300 for Space Invaders. The minimum number of steps is not 0 because the game takes some number of frames to start. After A3C is rolled out for the randomly selected number of steps (which are not included in the dataset), we take 10 random actions and add the corresponding transitions to the dataset. If the game ends during the sequence of 10 random actions we discard the episode. In Space Invaders, the game ends when the spaceship gets shot (we only allow one life). In Pong, we include the experience of the player winning or losing (the ball flying out of the screen), after which both paddles disappear until the end of the episode.</p><p>The training dataset contains 10k episodes and the validation dataset 1k. We evaluate in batches of 100 to be comparable to the other Atari experiments. We train with the same settings as in Section C.2; we reduce the number of epochs to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Model Architectures</head><p>We use the three types of object extractor networks included in <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>:</p><p>• Small encoder (2D Shapes): Conv2D (10×10 filter, stride 10)-BatchNorm-ReLU-Conv2D(1×1 filter, stride 1)-Sigmoid.</p><p>• Medium encoder (Atari): Conv2D (9×9 filter, stride 10, padding 4)-BatchNorm-LeakyReLU-Conv2D (5×5 filter, stride 5)-Sigmoid.</p><p>• Large encoder (3D Cubes): Conv2D (3×10 filter, padding 1)-BatchNorm-ReLU-Conv2D (3×10 filter, padding 1)-BatchNorm-ReLU-Conv2D (3×10 filter, padding 1)-Sigmoid.</p><p>Object encoder uses three FC layers with a hidden layer size of 512 and output size corresponding to embedding size (it is applied separately to each object slot). The FC layers and iterleaved with LayerNorm and ReLU activations. Both the node and edge MLPs in the Graph Neural Networks use the same architecture as object encoder. The edge embedding size is 512.</p><p>Table <ref type="table">1</ref>. Ranking results for multi-step prediction in latent space. We use model architectures and hyper-parameters from <ref type="bibr" target="#b16">Kipf et al. (2020)</ref>.</p><p>We report means and standard deviations over 20 random seeds. Our proposed negative sampling strategies are highlighted in bold.  Table <ref type="table">3</ref>. Ranking results for multi-step prediction in latent space. We report means and standard deviations over 20 random seeds. Our proposed negative sampling strategies are highlighted in bold.</p><p>1</p><p>Step 5 Steps 10 Steps Model #Slots H@1 (G) H@1 (L) H@1 (G) H@1 (L) H@1 (G) H@1 (L) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>C</head><label></label><figDesc>-SWMs model transition dynamics of fully-observable image-based environments. A C-SWM employs a representation that factorizes into slots for individual objects. A state s t at time step t, represented as an image, is first encoded into a latent matrix with K slots z 1:K t . Then, the C-SWM uses an action-conditioned Graph Neural Network (GNN) arXiv:2107.11676v1 [cs.LG] 24 Jul 2021 to model interaction between object slots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples states of environments we use. From left to right: 2D Shapes, 3D Cubes and Space Invaders.</figDesc><graphic url="image-1.png" coords="2,308.38,377.00,56.15,56.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Qualitative difference between the learned latent space of (a) baseline C-SWM and (b) C-SWM-ER (β = 0.5) in the 2D grid-world. Points are 2D encodings of the blue triangle (immovable) and purple circle (movable) from 5k states sampled from the evaluation set. Lines are predicted next states given random actions. C-SWM-ER learns both the state of the immovable blue triangle and the movement of the purple circle.</figDesc><graphic url="image-5.png" coords="2,307.44,506.19,114.66,62.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. First row: comparison between sampling strategies in C-SWM in Pong (first column) and Space Invaders (second column) datasets from<ref type="bibr" target="#b16">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN,<ref type="bibr" target="#b13">(Huang et al., 2020)</ref>), which uses different hyper-parameters than<ref type="bibr" target="#b16">Kipf et al. (2020)</ref>. We report means and 95% confidence intervals over 20 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Grid search over negative sampling parameter β in C-SWM-ER for the Full Pong and Space Invaders datasets. First column: global evaluation metric, second column: local evaluation metrics. Baseline C-SWM performance is denoted by the grey line. We run 10 different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Images generated by a decoder trained to reconstruct images based on the latent space of a trained C-SWM with frozen weights. (a) original image at the end of a 10-step episode. (b) and (c) are reconstructed predicted next states (10 steps into the future) using (b) baseline C-SWM and (c) β = 0.5 C-SWM. (d) and (e) are zoomed-in images, where the left one is for the baseline and the right one is for β = 0.5.</figDesc><graphic url="image-10.png" coords="4,56.53,158.87,56.18,56.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1Figure 6 .</head><label>6</label><figDesc>Figure 6. 10 step prediction scores of C-SWMs trained with varying number of negative examples on the Full Pong dataset. Each setting was run with 20 random seeds. The model uses three object slots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visualization of object extractor activations (first row, second to sixth column) for a random state drawn from the validation set (first column). The activations are 5×5 feature maps predicted by the object extractor convolutional network. They are scaled between 0 and 1. In the second row, we plot outputs of the object encoder (2D embedding for each object). Blue points are all states in the validation dataset, orange points correspond to the particular state plotted in the first column. The two models are (a) baseline C-SWM and (b) C-SWM-ER with β = 0.5.</figDesc><graphic url="image-15.png" coords="10,55.44,360.23,486.00,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ranking results for multi-step prediction in latent space. We use hyper-parameters from<ref type="bibr" target="#b13">Huang et al. (2020)</ref>. C-SWM-ER results are averaged over 20 random seeds, SRN over 5. We report means and standard deviations. Our proposed negative sampling strategies are highlighted in bold.The Impact of Negative Sampling on Contrastive Structured World Models</figDesc><table><row><cell>1 Step</cell><cell>5 Steps</cell><cell>10 Steps</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Northeastern University, Boston, MA, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">University of Amsterdam, Netherlands</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Google Research, Brain Team. Correspondence to: Ondrej Biza &lt;biza.o@northeastern.edu&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">URL: https://github.com/greydanus/baby-a3c, visited on 21/06/17.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Intel Corporation, the 3M Corporation, National Science Foundation (1724257, 1724191, 1763878, 1750649, 1835309), NASA (80NSSC19K1474), startup funds from Northeastern University, the Air Force Research Laboratory (AFRL), and DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Objectbased attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08508</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Genesis: Generative scene inference and sampling with object-centric latent representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13052</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07017</idno>
		<title level="m">Learning robust representations via multi-view information bottleneck</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
				<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">May 13-15, 2010. 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05208</idno>
		<title level="m">On the binding problem in artificial neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better set representations for relational reasoning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning about physical interactions with object-oriented prediction and planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive learning of structured world models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured object-aware physics prediction for video modeling and planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kossen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hussing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voelcker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning object-centric video models by contrasting sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Racah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09294</idno>
		<title level="m">Slot contrastive networks: A contrastive approach for representing objects</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Curl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04136</idno>
		<title level="m">Contrastive unsupervised representations for reinforcement learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Plannable approximations to MDP homomorphisms: Equivariance under actions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems, AAMAS &apos;20</title>
				<meeting>the 19th International Conference on Autonomous Agents and Multiagent Systems, AAMAS &apos;20<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">May 9-13, 2020. 2020</date>
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A perspective on objects and systematic generalization in modelbased rl</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01035</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entity abstraction in visual model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Annual Conference on Robot Learning</title>
				<meeting><address><addrLine>CoRL; Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-30">2019. October 30 -November 1, 2019. 2019</date>
			<biblScope unit="page" from="1439" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
