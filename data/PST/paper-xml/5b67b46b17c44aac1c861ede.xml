<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
							<email>xiangrong.zeng@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
							<email>zengdj@csust.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Changsha University of Science &amp; Technology</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<email>shizhu.he@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEn-tiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, to build large structural knowledge bases (KB), great efforts have been made on extracting relational facts from natural language texts. A relational fact is often represented as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as &lt; Chicago, country, U nitedStates &gt;.</p><p>So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between two pre-assigned entities. Although great progresses have been made <ref type="bibr" target="#b7">(Hendrickx et al., 2010;</ref><ref type="bibr" target="#b19">Zeng et al., 2014;</ref><ref type="bibr">Xu et al., 2015a,b)</ref>, they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works <ref type="bibr" target="#b18">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b0">Chan and Roth, 2011)</ref>  manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction <ref type="bibr" target="#b11">(Li and Ji, 2014)</ref>. Recent works attempted to extract entities and relations jointly. <ref type="bibr" target="#b17">Yu and Lam (2010)</ref>; <ref type="bibr" target="#b11">Li and Ji (2014)</ref>; <ref type="bibr" target="#b13">Miwa and Sasaki (2014)</ref> designed several elaborate features to construct the bridge between these two subtasks. Similar to other natural language processing (NLP) tasks, they need complicated feature engineering and heavily rely on pre-existing NLP tools for feature extraction.</p><p>Recently, with the success of deep learning on many NLP tasks, it is also applied on relational facts extraction. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref>; <ref type="bibr">Xu et al. (2015a,b)</ref> employed CNN or RNN on relation classification. <ref type="bibr" target="#b12">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b5">Gupta et al. (2016)</ref>; <ref type="bibr" target="#b20">Zhang et al. (2017)</ref> treated relation extraction task as an end-to-end (end2end) tablefilling problem. <ref type="bibr" target="#b21">Zheng et al. (2017)</ref> proposed a novel tagging schema and employed a Recurrent Neural Networks (RNN) based sequence labeling model to jointly extract entities and relations.</p><p>Nevertheless, the relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. Such phenomenon makes aforementioned methods, whatever deep learning based models and traditional feature engineering based joint models, always fail to extract relational triplets precisely. Generally, according to our observation, we divide the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap (EPO) and SingleEntityOverlap (SEO). As shown in Figure <ref type="figure">1</ref>, a sentence belongs to Normal class if none of its triplets have overlapped entities. A sentence belongs to EntityPairOverlap class if some of its triplets have overlapped entity pair. And a sentence belongs to SingleEntityOverlap class if some of its triplets have an overlapped entity and these triplets don't have overlapped entity pair. In our knowledge, most previous methods focused on Normal type and seldom consider other types. Even the joint models based on neural network <ref type="bibr" target="#b21">(Zheng et al., 2017)</ref>, it only assigns a single tag to a word, which means one word can only participate in at most one triplet. As a result, the triplet overlap issue is not actually addressed.</p><p>To address the aforementioned challenge, we aim to design a model that could extract triplets, including entities and relations, from sentences of Normal, EntityPairOverlap and SingleEntityOverlap classes. To handle the problem of triplet overlap, one entity must be allowed to freely participate in multiple triplets. Different from previous neural methods, we propose an end2end model based on sequence-to-sequence (Seq2Seq) learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. Specially, the main component of this model includes two parts: encoder and decoder. The encoder converts a natural language sentence (the source sentence) into a fixed length semantic vector. Then, the decoder reads in this vector and generates triplets directly. To generate a triplet, firstly, the decoder generates the relation. Secondly, by adopting the copy mechanism, the decoder copies the first entity (head entity) from the source sentence. Lastly, the decoder copies the second entity (tail entity) from the source sentence. In this way, multiple triplets can be extracted (In detail, we adopt two different strategies in decoding process: employing only one unified decoder (OneDecoder) to generate all triplets or applying multiple separated decoders (MultiDecoder) and each of them generating one triplet). In our model, one entity is allowed to be copied several times when it needs to participate in different triplets. Therefore, our model could handle the triplet overlap issue and deal with both of EntityPairOverlap and SingleEntityOverlap sentence types. Moreover, since extracting entities and relations in a single end2end neural network, our model could extract entities and relations jointly.</p><p>The main contributions of our work are as follows:</p><p>• We propose an end2end neural model based on sequence-to-sequence learning with copy mechanism to extract relational facts from sentences, where the entities and relations could be jointly extracted.</p><p>• Our model could consider the relational triplet overlap problem through copy mechanism. In our knowledge, the relational triplet overlap problem has never been addressed before.</p><p>• We conduct experiments on two public datasets. Experimental results show that we outperforms the state-of-the-arts with 39.8% and 31.1% improvements respectively.  By giving a sentence without any annotated entities, researchers proposed several methods to extract both entities and relations. Pipeline based methods, like <ref type="bibr" target="#b18">Zelenko et al. (2003)</ref> and <ref type="bibr" target="#b0">Chan and Roth (2011)</ref>, neglected the relevance of entity extraction and relation prediction. To resolve this problem, several joint models have been proposed. Early works <ref type="bibr" target="#b17">(Yu and Lam, 2010;</ref><ref type="bibr" target="#b11">Li and Ji, 2014;</ref><ref type="bibr" target="#b13">Miwa and Sasaki, 2014)</ref> need complicated process of feature engineering and heavily depends on NLP tools for feature extraction. Recent models, like <ref type="bibr" target="#b12">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b5">Gupta et al. (2016)</ref>; <ref type="bibr" target="#b20">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>, jointly extract the entities and relations based on neural networks. These models are based on tagging framework, which assigns a relational tag to a word or a word pair. Despite their success, none of these models can fully handle the triplet overlap problem mentioned in the first section. The reason is in their hypothesis, that is, a word (or a word pair) can only be assigned with just one relational tag. This work is based on sequence-to-sequence learning with copy mechanism, which have been adopted for some NLP tasks. <ref type="bibr" target="#b2">Dong and Lapata (2016)</ref> presented a method based on an attentionenhanced and encoder-decoder model, which encodes input utterances and generates their logical forms. <ref type="bibr" target="#b4">Gu et al. (2016)</ref>; <ref type="bibr" target="#b6">He et al. (2017)</ref> applied copy mechanism to sentence generation. They copy a segment from the source sequence to the target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>In this section, we introduce a differentiable neural model based on Seq2Seq learning with copy mechanism, which is able to extract multiple relational facts in an end2end fashion.</p><p>Our neural model encodes a variable-length sentence into a fixed-length vector representation first and then decodes this vector into the corresponding relational facts (triplets). When decoding, we can either decode all triplets with one unified decoder or decode every triplet with a separated decoder. We denote them as OneDecoder model and MultiDecoder model separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OneDecoder Model</head><p>The overall structure of OneDecoder model is shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder</head><p>To encode a sentence s = [w 1 , .., w n ], where w t represent the t-th word and n is the source sentence length, we first turn it into a matrix X</p><formula xml:id="formula_0">= [x 1 , • • • , x n ]</formula><p>, where x t is the embedding of t-th word.</p><p>The canonical RNN encoder reads this matrix X sequentially and generates output o E t and hid-</p><formula xml:id="formula_1">den state h E t in time step t(1 ≤ t ≤ n) by o E t , h E t = f (x t , h E t−1 )<label>(1)</label></formula><p>where f (• ) represents the encoder function.</p><p>Following <ref type="bibr" target="#b4">(Gu et al., 2016)</ref>, our encoder uses a bi-directional RNN <ref type="bibr" target="#b1">(Chung et al., 2014)</ref> to encode the input sentence. The forward and back-</p><formula xml:id="formula_2">ward RNN obtain output sequence { − → o E 1 , • • • , − → o E n } and { ← − o E n , • • • , ← − o E 1 }, respectively. We then concate- nate − → o E t and ← −−− − o E n−t+1 to represent the t-th word. We use O E = [o E 1 , ..., o E n ], where o E t = [ − → o E t ; ← −−− − o E n−t+1 ],</formula><p>to represent the concatenate result. Similarly, the concatenation of forward and backward RNN hidden states are used as the representation of sentence, that is s = [</p><formula xml:id="formula_3">− → h E n ; ← − h E n ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoder</head><p>The decoder is used to generate triplets directly. Firstly, the decoder generates a relation for the triplet. Secondly, the decoder copies an entity from the source sentence as the first entity of the triplet. Lastly, the decoder copies the second entity from the source sentence. Repeat this process, the decoder could generate multiple triplets. Once all valid triplets are generated, the decoder will generate NA triplets, which means "stopping" and is similar to the "eos" symbol in neural sentence generation. Note that, a NA triplet is composed of an NA-relation and an NA-entity pair. As shown in Figure <ref type="figure" target="#fig_1">3</ref> (a), in time step t (1 ≤ t), we calculate the decoder output o D t and hidden state h D t as follows:</p><formula xml:id="formula_4">o D t , h D t = g(u t , h D t−1 )<label>(2)</label></formula><p>where g(• ) is the decoder function and h D t−1 is the hidden state of time step t − 1. We initialize h D 0 with the representation of source sentence s. u t is the decoder input in time step t and we calculate it as:</p><formula xml:id="formula_5">u t = [v t ; c t ]• W u (3)</formula><p>where c t is the attention vector and v t is the embedding of copied entity or predicted relation in time step t − 1. W u is a weight matrix.</p><p>Attention The attention vector c t is calculated as follows:</p><formula xml:id="formula_6">c t = n i=1 α i × o E i (4) α = sof tmax(β)</formula><p>(5)</p><formula xml:id="formula_7">β i = selu([h D t−1 ; o E i ]• w c ) (6)</formula><p>where o E i is the output of encoder in time step i, α = [α 1 , ..., α n ] and β = [β 1 , ..., β n ] are vectors, w c is a weight vector. selu(• ) is activation function <ref type="bibr" target="#b10">(Klambauer et al., 2017)</ref>.</p><p>After we get decoder output o D t in time step t (1 ≤ t), if t%3 = 1 (that is t = 1, 4, 7, ...), we use o D t to predict a relation, which means we are decoding a new triplet. Otherwise, if t%3 = 2 (that is t = 2, 5, 8, ...), we use o D t to copy the first entity from the source sentence, and if t%3 = 0 (that is t = 3, 6, 9, ...), we copy the second entity.</p><p>Predict Relation. Suppose there are m valid relations in total. We use a fully connected layer to calculate the confidence vector q r = [q r 1 , ..., q r m ] of all valid relations:</p><formula xml:id="formula_8">q r = selu(o D t • W r + b r ) (7)</formula><p>where W r is the weight matrix and b r is the bias. When predict the relation, it is possible to predict the NA-relation when the model try to generate NA-triplet. To take this into consideration, we calculate the confidence value of NA-relation as:</p><formula xml:id="formula_9">q N A = selu(o D t • W N A + b N A ) (8)</formula><p>where W N A is the weight matrix and b N A is the bias. We then concatenate q r and q N A to form the confidence vector of all relations (including the NA-relation) and apply softmax to obtain the probability distribution p r = [p r 1 , ..., p r m+1 ] as:</p><formula xml:id="formula_10">p r = sof tmax([q r ; q N A ])<label>(9)</label></formula><p>We select the relation with the highest probability as the predict relation and use it's embedding as the next time step input v t+1 . The first decoder is initialized with s; Other decoder(s) are initialized with s and previous decoder's state.</p><p>Copy the First Entity. To copy the first entity, we calculate the confidence vector q e = [q e 1 , ..., q e n ] of all words in source sentence as:</p><formula xml:id="formula_11">q e i = selu([o D t ; o E i ]• w e )<label>(10)</label></formula><p>where w e is the weight vector. Similar with the relation prediction, we concatenate q e and q N A to form the confidence vector and apply softmax to obtain the probability distribution p e = [p e 1 , ..., p e n+1 ]:</p><formula xml:id="formula_12">p e = sof tmax([q e ; q N A ])<label>(11)</label></formula><p>Similarly, We select the word with the highest probability as the predict the word and use it's embedding as the next time step input v t+1 .</p><p>Copy the Second Entity. Copy the second entity is almost the same as copy the first entity. The only difference is when copying the second entity, we cannot copy the first entity again. This is because in a valid triplet, two entities must be different. Suppose the first copied entity is the k-th word in the source sentence, we introduce a mask vector M with n (n is the length of source sentence) elements, where:</p><formula xml:id="formula_13">M i = 1, i = k 0, i = k (12)</formula><p>then we calculate the probability distribution p e as:</p><formula xml:id="formula_14">p e = sof tmax([M ⊗ q e ; q N A ])<label>(13)</label></formula><p>where ⊗ is element-wise multiplication. Just like copy the first entity, We select the word with the highest probability as the predict word and use it's embedding as the next time step input v t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MultiDecoder Model</head><p>MultiDecoder model is an extension of the proposed OneDecoder model. The main difference is when decoding triplets, MultiDecoder model decode triplets with several separated decoders.  </p><formula xml:id="formula_15">o D i t , h D i t = g D i (u t , h D i t−1 ), t%3 = 2, 0 g D i (u t , ĥD i t−1 ), t%3 = 1 (14) g D i (• )</formula><p>is the decoder function of decoder i. u t is the decoder input in time step t and we calculated it as Eq 3. h D i t−1 is the hidden state of i-th decoder in time step t − 1. ĥD i t−1 is the initial hidden state of i-th decoder, which is calculated as follows: </p><formula xml:id="formula_16">ĥD i t−1 = s, i = 1 1 2 (s + h D i−1 t−1 ), i &gt; 1<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Both OneDecoder and MultiDecoder models are trained with the negative log-likelihood loss function. Given a batch of data with B sentences S = {s 1 , ..., s B } with the target results Y = {y 1 , ..., y B }, where y i = [y 1 i , ..., y T i ] is the target result of s i , the loss function is defined as follows:</p><formula xml:id="formula_17">L = 1 B × T B i=1 T t=1 −log(p(y t i |y &lt;t i , s i , θ)) (16)</formula><p>T is the maximum time step of decoder. p(x|y) is the conditional probability of x given y. θ denotes parameters of the entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Dataset</head><p>To evaluate the performance of our methods, we conduct experiments on two widely used datasets.</p><p>The first is New York Times (NYT) dataset, which is produced by distant supervision method <ref type="bibr" target="#b14">(Riedel et al., 2010)</ref>. This dataset consists of 1.18M sentences sampled from 294k 1987-2007 New York Times news articles. There are 24 valid relations in total. In this paper, we treat this dataset as supervised data as the same as <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>. We filter the sentences with more than 100 words and the sentences containing no positive triplets, and 66195 sentences are left. We randomly select 5000 sentences from it as the test set, 5000 sentences as the validation set and the rest 56195 sentences are used as train set.</p><p>The second is WebNLG dataset <ref type="bibr" target="#b3">(Gardent et al., 2017)</ref>. It is originally created for Natural Language Generation (NLG) task. This dataset contains 246 valid relations. In this dataset, a instance including a group of triplets and several standard sentences (written by human). Every standard sentence contains all triplets of this instance. We on-ly use the first standard sentence in our experiments and we filter out the instances if all entities of triplets are not found in this standard sentence. The origin WebNLG dataset contains train set and development set. In our experiments, we treat the origin development set as test set and randomly split the origin train set into validation set and train set. After filtering and splitting, the train set contains 5019 instances, the test set contains 703 instances and the validation set contains 500 instances.</p><p>The number of sentences of every class in NYT and WebNLG dataset are shown in Table <ref type="table" target="#tab_3">1</ref>. It's worthy noting that a sentence can belongs to both EntityPairOverlap class and SingleEntityOverlap class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>In our experiments, for both dataset, we use LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref> as the model cell; The cell unit number is set to 1000; The embedding dimension is set to 100; The batch size is 100 and the learning rate is 0.001; The maximum time steps T is 15, which means we predict at most 5 triplets for each sentence (therefore, there are 5 decoders in MultiDecoder model). These hyperparameters are tuned on the validation set. We use Adam <ref type="bibr" target="#b9">(Kingma and Ba, 2015)</ref> to optimize parameters and we stop the training when we find the best result in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline and Evaluation Metrics</head><p>We compare our models with NovelTagging model <ref type="bibr" target="#b21">(Zheng et al., 2017)</ref>, which conduct the best performance on relational facts extraction. We directly run the code released by <ref type="bibr" target="#b21">Zheng et al. (2017)</ref> to acquire the results.</p><p>Following <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>, we use the standard micro Precision, Recall and F1 score to evaluate the results. Triplets are regarded as correct when it's relation and entities are both correct. When copying the entity, we only copy the last word of it. A triplet is regarded as NA-triplet when and only when it's relation is NA-relation and it has an NA-entity pair. The predicted NA-triplets will be excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Table <ref type="table" target="#tab_4">2</ref> shows the Precision, Recall and F1 value of NovelTagging model <ref type="bibr" target="#b21">(Zheng et al., 2017)</ref>   As we can see, in NYT dataset, our MultiDecoder model achieves the best F1 score, which is 0.587. There is 39.8% improvement compared with the NovelTagging model, which is 0.420. Besides, our OneDecoder model also outperforms the NovelTagging model. In the WebNLG dataset, MultiDecoder model achieves the highest F1 score (0.371). MultiDecoder and OneDecoder models outperform the NovelTagging model with 31.1% and 7.8% improvements, respectively. These observations verify the effectiveness of our models.</p><p>We can also observe that, in both NYT and WebNLG dataset, the NovelTagging model achieves the highest precision value and lowest recall value. By contrast, our models are much more balanced. We think that the reason is in the structure of the proposed models. The NovelTagging method finds triplets through tagging the words. However, they assume that only one tag could be assigned to just one word. As a result, one word can participate at most one triplet. Therefore, the NovelTagging model can only recall a small number of triplets, which harms the recall performance. Different from the NovelTagging model, our models apply copy mechanism to find entities for a triplet, and a word can be copied many times when this word needs to participate in multiple different triplets. Not surprisingly, our models recall more triplets and achieve higher recall value. Further experiments verified this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Results on Different Sentence Types</head><p>To verify the ability of our models in handling the overlapping problem, we conduct further experiments on NYT dataset.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the results of NovelTagging, OneDecoder and MultiDecoder model in Normal, EntityPairOverlap and SingleEntityOverlap classes. As we can see, our proposed models perform much better than NovelTagging model in Entity-PairOverlap class and SingleEntityOverlap classes. Specifically, our models achieve much higher performance on all metrics. Another observation is that NovelTagging model achieves the best performance in Normal class. This is because the NovelTagging model is designed more suitable for Normal class. However, our proposed models are more suitable for the triplet overlap issues. Furthermore, it is still difficult for our models to judge how many triplets are needed for the input sentence. As a result, there is a loss in our models for Normal class. Nevertheless, the overall perfor- We also compare the model's ability of extracting relations from sentences that contains different number of triplets. We divide the sentences in NYT test set into 5 subclasses. Each class contains sentences that has 1,2,3,4 or &gt;= 5 triplets. The results are shown in Figure <ref type="figure">5</ref>. When extracting relation from sentences that contains 1 triplets, NovelTagging model achieve the best performance. However, when the number of triplets increases, the performance of NovelTagging model decreases significantly. We can also observe the huge decrease of recall value of NovelTagging model. These experimental results demonstrate the ability of our model in handling multiple relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">OneDecoder vs. MultiDecoder</head><p>As shown in the previous experiments (Table <ref type="table" target="#tab_4">2</ref>, Figure <ref type="figure" target="#fig_3">4</ref>   <ref type="table" target="#tab_5">3 and Table 4</ref>. We can observe that on both NYT and WebNLG datasets, these two models have comparable abilities on relation generation. However, MultiDecoder performs better than OneDecoder model when generating entities. We think that it is because MultiDecoder model utilizes different decoder to generate different triplets so that the entity generation results could be more diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>In this paper, we proposed an end2end neural model based on Seq2Seq learning framework with copy mechanism for relational facts extraction.</p><p>Our model can jointly extract relation and entity from sentences, especially when triplets in the sentences are overlapped. Moreover, we analyze the different overlap types and adopt two strategies for this issue, including one unified decoder and multiple separated decoders. We conduct experiments on two public datasets to evaluate the effectiveness of our models. The experiment results show that our models outperform the baseline method signif-icantly and our models can extract relational facts from all three classes. This challenging task is far from being solved. Our future work will concentrate on how to improve the performance further. Another future work is test our model in other NLP tasks like event extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The inputs and outputs of the decoder(s) of OneDecoder model and MultiDecoder model. (a) is the decoder of OneDecoder model. As we can see, only one decoder (the green rectangle with shadows) is used and this encoder is initialized with the sentence representation s. (b) is the decoders of MultiDecoder model. There are two decoders (the green rectangle and blue rectangle with shadows). The first decoder is initialized with s; Other decoder(s) are initialized with s and previous decoder's state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc> shows the inputs and outputs of decoders of MultiDecoder model. There are two decoders (the green and blue rectangle with shadows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of NovelTagging, OneDecoder, and MultiDecoder model in Normal, EntityPairOverlap and SingleEntityOverlap classes in NYT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and Figure 5), our MultiDecoder model performs better then OneDecoder model and Nov-values of relation generation. elTagging model. To find out why MultiDecoder model performs better than OneDecoder model, we analyzed their ability of entity generation and relation generation. The experiment results are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>adopted a pipeline</figDesc><table><row><cell>Normal</cell><cell>S1: Chicago is located in the United States. {&lt;Chicago, country, United</cell><cell cols="3">Chicago United States country</cell></row><row><cell></cell><cell>States&gt;}</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>S2: News of the list's existence</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EPO</cell><cell>unnerved officials in Khartoum, Sudan 's capital.</cell><cell>Sudan</cell><cell>capital</cell><cell>Khartoum</cell></row><row><cell></cell><cell>{&lt;Sudan, capital, Khartoum&gt;,</cell><cell cols="3">contains</cell></row><row><cell></cell><cell>&lt;Sudan, contains, Khartoum&gt;}</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>S3: Aarhus airport serves the</cell><cell cols="3">Jacob Bundsgaard</cell></row><row><cell></cell><cell>city of Aarhus who's leader is</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEO</cell><cell>{&lt;Aarhus, leaderName, Jacob Jacob Bundsgaard.</cell><cell>Aarhus</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bundsgaard&gt;,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>&lt;Aarhus Airport, cityServed,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Aarhus&gt;}</cell><cell></cell><cell cols="2">Aarhus Airport</cell></row></table><note>Figure 1: Examples of Normal, EntityPairOverlap (EPO) and SingleEntityOverlap (SEO) classes. The overlapped entities are marked in yellow. S1 belongs to Normal class because none of its triplets have overlapped entities; S2 belongs to EntityPairOverlap class since the entity pair &lt; Sudan, Khartoum &gt; of it's two triplets are overlapped; And S3 belongs to SingleEntityOverlap class because the entity Aarhus of it's two triplets are overlapped and these two triplets have no overlapped entity pair.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The number of sentences of Normal, En-tityPairOverlap (EPO) and SingleEntityOverlap (SEO) classes. It's worthy noting that a sentence can belongs to both EPO class and SEO class.</figDesc><table><row><cell>15)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>and our OneDecoder and MultiDecoder models. Results of different models in NYT dataset and WebNLG dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model</cell><cell></cell><cell></cell><cell cols="4">NYT Precision Recall</cell><cell>F1</cell><cell>WebNLG Precision Recall</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">NovelTagging</cell><cell cols="2">0.624</cell><cell></cell><cell>0.317 0.420</cell><cell>0.525</cell><cell>0.193 0.283</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">OneDecoder</cell><cell></cell><cell cols="2">0.594</cell><cell></cell><cell>0.531 0.560</cell><cell>0.322</cell><cell>0.289 0.305</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MultiDecoder</cell><cell cols="2">0.610</cell><cell></cell><cell>0.566 0.587</cell><cell>0.377</cell><cell>0.364 0.371</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Normal Class</cell><cell cols="3">NovelTagging OneDecoder MultiDecoder</cell><cell>1</cell><cell></cell><cell>EntityPairOverlap Class NovelTagging OneDecoder MultiDecoder</cell><cell>1</cell><cell>SingleEntityOverlap Class NovelTagging OneDecoder MultiDecoder</cell></row><row><cell></cell><cell>0.777</cell><cell>0.641</cell><cell>0.632</cell><cell>0.696</cell><cell>0.686</cell><cell>0.690</cell><cell>0.734</cell><cell>0.663</cell><cell>0.660</cell><cell></cell><cell>0.598</cell><cell>0.607</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.374</cell><cell>0.485</cell><cell>0.503</cell><cell>0.536</cell><cell>0.550</cell><cell>0.432</cell><cell>0.480</cell><cell>0.548</cell><cell>0.353</cell><cell>0.436</cell><cell>0.406</cell><cell>0.486</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.085</cell><cell>0.138</cell><cell>0.111</cell><cell>0.176</cell></row><row><cell>0</cell><cell cols="3">Precision</cell><cell cols="3">Recall</cell><cell></cell><cell>F1</cell><cell></cell><cell>0</cell><cell cols="2">Precision</cell><cell>Recall</cell><cell>F1</cell><cell>0</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>F1 values of entity generation.</figDesc><table><row><cell cols="2">0.8 0.777 0.645 0.636</cell><cell>0.471 0.572 0.624</cell><cell>0.581 0.586 Precision</cell><cell cols="2">0.464 0.546 0.584 NovelTagging 0.409 OneDecoder MultiDecoder</cell><cell>0.8</cell><cell>0.697 0.698 0.699</cell><cell>0.488 0.551</cell><cell>0.434 0.468 Recall</cell><cell cols="2">0.440 0.495 NovelTagging OneDecoder MultiDecoder</cell><cell>0.8</cell><cell>0.735 0.671 0.666</cell><cell>0.526 0.586</cell><cell>0.497 0.520 F1</cell><cell>0.487 0.536 NovelTagging OneDecoder MultiDecoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.296</cell><cell></cell><cell>0.295 0.316</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.237</cell><cell></cell><cell></cell><cell>0.272</cell><cell>0.300</cell></row><row><cell>0.0</cell><cell>1</cell><cell cols="3">2 Triplets number of a sentence 3 4</cell><cell>&gt;=5</cell><cell>0.0</cell><cell>1</cell><cell cols="3">2 Triplets number of a sentence 3 4 0.074 0.083 0.191</cell><cell>&gt;=5 0.038 0.149</cell><cell>0.0</cell><cell>1</cell><cell>2 Triplets number of a sentence 3 4 0.118 0.141</cell><cell>&gt;=5 0.068 0.203</cell></row><row><cell cols="15">Figure 5: Relation Extraction from sentences that contains different number of triplets. We divide the</cell></row><row><cell cols="15">sentences of NYT test set into 5 subclasses. Each class contains sentences that have 1,2,3,4 or &gt;= 5</cell></row><row><cell>triplets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell cols="4">NYT WebNLG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">OneDecoder 0.858</cell><cell cols="3">0.745</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">MultiDecoder 0.862</cell><cell cols="3">0.821</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>mance of the proposed models still outperforms NoverTagging. Moreover, we notice that the whole extracted performance of EntityPairOverlap and SingleEntityOverlap class is lower than that in Normal class. It proves that extracting relational facts from EntityPairOverlap and SingleEntity-Overlap classes are much more challenging than from Normal class.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Yi Chang from Huawei Tech. Ltm for his helpful discussions. This work is supported by the Natural Science Foundation of China (No.61533018 and No.61702512). This work is also supported in part by Beijing Unisound Information Technology Co., Ltd, and Huawei Innovation Research Program of Huawei Tech. Ltm.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planners</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schtze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning</title>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
				<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML PKDD</title>
				<meeting>ECML PKDD</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
