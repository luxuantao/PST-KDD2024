<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP</title>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_d73Cgh6">
					<orgName type="full">CAREER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-28">28 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><forename type="middle">Lisa</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Hall</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">St</forename><forename type="middle">Gregory</forename><surname>Hotel</surname></persName>
						</author>
						<title level="a" type="main">DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-28">28 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.14024v1[cs.CL]</idno>
					<note type="submission">? Retrieves a different building LM: &quot;Which castle did David Gregory inherit?&quot; RM: &quot;David Gregory inherited Kinnairdy Castle in 1664...&quot; LM: &quot;How many storyes does Kinnairdy Castle have?&quot; RM: &quot;Kinnairdy Castle is a tower house, having five storeys?&quot; LM: Kinnairdy Castle has five storeys.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>St. Gregory Hotel has nine storeys</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrievethen-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose DEMONSTRATE-SEARCH-PREDICT (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipelineaware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multihop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-200%, 8-40%, and 80-290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In-context learning adapts a frozen language model (LM) to tasks by conditioning the LM on a textual prompt including task instructions and a few demonstrating examples <ref type="bibr">(Mc-Cann et al., 2018;</ref><ref type="bibr" target="#b39">Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>. For knowledge-intensive tasks such as question answering, fact checking, and information-seeking dialogue, retrieval models (RM) are increasingly used to augment prompts with relevant information from a large corpus <ref type="bibr" target="#b25">(Lazaridou et al., 2022;</ref><ref type="bibr" target="#b37">Press et al., 2022;</ref><ref type="bibr" target="#b20">Khot et al., 2022)</ref>.  . On its own, the LM often makes false assertions. An increasingly popular retrieve-then-read pipeline fails when simple search can't find an answer. In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response. Texts edited for presentation.</p><p>Recent work has shown such retrieval-augmented in-context learning to be effective in simple "retrieve-then-read" pipelines: a query is fed to the RM and the retrieved passages become part of a prompt that provides context for the LM to use in its response. In this work, we argue that the fact that both LMs and RMs consume (and generate or retrieve) natural language texts creates an opportunity for much more sophisticated interactions between them. Fully realizing this would be transformative: frozen LMs and RMs could serve as infrastructure across tasks, enabling ML-and domain-experts alike to rapidly build grounded AI systems at a high level of abstraction and with lower deployment overheads and annotation costs.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> begins to illustrate the power of retrievalaugmented in-context learning, but also the limitations of "retrieve-then-read" <ref type="bibr" target="#b25">(Lazaridou et al., 2022;</ref><ref type="bibr" target="#b14">Izacard et al., 2022)</ref>. Our query is "How many storeys are in the castle David Gregory inherited?" When prompted to answer this, <ref type="bibr">GPT-3.5 (text-davinci-002;</ref><ref type="bibr" target="#b34">Ouyang et al. 2022</ref>) makes up a fictitious castle with incorrect attributes, highlighting the common observation that knowledge stored in LM parameters is often unreliable <ref type="bibr" target="#b44">(Shuster et al., 2021;</ref><ref type="bibr">Ishii et al., 2022)</ref>. Introducing an RM component helps, as the LM can ground its responses in retrieved passages, but a rigid retrieve-then-read strategy fails because the RM cannot find passages that directly answer the question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Demos</head><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hop1</head><p>Which castle did David Gregory inherit?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psg1</head><p>David Gregory inherited Kinnairdy Castle...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hop2</head><p>How many storeys are in Kinnairdy Castle?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psg2</head><p>Kinnairdy Castle [?] having five storeys...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q</head><p>How many storeys does the... x.hop1 = generate(hop_template)(x).pred</p><p>x.psg1 = retrieve(x.hop1, k=1) <ref type="bibr">[0]</ref> x.hop2 = generate(hop_template)(x).pred</p><p>x.psg2 = retrieve(x.hop2, k=1)[0] return x 2 Predict def predict(x: Example) -&gt; Example:</p><p>x.context <ref type="bibr">= [x.psg1, x.psg2]</ref> x.pred = generate(qa_template)(x).pred return x</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>"Five storeys" Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.</p><p>We introduce the DEMONSTRATE-SEARCH-PREDICT (DSP) framework for in-context learning, 1 which relies entirely on passing natural language text (and scores) between a frozen RM and LM. DSP introduces a number of composable functions that bootstrap training examples (DEMONSTRATE), gather information from a knowledge corpus (SEARCH), and generate grounded outputs (PREDICT), using them to systematically unify techniques from the retrieval-augmented NLP and the in-context learning literatures <ref type="bibr" target="#b27">(Lee et al., 2019;</ref><ref type="bibr">Khattab et al., 2021a;</ref><ref type="bibr" target="#b0">Anantha et al., 2020;</ref><ref type="bibr" target="#b8">Gao et al., 2022;</ref><ref type="bibr" target="#b14">Izacard et al., 2022;</ref><ref type="bibr" target="#b6">Dohan et al., 2022;</ref><ref type="bibr" target="#b58">Zelikman et al., 2022;</ref><ref type="bibr" target="#b59">Zhang et al., 2022)</ref>.</p><p>We use DSP to suggest powerful strategies for knowledgeintensive tasks with compositions of these techniques. This reveals new conceptual possibilities for in-context learning in general ( ?2), and it allows us to present rich programs that set new state-of-the-art results ( ?3).</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the path that a DSP program might take to arrive at an answer, and Figure <ref type="figure" target="#fig_3">2</ref>  stage uses labeled question-answer pairs to implement a form of weak supervision that programmatically annotates the transformations invoked within SEARCH and PREDICT.</p><p>We evaluate several DSP programs on answering questions in open-domain, multi-hop, and conversational settings. In them, we implement novel and reusable transformations such as bootstrapping annotations for all of our pipelines with weak supervision ( ?2.3), reliably rewriting questions to resolve conversational dependencies and iteratively decompose complex queries with summarization of intermediate hops ( ?2.4), and generating grounded responses from multiple passages with self-consistency ( ?2.5). We report preliminary results on Open-SQuAD, HotPotQA, and QReCC using the frozen LM <ref type="bibr">GPT-3.5 and RM ColBERTv2 (Khattab &amp; Zaharia, 2020;</ref><ref type="bibr">Santhanam et al., 2022b)</ref> with no fine-tuning. Our DSP programs deliver 37-200%, 8-40%, and 80-290% relative gains against corresponding vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline <ref type="bibr" target="#b37">(Press et al., 2022)</ref>, respectively. Future versions of this report will include additional test tasks and LM choices.</p><p>In summary, this work makes the following contributions. First, we argue that simple task-agnostic pipelines for incontext learning should give way to deliberate, task-aware strategies. Second, we show that this shift need not be a burden: with DSP, such strategies can be easily expressed as short programs using composable operators. Third, this composability spawns powerful capacities, like automatically annotating demonstrations for complex pipelines from end-task labels. Fourth, for three knowledge-intensive tasks, we implement rich programs that establish state-of-the-art results for in-context learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DEMONSTRATE-SEARCH-PREDICT</head><p>We now introduce the DSP framework and show its expressive power by suggesting a number of strategies in which the LM and RM can come together to tackle complex problems effectively. We show in ?3 that such strategies outperform existing in-context learning methods. We begin by discussing the LM and RM foundation modules on which DSP is built ( ?2.1) and then the datatypes and control flow within DSP ( ?2.2). Subsequently, we discuss each of the three inference stages: DEMONSTRATE ( ?2.3), SEARCH ( ?2.4), and PREDICT ( ?2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pretrained Modules: LM and RM</head><p>A DSP program defines the communication between the language model LM and the retrieval model RM.</p><p>Language Model We invoke a frozen language model LM to conditionally generate (or score) text. For each invocation, the program prepares a prompt that adapts the LM to a specific function (e.g., answering questions or generating queries). A prompt often includes instructions, a few demonstrations of the desired behavior, and an input query to be answered.</p><p>As in Figure <ref type="figure" target="#fig_3">2</ref>, the LM generates not only: (i) the final answer to the input question (in the PREDICT stage), but also (ii) intermediate "hop" queries to find useful information for the input question (SEARCH) as well as (iii) exemplar queries that illustrate how to produce queries for questions in the training set (DEMONSTRATE). This systematic use of the LM is a hallmark of DSP programs.</p><p>Retrieval Model DSP programs also invoke a frozen retrieval model RM to retrieve the top-k most "relevant" text sequences for a given query. The RM can index a massive set of pre-defined passages for scalable search, and those passages can be updated without changing the retrieval parameters. The RM accepts free-form textual inputs and specializes in estimating the relevance (or similarity) of a text sequence to a query.</p><p>As in Figure <ref type="figure" target="#fig_3">2</ref>, the RM is responsible for retrieving (i) passages for each query generated by the LM (during the SEARCH stage), but also (ii) passages that are used within demonstrations (DEMONSTRATE). In the latter case, the RM's contributions are less about providing directly relevant information to the input question and more about helping the LM adapt to the domain and task.</p><p>Though not utilized in this example, the RM is also used in DSP for functions like retrieving "nearest-neighbor" demonstrations from task training data (DEMONSTRATE) and selecting well-grounded generated sequences from the LM (PREDICT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Datatypes and Control Flow</head><p>We have implemented the DSP framework in Python. The present section introduces the core data types and composable functions provided by the framework. We use illustrative code snippets to ground the examples, and to convey the power that comes from being able to express complex interactions between the LM and RM in simple programs. This snippet contains two labeled examples, each with a multi-hop question (e.g., "In which city did Akeem Ellis play in 2017?") and its short answer ("Ellesmere Port"). Arbitrary keys and values are allowed within an Example, though typical values are strings or lists of strings.</p><p>In this task, we are unlikely to find an individual passage that provides the answer to any question. For example, the first training example can probably be resolved only by first answering the question of who discovered Palomar ("Edwin Hubble") and then addressing the question of Hubble's birth date using different evidence passages. We typically assume that the human-labeled training data do not include labels for intermediate transformations (e.g., queries for individual hops) that would be useful for following these steps, and so it is the job of the DSP program to discover these strategies via in-context learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DSP Program</head><p>The following code snippet is a complete program for resolving multi-hop questions like those in Figure <ref type="figure" target="#fig_0">1</ref>, with help from train examples like those above.</p><p>1 def multihop_program ( question : str ) -&gt; str : The program takes the input (here, a question) and outputs the system output (its short answer). It starts by creating an Example for the input question and assigning the train field to the training set from the previous snippet. Programs invoke and compose DSP primitives (i.e., built-in functions) to build the DEMONSTRATE, SEARCH, and PREDICT transformations that define the program.</p><p>Transformations A transformation is a function that takes an Example as input and returns an Example, populating new fields (or modifying existing fields) in it. This program invokes three developer-defined transformations, namely, multihop_demonstrate, multihop_search, and multihop_predict. Transformations may themselves invoke other transformations, and they act analogously to layers in standard deep neural network (DNN) programming frameworks such as PyTorch, except that they pass text data instead of tensors between each other and do not involve backpropagation.</p><p>Transformations are categorized under one of the DEMON-STRATE, SEARCH, and PREDICT stages, depending on their purpose, though DSP does not impose this categorization and even allows us to define functions that may blend these stages. We will discuss each of the three stages next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">DEMONSTRATE</head><p>It is known that including examples of the desired behavior from the LM in its prompt typically leads to better performance <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>. In DSP, a demonstration is a training example that has been prepared to illustrate specific desired behaviors from the LM. A DEMONSTRATE transformation takes as input x of type Example and prepares a list of demonstrations in x.demos, typically by selecting a subset of the training examples in x.train and bootstrapping new fields in them.</p><p>Bootstrapping Demonstrations Examples in the training set typically consist of the input text and the target output of the task. The DEMONSTRATE stage can augment a training example by programmatically bootstrapping annotations for intermediate transformations. In our running "multi-hop" example, the demonstrations illustrate three LM-based transformations: (i) how to break down the input question in order to gather information for answering it (i.e., first-hop retrieval), (ii) how to use information gathered in an earlier "hop" to ask follow-up questions, and (iii) how to use the information gathered to answer complex questions. Akin to a specialized map, the annotate primitive accepts a user-defined transformation fn and applies it over a list of training examples. Whenever fn returns an example (rather than None), annotate caches the intermediate predictions (i.e., the generated queries and retrieved passages). These predictions serve as successful demonstrations for the pipeline transformations. In simple uses, fn may attempt to answer the example "zero-shot" one or more times. This is typically done by invoking the SEARCH and PREDICT stages of the program. When an answer is produced, if fn assesses it as correct, it returns a populated example in which the intermediate predictions are present.</p><formula xml:id="formula_0">1 Examples = list [ Example ] 2 Transformation = Callable [[ Example ] , 3 Optional [ Example ]]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>The snippet below defines the function multihop_demonstrate, called in Line 3 of multihop_program, and illustrates the usage of annotate.</p><p>1 from dsp import sample , annotate Figure <ref type="figure" target="#fig_3">2</ref> illustrates this behavior. DEMONSTRATE transforms a training question-answer pair to a fully-populated demonstration, including fields such as hop1 and hop2 (i.e., queries for multi-hop search) as well as psg1 and psg2. When the LM is later invoked to conduct a transformation, say, generating a "second-hop" query during SEARCH, the psg1 field serves as context and the hop2 field serves as a label for this particular training example.</p><p>Discussion This simple case study illustrates the power of composition in the DSP abstraction. Because the pipeline is a well-defined program in which transformations communicate by passing text attached to Examples, a simple map-and-filter strategy can leverage the LM and RM to bootstrap annotations for a full pipeline from end-task labels. This is an extensible strategy, but even in its simplest form it generalizes the approaches explored recently by Zelikman et al. ( <ref type="formula">2022</ref>), <ref type="bibr" target="#b52">Wei et al. (2022)</ref>, <ref type="bibr" target="#b59">Zhang et al. (2022)</ref>, and <ref type="bibr" target="#b11">Huang et al. (2022)</ref> in which an LM self-generates chain-of-thought rationales for an individual prompt.</p><p>By bootstrapping pipelines, DEMONSTRATE makes it easy to explore complex strategies in SEARCH and PREDICT without writing examples for every transformation. This includes strategies that are challenging to explore without custom annotations in traditional retrieval-augmented NLP. For instance, <ref type="bibr">Khattab et al. (2021a)</ref> introduces a pipeline for multi-hop reasoning that is trained with weak supervision, extending work by <ref type="bibr" target="#b27">Lee et al. (2019)</ref> and <ref type="bibr">Khattab et al. (2021b)</ref>. In it, the target 3 or 4 passages that need to retrieved must be labeled but the system discovers the best order of "hops" automatically.</p><p>In contrast, DSP allows us to build complex pipelines without labels for intermediate steps, because we can compose programs out of small transformations. If LM and RM can accurately process such transformations "zero-shot" (i.e., without demonstrations) on at least one or two examples, these examples can be discovered with end-task labels and used as demonstrations.</p><p>To draw on our earlier analogy with DNN frameworks like PyTorch, DEMONSTRATE aims to replace the function of backpropagation in extensible ways by simulating the behavior of the program (corresponding to a "forward" pass) and programmatically learning from errors. In doing this with frozen models and with only end-task labels, DEMON-STRATE introduces a high degree of modularity. As a baseline choice, k demonstrations can be randomly sampled from train using the sample primitive, an approach used by <ref type="bibr" target="#b1">Brown et al. (2020)</ref> and much subsequent work. We can also leverage the RM's representations and select from the training set the k nearest neighbors to the input text, a strategy explored by <ref type="bibr" target="#b31">Liu et al. (2021)</ref>. Another strategy is to apply cross-validation to select among a number of sampled sets of demonstrations <ref type="bibr" target="#b36">(Perez et al., 2021)</ref>. Compositions &amp; Extensions By manipulating demonstrations and higher-order transformations, these simple selection and bootstrapping primitives can be combined to conduct larger novel strategies. If the training set is very large (e.g., |train| = 100, 000), we can conduct knn to find the nearest k = 16 examples and only annotate these, arriving at a system that learns incrementally in real-time. If the training set is moderately large (e.g., |train| = 1000), we can conduct crossval and cache the performance of all prompts it evaluates on each training example. At test time, we can use knn to find k = 50 similar examples to the test input and select the prompt that performs best on these k examples, producing an adaptive system that is informed by the quality of its pipeline on different types of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">SEARCH</head><p>The SEARCH stage gathers passages to support transformations conducted by the LM. We assume a large knowledge corpus-e.g., a snippet of Web, Wikipedia, or arXiv-that is divided into text passages. Providing passages to the LM facilitates factual responses, enables updating the knowledge store without retraining, and presents a transparency contract: when in doubt, users can check whether the system has faithfully used a reliable source in making a prediction.</p><p>In the simplest scenarios, SEARCH can directly query the RM, requesting the top-k passages (from a pre-defined index) that match an input question. This baseline instantiation of SEARCH simulates retrieval in most open-domain question answering systems, which implement a "retrievethen-read" pipeline, like <ref type="bibr" target="#b27">Lee et al. (2019)</ref>, <ref type="bibr">Khattab et al. (2021b)</ref>, <ref type="bibr" target="#b25">Lazaridou et al. (2022)</ref>, and many others.</p><p>1 from dsp import retrieve </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEARCH Strategies</head><p>In many scenarios, the complexity of the task demands more sophisticated SEARCH strategies that empower the RM to find relevant passages. Our running example (Figure <ref type="figure" target="#fig_3">2</ref>) is one such scenario, in which we suspect examples are likely to require multi-hop reasoning in particular. Other settings, for instance, pose conversational challenges, in which the information need expressed by a user can only be resolved by taking into account previous turns in the conversation, or demand more extensive planning <ref type="bibr">(Zhong et al., 2022)</ref>.</p><p>In the retrieval-augmented NLP literature, multi-hop search <ref type="bibr" target="#b54">(Xiong et al., 2020;</ref><ref type="bibr">Khattab et al., 2021a)</ref> and conversational search <ref type="bibr" target="#b5">(Del Tredici et al., 2021;</ref><ref type="bibr" target="#b41">Raposo et al., 2022)</ref> pipelines have received much attention. These systems are typically fine-tuned with many hand-labeled query "rewrites" <ref type="bibr" target="#b0">(Anantha et al., 2020)</ref>, "decompositions" <ref type="bibr" target="#b9">(Geva et al., 2021;</ref><ref type="bibr" target="#b33">Min et al., 2019)</ref>, or target hops <ref type="bibr" target="#b56">(Yang et al., 2018;</ref><ref type="bibr" target="#b15">Jiang et al., 2020)</ref>. Supported with automatic annotations from DEMONSTRATE, the SEARCH stage allows us to simulate many such strategies and many others in terms of passing queries, passages, and demonstrations between the RM and LM. More importantly, SEARCH facilitates our vision of advanced strategies in which the LM and RM cooperate to incrementally plan a research path for which the RM gathers information and the LM identifies next steps.</p><p>Case Study Let us build on our running multi-hop example as a case study. We can define multihop_search_v2 (Line 4 in our core program), a slightly more advanced version of the SEARCH transformation from Figure <ref type="figure" target="#fig_3">2</ref>. This transformation simulates the iterative retrieval component of fine-tuned retrieval-augmented systems like IRRR <ref type="bibr" target="#b38">(Qi et al., 2020)</ref>, which reads a retrieved passage in every hop and generates a follow-up query (or a termination condition to stop hopping), and Baleen <ref type="bibr">(Khattab et al., 2021a)</ref>, which summarizes the information from many passages in each hop for inclusion in subsequent hops.</p><p>1 from dsp import generate In multihop_search_v2, Line 7 calls the generate primitive, which invokes the LM to produce a query for each retrieval hop. The LM is conditioned on a prompt that is prepared using the hop_template template. (We discuss prompt templates and the generate primitive in ?2.5.) Here, this template may be designed to generate a prompt that has the following format (e.g., for the second hop).</p><p>As shown, the LM is instructed to read the context retrieved in earlier hops and a complex question. It is then prompted to write: (i) a summary of the supplied context and (ii) a search query that gathers information for answering that question. The generated text will be extracted and assigned to the summary and query variables in (multihop_search_v2; Line 7). On Line 10, we terminate the hops if the query is "N/A". Otherwise, Line 12 retrieves k = 5 passages using the query and Line 13 assigns the context for the subsequent hop (or for PREDICT), setting that to include the summary of all previous hops as well as the passages retrieved in the final hop so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing Retrieval Results</head><p>For improved recall and robustness, we can also fuse the retrieval across multiple generated queries. Fusion has a long history in information retrieval <ref type="bibr" target="#b7">(Fox &amp; Shaw, 1994;</ref><ref type="bibr" target="#b55">Xue &amp; Croft, 2013;</ref><ref type="bibr" target="#b23">Kurland &amp; Culpepper, 2018)</ref> and sequentially processing multiple queries was explored recently by <ref type="bibr" target="#b8">Gao et al. (2022)</ref> for retroactively attributing text generated by LMs to citations. Inspired by these, we include a fused_retrieval primitive to DSP to offer a versatile mechanism for interacting with frozen retrievers. It accepts an optional fusion function that maps multiple retrieval lists into one. By default, DSP uses a variant of CombSUM <ref type="bibr" target="#b7">(Fox &amp; Shaw, 1994)</ref>, assigning each passage the sum of its probabilities across retrieval lists.</p><p>To illustrate, the modification below generates n = 10 queries for the transformation multihop_search_v2. Compositions &amp; Extensions To illustrate a simple composition, we can equip a chatbot with the capacity for conversational multi-hop search by combining a query rewriting step, which produces a query that encompasses all of the relevant conversational context, with the multi-hop transformation, as follows.</p><p>1 def conversational_multihop_search ( x ) :</p><formula xml:id="formula_1">2 x . question = generate ( conv_rewriting_template )(x) 3 return multihop_search_v2 ( x )</formula><p>Similar approaches can be used for correcting spelling mistakes or implementing pseudo-relevance feedback <ref type="bibr" target="#b2">(Cao et al., 2008;</ref><ref type="bibr">Wang et al., 2022a)</ref>, in which retrieved passages are used to inform a better search query, though this has not been attempted with pretrained LMs to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">PREDICT</head><p>The PREDICT stage generates the system output, using in that zero or more demonstrations (e.g., in x.demos) and passages (e.g., in x.context). PREDICT tackles the challenges of reliably solving the downstream task, which integrates much of the work on in-context learning in general. Within DSP, it also has the more specialized function of systematically aggregating information across a large number of demonstrations, passages, and candidate predictions.</p><p>Generating Candidates Generally, PREDICT has to produce one or more candidate predictions for the end-task. To this end, the basic primitive in PREDICT is generate, which accepts a Template and (via currying) an Example and queries the LM to produce one or more completions, as explored earlier in ?2.4. A corresponding primitive that uses the RM in this stage is rank, which accepts a query and one or more passages and returns their relevance scores. A Template is an object that can produce prompts, that is, map an Example to a string, and extract fields out of completions. For instance, we can map an example x that has a question and retrieved passages to the following prompt:</p><p>1 My task is to answer questions using Web documents . As this illustrates, the LM will be asked to generate a chainof-thought rationale (CoT; <ref type="bibr" target="#b52">Wei et al. 2022;</ref><ref type="bibr" target="#b21">Kojima et al. 2022</ref>) and an answer, and the generated text will be extracted back into the rationale and answer keys of each completion.</p><p>Each invocation to the LM can sample multiple candidate predictions. Selecting a "best" prediction is the subject of much work on decoding <ref type="bibr" target="#b53">(Wiher et al., 2022;</ref><ref type="bibr" target="#b30">Li et al., 2022)</ref>, but a frozen and general-purpose LM may not support custom modifications to decoding. Within these constraints, we present several high-level strategies for selecting predictions and aggregating information in DSP via the LM and RM.</p><p>Selecting Predictions Among multiple candidates, we can simply extract the most popular prediction. When a CoT is used to arrive at the answer, this is the self-consistency method of <ref type="bibr">Wang et al. (2022c)</ref>, which seeks to identify predictions at which multiple distinct rationales arrive.</p><p>1 from dsp import generate , majority DSP generalizes this in two ways. First, we can sample multiple "pipelines of transformations" (PoT) within the program, rather than locally with "chains of thought" (CoT) in one transformation. These chains may even invoke different paths in the program, as illustrated below.</p><p>1 from dsp import branch In the snippet above, Line 10 invokes the primitive branch which samples n different PoTs with a high temperature (e.g., t = 0.7) and accumulates their intermediate and final predictions. In this example, our pipeline invokes multihop_search_v2 ( ?2.4), which applies a variable number of retrieval hops depending on the questions generated, before doing PREDICT. That is, PoT_program potentially invokes multiple distinct paths in the program (i.e., with different multi-hop queries and number of hops in each) across branches. It then selects the majority answer overall.</p><p>DSP generalizes self-consistency in a second way. When sampling our CoTs or PoTs provides multiple candidates, we can select the top-k (e.g., top-4) predictions and then compare them directly. For instance, we may prompt the LM to compare these choices as MCQ candidates, a transformation for which DEMONSTRATE can automatically prepare exemplars. This effectively simulates the LM recursion of <ref type="bibr" target="#b28">Levine et al. (2022)</ref>, though unlike their approach it does not require a large training set or updating any (prompttuning) weights. One such implementation is illustrated in openqa_predict below.</p><p>1 def openqa_predict ( x ) : x . passages = fused_retrieval ( queries ) 9</p><p>x . answer = generate ( TemplateMCQ ) ( x ) . answer 10 return x</p><p>As an alternative comparison approach, we can invoke the RM via rank to find the prediction that is most grounded in a retrieved contexts (i.e., most similar to the concatenation of the retrieved passages) or, given an RM that can score completions <ref type="bibr" target="#b22">(Krishna et al., 2022)</ref>, simply the prediction that has the highest score given the prompt.</p><p>Aggregating Information When only a few demonstrations or passages are selected, we can simply concatenate them all into the prompt. For instance, GPT-3.5 text-davinci-002 has a context window of 4097 tokens, which we find to be reasonably large for accommodating several (e.g., 3-5) demonstrations, which individually include their own passages and rationales.</p><p>To deal with a larger number of demonstrations or passages, we can branch in parallel to process individual subsets of the passages or demonstrations and then aggregate the individual answers using one of the scoring methods presented earlier. Indeed, <ref type="bibr" target="#b29">Lewis et al. (2020)</ref> and <ref type="bibr" target="#b25">Lazaridou et al. (2022)</ref> have explored marginalization as a way to combine scores across passages and <ref type="bibr" target="#b26">Le et al. (2022)</ref> ensemble prompts across demonstrations, which can be expressed in this way.</p><p>An alternative aggregation strategy is to accumulate information across passages sequentially, rather than independently. This is effectively how our multi-hop approach works ( ?2.4). Such a strategy has also been employed recently by <ref type="bibr" target="#b8">Gao et al. (2022)</ref> for retroactively attributing text generated by LMs to citations. They generate many queries but instead of fusion ( ?2.4), they run their pipeline on each query and use its outputs to alter the input to subsequent queries.<ref type="foot" target="#foot_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>We now consider how to implement DSP programs for three diverse knowledge-intensive NLP tasks: open-domain question answering (QA), multi-hop QA, and conversational QA. All of these tasks are "open-domain", in the sense that systems are given a short question or participate in a multi-turn conversation without being granted access to context that answers these questions.</p><p>We build and evaluate intuitive compositions of the functions explored in ?2 for each task. We show that, despite low development effort, the resulting DSP programs exhibit strong quality and deliver considerable empirical gains over vanilla in-context learning and a standard retrieve-then-read pipeline with in-context learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Methodology</head><p>In this report, we consider one development dataset for each of the tasks we consider, namely, the open-domain version of SQuAD <ref type="bibr" target="#b40">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b27">Lee et al., 2019)</ref>, the multi-hop HotPotQA <ref type="bibr" target="#b56">(Yang et al., 2018)</ref> dataset in the opendomain "fullwiki" setting, and the conversational question answering QReCC <ref type="bibr" target="#b0">(Anantha et al., 2020;</ref><ref type="bibr" target="#b48">Vakulenko et al., 2022)</ref> dataset, which we used for developing the DSP abstractions. We report the validation set accuracy on all three datasets and discuss them in detail ?3.5.</p><p>Unless otherwise stated, systems are given access to 16shot training examples, that is, each DSP program can use (up to) 16 questions-or conversations, where applicablerandomly sampled from the respective training set. We subsample the validation and test sets to 1000 questions (or 400 conversations, where applicable) and report average quality across five seeds where each seed fixes a single kshot training set of examples. To control the language model API spending budget, each seed processes one fifth of the evaluation examples (e.g., 200 questions per seed, for a total of 1000 unique questions).</p><p>We also dedicate held-out test datasets (e.g., Open-NaturalQuestions; <ref type="bibr" target="#b24">Kwiatkowski et al. 2019</ref>) and test tasks (e.g., claim verification, as in FEVER; <ref type="bibr" target="#b47">Thorne et al. 2018</ref>) that we only use for evaluating pre-defined DSP programs rather than development. We will include these results in a future version of this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pretrained Modules</head><p>RM We use ColBERTv2 <ref type="bibr">(Santhanam et al., 2022b)</ref>, a state-of-the-art retriever based on late interaction <ref type="bibr" target="#b17">(Khattab &amp; Zaharia, 2020)</ref>. We choose ColBERTv2 for its highly effective zero-shot search quality and efficient search <ref type="bibr">(Santhanam et al., 2022a)</ref>. However, our DSP programs are agnostic to how the retriever represents examples or scores passages, so essentially any retriever can be used.</p><p>In addition, by making retrieval a first-class construct, DSP allows us to change or update the search index over time. We simulate this in our experiments by aligning each of our datasets with the nearest Wikipedia corpus among the Dec 2016 Wikipedia dump from Chen et al. 2017, the Nov 2017 Wikipedia "abstracts" dump from <ref type="bibr" target="#b56">Yang et al. 2018</ref>, and the Dec 2018 Wikipedia dump from <ref type="bibr" target="#b16">Karpukhin et al. 2020</ref>.</p><p>LM We use the <ref type="bibr">GPT-3.5 (text-davinci-002;</ref><ref type="bibr" target="#b1">Brown et al. 2020;</ref><ref type="bibr" target="#b34">Ouyang et al. 2022</ref>) language model. Unless otherwise stated, we use greedy decoding when generating n = 1 prediction. We sample with temperature t = 0.7 when n &gt; 1, like related work <ref type="bibr">(Wang et al., 2022c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Baselines</head><p>Vanilla LM The vanilla LM baselines represent the fewshot in-context learning paradigm used by <ref type="bibr" target="#b1">Brown et al. (2020)</ref>. The open-domain QA and multi-hop QA base-lines randomly sample 16 demonstrations (i.e., all of the examples available to each program in our evaluation) from the training set and do not augment these demonstrations with evidence. Similarly, the conversational QA baseline samples four conversations. The vanilla baselines do not search for passages relevant to the input query.</p><p>1 def vanilla_LM_QA ( question : str ) : Retrieve-then-Read The "retrieve-then-read" baselines use the RM to support each example with a potentially relevant passage before submitting the prompt to the LM. This emulates the pipelines used by state-of-the-art open-domain question answering systems <ref type="bibr">(Khattab et al., 2021b;</ref><ref type="bibr" target="#b13">Izacard &amp; Grave, 2020;</ref><ref type="bibr" target="#b10">Hofst?tter et al., 2022)</ref>. In conversational QA, we concatenate the first turn and the final question, an approach that we found to perform much better than simply using the final turn. For multi-hop QA, we retrieve and concatenate two passages per question.</p><p>1 def retrieve_then_read_openQA ( question : str ) : Self-ask We also compare against self-ask <ref type="bibr" target="#b37">(Press et al., 2022)</ref>, a contemporaneous pipeline that can be thought of as a specific instantiation of DSP's SEARCH stage followed by a simple PREDICT step. For direct comparison with our methods, we modify the self-ask control flow to query the same ColBERTv2 index used in our DSP experiments instead of Google Search. We evaluate two configurations of self-ask. The first uses the original self-ask prompt template, which contains four hand-written demonstrations. In the second configuration, we modify the prompt template to apply a number of changes that we find are empirically useful for HotPotQA.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed DSP Programs</head><p>We build on transformations presented in ?2. Our programs for all three tasks have the following structure, illustrated for open-domain QA.</p><p>1 def openqa_program ( question : str ) -&gt; str : The exception is that the conversational QA program, convqa_program, accepts turns (i.e., a list of strings, representing the conversational history) instead of a single question. Unless otherwise stated, our programs default to greedy decoding during the DEMONSTRATE stage.</p><p>For SEARCH, our open-domain QA program uses the question directly for retrieving k = 7 passages and concatenates these passages into our QA prompt with CoT. For PREDICT, it generates n = 20 reasoning chains and uses self-consistency (SC; <ref type="bibr">Wang et al. 2022c</ref>) to select its final prediction. For DEMONSTRATE, our open-domain QA program uses the following approach, slightly simplified for presentation. In it, the parameter k = 3 passed to annotate requests annotating only three demonstrations, which will then be used in the prompts. Our multi-hop program adopts a very similar approach for DEMONSTRATE and PREDICT. For SEARCH, it uses the approach described in ?2.4, with the following adjustments. It uses result fusion across n = 10 queries per hop and, among the n predictions, uses the summary corresponding to the largest average log-probability. It uses a fixed number of hops for HotPotQA, i.e., two hops. In each prompt (i.e., each hop and QA), it concatenates the summaries of all previous hops (i.e., hop 1 onwards) and a total of k = 5 passages divided between the hops (i.e., five passages from the first hop or two passages from the first and three from the second).</p><p>Table <ref type="table">1</ref>. Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based rows use ColBERTv2. The results marked with ? are collected from related work as of mid-December 2022, and attributed to their individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons, since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Development Datasets &amp; Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-SQuAD</head><p>We conduct the open-domain version of SQuAD over the Wikipedia 2016 corpus from <ref type="bibr" target="#b3">Chen et al. (2017)</ref>, as processed by <ref type="bibr">Khattab et al. (2021b)</ref>. We use the same train/validation/test splits as in <ref type="bibr" target="#b16">Karpukhin et al. (2020)</ref> and <ref type="bibr">Khattab et al. (2021b)</ref>.</p><p>Table <ref type="table">1</ref> reports the answer EM and F1. The task-aware DSP program achieves 36.6% EM, outperforming the vanilla LM baseline by 126% EM relative gains. This indicates the importance of grounding the LM's predictions in retrieval, and it shows that state-of-the-art retrievers like ColBERTv2 have the capacity to do so off-the-shelf. The proposed DSP program also achieves relative gains of 8% in EM and 6% in F1 over the retrieve-then-read pipeline, highlighting that nontrivial gains are possible by aggregating information across several retrieved passages as we do with self-consistency.</p><p>These in-context learning results are competitive with a number of popular fine-tuned systems. For instance, on the Open-SQuAD test set, DPR achieves 29.8% EM, well below our 16-shot DSP program. On the Open-SQuAD dev set, the powerful Fusion-in-Decoder <ref type="bibr" target="#b13">(Izacard &amp; Grave, 2020</ref>) "base" approach achieves approximately 36% (i.e., very similar quality to our system) when invoked with five retrieved passages. Nonetheless, with the default setting of reading 100 passages, their system reaches 48% EM in this evaluation. This may indicate that similar gains are possible for our DSP program if the PREDICT stage is made to aggregate information across many more passages.</p><p>For comparison, we also evaluate the self-ask pipeline, which achieves 9.3% EM, suggesting that its fixed pipeline is ineffective outside its default multi-hop setting. Studying a few examples of its errors reveals that it often decomposes questions in tangential ways and answers these questions instead. For instance, when asked "When does The Kidnapping of Edgardo Mortara take place?", it asks "What is The Kidnapping of Edgardo Mortara" and then asks when it was published. Thus, self-ask answers 1997, instead of the time The Kidnapping of Edgardo Mortara takes place (1858).</p><p>For reference, Table <ref type="table">1</ref> also reports (as No-retrieval LM SoTA) the concurrent in-context learning results from <ref type="bibr" target="#b45">Si et al. (2022)</ref> using code-davinci-002, who achieve 20.2% EM without retrieval and 34.0% EM with retrieval, albeit on a different sample and split of the SQuAD data. Overall, their approaches are very similar to the baselines we implement (vanilla LM and retrieve-then-read), though their retrieval-augmented approach retrieves (and concatenates into the prompt) 10 passages from a Wikipedia dump.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HotPotQA</head><p>We use the open-domain "fullwiki" setting of HotPotQA using its official Wikipedia 2017 "abstracts" corpus. The HotPotQA test set is hidden, so we reserve the official validation set for our testing. We sub-divide the training set into 90%/10% train/validation splits. In the training (and thus validation) split, we keep only examples marked as "hard" in the original dataset, which matches the designation of the official validation and test sets.</p><p>We report the final answer EM and F1 in Table <ref type="table">1</ref>. On HotPotQA, the task-aware DSP program outperforms the baselines and existing work by very wide margins, exceeding the vanilla LM, the retrieve-then-read baseline, and the self-ask pipeline by 82%, 39%, and 80%, respectively, in EM. This highlights the effectiveness of building up more sophisticated programs that coordinate the LM and RM for the SEARCH step.</p><p>These results may be pegged against the evaluation on Hot-PotQA in a number of concurrent papers. We first compare with non-retrieval approaches, though our comparisons must be tentative due to variation in evaluation methodologies. <ref type="bibr" target="#b45">Si et al. (2022)</ref> achieve 25.2% EM with CoT prompting. With a "recite-and-answer" technique for PaLM-62B <ref type="bibr" target="#b4">(Chowdhery et al., 2022)</ref>, <ref type="bibr" target="#b46">Sun et al. (2022)</ref> achieve 26.5% EM. <ref type="bibr">Wang et al. (2022b)</ref> achieve 33.8% EM and 44.6 F1 when applying a self-consistency prompt for PaLM-540B. Next, we compare with a contemporaneous retrieval-based approach: <ref type="bibr" target="#b57">Yao et al. (2022)</ref> achieve 35.1% EM using a system capable of searching using a Wikipedia API. All of these approaches trail our task-aware DSP program, which achieves 51.4% EM, by large margins.</p><p>QReCC We use QReCC <ref type="bibr" target="#b0">(Anantha et al., 2020)</ref> in an opendomain setting over Wikipedia 2018. QReCC does not have an official development set, so we sub-divide the training set into 90%/10% train/validation splits. For the first question in every conversation, we use the rewritten question as the original question often assumes access to a groundtruth document. We also filter low-quality examples from QReCC. <ref type="foot" target="#foot_3">4</ref>We conduct the QReCC conversations in an auto-regressive manner. At turn t &gt; 1 of a particular conversation, the system sees its own responses (i.e., not the ground truth responses) to previous turns of the conversation. We report the novel-F1 metric (nF1; <ref type="bibr" target="#b35">Paranjape et al. 2022)</ref>, which computes the F1 overlap between the system response and the ground truth while discounting common stopwords and terms present in the question (or earlier questions). The results are shown in Table <ref type="table">1</ref>, and follow the same general pattern as SQuAD and HotPotQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>For a long time, the dominant paradigm for building models in AI has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. However, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.</p><p>The promise of in-context learning is that we can build complex systems from pretrained components using only natural language as the medium for giving systems instructions and, as we argue for, allowing components to communicate with each other. In this new paradigm, the building blocks are pretrained models and the core operations are natural language instructions and operations on natural language texts. If we can realize this potential, then we can broaden participation in AI system development, rapidly prototype systems for new domains, and maximize the value of specialized pretrained components.</p><p>In the current paper, we introduced the DEMONSTRATE-SEARCH-PREDICT (DSP) framework for retrieval augmented in-context learning. DSP consists of a number of simple, composable functions for implementing in-context learning systems as deliberate programs-instead of endtask prompts-for solving knowledge intensive tasks. We implemented DSP as a Python library and used it to write programs for Open-SQuAD, HotPotQA, and QReCC. These programs deliver substantial gains over previous in-context learning approaches. However, beyond any particular performance number, we argue that the central contribution of DSP is in helping to reveal a very large space of conceptual possibilities for in-context learning in general. tions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank Giuseppe Attanasio for his public L A T E X GitHub-style Python code formatting gist. 5  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. A comparison between three systems based on GPT-3.5 (text-davinci-002). On its own, the LM often makes false assertions. An increasingly popular retrieve-then-read pipeline fails when simple search can't find an answer. In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response. Texts edited for presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Q</head><label></label><figDesc>How many storeys are in the...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>many storeys does the castle David Gregory inherited have ? " ) 9 # = &gt; " five storeys "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>d . pred == d . answer else None 8 9 def multihop_demonstrate ( x : Example ) : 10 demos = annotate ( x . train , attempt_example ) 11 return Example (x , demos = demos ) In Line 10, multihop_demonstrate invokes annotate, which bootstraps missing fields in training examples by caching annotations from attempt_example. The transformation attempt_example takes a training example d and attempts to answer it in a zero-shot fashion: it creates a copy of d with no demonstrations (Line 4; i.e., zero-shot) and invokes the multi-hop search and predict pipeline (Lines 5 and 6). Each transformation returns an updated version of d with additional fields populated. If the pipeline answers correctly (Line 7), the updated d is returned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>In particular, without hand-labeling intermediate transformations, developers may swap the training domain, update the training examples, or modify the program's strategy, and use annotate to automatically populate all of the intermediate fields for demonstrations. Selecting Demonstrations It is not always possible to fit all of the training examples in the context window of the LM. DSP provides three primitives for selecting a subset of training examples, namely, sample, knn, and crossval. 1 sample ( train : Examples , k : int ) 2 -&gt; Examples 3 4 knn ( train : Examples , cast : Example -&gt; str ) 5 -&gt; fn ( example : Example , k : int ) # currying 6 -&gt; Examples 7 8 crossval ( train : Examples , n : int , k : int )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For example, given |train| = 100 training examples, crossval would select n subsets of k = 5 examples each, and return the set with which a transformation evaluate performs best on the remaining 95 examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>c = generate ( hop_template , n =10) ( x ) passages = fused_retrieval ( c . queries , k =5) summary = c . summaries [0] # highest -scoring summary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>with ( answer = majority ( candidates ) . answer )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>( pipeline , n =5 , t =0.7) ( x ) 11 return x . with ( answer = majority ( candidates ) . answer )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>illustrates how a deliberate program achieves this. Instead of asking the LM to answer this complex question, the program's SEARCH stage uses the LM to generate a query "Which castle did David Gregory inherit?" The RM retrieves a passage saying Gregory inherited the Kinnairdy Castle. After a second search "hop" finds the castle's number of storeys, the PREDICT stage queries the LM with these passages to answer the original question.</figDesc><table /><note><p>Although this program implements behaviors such as query generation, it requires no hand-labeled examples of these intermediate transformations (i.e., of the queries and passages of both retrieval hops). Instead, the DEMONSTRATE 1 We will release DSP as open source at https://github. com/stanford-futuredata/ColBERT</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>For conversational QA, we use a simple PREDICT which generates a response with greedy decoding, conditioned on all of the previous turns of the conversation and five retrieved passages. For SEARCH, our conversational QA pipeline generates n = 10 re-written queries (and also uses the simple query as the retrieve-and-read baseline; ?3.3) and fuses them as in ?2.4. We implement DEMONSTRATE similar to openqa_demonstrate, but sample only four examples (i.e., four conversational turns; instead of 16 questions as in open-domain QA) for demonstrating the task for the higherorder transformation convqa_attempt, which is passed to annotate (not shown for brevity).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">-SQuAD</cell><cell cols="2">HotPotQA</cell><cell>QReCC</cell></row><row><cell></cell><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>F1</cell><cell>nF1</cell></row><row><cell></cell><cell>Vanilla LM</cell><cell>16.2</cell><cell>25.6</cell><cell>28.3</cell><cell>36.4</cell><cell>29.8</cell><cell>18.4</cell></row><row><cell></cell><cell>No-retrieval LM SoTA</cell><cell>20.2  ?</cell><cell>-</cell><cell cols="2">33.8  ? 44.6  ?</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Retrieve-then-Read</cell><cell>33.8</cell><cell>46.1</cell><cell>36.9</cell><cell>46.1</cell><cell>31.6</cell><cell>22.2</cell></row><row><cell></cell><cell>Self-ask w/ ColBERTv2 Search</cell><cell>9.3</cell><cell>17.2</cell><cell>25.2</cell><cell>33.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>+ Refined Prompt</cell><cell>9.0</cell><cell>15.7</cell><cell>28.6</cell><cell>37.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Retrieval-augmented LM SoTA</cell><cell>34.0  ?</cell><cell>-</cell><cell>35.1  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Task-aware DSP Program</cell><cell>36.6</cell><cell>49.0</cell><cell>51.4</cell><cell>62.9</cell><cell>35.0</cell><cell>25.3</cell></row><row><cell cols="2">1 def convqa_attempt ( d ) :</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="3">x. demos = all_but ( demos , x ) # all examples that</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">don t intersect with the conversation of x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>x = convqa_search (x , k =2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>if max ( precision ( x . answer , p ) for p in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">x. passages ) &lt; .8: return None # skip examples</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>where search fails</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>x = convqa_predict (x , n =20)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>if max ( F1 (c. pred , x . answer ) for c in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">x. candidates ) &lt; .75: return None # skip</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">examples where predict fails out of n =20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>attempts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell>return x. without ( demos )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Stanford University. Correspondence to: Omar Khattab &lt;okhattab@cs.stanford.edu&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Though most of the functionality in this section is implemented, the primitives branch, knn, and crossval are currently work-in-progress.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In particular: (i) use ColBERTv2-style passages in the handcrafted demonstrations of self-ask (i.e., instead of the original Google-style snippets), (ii) concatenate 16-shot training examples from the task (i.e., question-answer pairs) as a prefix of the prompt, (iii) ask the model to generate a short intermediate answer per retrieval step, and (iv) explicitly ask the model to generate a followup "search query" at each step. We found the final item to be important because self-ask's default prompt often produces followup questions that are not self-contained (e.g., "what is the name of the national park?", which is not an informative search query). We also fix the casing in the prompt to be consistent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We remove conversations that have one or more empty groundtruth answers and conversations that have only one or two questions. We also find many conversations that include "what other interesting facts are in this article?", which conflict with the opendomain formulation and have no well-defined answer. Hence, we remove any conversation that includes the keywords "other interesting" or "else", which we found to be markers of low quality.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Ashwin Paranjape</rs>, <rs type="person">Amir Ziai</rs>, and <rs type="person">Rick Battle</rs> for valuable discussions and feedback. This work was partially supported by <rs type="funder">IBM</rs> as a founding member of the <rs type="institution">Stanford Institute for Human-Centered Artificial Intelligence (HAI)</rs>. This research was supported in part by affiliate members and other supporters of the <rs type="institution">Stanford DAWN project-Ant Financial, Facebook, Google</rs>, and VMware-as well as Cisco, SAP, and the <rs type="funder">NSF</rs> under <rs type="funder">CAREER</rs> grant <rs type="grantNumber">CNS-1651570</rs>. Any opinions, findings, and conclusions or recommenda-</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_d73Cgh6">
					<idno type="grant-number">CNS-1651570</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zhong, V., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa:</p><p>A benchmark for robust, multi-evidence, multi-answer question answering. arXiv preprint arXiv:2210.14353, 2022.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Open-domain question answering goes conversational via question rewriting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chappidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04898</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer open-domain questions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
		<ptr target="https://aclanthology.org/P17-1171" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Question rewriting for open-domain conversational qa: Best practices and limitations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Del Tredici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Gispert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2974" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10342</idno>
		<title level="m">Language model cascades</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combination of multiple searches</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST special publication SP</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08726</idno>
		<ptr target="https://gist.github.com" />
		<title level="m">be12ae02cfad4aa430d77dc940cb tributed text generation via post-hoc research and revision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
	<note>Transactions of the</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14290</idno>
		<title level="m">Fidlight: Efficient and effective retrieval-augmented text generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large language models can self-improve</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11610</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HoVer: A dataset for many-hop fact extraction and claim verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.309</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.309" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3441" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Colbert ; Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">July 25-30, 2020. 2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>SIGIR 2020, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust Multi-Hop Reasoning at Scale via Condensed Retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Baleen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relevance-guided supervision for openqa with ColBERT</title>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decomposed prompting: A modular approach for solving complex tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02406</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving text generation with large ranking models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><surname>Rankgen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09726</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fusion in information retrieval: Sigir 2018 half-day tutorial</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1383" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://aclanthology.org/Q19-1026" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Internet-augmented language models through few-shot prompting for open-domain question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grigorev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05115</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Few-shot anaphora resolution in scientific protocols via mixtures of in-context experts</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03690</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
		<ptr target="https://aclanthology.org/P19-1612" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Standing on the shoulders of giant frozen language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeldes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10019</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S H</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>b493230205f780e1bc26945df7481e5-Abstract. html</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15097</idno>
		<title level="m">Contrastive decoding: Open-ended text generation as optimization</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What makes good in-context examples for gpt-3?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<ptr target="https://arxiv.org/abs/1806.08730" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02916</idno>
		<title level="m">Multi-hop reading comprehension through question decomposition and rescoring</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hindsight: Posterior-guided Training of Retrievers for Improved Open-ended Generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Vr_BTpw3wz" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11054" to="11070" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03350</idno>
		<title level="m">Measuring and narrowing the compositionality gap in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12527</idno>
		<ptr target="https://arxiv.org/abs/2010.12527" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://aclanthology.org/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Question rewriting? assessing its importance for conversational question answering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coheur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">PLAID: An Efficient Engine for Late Interaction Retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09707</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ColBERTv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.272</idno>
		<ptr target="https://aclanthology.org/2022.naacl-main.272" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="page" from="3715" to="3734" />
		</imprint>
	</monogr>
	<note>b. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07567</idno>
		<title level="m">Retrieval augmentation reduces hallucination in conversation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09150</idno>
		<title level="m">Prompting gpt-3 to be reliable</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01296</idno>
		<title level="m">Recitation-augmented language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
		<ptr target="https://aclanthology.org/N18-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SCAI-QReCC shared task on conversational question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fr?be</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.525" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Colbert-prf: Semantic pseudo-relevance feedback for dense passage and document retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
	<note>Transactions on the Web, 2022a</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00747</idno>
		<title level="m">Rationale-augmented ensembles in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On decoding strategies for neural text generators</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15721</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions with multihop dense retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12756</idno>
		<ptr target="https://arxiv.org/abs/2009.12756" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling reformulation using query distributions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Hotpotqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>React</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">Synergizing reasoning and acting in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><surname>Star</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14465</idno>
		<title level="m">Bootstrapping reasoning with reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03493</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
