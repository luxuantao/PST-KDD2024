<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis on the Dropout Effect in Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sungheon</forename><surname>Park</surname></persName>
							<email>sungheonpark@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Convergence Science and Technology</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Convergence Science and Technology</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis on the Dropout Effect in Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FB528EF56D10287C85134AF87722C28</idno>
					<idno type="DOI">10.1007/978-3-319-54184-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regularizing neural networks is an important task to reduce overfitting. Dropout <ref type="bibr" target="#b0">[1]</ref> has been a widely-used regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization effect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the effect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image filters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout <ref type="bibr" target="#b1">[2]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) have been widely used for many computer vision tasks such as image classification, segmentation, and detection in recent years, mainly due to their high representation power and superior performance. Since deep neural networks are involved with a large number of parameters, regularization is a critical task to reduce overfitting. Other than a weight decay term, many algorithms have been presented to regularize neural networks. Dropout <ref type="bibr" target="#b0">[1]</ref> is the most commonly used technique for regularization. For CNNs, stochastic pooling <ref type="bibr" target="#b2">[3]</ref> or maxout networks <ref type="bibr" target="#b3">[4]</ref> are well known techniques to regularize convolutional layers. Though dropout has shown its effectiveness in convolutional layers in some cases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, it is still rarely used with convolutional layers in practice. Moreover, the effect of dropout in convolutional layers has not been studied thoroughly. Different from the fully connected layers, convolutional layers have smaller number of parameters compared to the size of feature maps. Hence, it is believed that convolutional layers suffer less from overfitting.</p><p>In this paper, we analyze the effect of dropout in convolutional layers. We found that dropout in convolutional layers as well as the fully connected layers are effective for regularization. The generalization effect of dropout in convolutional layers is due to the enhanced robustness by adding noise to the inputs of convolutional layers, not due to the model averaging in the case of fully connected layers. Based on this observation, we propose two variants of dropout which is suited for convolutional layers of CNNs. Similar to dropout <ref type="bibr" target="#b0">[1]</ref>, the proposed methods turn off the activations of convolutional layers. While dropout turns off the activations randomly, the first variant, max-drop, selectively drops the activation which is the maximum value within each feature map or within the same spatial position of feature maps. Since the neurons with high activation values contain key information about the problem at hand, dropping the maximum activation probabilistically can grant generalization power to the networks. The other variant, stochastic dropout, varies the dropout probability based on the probability distribution which makes the network robust to inputs with different levels of noise. Experimental results show that the proposed method effectively regularizes convolutional layers and shows competitive performance against dropout. This result indicates that unlike dropout, only dropping a small portion of activations in the network can lead to a powerful generalization performance.</p><p>The rest of the papers will be presented as follows. Related works are introduced in Sect. 2, and we analyze the effect of dropout in convolutional layers in Sect. <ref type="bibr" target="#b2">3</ref>. Based on the analysis, two variants of dropout, max-drop and stochastic dropout, are proposed in Sects. 4 and 5 respectively. Experiments on various datasets are conducted to compare the generalization performance of proposed methods with dropout, and the results are illustrated in Sect. 6. Finally, conclusions are made in Sect. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many efforts have been made for regularizing neural networks. Dropout <ref type="bibr" target="#b0">[1]</ref> is the most popular method for network regularization. It randomly drops the pre-designed portion of activations at each iteration to regularize the network. Dropout can be viewed as cooperation of multiple models trained on different subsets of data. From similar inspiration, DropConnect <ref type="bibr" target="#b6">[7]</ref> drops the connections of the network instead of activations. It showed comparable generalization performance with dropout.</p><p>Dropout works well in practice especially with fully connected layers. However, when applied to convolutional layers in a deep CNN, the performance of dropout has been thought to be questionable. It is argued that convolutional layers does not suffer from overfitting because the number of parameters for the convolutional layers is small relative to the number of activations. Nevertheless, dropout in convolutional layer is proven to improve generalization performance in some extent by adding noise to the activations <ref type="bibr" target="#b0">[1]</ref>. Network-in-Network <ref type="bibr" target="#b7">[8]</ref> efficiently integrated dropout in convolutional layer by using 1 × 1 convolutional layer followed by dropout, which enhances both representation and generalization power. On the other hand, spatial dropout <ref type="bibr" target="#b1">[2]</ref> has been suggested to consider the correlated activations in convolutional layers. The method drops the entire feature maps rather than individual activations. Since spatially close activations in the same feature map are tend to be correlated, the paper argues that dropout does not suitably applied to volumetric feature map since it assumes independence between the activations.</p><p>Various pooling methods have been proposed to regularize CNNs. Stochastic pooling <ref type="bibr" target="#b2">[3]</ref> determines the elements to pool probabilistically based on the input activation values. Generalized pooling functions <ref type="bibr" target="#b8">[9]</ref> learn parameters to combine average and max pooling. Strided convolution <ref type="bibr" target="#b4">[5]</ref> can also be viewed as generalization of pooling operations. Wu and Gu <ref type="bibr" target="#b9">[10]</ref> proposed probabilistic weighted pooling which combines dropout in convolutional layers and max pooling together.</p><p>Adding noise in the training step or to the activation function also helps enhancing generalization power. Neelakantan et al. <ref type="bibr" target="#b10">[11]</ref> found that adding noise to the gradient during backpropagation helps deep networks converge faster and prevent overfitting. Audhkhasi et al. <ref type="bibr" target="#b11">[12]</ref> and Gulcehre et al. <ref type="bibr" target="#b12">[13]</ref> showed that adding carefully chosen noise to the activation can speed up training procedure. Maxout networks <ref type="bibr" target="#b3">[4]</ref> regularize networks by propagating only maximum activations. Huang et al. <ref type="bibr" target="#b13">[14]</ref> proposed the regularization techniques which combine maxout and dropout. Opposed to the maxout networks, our method prohibits maximum activations from forward and backward propagation.</p><p>Among the numerous regularization methods, dropout is still used in most neural networks due to its simplicity and reasonable performance. Though Srivastava et al. <ref type="bibr" target="#b0">[1]</ref> empirically proved the effectiveness of dropout in the convolutional layers, dropout is not preferable to apply every layer in a deep convolutional neural network since the scale of backpropagated error drops whenever it passes the layer with dropout, which slows down the learning speed in the lower convolutional layers. Therefore, dropout has been applied only to the fully connected networks in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Effectiveness of Dropout in Convolutional Layer</head><p>We first investigate the effect of dropout in convolutional layers of CNNs. Dropout is interpreted as bagging of different models which is trained on different subsets of data. On the other hand, it is believed that the regularization effect of dropout in convolutional layers is mainly obtained from the robustness to noisy inputs. To analyze the characteristics of dropout that actually help generalizing convolutional layers, we scrutinized the distribution of activations in a CNN trained on the CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref> with and without dropout in convolutional layers. The network model used in this section consists of 10 convolutional layers and 4 pooling layers. All convolutional layers have kernels of 3 × 3 size, and inputs to the convolutional layers are padded by 1 pixels for both sides. All pooling layers use 2 × 2 max pooling with stride of 2 except the last layer for which we used 4× 4 mean pooling. Dropout after pool4 with probability of 0.5 is applied regardless of using dropout in convolutional layers or not. The number of filters is doubled after each pooling layer, which is a similar approach to the VGGnet <ref type="bibr" target="#b15">[16]</ref>. Rectified linear unit (ReLU) is used as a activation function in all layers. Detailed configuration is illustrated in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. While the CNN that does not use dropout achieved 83.16% accuracy, when dropout is applied to the output of every convolutional layer except the last conv4 3 layer with ratio of 0.1, the network achieved 87.78% accuracy. We analyzed the reason of accuracy improvement by looking into the behavior of the activated neurons in the convolutional layers.  First, we investigated that every neuron in CNNs are working effectively, which means that the filters in CNNs do not learn redundant or useless information. One of the difficulties for training deep CNNs is that there exist dead neurons in the convolutional layer that are never activated. Using variants of ReLU activation functions such as leaky ReLU <ref type="bibr" target="#b16">[17]</ref> or parametric ReLU <ref type="bibr" target="#b17">[18]</ref> is one of the solutions to avoid dead neurons. We verify that dropout is also useful for avoiding dead neurons while the network still uses ReLU activation function. We counted the number of neurons that are not activated at the test time for each layer. The portions of never activated neurons with and without dropout are shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Large number of dead neurons are observed in most layers when dropout is not applied. On the other hand, when dropout is applied to the convolutional layers, almost all neurons are activated. Therefore, we verify that dropout in convolutional layers helps filters to learn informative features of images, which improves representation power of the network and classification performance as a consequence.</p><p>Next, as discussed in <ref type="bibr" target="#b0">[1]</ref>, we compared the sparsity of the activations. It is verified from <ref type="bibr" target="#b0">[1]</ref> that in the fully connected layer, the activations are sparser when dropout is used. To confirm that this statement also holds for the convolutional layers in both lower and higher layers, we calculated the mean activation of all neurons in each layer. The mean activation of lower convolutional layers, conv1 1 (Fig. <ref type="figure" target="#fig_1">2</ref>(a) and (b)), and that of higher convolutional layers, conv4 2 (Fig. <ref type="figure" target="#fig_1">2(c</ref>) and (d)) are shown. In the lower convolutional layers, the histogram is almost flat when dropout is not used while it is bell shape when dropout is applied. This indicates that some neurons are activated frequently or have larger activation values, and others are activated less frequently or have small activation values when dropout is not applied. Meanwhile, with dropout, every neuron has similar mean activation value, which means that every neuron is similarly activated. Since lower layers in CNN usually captures common features, it is preferable behavior that neurons have similar mean activation values. In the higher convolutional layer, we could verify the sparsity of activations with dropout. A high peak near zero value is observed, which implies that the mean activation of most neurons are concentrated at small values when dropout is applied.</p><p>Based on these observations, we conclude that dropout in convolutional layers helps filters to learn informative features. However, when dropout is applied to every convolutional layers in deep CNNs, training process can be slow since activation signals are dropped exponentially as dropout is applied repeatedly. If higher drop probability such as 0.5 is applied in convolutional layers, CNNs perform poor or cannot be trained at all. In the next sections, we propose two variants of dropout to deal with this problem while maintaining the competitive generalization power with dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Max-Drop Layer</head><p>In this section, we will explain a new regularization method named as max-drop. Based on the information from Sect. 3, we note that neurons with high activation contain important information in the network. Max-drop layer selectively drops only the maximum activations. While dropout is motivated by model averaging, max-drop layers originate from different inspiration from CNNs. Different images of the same class often do not share the same features due to the occlusion, viewpoint changes, illumination variation, and so on. For instance, human face images may contain one eye or two eyes depending on the viewpoint. Therefore, a feature which plays an important role in an image may not appear in different images of the same class. Max-drop aims to simulate these cases by dropping high activations deterministically, rather than randomly select activations to drop off. In the lower layer of convolutional layers, this procedure of dropping the maximum activations simulates the case that important features are not present due to occlusion or other types of variations. In the higher layer, each feature map learns more abstracted and class-specific information <ref type="bibr" target="#b18">[19]</ref>. Therefore, turning off high activations helps other neurons to learn the class-specific characteristics. It is intuitively uncertain that dropped high activations on the higher convolutional layers give generalization power, but we empirically prove that max-drop layers effectively regularize higher convolutional layers similar to dropout. In the max-drop layer, maximum element is found in the activations of convolutional layers and the maximum activation is dropped to zero with the probability of p of f . Max-drop can be applied to both outputs of convolutional layers or pooling layers as in the case of dropout. We propose two different strategies to find maximum value which is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. The first strategy is to find maximum within each feature map, which will be called as feature-wise max-drop. This scheme turns off the most informative feature within the feature map and drop the value to 0 with the probability p of f . The portion of dropped activations with respect to the entire activations of the convolutional layer is calculated as</p><formula xml:id="formula_0">p f = 1 n w × n f p of f ,<label>(1)</label></formula><p>where n w , n h is the width and height of the feature map respectively. For instance, if convolutional layer outputs 4 × 4 size feature map, then the maximum probability of drop will be 0.0625, which is smaller than the typical dropout ratio.</p><p>Another strategy is to find maximum across the channels in the same position of feature map, which is denoted as channel-wise max-drop. This scheme prevents the highest activation to be propagated to the next layer on a certain spatial position of the feature map. The actual drop probability for the channel-wise max-drop is</p><formula xml:id="formula_1">p c = 1 n c p of f , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where n c is the number of channels in the convolutional layer outputs. With the same drop rate, channel-wise max-drop will drop smaller number of activations than the feature-wise max-drop in higher layers where the size of feature map is much smaller than the number of channels, and vice versa in the lower layers.</p><p>Dropping small number of neurons has an advantage over conventional dropout. Max-drop does not suffer from slow training since gradients are propagated through all activations except the maximum activations that are selected to turn off. Empirically, when max-drop is applied to every convolutional layer, test error decreases faster in the early stage of training than the case when dropout is applied. Moreover, with the same learning rate, the network with max-drop can be trained when high p of f (larger than 0.5) is used while the network with dropout usually failed to be trained when the drop probability exceeds 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stochastic Dropout</head><p>If we interpret the effect of dropout as gaining robustness by putting in noisy inputs, giving different degrees of noise might be helpful. Also, it is hard to determine an appropriate drop rate for the convolutional layers in most cases. If dropout ratio is determined differently for every iteration, we believe that CNN can be learned to handle different amount of information. Based on this idea, we propose a stochastic dropout in which dropout ratio is determined from probability distribution. In stochastic dropout, probability of dropping neurons is drawn from the uniform distribution or normal distribution, i.e.,</p><formula xml:id="formula_3">p of f ∼ N (μ, σ) or p of f ∼ U (a, b)<label>(3)</label></formula><p>where N (μ, σ) is the normal distribution with mean μ and standard deviation σ, and U (a, b) is the uniform distribution whose range is [a, b]. For our implementation, if μ = 0 for normal distribution, we used the absolute value of drawn probability as p of f . When μ is non-zero, we set p of f = 0 if negative number is drawn.</p><p>We implemented max-drop and stochastic dropout using Caffe framework <ref type="bibr" target="#b19">[20]</ref>. Like the dropout implementation, the activations scale up by inverse of drop probability when max-drop or stochastic dropout is applied. <ref type="foot" target="#foot_0">1</ref> Note that the scale factor is almost 1 for max-drop since the actual drop probability is near 0. We also found that the performance is almost the same for max-drop regardless of scale factor multiplication. GPU implementation of max-drop showed similar computation time for one iteration of backpropagation with dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We examined the regularization performance of various algorithms using MNIST <ref type="bibr" target="#b20">[21]</ref>, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b14">[15]</ref>, and the street view house numbers (SVHN) <ref type="bibr" target="#b21">[22]</ref> dataset. Max-drop and stochastic dropout are compared to dropout <ref type="bibr" target="#b0">[1]</ref> and spatial dropout <ref type="bibr" target="#b1">[2]</ref> to validate the generalization performance of the proposed methods against the conventional algorithms. To verify the regularization effect on the recently proposed very deep neural networks, we also conducted an experiment with ResNet <ref type="bibr" target="#b22">[23]</ref> on CIFAR-10 dataset.</p><p>The baseline model structure of the CNN is the same as the model described in Fig. <ref type="figure" target="#fig_0">1</ref>(a) except the MNIST dataset in Sect. 6.1 and ResNet <ref type="bibr" target="#b22">[23]</ref> experiment in Sect. 6.2. For the MNIST dataset, the number of channels for the network is reduced from 64, 128, 256, 512 to 64, 96, 128, 256. Also, pool3 layer has 3 × 3 kernels with a stride of 2, and pool4 has 3 × 3 kernels to deal with the 28 × 28 input size. For ResNet experiment, we used the same 32-layer model suggested in <ref type="bibr" target="#b22">[23]</ref> except that the number of feature maps in every convolutional layer is doubled. Mean substraction is the only preprocessing for the whole experiments.</p><p>For fair comparison, we searched the best parameter (e.g. drop probability) for each method. To ease the parameter tuning process, we applied the regularization algorithms for every convolutional layer with the same parameters. For all models in the experiments, dropout with probability of 0.5 is applied after pool4 and before the softmax. Dropout, spatial dropout, max-drop, or stochastic dropout is applied after every convolution layers except the last conv4 3 layer. When batch normalization <ref type="bibr" target="#b23">[24]</ref> is applied, dropout after pool4 is removed and the regularization methods are applied after conv4 3. Since drop probability of max-drop has large values, we searched the parameter for max-drop with the interval of 0.1, ranging from 0.1 to 0.9, and we used the interval of 0.05 for dropout and spatial dropout, ranging from 0.05 to 0.5. We reported the selected parameter together with the regularization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MNIST Dataset</head><p>As a sanity check, we experimented the proposed max-drop and stochastic dropout on the MNIST dataset. MNIST has 60,000 training images and 10,000 test images with 28 × 28 size. We trained CNNs for 60 epochs with the initial learning rate of 0.01 and the batch size of 128. The learning rate is decreased by 0.1 for every 20 epochs. Since MNIST classification is an easy task, and the performance is highly saturated, we conducted training 5 times for each model. We reported the average classification error for each method with the standard deviation as well as the classification of the ensemble classification error by averaging the predictions of 5 models. The results are shown in Table <ref type="table" target="#tab_1">1</ref>. It is shown that all regularization methods significantly improve the performance of the baseline. Dropout has higher improvement on classification accuracy than max-drop. For average performance, stochastic dropout also showed the best performance.</p><p>We also analyzed the effect of regularization methods with small amount of training data. We randomly select 20% of the training images from the MNIST dataset and trained with the small dataset. The performance is illustrated in Table <ref type="table" target="#tab_2">2</ref>, which shows similar tendency to Table <ref type="table" target="#tab_1">1</ref>. In general, regularization methods reduced the classification error by 20 ∼ 30%. Also, standard deviation has smaller values when regularization methods are applied, which means that regularization in convolutional layers provides stable results. Dropout shows superior performance to max-drop in MNIST dataset. Though stochastic dropout works slightly better than dropout with fixed probability, it seems that giving different levels of noise does not take much advantage against dropout. Spatial dropout showed inferior performance, which indicates that independence between feature map does not play an important role in regularization of convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CIFAR-10 and CIFAR-100 Dataset</head><p>CIFAR-10 and CIFAR-100 datasets are image classification dataset which consist of 10 and 100 classes respectively. Each dataset has 50,000 training images and 10,000 test images with 32 × 32 size. For the CIFAR datasets, we reported the classification error of a single model for each method. To ensure convergence of models, we trained CNNs for 250 epochs with the initial learning rate of 0.02 and the batch size of 128. The learning rate is decreased by 0.5 for every 25 epochs.</p><p>The classification accuracy is illustrated in Table <ref type="table" target="#tab_3">3</ref>. In CIFAR-10, channelwise max-drop showed better result than dropout. Note that despite the high drop probability, the actual drop probability of channel-wise max-drop is very small, about 0.01 for the first convolutional layer and about 0.001 for the last convolutional layer. The result verifies that dropping only high activations results in similar regularization effect to random drop. Also, unlike MNIST experiment, stochastic dropout of normal distribution with zero mean showed best performance. One possible interpretation is that giving different levels of noise to the input of the convolutional layers makes the layers robust to intra-class variations, thus obtaining enhanced generalization power. Next, we compared the progress of training for baseline, dropout, and channel-wise max-drop models. The losses on the training set and the test set, and the accuracies on the test set is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. The training losses are plotted in log-scale (Fig. <ref type="figure" target="#fig_3">4(a)</ref>). It is shown that the training loss of dropout and max-drop fluctuates heavily compared to the baseline model since each layer in those models takes noisy inputs. Dropout has larger variations than max-drop since the number of dropped activations is larger. These fluctuations does not affect the test loss or accuracy, as shown in Fig. <ref type="figure" target="#fig_3">4(b)</ref>. It is interesting that the test loss of max-drop is even higher than the baseline model while maintaining similar accuracy with dropout. Since max-drop drops the highest activation which contains important information, the model is learned to classify an image with less informative feature. This will increase the uncertainty of the prediction, which leads to high softmax loss values. We also analyzed the activation behavior of max-drop following the analysis of dropout in Sect. 3. The number of neurons that are never activated at the test time are counted for each regularization method and reported in Fig. <ref type="figure">5</ref>. Similar to dropout, all regularization methods have little number of never activated neurons for all layers. Thus, it is verified that max-drop helps neurons to learn discriminative features as in the case of dropout.</p><p>The histogram of mean activation in the lower and higher convolutional layers for max-drop models are shown in Fig. <ref type="figure">6</ref>. As observed in Fig. <ref type="figure">6</ref>(a) and (b), the histogram is bell-shaped in the lower convolutional layer like dropout, which indicates that max-drop also make neurons evenly activated. Meanwhile, for the higher convolutional layer, number of neurons that has mean activation near zero is small unlike either dropout or no regularization case. Max-drop pushes neurons to have similar mean activations, but it does not prefer sparse activations.</p><p>To investigate the usefulness of the regularization methods in the specific layers, we trained the model by applying dropout and max-drop only to the lower layers (conv1 1 and conv1 2) and only to the higher layers (conv4 1 and conv4 2). The classification errors for both cases are shown in Table <ref type="table" target="#tab_4">4</ref>. Regularization methods improves the network in both lower and higher layers, but the regularization effect is more powerful in the higher layers. We found that high drop ratio is preferable for higher layers, while low drop ratio showed better performance in lower layers. Feature-wise max-drop in higher layers and channelwise max-drop in lower layers showed better regularization performance. Spatial dropout also proved its effectiveness in higher layers. Next, we combined the regularization methods with other methods that improves generalization performance. Batch normalization <ref type="bibr" target="#b23">[24]</ref> improves training speed and the performance of network by normalizing the activations of each layer in neural networks. We applied the regularization methods after batch normalization is performed. Data augmentation is also a simple way to grant gen-eralization power to neural networks. Following the previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref>, we applied data augmentation to training data by padding 4 pixels on all sides of images and by flipping images horizontally. The classification errors are shown in Table <ref type="table" target="#tab_5">5</ref>. With batch normalization, dropout showed the best performance. After activations are normalized, it seems that the importance of maximum value is decreased, which leads to the poor generalization performance of max-drop compared to dropout or spatial dropout. With data augmentation, spatial dropout showed the smallest error, but the improvement of all regularization methods from the baseline is very small. The result indicates that data augmentation imposes generalization power to the CNNs which make the regularization methods less effective. Recently, deep residual learning <ref type="bibr" target="#b22">[23]</ref> enabled training of very deep networks. To investigate the regularization performance in the very deep CNNs, we trained 32-layer ResNet on CIFAR-10 dataset. We followed the training procedure and hyper parameters selection from <ref type="bibr" target="#b22">[23]</ref> without data augmentation. The result is illustrated in Table <ref type="table" target="#tab_6">6</ref>. All of the tested methods showed superior performance over the baseline with a margin of 2 ∼ 4% except spatial dropout. This indicates that dropout and max-drop is still effective for regularizing very deep networks. Lastly, we evaluated the regularization methods on CIFAR-100 dataset. The dataset has much less training samples for each class than CIFAR-10. The classification errors without data augmentation are shown in Table <ref type="table" target="#tab_7">7</ref>. Regularization effect is much stronger than CIFAR-10 mainly due to the small amount of training samples, which reduced the classification error up to 15%. Without batch normalization, max-drop methods outperforms dropout. When batch normalization is used, dropout shows more improvements.  <ref type="table" target="#tab_8">8</ref>. Huge number of training samples weakens the effect of regularization. Without batch normalization, max-drop methods showed a small improvement, while dropout and spatial dropout worsen the performance of the network. Dropout showed the best performance when batch normalization is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have investigated and verified the usefulness of dropout-like methods in convolutional layers. Usage of dropout in convolutional layers is justified by looking into the activation behavior of neurons. Regularization effect in the convolutional layers is strong when training samples are small and when data augmentation is not used. Also, newly-proposed max-drop and stochastic dropout methods showed competitive results to the conventional dropout, which implies that these methods can substitute dropout in convolutional layers of CNNs. Max-drop layer can be generalized such as dropping largest k activations or suppress the activations by multiplying constant value instead of dropping them to zero. We expect that carefully adjusted parameters increase the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Structure of CNN used in the experiments. (b) Number of neurons that never activated in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Histogram of mean activation of (a) conv1 1 layer without dropout. (b) conv1 1 layer with dropout. (c) conv4 2 layer without dropout. (d) conv4 2 layer with dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of max-drop layer. Two different ways to find maximum value is proposed in this paper: (a) feature-wise max-drop finds maximum value within each feature map and drops the maximum values with probability p of f . (b) Channel-wise max-drop finds maximum value across each channel in the same spatial position and drops the maximum values with probability p of f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Training error of baseline, dropout (p = 0.1), and channel-wise max-drop (p = 0.7). (b) Test error and test set accuracy of the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Number of never activated neurons in the models with dropout, max-drop, and spatial dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification error on MNIST</figDesc><table><row><cell>Method</cell><cell cols="2">Classification error (%)</cell></row><row><cell></cell><cell cols="2">Average of 5 models Ensemble of 5 models</cell></row><row><cell>Baseline (without dropout)</cell><cell>0.604 ± 0.0829</cell><cell>0.57</cell></row><row><cell>Dropout (p = 0.2)</cell><cell>0.430 ± 0.0212</cell><cell>0.38</cell></row><row><cell>Spatial dropout (p = 0.1)</cell><cell>0.504 ± 0.0493</cell><cell>0.42</cell></row><row><cell cols="2">Feature-wise max-drop (p = 0.2) 0.488 ± 0.0657</cell><cell>0.42</cell></row><row><cell cols="2">Channel-wise max-drop (p = 0.5) 0.502 ± 0.0148</cell><cell>0.40</cell></row><row><cell cols="2">Stochastic dropout (N (0.2, 0.05)) 0.410 ± 0.0122</cell><cell>0.38</cell></row><row><cell cols="2">Stochastic dropout (U (0.1, 0.3)) 0.448 ± 0.0363</cell><cell>0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification error on MNIST with 20% of training data.</figDesc><table><row><cell>Method</cell><cell cols="2">Classification error (%)</cell></row><row><cell></cell><cell cols="2">Average of 5 models Ensemble of 5 models</cell></row><row><cell>Baseline</cell><cell>1.126 ± 0.0802</cell><cell>0.92</cell></row><row><cell>Dropout (p = 0.2)</cell><cell>0.808 ± 0.0740</cell><cell>0.76</cell></row><row><cell>Spatial dropout (p = 0.1)</cell><cell>0.872 ± 0.0335</cell><cell>0.78</cell></row><row><cell cols="2">Feature-wise max-drop (p = 0.4) 0.882 ± 0.0676</cell><cell>0.83</cell></row><row><cell cols="2">Channel-wise max-drop (p = 0.5) 0.888 ± 0.0638</cell><cell>0.79</cell></row><row><cell cols="2">Stochastic dropout (N (0.2, 0.05)) 0.810 ± 0.0534</cell><cell>0.80</cell></row><row><cell cols="2">Stochastic dropout (U (0.1, 0.3)) 0.802 ± 0.0444</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification error on CIFAR-10 dataset.</figDesc><table><row><cell>Method</cell><cell>Classification error (%)</cell></row><row><cell>Baseline</cell><cell>16.84%</cell></row><row><cell>Dropout (p = 0.1)</cell><cell>12.22%</cell></row><row><cell>Spatial dropout (p = 0.05)</cell><cell>13.78%</cell></row><row><cell cols="2">Feature-wise max-drop (p = 0.2) 12.55%</cell></row><row><cell cols="2">Channel-wise max-drop (p = 0.7) 12.00%</cell></row><row><cell cols="2">Stochastic dropout (N (0.0, 0.2)) 11.79%</cell></row><row><cell cols="2">Stochastic dropout (U (0.0, 0.4)) 12.86%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effect of regularization in lower and higher convolutional layers.</figDesc><table><row><cell>Method</cell><cell cols="2">conv1 regularization</cell><cell cols="2">conv4 regularization</cell></row><row><cell></cell><cell cols="2">Parameter Classification</cell><cell cols="2">Parameter Classification</cell></row><row><cell></cell><cell></cell><cell>err. (%)</cell><cell></cell><cell>err. (%)</cell></row><row><cell>Dropout</cell><cell>p = 0.05</cell><cell>15.69</cell><cell>p = 0.3</cell><cell>15.14</cell></row><row><cell>Spatial dropout</cell><cell>p = 0.05</cell><cell>16.16</cell><cell>p = 0.25</cell><cell>14.48</cell></row><row><cell cols="2">Feature-wise max-drop p = 0.1</cell><cell>15.93</cell><cell>p = 0.7</cell><cell>15.06</cell></row><row><cell cols="2">Channel-wise max-drop p = 0.1</cell><cell>15.02</cell><cell>p = 0.4</cell><cell>15.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of regularization when combined with batch normalization and data augmentation.</figDesc><table><row><cell>Method</cell><cell cols="4">With batch normalization With data augmentation</cell></row><row><cell></cell><cell cols="2">Parameter Classification</cell><cell cols="2">Parameter Classification</cell></row><row><cell></cell><cell></cell><cell>err. (%)</cell><cell></cell><cell>err. (%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>12.10</cell><cell>-</cell><cell>8.01</cell></row><row><cell>Dropout</cell><cell>p = 0.1</cell><cell>9.85</cell><cell>p = 0.05</cell><cell>8.54</cell></row><row><cell>Spatial dropout</cell><cell>p = 0.15</cell><cell>10.69</cell><cell>p = 0.05</cell><cell>7.17</cell></row><row><cell cols="2">Feature-wise max-drop p = 0.2</cell><cell>10.67</cell><cell>p = 0.2</cell><cell>7.73</cell></row><row><cell cols="2">Channel-wise max-drop p = 0.2</cell><cell>11.15</cell><cell>p = 0.4</cell><cell>7.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Classification error on CIFAR-10 dataset using ResNet-32.</figDesc><table><row><cell>Method</cell><cell>Classification error (%)</cell></row><row><cell>Baseline</cell><cell>12.84%</cell></row><row><cell>Dropout (p = 0.1)</cell><cell>9.14%</cell></row><row><cell>Spatial dropout (p = 0.1)</cell><cell>16.33%</cell></row></table><note><p>Feature-wise max-drop (p = 0.2) 10.72% Channel-wise max-drop (p = 0.1) 11.15%</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Classification errors on CIFAR-100 dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">W/O batch normalization W/batch normalization</cell></row><row><cell></cell><cell cols="2">Parameter Classification</cell><cell cols="2">Parameter Classification</cell></row><row><cell></cell><cell></cell><cell>err. (%)</cell><cell></cell><cell>err. (%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>50.26</cell><cell>-</cell><cell>38.84</cell></row><row><cell>Dropout</cell><cell>p = 0.3</cell><cell>37.23</cell><cell>p = 0.15</cell><cell>32.46</cell></row><row><cell>Spatial dropout</cell><cell>p = 0.15</cell><cell>42.07</cell><cell>p = 0.1</cell><cell>35.28</cell></row><row><cell cols="2">Feature-wise max-drop p = 0.4</cell><cell>36.22</cell><cell>p = 0.2</cell><cell>34.27</cell></row><row><cell cols="2">Channel-wise max-drop p = 0.7</cell><cell>35.33</cell><cell>p = 0.3</cell><cell>34.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Classification errors on SVHN dataset.SVHN dataset contains much more training samples than the previous datasets. The dataset consists of over 600,000 training images and 26,032 test images. We trained CNN for 15 epochs with the initial learning rate of 0.01 and the batch size of 128 for the experiments on SVHN dataset. The learning rate is decreased by 0.1 for every 5 epochs. Data augmentation is not applied. The classification errors are reported in Table</figDesc><table><row><cell>Method</cell><cell cols="4">W/O batch normalization W/batch normalization</cell></row><row><cell></cell><cell cols="2">Parameter Classification</cell><cell cols="2">Parameter Classification</cell></row><row><cell></cell><cell></cell><cell>err. (%)</cell><cell></cell><cell>err. (%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>2.46</cell><cell>-</cell><cell>2.34</cell></row><row><cell>Dropout</cell><cell>p = 0.25</cell><cell>2.46</cell><cell>p = 0.1</cell><cell>2.02</cell></row><row><cell>Spatial dropout</cell><cell>p = 0.05</cell><cell>2.58</cell><cell>p = 0.15</cell><cell>2.07</cell></row><row><cell cols="2">Feature-wise max-drop p = 0.4</cell><cell>2.29</cell><cell>p = 0.2</cell><cell>2.14</cell></row><row><cell cols="2">Channel-wise max-drop p = 0.4</cell><cell>2.30</cell><cell>p = 0.7</cell><cell>2.28</cell></row><row><cell>6.3 SVHN Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Caffe implementation of dropout scales up the activations at training time instead of scaling down them at test time unlike the original dropout paper<ref type="bibr" target="#b0">[1]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This research was supported by Basic Research Program through the National Research Foundation of Korea (NRF-2016R1A1A1A05005442).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: the all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML 2013)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08985</idno>
		<title level="m">Generalizing pooling functions in convolutional neural networks: mixed, gated, and tree</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards dropout training for convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adding gradient noise improves learning for very deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06807</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-enhanced convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Osoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kosko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00391</idno>
		<title level="m">Noisy activation functions</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel-max, channel-drop and stochastic max-pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CAFFE: convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
