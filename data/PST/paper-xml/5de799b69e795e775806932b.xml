<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Mean-Field Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
							<email>xinguo@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anran</forename><surname>Hu</surname></persName>
							<email>anran_hu@berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renyuan</forename><surname>Xu</surname></persName>
							<email>renyuanxu@berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzi</forename><surname>Zhang</surname></persName>
							<email>junziz@stanford.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Mean-Field Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3AB44BA6249F9A132CB26BA1BA81BC8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a general mean-field game (GMFG) framework for simultaneous learning and decision-making in stochastic games with a large population. It first establishes the existence of a unique Nash Equilibrium to this GMFG, and explains that naively combining Q-learning with the fixed-point approach in classical MFGs yields unstable algorithms. It then proposes a Q-learning algorithm with Boltzmann policy (GMF-Q), with analysis of convergence property and computational complexity. The experiments on repeated Ad auction problems demonstrate that this GMF-Q algorithm is efficient and robust in terms of convergence and learning accuracy. Moreover, its performance is superior in convergence, stability, and learning ability, when compared with existing algorithms for multi-agent reinforcement learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motivating example. This paper is motivated by the following Ad auction problem for an advertiser. An Ad auction is a stochastic game on an Ad exchange platform among a large number of players, the advertisers. In between the time a web user requests a page and the time the page is displayed, usually within a millisecond, a Vickrey-type of second-best-price auction is run to incentivize interested advertisers to bid for an Ad slot to display advertisement. Each advertiser has limited information before each bid: first, her own valuation for a slot depends on an unknown conversion of clicks for the item; secondly, she, should she win the bid, only knows the reward after the user's activities on the website are finished. In addition, she has a budget constraint in this repeated auction.</p><p>The question is, how should she bid in this online sequential repeated game when there is a large population of bidders competing on the Ad platform, with unknown distributions of the conversion of clicks and rewards? Besides the Ad auction, there are many real-world problems involving a large number of players and unknown systems. Examples include massive multi-player online role-playing games <ref type="bibr" target="#b18">[19]</ref>, high frequency tradings <ref type="bibr" target="#b23">[24]</ref>, and the sharing economy <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our work. Motivated by these problems, we consider a general framework of simultaneous learning and decision-making in stochastic games with a large population. We formulate a general mean-fieldgame (GMFG) with incorporation of action distributions, (randomized) relaxed policies, and with unknown rewards and dynamics. This general framework can also be viewed as a generalized version of MFGs of McKean-Vlasov type <ref type="bibr" target="#b0">[1]</ref>, which is a different paradigm from the classical MFG. It is also beyond the scope of the existing Q-learning framework for Markov decision problem (MDP) with unknown distributions, as MDP is technically equivalent to a single player stochastic game.</p><p>On the theory front, this general framework differs from all existing MFGs. We establish under appropriate technical conditions, the existence and uniqueness of the Nash equilibrium (NE) to this GMFG. On the computational front, we show that naively combining Q-learning with the three-step fixed-point approach in classical MFGs yields unstable algorithms. We then propose a Q-learning algorithm with Boltzmann policy (GMF-Q), establish its convergence property and analyze its computational complexity. Finally, we apply this GMF-Q algorithm to the Ad auction problem, where this GMF-Q algorithm demonstrates its efficiency and robustness in terms of convergence and learning. Moreover, its performance is superior, when compared with existing algorithms for multi-agent reinforcement learning for convergence, stability, and learning accuracy.</p><p>Related works. On learning large population games with mean-field approximations, <ref type="bibr" target="#b38">[39]</ref> focuses on inverse reinforcement learning for MFGs without decision making, <ref type="bibr" target="#b39">[40]</ref> studies an MARL problem with a first-order mean-field approximation term modeling the interaction between one player and all the other finite players, and <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b40">[41]</ref> consider model-based adaptive learning for MFGs in specific models (e.g., linear-quadratic and oscillator games). More recently, <ref type="bibr" target="#b25">[26]</ref> studies the local convergence of actor-critic algorithms on finite time horizon MFGs, and <ref type="bibr" target="#b33">[34]</ref> proposes a policy-gradient based algorithm and analyzes the so-called local NE for reinforcement learning in infinite time horizon MFGs. For learning large population games without mean-field approximation, see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> and the references therein. In the specific topic of learning auctions with a large number of advertisers, <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b19">[20]</ref> explore reinforcement learning techniques to search for social optimal solutions with real-word data, and <ref type="bibr" target="#b17">[18]</ref> uses MFGs to model the auction system with unknown conversion of clicks within a Bayesian framework.</p><p>However, none of these works consider the problem of simultaneous learning and decision-making in a general MFG framework. Neither do they establish the existence and uniqueness of the (global) NE, nor do they present model-free learning algorithms with complexity analysis and convergence to the NE. Note that in principle, global results are harder to obtain compared to local results.</p><p>2 Framework of General MFG (GMFG)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: classical N -player Markovian game and MFG</head><p>Let us first recall the classical N -player game. There are N players in a game. At each step t, the state of player i (= 1, 2, • • • , N ) is s i t ∈ S ⊆ R d and she takes an action a i t ∈ A ⊆ R p . Here d, p are positive integers, and S and A are compact (for example, finite) state space and action space, respectively. Given the current state profile of N -players s t = (s<ref type="foot" target="#foot_1">1</ref> t , . . . , s N t ) ∈ S N and the action a i t , player i will receive a reward r i (s t , a i t ) and her state will change to s i t+1 according to a transition probability function P i (s t , a i t ). A Markovian game further restricts the admissible policy/control for player i to be of the form a i t = π i t (s t ). That is, π i t : S N → P(A) maps each state profile s ∈ S N to a randomized action, with P(X ) the space of probability measures on space X . The accumulated reward (a.k.a. the value function) for player i, given the initial state profile s and the policy profile sequence π π π := {π π π t } ∞ t=0 with π π π t = (π 1 t , . . . , π N t ), is then defined as</p><formula xml:id="formula_0">V i (s, π π π) := E ∞ t=0 γ t r i (s t , a i t ) s 0 = s ,<label>(1)</label></formula><p>where γ ∈ (0, 1) is the discount factor, a i t ∼ π i t (s t ), and s i t+1 ∼ P i (s t , a i t ). The goal of each player is to maximize her value function over all admissible policy sequences.</p><p>In general, this type of stochastic N -player game is notoriously hard to analyze, especially when N is large <ref type="bibr" target="#b27">[28]</ref>. Mean field game (MFG), pioneered by <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b22">[23]</ref> in the continuous settings and later developed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> for discrete settings, provides an ingenious and tractable aggregation approach to approximate the otherwise challenging N -player stochastic games. The basic idea for an MFG goes as follows. Assume all players are identical, indistinguishable and interchangeable, when N → ∞, one can view the limit of other players' states s -i t = (s where π π π := {π t } ∞ t=0 denotes the policy sequence and µ µ µ := {µ t } ∞ t=0 the distribution flow. In this MFG setting, at time t, after the representative player chooses her action a t according to some policy π t , she will receive reward r(s t , a t , µ t ) and her state will evolve under a controlled stochastic dynamics of a mean-field type P (•|s t , a t , µ t ). Here the policy π t depends on both the current state s t and the current population state distribution µ t such that π : S × P(S) → P(A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">General MFG (GMFG)</head><p>In the classical MFG setting, the reward and the dynamic for each player are known. They depend only on s t the state of the player, a t the action of this particular player, and µ t the population state distribution. In contrast, in the motivating auction example, the reward and the dynamic are unknown; they rely on the actions of all players, as well as on s t and µ t .</p><p>We therefore define the following general MFG (GMFG) framework. At time t, after the representative player chooses her action a t according to some policy π : S × P(S) → P(A), she will receive a reward r(s t , a t , L t ) and her state will evolve according to P (•|s t , a t , L t ), where r and P are possibly unknown. The objective of the player is to solve the following control problem:</p><formula xml:id="formula_1">maximize π π π V (s, π π π, L L L) := E ∞ t=0 γ t r(s t , a t , L t )|s 0 = s subject to s t+1 ∼ P (s t , a t , L t ), a t ∼ π t (s t , µ t ). (GMFG)</formula><p>Here, L L L := {L t } ∞ t=0 , with L t = P st,at ∈ P(S × A) the joint distribution of the state and the action (i.e., the population state-action pair). L t has marginal distributions α t for the population action and µ t for the population state. Notice that {L t } ∞ t=0 could depend on time. Namely, an infinite time horizon MFG could still have time-dependent NE solution due to the mean information process (game interaction) in the MFG. This is fundamentally different from the theory of single-agent MDP where the optimal control, if exists uniquely, would be time independent in an infinite time horizon setting.</p><p>In this framework, we adopt the well-known Nash Equilibrium (NE) for analyzing stochastic games. Definition 2.1 (NE for GMFGs). In (GMFG), a player-population profile</p><formula xml:id="formula_2">(π π π , L L L ) := ({π t } ∞ t=0 , {L t } ∞ t=0 ) is called an NE if 1. (Single player side) Fix L L L , for any policy sequence π π π := {π t } ∞ t=0 and initial state s ∈ S, V (s, π π π , L L L ) ≥ V (s, π π π, L L L ) .<label>(2)</label></formula><p>2. (Population side) P st,at = L t for all t ≥ 0, where {s t , a t } ∞ t=0 is the dynamics under the policy sequence π π π starting from s 0 ∼ µ 0 , with a t ∼ π t (s t , µ t ), s t+1 ∼ P (•|s t , a t , L t ), and µ t being the population state marginal of L t .</p><p>The single player side condition captures the optimality of π π π , when the population side is fixed. The population side condition ensures the "consistency" of the solution: it guarantees that the state and action distribution flow of the single player does match the population state and action sequence L L L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Example: GMFG for the repeated auction</head><p>Now, consider the repeated Vickrey auction with a budget constraint in Section 1. Take a representative advertiser in the auction. Denote s t ∈ {0, 1, 2, • • • , s max } as the budget of this player at time t, where s max ∈ N + is the maximum budget allowed on the Ad exchange with a unit bidding price. Denote a t as the bid price submitted by this player and α t as the bidding/(action) distribution of the population. The reward for this advertiser with bid a t and budget s t is</p><formula xml:id="formula_3">r t = I w M t =1 (v t -a M t ) -(1 + ρ)I st&lt;a M t (a M t -s t ) .<label>(3)</label></formula><p>Here w M t takes values 1 and 0, with w M t = 1 meaning this player winning the bid and 0 otherwise. The probability of winning the bid would depend on M , the index for the game intensity, and α t . (See discussion on M in Appendix H.1.) The conversion of clicks at time t is v t and follows an unknown distribution. a M t is the value of the second largest bid at time t, taking values from 0 to s max , and depends on both M and L t . Should the player win the bid, the reward r t consists of two parts, corresponding to the two terms in <ref type="bibr" target="#b2">(3)</ref>. The first term is the profit of wining the auction, as the winner only needs to pay for the second best bid a M t in a Vickrey auction. The second term is the penalty of overshooting if the payment exceeds her budget, with a penalty rate ρ. At each time t, the budget dynamics s t follows,</p><formula xml:id="formula_4">s t+1 =    s t , w M t = 1, s t -a M t , w M t = 1 and a M t ≤ s t , 0, w M t = 1 and a M t &gt; s t .</formula><p>That is, if this player does not win the bid, the budget will remain the same. If she wins and has enough money to pay, her budget will decrease from s t to s t -a M t . However, if she wins but does not have enough money, her budget will be 0 after the payment and there will be a penalty in the reward function. Note that in this game, both the rewards r t and the dynamics s t are unknown a priori.</p><p>In practice, one often modifies the dynamics of s t+1 with a non-negative random budget fulfillment ∆(s t+1 ) after the auction clearing <ref type="bibr" target="#b10">[11]</ref>, such that ŝt+1 = s t+1 + ∆(s t+1 ). One may see some particular choices of ∆(s t+1 ) in the experiment section (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Solution for GMFGs</head><p>We now establish the existence and uniqueness of the NE to (GMFG), by generalizing the classical fixed-point approach for MFGs to this GMFG setting. (See <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b22">[23]</ref> for the classical case). It consists of three steps.</p><p>Step A. Fix L L L := {L t } ∞ t=0 , (GMFG) becomes the classical optimization problem. Indeed, with L L L fixed, the population state distribution sequence µ µ µ := {µ t } ∞ t=0 is also fixed, hence the space of admissible policies is reduced to the single-player case. Solving (GMFG) is now reduced to finding a policy sequence π t,L L L ∈ Π := {π | π : S → P(A)} over all admissible π π π L L L = {π t,L L L } ∞ t=0 , to maximize</p><formula xml:id="formula_5">V (s, π π π L L L , L L L) := E ∞ t=0 γ t r(s t , a t , L t )|s 0 = s , subject to s t+1 ∼ P (s t , a t , L t ), a t ∼ π t,L L L (s t ).</formula><p>Notice that with L L L fixed, one can safely suppress the dependency on µ t in the admissible policies. Moreover, given this fixed L L L sequence and the solution π π π L L L := {π t,L L L } ∞ t=0 , one can define a mapping from the fixed population distribution sequence L L L to an arbitrarily chosen optimal randomized policy sequence. That is,</p><formula xml:id="formula_6">Γ 1 : {P(S × A)} ∞ t=0 → {Π} ∞ t=0 , such that π π π L L L = Γ 1 (L L L).</formula><p>Note that this π π π L L L sequence satisfies the single player side condition in Definition 2.1 for the population state-action pair sequence L L L. That is, V (s, π π π L L L , L L L) ≥ V (s, π π π, L L L) , for any policy sequence π π π = {π t } ∞ t=0 and any initial state s ∈ S. As in the MFG literature <ref type="bibr" target="#b16">[17]</ref>, a feedback regularity condition is needed for analyzing Step A.</p><p>Assumption 1. There exists a constant d 1 ≥ 0, such that for any</p><formula xml:id="formula_7">L L L, L L L ∈ {P(S × A)} ∞ t=0 , D(Γ 1 (L L L), Γ 1 (L L L )) ≤ d 1 W 1 (L L L, L L L ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">D(π π π, π π π ) := sup s∈S W 1 (π π π(s), π π π (s)) = sup s∈S sup t∈N W 1 (π t (s), π t (s)), W 1 (L L L, L L L ) := sup t∈N W 1 (L t , L t ),<label>(5)</label></formula><p>and W 1 is the 1 -Wasserstein distance between probability measures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Step B. Based on the analysis in Step A and π π π L L L = {π t,L L L } ∞ t=0 , update the initial sequence L L L to L L L following the controlled dynamics P (•|s t , a t , L t ).</p><p>Accordingly, for any admissible policy sequence π π π ∈ {Π} ∞ t=0 and a joint population state-action pair sequence L L L ∈ {P(S ×A)} ∞ t=0 , define a mapping Γ 2 : {Π} ∞ t=0 ×{P(S ×A)} ∞ t=0 → {P(S ×A)} ∞ t=0 as follows:</p><formula xml:id="formula_9">Γ 2 (π π π, L L L) := L L L = {P st,at } ∞ t=0 ,<label>(6)</label></formula><p>where s t+1 ∼ µ t P (•|•, a t , L t ), a t ∼ π t (s t ), s 0 ∼ µ 0 , and µ t is the population state marginal of L t .</p><p>One needs a standard assumption in this step. Assumption 2. There exist constants d 2 , d 3 ≥ 0, such that for any admissible policy sequences π π π, π π π 1 , π π π 2 and joint distribution sequences</p><formula xml:id="formula_10">L L L, L L L 1 , L L L 2 , W 1 (Γ 2 (π π π 1 , L L L), Γ 2 (π π π 2 , L L L)) ≤ d 2 D(π π π 1 , π π π 2 ),<label>(7)</label></formula><formula xml:id="formula_11">W 1 (Γ 2 (π π π, L L L 1 ), Γ 2 (π π π, L L L 2 )) ≤ d 3 W 1 (L L L 1 , L L L 2 ).<label>(8)</label></formula><p>Assumption 2 can be reduced to Lipschitz continuity and boundedness of the transition dynamics P . (See the Appendix for more details.)</p><p>Step C. Repeat Step A and Step B until L L L matches L L L.</p><p>This step is to take care of the population side condition. To ensure the convergence of the combined step A and step B, it suffices if Γ :</p><formula xml:id="formula_12">{P(S × A)} ∞ t=0 → {P(S × A)} ∞ t=0 is a contractive mapping under the W 1 distance, with Γ(L L L) := Γ 2 (Γ 1 (L L L), L L L).</formula><p>Then by the Banach fixed point theorem and the completeness of the related metric spaces, there exists a unique NE to the GMFG.</p><p>In summary, we have Theorem 1 (Existence and Uniqueness of GMFG solution). Given Assumptions 1 and 2, and assuming that d 1 d 2 + d 3 &lt; 1, there exists a unique NE to (GMFG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RL Algorithms for (stationary) GMFGs</head><p>In this section, we design the computational algorithm for the GMFG. Since the reward and transition distributions are unknown, this is simultaneously learning the system and finding the NE of the game. We will focus on the case with finite state and action spaces, i.e., |S|, |A| &lt; ∞. We will look for stationary (time independent) NEs. Accordingly, we abbreviate π π π := {π} ∞ t=0 and L L L := {L} ∞ t=0 as π and L, respectively. This stationarity property enables developing appropriate time-independent Q-learning algorithm, suitable for an infinite time horizon game. Modification from the GMFG framework to this special stationary setting is straightforward, and is left to Appendix B. Note that the assumptions to guarantee the existence and uniqueness of GMFG solutions are slightly different between the stationary and non-stationary cases. For instance, one can compare ( <ref type="formula" target="#formula_10">7</ref>)-( <ref type="formula" target="#formula_11">8</ref>) with ( <ref type="formula">21</ref>)- <ref type="bibr" target="#b21">(22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The algorithm consists of two steps, parallel to</head><p>Step A and Step B in Section 3.</p><p>Step 1: Q-learning with stability for fixed L. With L fixed, it becomes a standard learning problem for an infinite horizon MDP. We will focus on the Q-learning algorithm <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>The Q-learning algorithm approximates the value iteration by stochastic approximation. At each step with the state s and an action a, the system reaches state s according to the controlled dynamics and the Q-function is updated according to</p><formula xml:id="formula_13">Q L (s, a) ← (1 -β t (s, a))Q L (s, a) + β t (s, a) [r(s, a, L) + γ max ã Q L (s , ã)] ,<label>(9)</label></formula><p>where the step size β t (s, a) can be chosen as (cf. <ref type="bibr" target="#b6">[7]</ref>)</p><formula xml:id="formula_14">β t (s, a) = |#(s, a, t) + 1| -h , (s, a) = (s t , a t ), 0, otherwise.</formula><p>with h ∈ (1/2, 1). Here #(s, a, t) is the number of times up to time t that one visits the pair (s, a). The algorithm then proceeds to choose action a based on Q L with appropriate exploration strategies, including the -greedy strategy.</p><p>After obtaining the approximate Q L , in order to retrieve an approximately optimal policy, it would be natural to define an argmax-e operator so that actions with equal maximum Q-values would have equal probabilities to be selected. Unfortunately, the discontinuity and sensitivity of argmax-e could lead to an unstable algorithm (see Figure <ref type="figure" target="#fig_1">4</ref> for the corresponding naive Algorithm 2 in Appendix). <ref type="foot" target="#foot_2">2</ref>Instead, we consider a Boltzmann policy based on the operator softmax c : R n → R n , defined as</p><formula xml:id="formula_15">softmax c (x) i = exp(cx i ) n j=1 exp(cx j ) . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>This operator is smooth and close to the argmax-e (see Lemma 7 in the Appendix). Moreover, even though Boltzmann policies are not optimal, the difference between the Boltzmann and the optimal one can always be controlled by choosing the hyper-parameter c appropriately in the softmax operator. Note that other smoothing operators (e.g., Mellowmax <ref type="bibr" target="#b1">[2]</ref>) may also be considered in the future.</p><p>Step 2: error control in updating L. Given the sub-optimality of the Boltzmann policy, one needs to characterize the difference between the optimal policy and the non-optimal ones. In particular, one can define the action gap between the best action and the second best action in terms of the Q-value as</p><formula xml:id="formula_17">δ s (L) := max a ∈A Q L (s, a ) -max a / ∈argmax a∈A Q L (s,a) Q L (s, a) &gt; 0.</formula><p>Action gap is important for approximation algorithms <ref type="bibr" target="#b2">[3]</ref>, and are closely related to the problem-dependent bounds for regret analysis in reinforcement learning and multi-armed bandits, and advantage learning algorithms including A2C <ref type="bibr" target="#b26">[27]</ref>.</p><p>The problem is: in order for the learning algorithm to converge in terms of L (Theorem 2), one needs to ensure a definite differentiation between the optimal policy and the sub-optimal ones. This is problematic as the infimum of δ s (L) over an infinite number of L can be 0. To address this, the population distribution at step k, say L k , needs to be projected to a finite grid, called -net. The relation between the -net and action gaps is as follows:</p><p>For any &gt; 0, there exist a positive function φ( ) and an -net S := {L (1) , . . . , L (N ) } ⊆ P(S × A), with the properties that min i=1,...,N d T V (L, L (i) ) ≤ for any L ∈ P(S × A), and that max a ∈A Q L</p><formula xml:id="formula_18">(i) (s, a ) -Q L (i) (s, a) ≥ φ( ) for any i = 1, . . . , N , s ∈ S, and any a / ∈ argmax a∈A Q L (i) (s, a).</formula><p>Here the existence of -nets is trivial due to the compactness of the probability simplex P(S × A), and the existence of φ( ) comes from the finiteness of the action set A. In practice, φ( ) often takes the form of D α with D &gt; 0 and the exponent α &gt; 0 characterizing the decay rate of the action gaps.</p><p>Finally, to enable Q-learning, it is assumed that one has access to a population simulator (See <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>). That is, for any policy π ∈ Π, given the current state s ∈ S, for any population distribution L, one can obtain the next state s ∼ P (•|s, π(s, µ), L), a reward r = r(s, π(s, µ), L), and the next population distribution L = P s ,π(s ,µ) . For brevity, we denote the simulator as (s , r, L ) = G(s, π, L). Here µ is the state marginal distribution of L.</p><p>In summary, we propose the following Algorithm 1.</p><p>Algorithm 1 Q-learning for GMFGs (GMF-Q)  </p><formula xml:id="formula_19">1: Input: Initial L 0 , tolerance &gt; 0. 2: for k = 0, 1, • • • do 3: Perform Q-learning for T k iterations to find the approximate Q-function Q k (s, a) = Q L k (s,</formula><formula xml:id="formula_20">: Compute π k ∈ Π with π k (s) = softmax c ( Q k (s, •)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Sample s ∼ µ k (µ k is the population state marginal of L k ), obtain Lk+1 from G(s, π k , L k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Find L k+1 = Proj S ( Lk+1 ) 7: end for Note that softmax is applied only at the end of each outer iteration when a good approximation of Q function is obtained. Within the outer iteration for the MDP problem with fixed mean-field information, standard Q-learning method is applied.</p><p>Here Proj S (L) = argmin L (1) ,...,L (N ) d T V (L (i) , L). For computational tractability, it would be sufficient to choose S as a truncation grid so that projection of Lk onto the epsilon-net reduces to truncating Lk to a certain number of digits. For instance, in our experiment, the number of digits is chosen to be 4. The choices of the hyper-parameters c and T k can be found in Lemma 8 and Theorem 2. In practice, the algorithm is rather robust with respect to these hyper-parameters.</p><p>In the special case when the rewards r L and transition dynamics P (•|s, a, L) are known, one can replace the Q-learning step in the above Algorithm 1 by a value iteration, resulting in the GMF-V Algorithm 3 in the Appendix.</p><p>We next show the convergence of this GMF-Q algorithm (Algorithm 1) to an -Nash of (GMFG), with complexity analysis. Theorem 2 (Convergence and complexity of GMF-Q). Assume the same conditions in Theorem 4 and Lemma 8 in the Appendix. For any tolerances , δ &gt; 0, set δ k = δ/K ,η , k = (k + 1) -(1+η) for some η ∈ (0, 1] (k = 0, . . . , K ,η -1), T k = T M L k (δ k , k ) (defined in Lemma 8 in the Appendix) and c = log (1/ )  φ( ) . Then with probability at least 1 -2δ, W 1 (L K ,η , L ) ≤ C . Moreover, the total number of iterations T =</p><formula xml:id="formula_21">K ,η -1 k=0 T M L k (δ k , k ) is bounded by 3 T = O K 1+ 4 h ,η (log(K ,η /δ)) 2 1-h + 2 h +3 . (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>Here K ,η := 2 max (η ) -1/η , log d ( /max{diam(S)diam(A), 1}) + 1) is the number of outer iterations, h is the step-size exponent in Q-learning (defined in Lemma 8 in the Appendix), and the constant C is independent of δ, and η.</p><p>The proof of Theorem 2 in the Appendix depends on the Lipschitz continuity of the softmax operator <ref type="bibr" target="#b7">[8]</ref>, the closeness between softmax and the argmax-e (Lemma 7 in the Appendix), and the complexity of Q-learning for the MDP (Lemma 8 in the Appendix).   Learning accuracy of GMF-Q. GMF-Q learns well. Its learning accuracy is tested against its special form GMF-V (Appendix G), with the latter assuming a known distribution of conversion rate v and the dynamics P for the budget s. The relative L 2 distance between the Q-tables of these two algorithms is ∆Q := QGMF-V-QGMF-Q 2 QGMF-V 2 = 0.098879. This implies that GMF-Q learns the true GMFG solution with 90-percent accuracy with 10000 inner iterations.</p><p>The heatmap in Figure <ref type="figure" target="#fig_3">1</ref>(a) is the Q-table for GMF-Q Algorithm after 20 outer iterations. Within each outer iteration, there are T GMF-Q k = 10000 inner iterations. The heatmap in Figure <ref type="figure" target="#fig_3">1</ref>(b) is the Q-table for GMF-Q Algorithm after 20 outer iterations. Within each outer iteration, there are T GMF-V k = 5000 inner iterations.</p><p>Comparison with existing algorithms for N -player games. To test the effectiveness of GMF-Q for approximating N -player games, we next compare GMF-Q with IL algorithm and MF-Q algorithm. IL algorithm <ref type="bibr" target="#b35">[36]</ref> considers N independent players and each player solves a decentralized reinforcement learning problem ignoring other players in the system. The MF-Q algorithm <ref type="bibr" target="#b39">[40]</ref> extends the NASH-Q Learning algorithm for the N -player game introduced in <ref type="bibr" target="#b14">[15]</ref>, adds the aggregate actions (ā a a -i = j =i aj N -1 ) from the opponents, and works for the class of games where the interactions are only through the average actions of N players.      Performance metric. We adopt the following metric to measure the difference between a given policy π and an NE (here 0 &gt; 0 is a safeguard, and is taken as 0.1 in the experiments):</p><formula xml:id="formula_23">C(π π π) = 1 N |S| N N i=1</formula><p>s s s∈S N max π i V i (s s s, (π π π -i , π i )) -V i (s s s, π π π) | max π i V i (s s s, (π π π -i , π i ))| + 0 .</p><p>Clearly C(π π π) ≥ 0, and C(π π π * ) = 0 if and only if π π π * is an NE. Policy arg max πi V i (s s s, (π π π -i , π i )) is called the best response to π π π -i . A similar metric without normalization has been adopted in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Our experiment (Figure <ref type="figure" target="#fig_9">5</ref>) shows that GMF-Q is superior in terms of convergence rate, accuracy, and stability for approximating an N -player game: GMF-Q converges faster than IL and MF-Q, with the smallest error, and with the lowest variance, as -net improves the stability.</p><p>For instance, when N = 20, IL Algorithm converges with the largest error 0.220. The error from MF-Q is 0.101, smaller than IL but still bigger than the error from GMF-Q. The GMF-Q converges with the lowest error 0.065. Moreover, as N increases, the error of GMF-Q deceases while the errors of both MF-Q and IL increase significantly. As |S| and |A| increase, GMF-Q is robust with respect to this increase of dimensionality, while both MF-Q and IL clearly suffer from the increase of the dimensionality with decreased convergence rate and accuracy. Therefore, GMF-Q is more scalable than IL and MF-Q, when the system is complex and the number of players N is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper builds a GMFG framework for simultaneous learning and decision-making, establishes the existence and uniqueness of NE, and proposes a Q-learning algorithm GMF-Q with convergence and complexity analysis. Experiments demonstrate superior performance of GMF-Q.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) of an MDP with dynamics P L k (s |s, a) and rewards r L k (s, a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Q-tables: GMF-Q vs. GMF-V.</figDesc><graphic coords="8,298.45,132.96,189.72,126.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence with different number of inner iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence with different number of states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>fluctuation in l∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fluctuations of Naive Algorithm (30 sample paths).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning accuracy based on C(π π π).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, a t , µ t )|s 0 = s subject to s t+1 ∼ P (s t , a t , µ t ), a t ∼ π t (s t , µ t ),</figDesc><table><row><cell cols="4">of the players, one can then focus on a single (representative) player. That is, in an MFG, one may</cell></row><row><cell cols="2">consider instead the following optimization problem,</cell><cell></cell></row><row><cell>maximize π π π V (s, π π π, µ µ µ) := E</cell><cell>∞</cell><cell cols="2">γ t r(s t</cell></row><row><cell></cell><cell>t=0</cell><cell></cell></row><row><cell cols="3">a population state distribution µ t with µ t (s) := lim N →∞</cell><cell>1 t , . . . , s i-1 t j=1,j =i 1s(s j N t ) N 1 . Due to the homogeneity , s i+1 t , . . . , s N t ) as</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Q-table with T GMF-V</figDesc><table><row><cell>T GMF-Q k</cell><cell>1000</cell><cell>3000</cell><cell>k</cell><cell>= 5000. 5000 10000</cell></row><row><cell>∆Q</cell><cell cols="4">0.21263 0.1294 0.10258 0.0989</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Here the indicator function 1s(s j t ) = 1 if s j t = s and 0 otherwise.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>argmax-e is not continuous: Let x = (1, 1), then argmax-e(x) = (1/2, 1/2). For any &gt; 0, let y = (1, 1 -), then argmax-e(y) = (1, 0).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>). Note that this bound may not be tight.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Haoran Tang for the insightful early discussion on stabilizing the Q-learning algorithm and sharing the ideas of his work on soft-Q-learning <ref type="bibr" target="#b11">[12]</ref>, which motivates our adoption of the soft-max operators. We also thank the anonymous NeurIPS 2019 reviewers for the valuable suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment: repeated auction game</head><p>In this section, we report the performance of the proposed GMF-Q Algorithm. The objectives of the experiments include 1) testing the convergence, stability, and learning ability of GMF-Q in the GMFG setting, and 2) comparing GMF-Q with existing multi-agent reinforcement learning algorithms, including IL algorithm and MF-Q algorithm.</p><p>We take the GMFG framework for the repeated auction game from Section 2.3. Here each advertiser learns to bid in the auction with a budget constraint.</p><p>Parameters. The model parameters are set as: |S| = |A| = 10, the overbidding penalty ρ = 0.2, the distributions of the conversion rate v ∼ uniform <ref type="bibr" target="#b3">[4]</ref>, and the competition intensity index M = 5. The random fulfillment is chosen as: if s &lt; s max , ∆(s) = 1 with probability 1  2 and ∆(s) = 0 with probability 1 2 ; if s = s max , ∆(s) = 0. The algorithm parameters are (unless otherwise specified): the temperature parameter c = 4.0, the discount factor γ = 0.8, the parameter h from Lemma 8 in the Appendix being h = 0.87, and the baseline inner iteration being 2000. Recall that for GMF-Q, both v and the dynamics of P for s are unknown a priori. The 90%-confidence intervals are calculated with 20 sample paths.</p><p>Performance evaluation in the GMFG setting. Our experiment shows that the GMF-Q Algorithm is efficient and robust, and learns well.</p><p>Convergence and stability of GMF-Q. GMF-Q is efficient and robust. First, GMF-Q converges after about 10 outer iterations; secondly, as the number of inner iterations increases, the error decreases (Figure <ref type="figure">2</ref>); and finally, the convergence is robust with respect to both the change of number of states and the initial population distribution (Figure <ref type="figure">3</ref>).</p><p>In contrast, the Naive algorithm does not converge even with 10000 inner iterations, and the joint distribution L t keeps fluctuating (Figure <ref type="figure">4</ref>). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Extended mean field control problems: stochastic maximum principle and transport perspective</title>
		<author>
			<persName><forename type="first">B</forename><surname>Acciaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Backhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carmona</surname></persName>
		</author>
		<idno>Arxiv Preprint:1802.05754</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An alternative softmax operator for reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Increasing the action gap: new operators for reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1476" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A class of mean field interaction models for computer and communication systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Le Boudec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance evaluation</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="823" to="838" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Separability and completeness for the Wasserstein distance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Séminaire de Probabilités XLI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="371" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time bidding by reinforcement learning in display advertising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Malialis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning rates for Q-learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Even-Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the properties of the softmax function with application in game theory and reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
		<idno>Arxiv Preprint:1704.00805</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On choosing and bounding probability metrics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="435" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discrete time, finite state space mean field games</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de mathématiques pures et appliquées</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="308" to="328" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Repeated auctions under budget constraints: Optimal bidding strategies and equilibria</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Proutiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Eighth Ad Auction Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>Arxiv Preprint:1702.08165</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The sharing economy: Why people participate in collaborative consumption</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hamari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sjöklint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2047" to="2059" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Is multiagent deep reinforcement learning the answer or the question? A brief survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<idno>Arxiv Preprint:1810.05587</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nash Q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mean field stochastic games with binary action spaces and monotone costs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>ArXiv Preprint:1701.06661</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large population stochastic dynamic games: closedloop McKean-Vlasov systems and the Nash certainty equivalence principle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Malhamé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Information &amp; Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="252" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mean field equilibria of dynamic auctions with learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGecom Exchanges</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis of game bot&apos;s behavioral characteristics in social interaction networks of MMORPG</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="99" to="100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Real-time bidding with multi-agent reinforcement learning in display advertising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>Arxiv Preprint:1802.09756</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-agent reinforcement learning: A report on challenges and approaches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</author>
		<idno>Arxiv Preprint:1807.09427</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mean field stochastic adaptive control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kizilkale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="905" to="920" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mean field games</title>
		<author>
			<persName><forename type="first">J-M</forename><surname>Lasry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="260" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A mean field game of portfolio trading and its consequences on perceived correlations</title>
		<author>
			<persName><forename type="first">C-A</forename><surname>Lehalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mouzouni</surname></persName>
		</author>
		<idno>ArXiv Preprint:1902.09606</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discrete time mean field games: The short-stage limit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dynamics &amp; Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="101" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decentralised learning in systems with many, many strategic agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mguni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>De Cote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Computing equilibria in multi-player games</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roughgarden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the sixteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Actor-critic fictitious play in simultaneous move multistage games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning Nash equilibrium for general-sum Markov games from batch data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pérolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<idno>Arxiv Preprint:1606.08718</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Computational optimal transport</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="355" to="607" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A tour of reinforcement learning: The view from continuous control</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics, and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Annual Review of Control</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Markov-Nash equilibria in mean-field games with discounted cost</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4256" to="4287" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforcement learning in stationary mean-field games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Autonomous Agents and Multiagent Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="251" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: independent vs. cooperative agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning via double averaging primal-dual optimization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9672" to="9683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep mean field games for learning optimal behavior policy of large populations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno>Arxiv Preprint:1711.03156</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>Arxiv Preprint:1802.05438</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning in mean-field games</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="629" to="644" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
