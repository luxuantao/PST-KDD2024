<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Graph Neural Networks with 1000 Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><forename type="middle">M</forename><surname>Üller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
						</author>
						<title level="a" type="main">Training Graph Neural Networks with 1000 Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of 87.74 ± 0.13 and 88.14 ± 0.15 on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graphs are all around us. Whether we watch a show on Netflix, browse through friends' feeds on Facebook, buy something on Amazon, or look up a researcher on Google, chances are that our actions trigger queries on a large graph. Movie and book databases are often encoded as knowledge graphs for efficient recommendation systems. Social media services rely on social graphs. Shopping platforms leverage product co-purchasing networks to boost sales. Citation indices like Web of Science, Scopus, and Google Scholar 1 Intel Labs 2 King Abdullah University of Science and Technology. Correspondence to: Guohao Li &lt;guohao.li@kaust.edu.sa&gt;.  . ROC-AUC score vs. GPU memory consumption on the ogbn-proteins dataset. We find that deep reversible GNNs are very powerful and outperform existing models by a margin; our best models are RevGNN-Deep and RevGNN-Wide. We also compare reversible connections (RevGNN-x), weight-tying (WTx), and equilibrium models (DEQ-x) for 112-layer deep GNNs (x denotes the number of channels per layer). Reversible models consistently achieve the same or better performance as the baseline using only a fraction of the memory. Weight-tied and equilibrium models offer a good performance to parameter efficiency trade-off. Datapoint size is proportional to √ p, where p is the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the</head><p>construct large citation graphs. Even the Internet itself is in essence a vast graph with billions of nodes and edges. Graphs are also powerful tools for representing 3D data such as point clouds and meshes or biological data such as molecular structures and protein interactions.</p><p>One prominent and powerful approach to process such graphs are graph neural networks (GNNs). GNNs have achieved impressive performance on relatively small graph datasets <ref type="bibr" target="#b69">(Yang et al., 2016;</ref><ref type="bibr" target="#b75">Zitnik &amp; Leskovec, 2017;</ref><ref type="bibr" target="#b58">Shchur et al., 2018)</ref>. Unfortunately, the most interesting and impactful real-world problems rely on very large graphs where limited GPU memory quickly becomes a bottleneck. In order to train GNN models on large graphs, the common practice is to reduce the number of model parameters. This is counterproductive, since processing larger graphs would likely benefit from more parameters. While there is evidence that over-parameterized models generalize better <ref type="bibr" target="#b48">(Neyshabur et al., 2019;</ref><ref type="bibr" target="#b3">Belkin et al., 2019)</ref>, the relationship between performance and parameters is best illustrated in the context arXiv:2106.07476v1 <ref type="bibr">[cs.</ref>LG] 14 Jun 2021 of language modelling. Recent progress in natural language processing (NLP) has been enabled by a massive increase in parameter counts: GPT (110M) <ref type="bibr">(Radford et al.)</ref>, BERT (340M) <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>,   <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>, Gshard-M4 (600B) <ref type="bibr" target="#b38">(Lepikhin et al., 2021)</ref>, and Deep-Speed (1T) <ref type="bibr" target="#b54">(Rasley et al., 2020)</ref>. More parameters mean deeper or wider networks that consume more memory.</p><p>GNNs have shown a lot of promise on recent large-scale graph datasets such as Benchmarking GNNs <ref type="bibr" target="#b15">(Dwivedi et al., 2020)</ref>, Open Graph Benchmark (OGB) <ref type="bibr" target="#b27">(Hu et al., 2020;</ref><ref type="bibr">2021)</ref> and Microsoft Academic Graph (MAG) <ref type="bibr" target="#b61">(Wang et al., 2020)</ref>. Recent works <ref type="bibr" target="#b40">(Li et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b11">Chen et al., 2020)</ref> have successfully trained deep models with a large number of parameters and achieved state-of-the-art performance. However, these models have large memory footprints and operate at the physical limits of current hardware. In order to apply deeper and wider GNNs with more parameters, we need either different hardware or more efficient architectures that consume less memory.</p><p>Existing works try to overcome the memory bottleneck by mini-batch training, i.e. sampling a smaller set of nodes <ref type="bibr" target="#b23">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b10">Chen et al., 2018b;</ref><ref type="bibr">a)</ref> or partitioning large graphs <ref type="bibr" target="#b12">(Chiang et al., 2019;</ref><ref type="bibr" target="#b71">Zeng et al., 2020)</ref> into smaller subgraphs and sampling from those. These approaches have proven successful, but they introduce further hyperparameters that need to be tuned. For instance, if the sampled size of nodes or subgraphs is too small, it may break important structures in the graph. While these methods are a step in the right direction, they do not scale well as the models become deeper or wider, since memory consumption is still dependent on the number of layers. Another approach is efficient propagation via K-power adjacency matrices or graph diffusion matrices <ref type="bibr" target="#b64">(Wu et al., 2019;</ref><ref type="bibr" target="#b35">Klicpera et al., 2019;</ref><ref type="bibr" target="#b4">Bojchevski et al., 2020;</ref><ref type="bibr" target="#b46">Liu et al., 2020;</ref><ref type="bibr" target="#b17">Frasca et al., 2020)</ref>. However, the propagation schemes of these methods are non-trainable, which may lead to sub-optimality.</p><p>Inspired by efficient architectures from computer vision and natural language processing <ref type="bibr" target="#b20">(Gomez et al., 2017;</ref><ref type="bibr" target="#b65">Xie et al., 2017;</ref><ref type="bibr" target="#b0">Bai et al., 2018;</ref><ref type="bibr">2019)</ref>, here we investigate several methods to obtain more efficient GNNs that use less memory than conventional architectures while achieving state-of-theart results (see Figure <ref type="figure">1</ref>). We explore grouped reversible graph connections in order to reduce the memory complexity with respect to the number of layers from O(L) to O(1). In other words, the memory consumption is independent of the depth. This allows us to train very deep, over-parameterized models with constant memory consumption.</p><p>We also investigate parameter-efficient architectures. We study deep weight-tied GNNs that have the parameter cost of only a single layer. We also develop a deep graph equilibrium GNN, which is essentially a weight-tied network with infinite depth. We directly solve for the equilibrium point of this infinite-layer network using a root-finding method. We backpropagate through the equilibrium point using implicit differentiation. Hence, we do not need to store intermediate states and get an infinite-depth network at the memory and parameter cost of a single layer.</p><p>Our analysis of these methods shows that deep reversible architectures are the most powerful in terms of achieving state-of-the-art performance on benchmark datasets. This is due to their very large capacity at low memory cost and only slightly increased training time. Weight-tied models offer constant parameter size regardless of depth. However, due to the smaller number of parameters, performance on large datasets suffers and has to be compensated by increasing the width. Finally, graph equilibrium models have the same memory efficiency as reversible models and the same parameter efficiency as weight-tied models. They perform similarly to weight-tied models, and the training time vs. performance tradeoff can be further adjusted by tuning the number of iterations in each optimization step.</p><p>Our methods can be applied to different GNN operators. In our experiments, we successfully apply them to GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b60">(Veličković et al., 2018)</ref>, and DeepGCN <ref type="bibr" target="#b40">(Li et al., 2019)</ref>. We can also combine the proposed techniques with samplingbased methods to further reduce memory and boost performance on some datasets. To the best of our knowledge, we are the first to train a GNN with more than 1000 layers. Our model RevGNN-Deep, outperforms all state-of-the-art approaches on the ogbn-proteins dataset <ref type="bibr" target="#b27">(Hu et al., 2020)</ref> with an ROC-AUC of 87.74%, while only consuming 2.86 GB of GPU memory during training, one order of magnitude less than the current top performer. We can also trade our memory savings for larger width, pushing performance to new heights. Our RevGNN-Wide achieves an ROC-AUC of 88.24% on the ogbn-proteins dataset, ranking first on the leaderboard by a large margin.</p><p>In summary, we investigate several techniques to increase the memory efficiency of GNNs and perform an extensive analysis. We significantly outperform current state-of-theart methods on several datasets by employing reversible connections to train deeper and wider models. Further, we demonstrate the generality of these techniques by applying them to multiple GNN operators. We release our implementation, which supports PyTorch Geometric <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019)</ref> and the Deep Graph Library <ref type="bibr" target="#b62">(Wang et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GNNs were introduced by <ref type="bibr" target="#b22">Gori et al. (2005)</ref> and <ref type="bibr" target="#b56">Scarselli et al. (2008)</ref> for learning a graph representation by finding stable states through fixed-point iterations. <ref type="bibr">Bruna et al. (2014) generalized Convolutional Neural Networks (CNNs)</ref> to graphs using the Fourier basis by computing the eigendecomposition of the graph Laplacian. Follow-up works <ref type="bibr" target="#b26">(Henaff et al., 2015;</ref><ref type="bibr" target="#b13">Defferrard et al., 2016;</ref><ref type="bibr" target="#b33">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b39">Levie et al., 2018;</ref><ref type="bibr">Li et al., 2018)</ref> propose different ways to improve and extend spectral GNNs. For instance, <ref type="bibr" target="#b33">Kipf &amp; Welling (2017)</ref> simplify spectral graph convolutions by limiting the filters to operate on 1-hop neighborhoods. Instead of defining GNNs from the spectral view, <ref type="bibr" target="#b19">Gilmer et al. (2017)</ref> introduce a general framework termed Message Passing Neural Networks (MPNNs) to unify GNNs. MPNNs define convolutions on the graph by propagating messages from spatial neighbors. Many subsequent works <ref type="bibr" target="#b23">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b47">Monti et al., 2017;</ref><ref type="bibr" target="#b49">Niepert et al., 2016;</ref><ref type="bibr" target="#b18">Gao et al., 2018;</ref><ref type="bibr" target="#b67">Xu et al., 2019;</ref><ref type="bibr" target="#b60">Veličković et al., 2018;</ref><ref type="bibr" target="#b63">Wang et al., 2019b)</ref> fall into this framework.</p><p>Most prior state-of-the-art (SOTA) works are limited to shallow depths due to the over-smoothing <ref type="bibr">(Li et al., 2018)</ref> and vanishing gradient problems <ref type="bibr" target="#b40">(Li et al., 2019;</ref><ref type="bibr">2021)</ref>   <ref type="bibr" target="#b25">(He et al., 2016;</ref><ref type="bibr" target="#b29">Huang et al., 2017)</ref> and dilated convolutions <ref type="bibr" target="#b70">(Yu &amp; Koltun, 2016)</ref> from CNNs to GNNs; they successfully train GNNs with up to 56 layers on 3D point clouds and achieve SOTA results. <ref type="bibr" target="#b41">Li et al. (2020)</ref> train a 112-layer residual GNN and achieve SOTA performance on the large-scale ogbn-proteins dataset <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. Other works also demonstrate that residual connections aid in training deeper GNNs <ref type="bibr" target="#b21">(Gong et al., 2020;</ref><ref type="bibr" target="#b11">Chen et al., 2020;</ref><ref type="bibr" target="#b68">Xu et al., 2021)</ref>. We push further along the path of deep residual GNNs and train a SOTA model with more than 1000 layers by addressing the memory bottleneck of current approaches.</p><p>Researchers have also studied different normalization and regularization techniques such as DropEdge <ref type="bibr" target="#b55">(Rong et al., 2020)</ref>, DropConnect <ref type="bibr" target="#b24">(Hasanzadeh et al., 2020)</ref>, PairNorm <ref type="bibr" target="#b73">(Zhao &amp; Akoglu, 2019)</ref>, weight normalization <ref type="bibr" target="#b50">(Oono &amp; Suzuki, 2019)</ref>, differentiable group normalization <ref type="bibr" target="#b74">(Zhou et al., 2020)</ref>, and GraphNorm <ref type="bibr" target="#b8">(Cai et al., 2020)</ref>. Another line of work <ref type="bibr" target="#b64">(Wu et al., 2019;</ref><ref type="bibr" target="#b35">Klicpera et al., 2019;</ref><ref type="bibr" target="#b4">Bojchevski et al., 2020;</ref><ref type="bibr" target="#b46">Liu et al., 2020;</ref><ref type="bibr" target="#b17">Frasca et al., 2020)</ref> proposes an efficient propagation scheme to avoid stacking layers by incorporating multi-hop information into a single GNN layer via K-th power adjacency matrices or personalized PageRank diffusion matrices. However, the propagation scheme of these methods is non-trainable, thus making it difficult to learn hierarchical features and limiting capacity.</p><p>Many applications yield huge graphs with millions of nodes and edges. Memory limitations preclude full-batch training in such settings. Specifically, to train an L-layer GNN with D hidden channels on a graph G with N nodes and M edges, the memory complexity of storing activations is O(LN D). GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref> incorporates a recursive node-wise sampling scheme to improve the scalability of GNNs. Instead of training on the whole graph, Graph-SAGE uniformly samples a fixed number of neighbors for a batch of nodes. However, the recursive neighborhood leads to exponential memory complexity as the number of GNN layers increases. If B is the batch size of nodes and R is the number of sampled neighbors per node, the memory complexity of GraphSAGE is O(R L BD). VR-GCN <ref type="bibr" target="#b10">(Chen et al., 2018b)</ref> enhances GraphSAGE by reducing the variance of mini-batch training to increase the convergence rate. However, to reduce the estimation variance, historical activations need to be stored in memory (O(LN D)).</p><p>To avoid recursive neighborhood expansion, FastGCN <ref type="bibr" target="#b9">(Chen et al., 2018a)</ref> performs layer-wise sampling to subsample nodes for each layer independently with a degreebased distribution. While this further reduces the memory complexity to O(LRBD), it leads to sparse connections. To better maintain the graph structure, Cluster-GCN <ref type="bibr" target="#b12">(Chiang et al., 2019)</ref> and GraphSAINT <ref type="bibr" target="#b71">(Zeng et al., 2020)</ref> propose subgraph-wise sampling methods for mini-batch training with memory complexity O(LBD). However, even with smart sampling methods, GNNs still do not scale as the number of layers increases (i.e. the L dimension). We tackle this issue and study orthogonal approaches that eliminate the L dimension from the memory complexity. Our techniques enable unlimited network depth at no memory cost and can be combined with existing sampling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Building Deep GNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>A graph G is represented by a tuple G = V, E , where</p><formula xml:id="formula_0">V = { v 1 , v 2 , ..., v N } is an unordered set of vertices and E ⊆ V × V is a set of edges.</formula><p>Let N and M denote the number of vertices and edges, respectively. For convenience, a graph can be equivalently defined as an adjacency matrix A ∈ A ⊂ R N ×N , where a i,j denotes the link relation between vertex v i and v j . In some scenarios, vertices and edges are associated with a vertex feature matrix X ∈ X ⊂ R N ×D and an edge feature matrix U ∈ U ⊂ R M ×F , respectively. We use GNN operators that map the vertex feature matrix X, the adjacency matrix A, and the edge feature matrix U (optional) to a transformed vertex feature matrix X :</p><formula xml:id="formula_1">f w : X × A × U → X ,<label>(1)</label></formula><p>where f w (X, A, U ) is parameterized by learnable parameters w. For simplicity, we assume that the transformed vertex feature matrix X has the same dimension as the input vertex feature matrix X. We also assume that the adjacency matrix A is the same for all GNN layers. When the edge feature matrix U is present, it is fed into each layer with its initial values U (0) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Over-parameterized GNNs</head><p>Deep GNNs. Recent works <ref type="bibr" target="#b40">(Li et al., 2019;</ref><ref type="bibr">2020)</ref> show how adding residual connections <ref type="bibr" target="#b25">(He et al., 2016)</ref> to vertex features (X = f w (X, A, U ) + X) enables training of deep GNNs that achieve promising results on graph datasets. However, the memory complexity of the activations remains O(LN D), where L is the number of GNN layers, N is the number of vertices, and D is the size of vertex features.</p><p>Hence, the memory consumption of deep GNNs scales linearly with the number of layers. Since the memory footprint of the network parameters is usually negligible, we focus on memory consumption induced by the activations.</p><p>Grouped Reversible GNNs. Inspired by reversible networks <ref type="bibr" target="#b20">(Gomez et al., 2017;</ref><ref type="bibr" target="#b45">Liu et al., 2019;</ref><ref type="bibr" target="#b34">Kitaev et al., 2019)</ref> and grouped convolutions <ref type="bibr" target="#b36">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b65">Xie et al., 2017)</ref>, we generalize reversible residual connections to grouped reversible residual connections for GNNs. Specifically, the input vertex feature matrix X is uniformly partitioned into C groups X 1 , X 2 , ..., X C across the channel dimension, where</p><formula xml:id="formula_2">X i ∈ R N × D C .</formula><p>A grouped reversible GNN block operates on a group of inputs and produces a group of outputs:</p><formula xml:id="formula_3">X 1 , X 2 , ..., X C → X 1 , X 2 , ..., X C .</formula><p>The forward pass is defined as follows:</p><formula xml:id="formula_4">X 0 = C i=2 X i</formula><p>(2)</p><formula xml:id="formula_5">X i = f wi (X i−1 , A, U ) + X i , i ∈ {1, • • • , C},<label>(3)</label></formula><p>where X 0 is designed for exchanging information across groups. Unlike conventional GNNs, grouped reversible GNNs only need to save the output vertex features of the last GNN block in GPU memory for backpropagation. Therefore, the memory complexity of activations becomes O(N D), which is independent of the depth of the network. Note that the adjacency matrix A and the edge feature matrix U are not updated during message passing. During the backward pass, only the input vertex features are reconstructed, on the fly, from the output vertex features X 1 , X 2 , ..., X C for backpropagation:</p><formula xml:id="formula_6">X i = X i − f wi (X i−1 , A, U ), i ∈ {2, • • • , C} (4) X 0 = C i=2 X i (5) X 1 = X 1 − f w1 (X 0 , A, U ).<label>(6)</label></formula><p>In practice, X i for i ∈ {2, • • • , C} can be computed in parallel. To reconstruct X 1 , X 0 needs to be computed in advance. After reconstructing the original input vertex features, gradients can be derived through backpropagation.</p><p>Owing to the group processing, the number of parameters reduces as the group size increases. Note that in the special case where the group size C = 2, we obtain a similar form to the reversible residual connections proposed for CNNs <ref type="bibr" target="#b20">(Gomez et al., 2017)</ref>. The definition above is independent of the choice of f w . However, we find that normalization layers and dropout layers are essential for training deep GNNs.</p><p>To avoid extra memory usage, normalization layers and dropout layers are embedded into the reversible GNN block.</p><p>The GNN block f wi is designed similar to the pre-activation residual GNN block proposed by <ref type="bibr" target="#b41">Li et al. (2020)</ref>:</p><formula xml:id="formula_7">X i = Dropout(ReLU(Norm(X i−1 )))<label>(7)</label></formula><formula xml:id="formula_8">X i = GraphConv( X i , A, U ). (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>The stochasticity of vanilla dropout layers causes reconstruction errors in the reverse pass. A naive solution would be to store the dropout pattern for all layers. However, the dropout patterns have the same dimension as the activations, which would lead to O(LN D) memory consumption. As an alternative, we adopt a modified dropout layer in which the dropout pattern is shared across layers. Therefore, we only need to store one dropout pattern in every SGD iteration; its memory complexity is independent of the depth: O(N D). During the reverse pass, the saved dropout pattern is reactivated to reconstruct the input vertex features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter-Efficient GNNs</head><p>Weight-tied GNNs. Weight-tying is a powerful tool for improving the parameter efficiency of language models <ref type="bibr" target="#b52">(Press &amp; Wolf, 2017;</ref><ref type="bibr" target="#b30">Inan et al., 2017;</ref><ref type="bibr" target="#b0">Bai et al., 2018)</ref>. We take these works as inspiration to study how weight-tying can improve the parameter efficiency of GNNs. Instead of having different weights for different GNN blocks, the weight-tied GNN shares weights across layers. The GNN model proposed by <ref type="bibr" target="#b56">Scarselli et al. (2008)</ref> can be considered the first weight-tied GNN, in which a learned transition function is used to find stable node states by Banach's fixed point theorem <ref type="bibr" target="#b32">(Khamsi &amp; Kirk, 2001</ref>). Here we experiment with weight-tied residual GNNs and weight-tied reversible GNNs. For weight-tied residual GNNs, we define:</p><formula xml:id="formula_10">f (1) w := f (2) w . . . := f (L) w , (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where L is the number of layers. For weight-tied reversible GNNs, weights are shared in a group-wise manner:</p><formula xml:id="formula_12">f (1) wi := f (2) wi . . . := f (L) wi , i ∈ {1, • • • , C}<label>(10)</label></formula><p>Note that we use the same pre-activation GNN block described in Section 3.2 instead of a contraction map used by <ref type="bibr" target="#b56">Scarselli et al. (2008)</ref>. Both weight-tied GNNs have explicit layers and are trained with backpropagation. But the weighttied reversible GNN reconstructs input vertex features on the fly during backpropagration. Therefore, the memory complexities of the weight-tied residual GNN and weight-tied reversible GNN are O(LN D) and O(N D), respectively.</p><p>Deep Equilibrium GNNs. An alternative way to train weight-tied GNNs with O(N D) memory consumption is to use implicit differentiation <ref type="bibr" target="#b56">(Scarselli et al., 2008;</ref><ref type="bibr" target="#b1">Bai et al., 2019)</ref>, which assumes that the network can reach an equilibrium state. We construct a GNN model that is assumed to converge to a fixed point Z * for any given input:</p><formula xml:id="formula_13">Z * = f DEQ w (Z * , X, A, U ),<label>(11)</label></formula><p>where the state Z represents the transformed node features.</p><p>To construct a stable or contractive GNN block, we mimic the design of MDEQ <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>. We build a GNN block as follows:</p><formula xml:id="formula_14">Z = GraphConv(Z in , A, U )<label>(12)</label></formula><formula xml:id="formula_15">Z = Norm(Z + X) (13) Z = GraphConv(Dropout(ReLU(Z )), A, U ) (14) Z o = Norm(ReLU(Z + Z )),<label>(15)</label></formula><p>where Z in is the input node state, Z o is the output node state, X serves as the injected input, and Z forms an internal residual signal to the output Z o . In practice, X represents the initial node features and Z in is initialized to zeros for the first iteration. Similar to DEQ <ref type="bibr" target="#b1">(Bai et al., 2019)</ref>, the forward pass of DEQ-GNN is implemented with a root-finding algorithm (e.g. Broyden's method) and the gradients are obtained by implicitly differentiating through the equilibrium node state for the backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis of Different Deep GNN Architectures</head><p>In the following, we compare the different techniques for building deep GNNs with respect to their performance, memory efficiency and parameter efficiency on the ogbnproteins dataset from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. We use the same GNN operator <ref type="bibr" target="#b41">(Li et al., 2020)</ref>, hyper-parameters (e.g. learning rate, dropout rate, training epoch, etc.), and optimizers to make the comparison as fair as possible. We use mini-batch training with random partitioning where graphs are split into 10 parts during training and 5 parts during testing <ref type="bibr" target="#b41">(Li et al., 2020)</ref>. One subgraph is sampled to form a mini-batch at each step.</p><p>The reported GPU memory corresponds to the peak GPU memory usage during the first training epoch.</p><p>Baseline GNN. We use a recent pre-activation residual GNN <ref type="bibr" target="#b41">(Li et al., 2020)</ref>, which achieves state-of-the-art performance across several OGB datasets, as our baseline. For simplicity, we use the max aggregation function for all the ablated models on the ogbn-proteins dataset. We refer to the baseline model as ResGNN. As shown in Figure <ref type="figure">2</ref>, the  Equilibrium GNN. Equilibrium networks implicitly model a weight-tied network with infinite depth. As a result, they only have the parameter and memory footprint of a single layer, but the expressiveness of a very deep network. The initial node features X are used as input injection and the initial node states Z are set to zero. We implement our DEQ-GNN based on the original DEQ implementation for CNNs <ref type="bibr" target="#b1">(Bai et al., 2019</ref>). Broyden's root-finding method is used <ref type="bibr" target="#b6">(Broyden, 1965)</ref> in both the forward pass and the backward pass to find the equilibrium node states and approximate the inverse Jacobian. The Broyden iterations terminate when the norm of the objective is smaller than a tolerance or a maximum iteration threshold is reached. is set to 10 −6 • √ BD and 2 × 10 −10 • √ BD for the forward pass and the backward pass respectively, where B is the number of nodes in the sampled subgraph and D is the channel size. The iteration thresholds in the forward pass and the backward pass are set to the same value. We examine different iteration thresholds for DEQ-GNN with a channel size of 64 (DEQ-GNN-64) and a channel size of 224 (DEQ-GNN-224) in Figure <ref type="figure" target="#fig_2">4</ref>. It can be seen that the iteration threshold is crucial for good performance, since it affects the convergence to the equilibrium. We find that DEQ-GNN-64 performs similarly to WT-RevGNN-80 with nearly the same number of parameters and memory consumption. Discussion. Reversible networks emerge as the most promising approach for building deep GNNs that achieve state-of-the-art performance. They consume much less memory than the baseline while achieving comparable performance with the same number of parameters (see Figure <ref type="figure">1</ref>). However, in contrast to the baseline method, we are able to train very deep networks with more parameters and much better performance without running out of memory. While it is possible to go to arbitrary depths with constant memory consumption, the training time increases. In order to reduce the number of parameters and inference time, it is possible to increase the group size. However, we find that group sizes larger than 2 do not lead to a performance increase in practice and may even degrade model performance. We conjecture that this is due to the increased receptive field of early layers and the smaller number of parameters per memory. We provide an ablation study in the appendix.</p><p>The weight-tied network limits the number of parameters to a single layer regardless of the effective depth. We find that going deeper with tied weights boosts performance, but returns eventually diminish (see Figure <ref type="figure">3</ref>). While parameters stay constant, going deeper still increases memory consumption, unless the reversible GNN block is used. An extension of the weight-tied network is the graph equilibrium model. It represents an infinite-depth weight-tied network and uses fixed-point iterations to solve for the optimal weights. This allows for much wider channel size with the same amount of memory. DEQ-GNNs are faster to train, but have more hyper-parameters to tune. Please refer to the appendix for a training time ablation. We find that pretraining is not necessary, but can further improve the results. The results reported in this paper are without pretraining for fair comparison. While weight-tied GNNs and equilibrium GNNs are not able to achieve state-of-the-art performance, they are very parameter-efficient, which may be relevant for applications on embedded devices where model size matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Over-parameterized Deep GNNs</head><p>We conduct experiments on several datasets from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. We first show state-of-the art results on several datasets with the proposed deep reversible GNN. We then apply the reversible GNN framework to different GNN operators on the ogbn-arxiv dataset. To show how mini-batch sampling can further aid the training of deep GNNs, we compare full-batch and minibatch training of reversible GNNs on the ogbn-products dataset. The data splits and evaluation metrics on all datasets follow the OGB evaluation protocol. Mean and standard deviation are obtained across 10 trials. All the ablated models use the same hyper-parameters (e.g. learning rate, dropout rate, training epoch, etc.) and optimizers as the baseline models. The implementation of all the reversible models is based on PyTorch <ref type="bibr" target="#b51">(Paszke et al., 2019)</ref> and supports both PyTorch Geometric (PyG) <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019)</ref> and Deep Graph Libray (DGL) <ref type="bibr" target="#b62">(Wang et al., 2019a)</ref> frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State-of-the-art Results</head><p>We briefly describe two variants of our RevGNN with reversible connections, which reach new SOTA results on the ogbn-proteins dataset of the OGB leaderboard <ref type="bibr" target="#b27">(Hu et al., 2020)</ref> (Table <ref type="table" target="#tab_3">1</ref>) at the time of submission. RevGNN-Deep has 1001 layers and a channel size of 80. It outperforms the previous SOTA method UniMP+CEF by 0.83% ROC-AUC, while using only 10.5% of the GPU memory for training. RevGNN-Wide uses 448 GNN layers and 224 hidden channels and significantly outperforms UniMP+CEF by 1.33% ROC-AUC with about 29% of the GPU memory. RevGNN-Deep uses the same training setting as mentioned in Section 3.4. The RevGNN-Wide uses a larger dropout rate of 0.2 to prevent overfitting. To boost the performance, the results of RevGNN-Deep and RevGNN-Wide are obtained using multi-view inferences with 10 views on larger subgraphs with a partition size of 3. We perform the inferences on a NVIDIA RTX A6000 (48GB). Please refer to the appendix for the details of multi-view inference. Larger and deeper models incur a cost in terms of training and inference time. RevGNN-Deep and RevGNN-Wide take 13.5 days and 17.1 days, respectively, to train for 2000 epochs on a single NVIDIA V100. Nonetheless, it is affordable for accuracycritical applications in scientific research such as predicting protein structures <ref type="bibr" target="#b57">(Senior et al., 2020)</ref>. We demonstrate that it is possible to train huge over-parameterized GNN models on a single GPU. The RevGNN-Wide model has 68.47 million parameters, which is about half the size of the GPT language model <ref type="bibr">(Radford et al.)</ref>. We believe that this is an important step forward in developing over-parameterized GNNs for graphs.</p><p>Our RevGNNs also achieve new SOTA results on the ogbnarxiv dataset (see Table <ref type="table" target="#tab_4">2</ref>). RevGCN-Deep uses 28 GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref> layers with 128 channels each and achieves an accuracy of 73.01%, while only using 1.84GB of GPU memory. RevGAT-Wide uses 5 attention layers with 3 heads and 356 channels for each head and outperforms the current top performer UniMP v2 <ref type="bibr" target="#b59">(Shi et al., 2020)</ref> (74.05% vs. 73.97%) while using about a third of the memory (8.49GB vs. 25.0GB). RevGAT-SelfKD uses selfknowledge distillation <ref type="bibr" target="#b72">(Zhang et al., 2019)</ref> with 5 attention layers, 3 heads, and 256 channels each. The teacher models achieve an accuracy of 74.02%. After training with distillation, the student models set a new SOTA with 74.26% test accuracy. Please refer to the appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Application to Different GNN Operators</head><p>The proposed techniques are generic and can in principle be applied to any SOTA GNN to further boost the performance with deeper and wider architectures while saving GPU memory. We show this for the example of reversible GNNs and build RevGNNs with different SOTA GNN operators: GAT <ref type="bibr" target="#b60">(Veličković et al., 2018)</ref>, GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017), and</ref><ref type="bibr">ResGEN (Li et al., 2020)</ref>. We compare them to their non-reversible residual counterparts on the ogbn-arxiv in Table <ref type="table" target="#tab_6">3</ref>. Since ogbn-arxiv is much smaller than the ogbn-proteins, we are able to run all experiments with full-batch training and report statistics across 10 training runs. We observe that all of the RevGNNs consistently outperform their vanilla residual counterparts with the same channel size. The RevGNNs use less memory due to reversible connections and fewer parameters due to grouping. We increase the channel size of RevGNNs to roughly match the number of parameters of the  <ref type="bibr">(Liu et al.)</ref> 72.09 ± 0.25 2.40 43.9k DeeperGCN <ref type="bibr">(Li et al.)</ref> 72.32 ± 0.27 21.6 491k GCNII <ref type="bibr">(Chen et al.)</ref> 72.74 ± 0.16 17.0 2.15M GAT <ref type="bibr">(Veličković et al.)</ref> 73.91 ± 0.12 5.52 1.44M UniMP v2 <ref type="bibr">(Shi et al.)</ref> 73.97 ± 0.15 25.0 687k   <ref type="bibr" target="#b41">(Li et al., 2020)</ref> and RevGNN with full-batch and mini-batch training on the ogbn-products. RevGNN uses constant memory for deeper networks with more parameters and better performance. We use a width of 128 and 160 for ResGNN and RevGNN, respectively, to ensure a similar number of parameters per network depth. Datapoints are annotated with the accuracy of the model and their size is proportional to √ p, where p is the number of parameters.</p><p>corresponding ResGNNs, thus increasing the performance gap further. For instance, the RevGCN with 28 layers and 180 channels reduces the memory footprint by more than 75% while improving the accurracy by 0.76%, as compared to the ResGCN with 28 layers and 128 channels. Utilizing label propagation, the RevGAT with 5 layers and 1068 channels (3 attention heads with 356 channels for each head) achieves SOTA results on ogbn-arxiv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Full-batch vs. Mini-batch Training</head><p>Our method is orthogonal to existing sampling-based approaches, which also reduce memory consumption. Hence,  <ref type="figure" target="#fig_3">5</ref> with full-batch inference. Compared to full-batch training, we find that mini-batch training further reduces the memory consumption of RevGNN to 44% and improves its accuracy from 78.77% to 82.16%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of Complexities</head><p>We have discussed the memory complexity of full-batch GNNs, GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref>, VR-GCN <ref type="bibr" target="#b10">(Chen et al., 2018b)</ref>, FastGCN <ref type="bibr" target="#b9">(Chen et al., 2018a)</ref>, Cluster-GCN <ref type="bibr" target="#b12">(Chiang et al., 2019)</ref> and GraphSAINT <ref type="bibr" target="#b71">(Zeng et al., 2020)</ref> in the related work section, and the memory complexity of our Reversible GNN, Weight-tied GNN and DEQ-GNN in the methodology section. We summarize the theoretical memory complexity in Table <ref type="table" target="#tab_7">4</ref>, where L is the number of layers of the GNN, D is the size of hidden channels, N is of the number of nodes in the graph, B is the batch size of nodes and R is the number of sampled neighbors of each node. K is the maximum Broyden iterations for equilibrium GNNs. We only discuss the memory complexity for storing intermediate node features in each layer since the memory footprint of the network parameters is negligible. All prior works suffer from memory consumption with respect to the number of layers, while the memory consumption of our methods is independent of the depth. Our methods can also be combined with mini-batch sampling methods to further reduce the memory complexity with respect to the number of nodes. We also include the parameter complexity and time complexity in Table <ref type="table" target="#tab_7">4</ref>. Note that although the </p><formula xml:id="formula_16">(LN D) O(LD 2 ) O(L A 0 D + LN D 2 ) GraphSAGE O(R L BD) O(LD 2 ) O(R L N D 2 ) VR-GCN O(LN D) O(LD 2 ) O(L A 0 D + LN D 2 + R L N D 2 ) FastGCN O(LRBD) O(LD 2 ) O(RLN D 2 ) Cluster-GCN O(LBD) O(LD 2 ) O(L A 0 D + LN D 2 ) GraphSAINT O(LBD) O(LD 2 ) O(L A 0 D + LN D 2 ) Weight-tied GNN O(LN D) O(D 2 ) O(L A 0 D + LN D 2 ) RevGNN O(N D) O(LD 2 ) O(L A 0 D + LN D 2 ) WT-RevGNN O(N D) O(D 2 ) O(L A 0 D + LN D 2 ) DEQ-GNN O(N D) O(D 2 ) O(K A 0 D + KN D 2 ) RevGNN + Subgraph Sampling O(BD) O(LD 2 ) O(L A 0 D + LN D 2 ) WT-RevGNN + Subgraph Sampling O(BD) O(D 2 ) O(L A 0 D + LN D 2 ) DEQ-GNN + Subgraph Sampling O(BD) O(D 2 ) O(K A 0 D + KN D 2 )</formula><p>time complexity of RevGNN is the same as for the vanilla GNNs, it has a larger constant term. This is due to the input that needs to be reconstructed during the backward pass.</p><p>Both the memory complexity and parameter complexity of WT-RevGNN and DEQ-GNN is independent of L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Ablation Studies</head><p>Comparison with SGC &amp; SIGN on Ogbn-products. We compare RevGNN with SGC <ref type="bibr" target="#b64">(Wu et al., 2019)</ref> and SIGN <ref type="bibr" target="#b17">(Frasca et al., 2020)</ref> on ogbn-products. The test accuracies (%) of SGC, SIGN and RevGNN on ogbn-products are: 74.87 ± 0.25 (SGC), 77.60 ± 0.13 (SIGN) and 82.16 ± 0.15 (RevGNN) respectively. The results of SGC and SIGN are obtained from the SIGN paper <ref type="bibr" target="#b17">(Frasca et al., 2020)</ref>. We train the RevGNN with 56 layers and 160 hidden channels in a random-clustering mini-batch training fashion. We find that RevGNN outperforms SGC and SIGN by a large margin.</p><p>Results of RevGNN on Ogbg-ppa. We trained ResGNN and RevGNN with 28 layers and similar number of parameters (∼ 2.3M) on the ogbg-ppa dataset. Both achieve around 77% test accuracy, but RevGNN uses only 16% GPU memory compared to ResGNN.</p><p>Comparison of Normalizations on Ogbg-molhiv. We conduct an ablation study on ogbg-molhiv for comparing using BatchNorm <ref type="bibr" target="#b31">(Ioffe &amp; Szegedy, 2015)</ref> or GraphNorm <ref type="bibr" target="#b8">(Cai et al., 2020)</ref> for RevGNN on graph property prediction. We train RevGNN with 14 layers and 256 hidden channels. Dropout layers with a rate of 0.3 are used. Learnable SoftMax aggregation functions <ref type="bibr" target="#b41">(Li et al., 2020)</ref> are used for message aggregation. The RevGNN with GraphNorm achieves 78.62 ROC-AUC which outperforms the RevGNN with BatchNorm (77.82 ROC-AUC) slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>This work addresses a fundamental bottleneck of current deep GNNs: their high GPU memory consumption. We investigate several techniques to reduce the memory complexity with respect to network depth (L) from O(L) to O(1). In particular, reversible GNNs enable networks that are an order of magnitude deeper than current SOTA models. Since there is no memory cost associated with depth, the width can be increased for additional representational power.</p><p>As a result, we can train over-parameterized networks that significantly outperform current models on standard benchmarks while consuming less memory.</p><p>However, this comes at an additional cost in training time. While this cost is less than 40%, it can extend the training time by several days on very large datasets. In addition, current GNNs are usually trained for thousands of epochs to achieve SOTA performance. Reducing the training time of GNNs is an interesting avenue for future research. Please refer to the appendix for more details about the opportunities for future work and further discussion on topics like gradient checkpointing, model parallelism, etc. In the appendix, we also provide all the experimental details and further ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grouping of RevGNN</head><p>Grouped convolution is an effective way to reduce the parameter complexity in CNNs. We provide an ablation study to show how grouping reduces the number of parameters of RevGNNs. We conduct experiments on the ogbn-proteins dataset with different group sizes and report the results in Table <ref type="table" target="#tab_8">5</ref>. The number of hidden channels for all of these models is set to 224. We find that a larger group size reduces the number of parameters. As the group size increases from 2 to 4, the number of parameters reduces by more than 30%.</p><p>The performance of models with 3 to 56 layers decreases slightly. The 112-layer networks achieve the same performance while the model with group size 4 uses only around 67% parameters compared to the model with group size 2. However, we observe that the GPU memory usage increases from 7.30 GB to 11.05 GB as the group size increases from 2 to 4 with our current implementation. We conjecture that this is due to our inefficient implementation. Optimizing our code for larger group sizes and conducting a more rigorous analysis is an interesting avenue for future investigation. We conduct experiments on three OGB datasets <ref type="bibr" target="#b27">(Hu et al., 2020)</ref> including ogbn-proteins, ogbn-arxiv and ogbnproducts. We follow the standard data splits and evaluation protocol of OGB 1.2.4. Please refer to the OGB website<ref type="foot" target="#foot_0">1</ref> for more details. Our code implementation relies on the deep learning framework Pytorch 1.6.0. We use Pytorch Geometric 1.6.1 <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019)</ref> for all experiments except for the experiments with GATs where we use DGL 0.5.3 <ref type="bibr" target="#b62">(Wang et al., 2019a)</ref>. The reversible module is implemented based on MemCNN <ref type="bibr" target="#b37">(Leemput et al., 2019)</ref>. The deep equilibrium module is implemented based on DEQ <ref type="bibr" target="#b1">(Bai et al., 2019)</ref>.  Multi-view Inference on Ogbn-proteins. To further improve the evaluation results, we propose multi-view inference which reduces the negative effects of random partitioning and noisy neighbors. During different inference passes, each vertex will see a different set of neighbors. We refer to this as multi-view inference and implement it by partitioning the graphs into different subgraphs in each inference pass. In Table <ref type="table" target="#tab_10">7</ref>, we find that performing inference with more views yields better results. We observe a substantial improvement with increasing number of views for both the RevGNN-Deep and RevGNN-Wide models. The results increase by about 0.4% in terms of ROC-AUC going from 1 view to 10 views. We also observe that a smaller number of partitions is favorable for evaluation. To reduce memory cost, automatic mixed precision<ref type="foot" target="#foot_1">2</ref> by NVIDIA is used for inference.</p><p>Ogbn-arxiv. The directed graph is converted into an undirected graph and self-loops are added. We use the full-batch setting for both training and testing. For the GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref>, SAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref> and GEN <ref type="bibr" target="#b41">(Li et al., 2020</ref>) models, batch normalization and dropout with a rate of 0.5 is applied to each layer and the Adam optimizer with a learning rate of 0.001 is used to train the models for 2000 epochs. The GAT-based <ref type="bibr" target="#b60">(Veličković et al., 2018)</ref> models are implemented based on the OGB leaderboard submission GAT + norm. adj. + label reuse<ref type="foot" target="#foot_2">3</ref> . The RevGAT models with self-knowledge distillation are implemented based on the submission GAT + label reuse + self KD<ref type="foot" target="#foot_3">4</ref> . The teacher models and student models have the same architecture. A knowledge distillation loss is added to the student model to minimize the Kullback-Leibler divergence between the teacher's predictions and the student's predictions during training. Please refer to the Github repositories for more details about the implementation.</p><p>Ogbn-products. Self-loops are added to the graph. We compare RevGNNs with full-batch training and mini-batch training. For mini-batch training, the graph is randomly partitioned into 10 subgraphs and one subgraph is sampled at each SGD step. We use full-batch testing in both scenarios. Batch normalization and dropout with a rate of 0.5 are used for each GNN block. The model is trained using the Adam optimizer with a learning rate of 0.001 for 1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. GPU Memory Measurement</head><p>In all the experiments, the GPU memory usage is measured as the peak GPU memory during the first training epoch. Note that the measured GPU memory is larger than the GPU memory for storing node features due to the intermediate computation and network parameters. We consider the peak GPU memory usage as a practical metric since it is the bottleneck for training neural networks. As is common practice, we use torch.cuda.max memory allocated() for the memory measurement. However, note that the measured peak GPU memory obtained using torch.cuda.max memory allocated() is usually smaller than the actual peak GPU memory obtained with NVIDIA-SMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Correlation of Model Predictions</head><p>We of RevGNN. For 112-layer models with 64 hidden channels, ResGNN with gradient checkpointing consumes 2.5X the memory compared to <ref type="bibr">RevGNN (5.22 G vs.</ref> 2.09 G) while reaching similar performance on ogbn-proteins with similar training time. Model parallelism is orthogonal to our approach. It would be interesting to investigate model parallelism to make RevGNN even wider with multiple GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Going Deeper and Datasets</head><p>In our experiments, we find that going deeper is very effective on ogbn-proteins. It would probably be beneficial to pre-train overparameterized GNNs on larger-scale protein datasets and then apply the pre-trained models to scientific applications such as drug discovery, protein structure prediction and gene-disease associations. For the other datasets such as ogbn-products and ogbn-arxiv, we observe less improvement when going very deep. It is still unclear what kind of datasets benefit more from depth and overparameterization. Investigating the relationship between overparameterization and factors such as dataset size, graph modality and graph learning task is an important direction of future work to better understand when overparameterized models are beneficial. We also anticipate that overparameterized GNNs will be a promising solution to even larger datasets such as OGB-LSC <ref type="bibr" target="#b28">(Hu et al., 2021)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure1. ROC-AUC score vs. GPU memory consumption on the ogbn-proteins dataset. We find that deep reversible GNNs are very powerful and outperform existing models by a margin; our best models are RevGNN-Deep and RevGNN-Wide. We also compare reversible connections (RevGNN-x), weight-tying (WTx), and equilibrium models (DEQ-x) for 112-layer deep GNNs (x denotes the number of channels per layer). Reversible models consistently achieve the same or better performance as the baseline using only a fraction of the memory. Weight-tied and equilibrium models offer a good performance to parameter efficiency trade-off. Datapoint size is proportional to √ p, where p is the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. GPU memory vs. number of layers/iterations for Res-GNN and DEQ-GNN. Datapoints are annotated with the ROC-AUC score of the model and their size is proportional to √ p, where p is the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. GPU memory consumption vs. number of layers for ResGNN<ref type="bibr" target="#b41">(Li et al., 2020)</ref> and RevGNN with full-batch and mini-batch training on the ogbn-products. RevGNN uses constant memory for deeper networks with more parameters and better performance. We use a width of 128 and 160 for ResGNN and RevGNN, respectively, to ensure a similar number of parameters per network depth. Datapoints are annotated with the accuracy of the model and their size is proportional to √ p, where p is the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>perform a correlation analysis on model predictions of RevGNN, Weight-tied RevGNN and DEQ-GNN. The pearson correlations of RevGNN with 1000 layers, WT-RevGNN-224 with 7 layers and DEQ-GNN-224 with 56 iterations are: 0.8571 (RevGNN vs. WT-RevGNN), 0.8565 (RevGNN vs. DEQ-GNN) and 0.8948 (WT-RevGNN vs. DEQ-GNN). which is still higher than the memory complexity O(N D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure2. GPU memory consumption vs. number of layers for ResGNN<ref type="bibr" target="#b41">(Li et al., 2020)</ref> and RevGNN, our adaptation with reversible connections. RevGNN uses constant memory for deeper networks with more parameters and better performance. We use width 64 and 80 for ResGNN and RevGNN, respectively, to ensure a similar number of parameters per network depth. Datapoints are annotated with the ROC-AUC score of the model and their size is proportional to √ p, where p is the number of parameters. Since the parameters are shared across GNN layers, the number of parameters stays constant as the number of layers increases. For example, with 112 layers, WT-RevGNN-224 has only 337k parameters, while RevGNN-224 has 17.1M parameters. However, training time and GPU memory consumption are similar while the performance of the weight-tied model is worse (85.28% vs. 87.41%) due to diminishing returns after more than 7 layers. Similar to the results in Figure2, this is evidence of a clear correlation between the number of parameters and performance.Figure3. GPU memory consumption vs. number of layers for WT-ResGNN and WT-RevGNN. For a given filter size, the number of parameters is constant regardless of network depth. Our reversible architecture has a constant memory footprint regardless of network depth, while yielding similar accuracy. Datapoints are annotated with the ROC-AUC score of the model and their size is proportional to √ p, where p is the number of parameters.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell cols="2">ResGNN-64 (Baseline)</cell><cell>85.94</cell><cell>Out of Memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RevGNN-80 (Ours)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell></cell><cell cols="2">RevGNN-224 (Ours)</cell><cell></cell></row><row><cell>GPU Memory (GB)</cell><cell>8 16 32 24</cell><cell>84.95</cell><cell cols="2">85.49 WT-ResGNN-64 (Baseline) 85.33 WT-RevGNN-80 (Ours) WT-RevGNN-224 (Ours)</cell><cell>83.35</cell><cell>85.37 82.91</cell><cell>85.28 11GB 83.3</cell><cell>GPU Memory (GB)</cell><cell>0 8 16</cell><cell>3 83.47 83.32 85.09</cell><cell>7 84.65 84.71 85.16 14 84.96 85.68 86.62</cell><cell cols="2">28 Number of Layers 56 112 85.31 85.96 85.97 86.68 85.26 86.05 86.9 87.02</cell><cell>224 86.23 87.33</cell><cell>448 87.41 11GB 1001 86.83 87.06</cell></row><row><cell></cell><cell></cell><cell>82.55</cell><cell>83.41</cell><cell>83.67</cell><cell>85.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPU Memory (GB)</cell><cell>0 0 8 16 32 24</cell><cell cols="13">Reversible GNN. Our reversible GNN enables training of very deep networks with constant memory footprint, as illustrated in Figure 2 (RevGNN). We use a group size of 2; since grouping reduces the number of parameters, RevGNN-80 has roughly the same number of parameters as ResGNN-64 for the same number of layers. While the baseline model ResGNN-64 cannot go beyond 112 layers due to memory limitations, RevGNN-80 can go to more than 1000 layers without additional memory cost and achieves much better accuracy (87.06% ROC-AUC). We can invest the saved GPU memory to increase the network width and train a higher-capacity model. This model is not only deep (448 layers) but also wide (224 channels) and further improves performance (87.41% ROC-AUC). Weight-tied GNN. We compare the weight-tied ResGNN (WT-ResGNN) and its reversible counterpart RevGNN (WT-RevGNN) in Figure 3. Both models have approximately the same number of parameters and achieve a similar accuracy while the reversible model uses significantly less GPU mem-ory. 3 7 14 28 56 112 Number of Layers 82.76 83.28 83.53 83.1 83.07 83.46 3 7 14 28 56 112 Number of Layers/Iterations 82.76 83.41 83.67 83.35 82.91 83.3 79.04 82.82 82.88 83.66 83.17 83.38 81.67 85.06 85.6 85.32 85.84 85.55 11GB WT-ResGNN-64 (Baseline) DEQ-GNN-64 (Ours) DEQ-GNN-224 (Ours)</cell></row></table><note>ResGNN with 112 layers and a channel size of 64 achieves 85.94% ROC-AUC on ogbn-proteins. However, the memory consumption of ResGNN-64 increases linearly with the number of layers. ResGNN-64 runs out of memory beyond 112 layers, making it impossible to investigate deeper models with current commodity hardware.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Results on the ogbn-proteins dataset compared to SOTA. RevGNN-Deep has 1001 layers with 80 channels each. It achieves SOTA performance with minimal GPU memory for training. RevGNN-Wide has 448 layers with 224 channels each. It achieves the best accuracy while consuming a moderate amount of GPU memory.</figDesc><table><row><cell>Model</cell><cell>ROC-AUC ↑ Mem ↓ Params</cell></row><row><cell>GCN (Kipf &amp; Welling)</cell><cell>72.51 ± 0.35 4.68 96.9k</cell></row><row><cell cols="2">GraphSAGE (Hamilton et al.) 77.68 ± 0.20 3.12 193k</cell></row><row><cell>DeeperGCN (Li et al.)</cell><cell>86.16 ± 0.16 27.1 2.37M</cell></row><row><cell>UniMP (Shi et al.)</cell><cell>86.42 ± 0.08 27.2 1.91M</cell></row><row><cell>GAT (Veličković et al.)</cell><cell>86.82 ± 0.21 6.74 2.48M</cell></row><row><cell cols="2">UniMP+CEF (Shi et al.) 86.91 ± 0.18 27.2 1.96M</cell></row><row><cell>Ours (RevGNN-Deep)</cell><cell>87.74 ± 0.13 2.86 20.03M</cell></row><row><cell>Ours (RevGNN-Wide)</cell><cell>88.24 ± 0.15 7.91 68.47M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Results on the ogbn-arxiv dataset compared to SOTA. RevGCN-Deep has 28 layers with 128 channels each. It achieves SOTA performance with minimal GPU memory. RevGAT-Wide has 5 layers with 1068 channels each. RevGAT-SelfKD denotes the student models with 5 layers and 768 channels. It achieves the best accuracy while consuming a moderate amount of GPU memory.</figDesc><table><row><cell>Model</cell><cell>ACC ↑ Mem ↓ Params</cell></row><row><cell cols="2">GraphSAGE (Hamilton et al.) 71.49 ± 0.27 1.99 219k</cell></row><row><cell>GCN (Kipf &amp; Welling)</cell><cell>71.74 ± 0.29 1.90 143k</cell></row><row><cell>DAGNN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Results with different GNN operators on the ogbnarxiv. All GAT models use label propagation. #L and #Ch denote the number of layers and channels respectively. Baselines are in italic.</figDesc><table><row><cell>Model</cell><cell cols="2">#L #Ch</cell><cell>ACC ↑</cell><cell cols="2">Mem ↓ Params</cell></row><row><cell cols="5">ResGCN 28 128 72.46 ± 0.29 11.15</cell><cell>491k</cell></row><row><cell cols="4">RevGCN 28 128 73.01 ± 0.31</cell><cell>1.84</cell><cell>262k</cell></row><row><cell cols="4">RevGCN 28 180 73.22 ± 0.19</cell><cell>2.73</cell><cell>500k</cell></row><row><cell cols="4">ResSAGE 28 128 72.46 ± 0.29</cell><cell>8.93</cell><cell>950k</cell></row><row><cell cols="4">RevSAGE 28 128 72.69 ± 0.23</cell><cell>1.17</cell><cell>491k</cell></row><row><cell cols="4">RevSAGE 28 180 72.73 ± 0.10</cell><cell>1.57</cell><cell>953k</cell></row><row><cell cols="5">ResGEN 28 128 72.32 ± 0.27 21.63</cell><cell>491k</cell></row><row><cell cols="4">RevGEN 28 128 72.34 ± 0.18</cell><cell>4.08</cell><cell>262k</cell></row><row><cell cols="4">RevGEN 28 180 72.93 ± 0.10</cell><cell>5.67</cell><cell>500k</cell></row><row><cell>ResGAT</cell><cell>5</cell><cell cols="2">768 73.76 ± 0.13</cell><cell>9.96</cell><cell>3.87M</cell></row><row><cell>RevGAT</cell><cell>5</cell><cell cols="2">768 74.02 ± 0.18</cell><cell>6.30</cell><cell>2.10M</cell></row><row><cell>RevGAT</cell><cell cols="3">5 1068 74.05 ± 0.11</cell><cell>8.49</cell><cell>3.88M</cell></row></table><note>we can use our techniques in conjunction with mini-batch training to further optimize memory. We conduct an ablation study on the ogbn-products dataset<ref type="bibr" target="#b27">(Hu et al., 2020)</ref> with full-batch training and a simple random-clustering minibatch training for ResGNNs and RevGNNs. The test results are reported in Figure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparison of complexities. L is the number of layers, D is the number of hidden channels, N is of the number of nodes, B is the batch size of nodes and R is the number of sampled neighbors of each node. K is the maximum Broyden iterations.</figDesc><table><row><cell>Method</cell><cell>Memory</cell><cell>Params</cell><cell>Time</cell></row><row><cell>Full-batch GNN</cell><cell>O</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Ablation of the group size of group reversible GNNs on the ogbn-protein dataset. L is the number of layers. The number of hidden channels is 224 for all the models.</figDesc><table><row><cell></cell><cell cols="2">Group=2</cell><cell cols="2">Group=4</cell></row><row><cell cols="5">#L Params ROC-AUC ↑ Params ROC-AUC ↑</cell></row><row><cell>3</cell><cell>490k</cell><cell>85.09</cell><cell>339k</cell><cell>84.86</cell></row><row><cell>7</cell><cell>1.1M</cell><cell>85.68</cell><cell>750k</cell><cell>85.25</cell></row><row><cell>14</cell><cell>2.2M</cell><cell>86.62</cell><cell>1.5M</cell><cell>85.79</cell></row><row><cell>28</cell><cell>4.3M</cell><cell>86.68</cell><cell>2.9M</cell><cell>86.30</cell></row><row><cell>56</cell><cell>8.6M</cell><cell>86.90</cell><cell>5.8M</cell><cell>86.76</cell></row><row><cell cols="2">112 17.2M</cell><cell>87.02</cell><cell>11.5M</cell><cell>87.09</cell></row><row><cell cols="5">B. Experimental Details and More Ablations</cell></row><row><cell cols="3">B.1. Datasets and Frameworks</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Results on the ogbn-proteins dataset for various 112layer networks. Note that DEQ-GNN always has only a single layer that approximates an infinitely deep network. Each network is trained on one V100 GPU with 32GB of memory. The column Mem reports the GPU memory in GB, Params reports the number of model parameters, and Time reports the training time in days. The node features are initialized through aggregating connected edge features by a sum aggregation at the first layer. We use random partitioning for mini-batch training. The number of partitions is set to 10 for training and 5 for validation for all the ablated models. One subgraph is sampled at each SGD step. One layer normalization is used in the GNN block. Dropout with a rate of 0.1 is used for each layer. We use max as the message aggregator. Each model is trained for 2000 epochs using the Adam optimizer with a learning rate of 0.001.</figDesc><table><row><cell cols="2">Baselines are in italic.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">#Ch ROC-AUC ↑ Mem ↓ Params Time ↓</cell></row><row><cell>ResGNN</cell><cell>64</cell><cell>85.94</cell><cell cols="2">27.1 2.37M 1.3</cell></row><row><cell>ResGNN</cell><cell>224</cell><cell>-</cell><cell>OOM 28.4M</cell><cell>-</cell></row><row><cell cols="2">WT-ResGNN 64</cell><cell>83.30</cell><cell>27.4 51.2k</cell><cell>1.2</cell></row><row><cell cols="2">WT-ResGNN 224</cell><cell>-</cell><cell>OOM 537k</cell><cell>-</cell></row><row><cell cols="2">DEQ-GNN 64</cell><cell>83.17</cell><cell>2.22 51.3k</cell><cell>1.3</cell></row><row><cell cols="2">DEQ-GNN 224</cell><cell>85.84</cell><cell>7.60 537k</cell><cell>2.9</cell></row><row><cell>RevGNN</cell><cell>64</cell><cell>85.48</cell><cell cols="2">2.09 1.46M 1.8</cell></row><row><cell>RevGNN</cell><cell>80</cell><cell>85.97</cell><cell cols="2">2.56 2.25M 2.2</cell></row><row><cell cols="2">RevGNN 224</cell><cell>87.02</cell><cell cols="2">7.30 17.1M 4.9</cell></row><row><cell cols="2">WT-RevGNN 64</cell><cell>82.89</cell><cell>1.60 35.0k</cell><cell>1.7</cell></row><row><cell cols="2">WT-RevGNN 80</cell><cell>83.46</cell><cell>2.08 51.4k</cell><cell>2.0</cell></row><row><cell cols="2">WT-RevGNN 224</cell><cell>85.28</cell><cell>5.55 337k</cell><cell>4.8</cell></row><row><cell cols="4">B.2. Hyperparameters and Experimental Settings</cell><cell></cell></row><row><cell cols="5">We describe all important hyperparameters and training</cell></row><row><cell cols="5">settings that were not mentioned in the main paper for re-</cell></row><row><cell cols="5">producibility. The settings are slightly different for each</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Ogbn-proteins. Ablations on Ogbn-proteins. A detailed comparison of</cell></row><row><cell cols="5">ResGNN, Weight-tied ResGNN, DEQ-GNN, RevGNN and</cell></row><row><cell cols="5">Weight-tied RevGNN is shown in Table 6. Except for the</cell></row><row><cell cols="5">DEQ-GNN, all the other models have an explicit depth of</cell></row><row><cell cols="5">112 layers. The reversible connections reduce the mem-</cell></row><row><cell cols="5">ory consumption significantly and enable training of wider</cell></row><row><cell cols="5">RevGNNs. A 112-layer RevGNN achieves the best per-</cell></row><row><cell cols="5">formance (87.02 ROC-AUC) among the compared models.</cell></row></table><note>DEQ-GNN with 64 channels and WT-RevGNN with 80 channels have a similar number of parameters and memory consumption and also perform similarly. However, training DEQ-GNN is significantly faster than training WT-RevGNN (1.3 days vs. 2 days).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Ablations for multi-view inference with RevGNN-Deep and RevGNN-Wide on the ogbn-proteins dataset. L, Ch, Views and Parts denote the numbers of layers, channels, views and parts respectively. Doing inference with more views and less parts is favorable.</figDesc><table><row><cell>Model</cell><cell cols="4">#L #Ch #Views #Parts ROC-AUC ↑</cell></row><row><cell cols="2">RevGNN-Deep 1001 80</cell><cell>1</cell><cell>3</cell><cell>87.29 ± 0.16</cell></row><row><cell cols="2">RevGNN-Deep 1001 80</cell><cell>5</cell><cell>3</cell><cell>87.68 ± 0.13</cell></row><row><cell cols="2">RevGNN-Deep 1001 80</cell><cell>10</cell><cell>3</cell><cell>87.74 ± 0.13</cell></row><row><cell cols="2">RevGNN-Wide 448 224</cell><cell>1</cell><cell>3</cell><cell>87.84 ± 0.21</cell></row><row><cell cols="2">RevGNN-Wide 448 224</cell><cell>5</cell><cell>3</cell><cell>88.20 ± 0.16</cell></row><row><cell cols="3">RevGNN-Wide 448 224 10</cell><cell>3</cell><cell>88.24 ± 0.15</cell></row><row><cell cols="2">RevGNN-Wide 448 224</cell><cell>1</cell><cell>3</cell><cell>87.84 ± 0.21</cell></row><row><cell cols="2">RevGNN-Wide 448 224</cell><cell>1</cell><cell>5</cell><cell>87.62 ± 0.18</cell></row><row><cell cols="2">RevGNN-Wide 448 224</cell><cell>1</cell><cell cols="2">10 87.23 ± 0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://ogb.stanford.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://developer.nvidia.com/automatic-mixed-precision</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/Espylapiza/dgl/tree/master/examples /pytorch/ogb/ogbn-arxiv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/ShunliRen/dgl/tree/master/examples /pytorch/ogb/ogbn-arxiv</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Shaojie Bai, researchers at Intel ISL, the reviewers, and area chairs for their helpful suggestions. This project is partially funded by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research under Award No. OSR-CRG2019-4033.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale deep equilibrium models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A class of methods for solving nonlinear simultaneous equations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Broyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">92</biblScope>
			<biblScope unit="page" from="577" to="593" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Graphnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03294</idno>
		<title level="m">A principled approach to accelerating graph neural network training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometrically principled connections in graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11415" to="11424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4094" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<title level="m">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An introduction to metric spaces and fixed point theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kirk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Memcnn: A python/pytorch package for creating memory-efficient invertible neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C V</forename><surname>Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manniesing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<idno type="ISSN">2475-9066</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page">1576</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Cayleynets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><surname>Deepgcns</surname></persName>
		</author>
		<title level="m">Making gcns go as deep as cnns. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph normalizing flows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The role of over-parametrization in generalization of neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: when experts are not enough</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Optimization of graph neural networks: Implicit acceleration by skip connections and more depth</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04550</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
