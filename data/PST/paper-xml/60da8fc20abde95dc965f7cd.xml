<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blocking-based Neighbor Sampling for Large-scale Graph Neural Networks</title>
				<funder ref="#_DCNytzE">
					<orgName type="full">NSFC-NRF Joint Research Project</orgName>
				</funder>
				<funder ref="#_Qx7f8qb">
					<orgName type="full">NSFC Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai-Lang</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory for Novel Software Technology Collaborative Innovation Center of Novel Software Technology and Industrialization Department of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
							<email>liwujun@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory for Novel Software Technology Collaborative Innovation Center of Novel Software Technology and Industrialization Department of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Blocking-based Neighbor Sampling for Large-scale Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The exponential increase in computation and memory complexity with the depth of network has become the main impediment to the successful application of graph neural networks (GNNs) on largescale graphs like graphs with hundreds of millions of nodes. In this paper, we propose a novel neighbor sampling strategy, dubbed blocking-based neighbor sampling (BNS), for efficient training of GNNs on large-scale graphs. Specifically, BNS adopts a policy to stochastically block the ongoing expansion of neighboring nodes, which can reduce the rate of the exponential increase in computation and memory complexity of GNNs. Furthermore, a reweighted policy is applied to graph convolution, to adjust the contribution of blocked and non-blocked neighbors to central nodes. We theoretically prove that BNS provides an unbiased estimation for the original graph convolution operation. Extensive experiments on three benchmark datasets show that, on large-scale graphs, BNS is 2? ? 5? faster than state-of-the-art methods when achieving the same accuracy. Moreover, even on the small-scale graphs, BNS also demonstrates the advantage of low time cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph has been widely used for describing unstructured data in real applications such as social networks, brain networks, molecular graphs, and knowledge graphs. Edges in graphs depict the complex relationships between samples, and rich relational information between samples is contained in graphs. Making good use of the rich relational information between samples in graphs has great potential in boosting the performance of traditional machine learning methods which are mainly designed for modeling independent and identically distributed <ref type="bibr">(i.i.d.)</ref> data. In addition, graph data is now widely available in many applications. Therefore, developing advanced graph learning algorithms is a topic of great interest.</p><p>Among various algorithms of representation learning for graphs, graph neural networks (GNNs) <ref type="bibr" target="#b7">[Gori et al., 2005;</ref><ref type="bibr" target="#b0">Bruna et al., 2014]</ref> have recently become the most successful and popular ones, due to their powerful ability in modeling complex relationships between samples. Although many advanced GNN models <ref type="bibr" target="#b12">[Kipf and Welling, 2017;</ref><ref type="bibr" target="#b8">Hamilton et al., 2017;</ref><ref type="bibr">Velickovic et al., 2018]</ref> have been proposed, most of them are limited to the successful application on smallscale graphs (e.g., graphs with hundreds of thousands of nodes). There are significant challenges in applying existing GNN methods to applications with large-scale graphs (e.g., graphs with hundreds of millions of nodes) because of the expensive computation and memory cost during the training process. Due to the iteratively dependent nature of nodes in GNNs, the number of nodes supporting the computation of output layer exponentially increases with the depth of network. Hence, the computation and memory complexity grow exponentially. Moreover, recent works <ref type="bibr">[Li et al., 2019;</ref><ref type="bibr" target="#b15">Verma and Zhang, 2020;</ref><ref type="bibr">Chen et al., 2020c]</ref> show the potential to improve the performance of GNN models as the network becomes deeper, which will undoubtedly exacerbate the problem of expensive cost on large-scale graphs. Nowadays, in order to speed up the training process, it is a dominant trend to perform training on GPUs. However, many GPUs have limited graphics memory, which hinders GNN models from training with large batch size and as a result leads to a sharp increase in time cost for training. Solutions for the above problem mainly include model simplification methods and sampling-based methods. For model simplification methods <ref type="bibr" target="#b16">[Wu et al., 2019;</ref><ref type="bibr" target="#b12">Klicpera et al., 2019;</ref><ref type="bibr">Chen et al., 2020b]</ref>, the main idea is to remove the nonlinear transformation between graph convolution layers such that the graph convolution on node features can be preprocessed before training. Although model simplification methods are efficient in training, as stated in <ref type="bibr">[Chen et al., 2020a]</ref>, it is still an open question whether simplified GNNs' expressive power can match that of the original GNNs. For sampling-based methods, existing works can be broadly categorized into node-wise sampling <ref type="bibr" target="#b8">[Hamilton et al., 2017;</ref><ref type="bibr">Chen et al., 2018a;</ref><ref type="bibr" target="#b6">Cong et al., 2020]</ref>, layer-wise sampling <ref type="bibr">[Chen et al., 2018b;</ref><ref type="bibr" target="#b10">Huang et al., 2018;</ref><ref type="bibr" target="#b18">Zou et al., 2019]</ref>, and subgraph sampling <ref type="bibr" target="#b5">[Chiang et al., 2019;</ref><ref type="bibr">Zeng et al., 2020]</ref>. For node-wise sampling, the main idea is to sample a number of neighbors for each node of each layer in a top-down manner. For layer-wise sampling, the main idea is to independently sample a number of nodes from a candidate set for each layer based on the importance probabilities of nodes. All connections between the nodes of two adjacent layers are used to perform approximate graph convolution. For subgraph sampling, the main idea is to sample a subgraph and feed it to GNN models before each round of mini-batch training. Although the above sampling strategies are applicable to large-scale GNNs, they have some deficiencies or limitations in terms of accuracy, total time cost, or memory cost. For example, existing node-wise sampling strategies need to sample a large number of neighbors for high accuracy, which will lead to a sharp increase in time cost. Layer-wise sampling strategies have a high time cost of preparing data (including sampling) and may suffer from sparse connection between two adjacent layers. Subgraph sampling strategies may also suffer from sparse connection in subgraphs.</p><p>In this paper, we propose a novel node-wise sampling strategy, called blocking-based neighbor sampling (BNS), for large-scale training of GNNs. The contributions of this paper are listed as follows:</p><p>? We propose a novel blocking mechanism in BNS to stochastically block the ongoing expansion of neighboring nodes, dramatically reducing the computation and memory complexity.</p><p>? We further propose a reweighted policy to adjust the contribution of blocked and non-blocked neighboring nodes to central nodes.</p><p>? We theoretically prove that BNS provides an unbiased estimation for the original graph convolution operation.</p><p>? Extensive experiments on large-scale graphs show that BNS is 2? ? 5? faster than existing state-of-the-art methods when achieving the same accuracy. Even on the small-scale graph, BNS also demonstrates the advantage of low time cost.</p><p>2 Notations and Problem Definition </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Definition</head><p>Suppose we have a graph with N nodes. Let A ? {0, 1} N ?N denote the adjacency matrix of the graph. A ij = 1 denotes there exists an edge between node i and node j, and A ij = 0 denotes there is no edge between them. Let X ? R N ?u denote the node feature matrix, where u denotes the dimension of node feature. Suppose the average number of neighbors per node in the graph is s. Suppose the mini-batch size of nodes at output layer is B. We use L to denote the layer number of GNNs.</p><p>We take GCN <ref type="bibr" target="#b12">[Kipf and Welling, 2017]</ref> as an example to describe the problem of the exponential increase in computation and memory complexity. Let A = A + I and ? = D -1 2 A D -1 2 , where D denotes the diagonal degree matrix of A and D ii = n j=1 A ij . Then GCN can be formulated as follows:</p><formula xml:id="formula_0">Z ( ) i * = j?N (i) ?ij H ( -1) j * , H ( ) i * = f (Z ( ) i * W ( ) ),<label>(1)</label></formula><p>where H (0) = X, f (?) is the activation function, N (i) denotes the set of neighbors of node i. W ( ) ? R r?r is a learnable parameter.</p><p>From (1), we can see that the output of a node at the Lth layer iteratively depends on the information of its 1, ? ? ? , Lhop neighbors. Such an iteratively dependent nature of nodes leads to the exponential increase in computation and memory complexity with the depth of network. Let r denote the feature dimension of hidden layer. Then, the computation and memory complexity during a mini-batch training are O(s L-1 ? (sBr + Br 2 )) and O(Lr 2 + s L ? Br), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Blocking-based Neighbor Sampling</head><p>In this section, we present the details of BNS. Firstly, we sample a fixed number of neighbors for each node at the current layer . Secondly, we adopt a policy to stochastically block the ongoing expansion of neighboring nodes at the preceding layers {1, ? ? ? , -1}. Note that once a node is blocked, all its ways out to all other nodes are blocked, and it is trapped at its current position. Thirdly, after sampling finishes, reweighted graph convolution is performed to obtain the outputs, in which a reweighted policy is adopted to adjust the contribution of blocked and non-blocked neighbors to central nodes. A visual illustration of BNS is presented in Figure <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling Algorithm</head><p>The entire sampling process is performed in a top-down manner, and it is summarized in Algorithm 1. Suppose V in denote a mini-batch of nodes in the output layer. Firstly, we sample s n neighbors for each node i at layer in line 5. Sample(N (i), s n ) in line 5 is an operation that uniformly samples s n elements from N (i). Then, we randomly select s n ? ? (0 ? ? ? 1) nodes from N (i) in line 6, and stop sampling neighbors for them at the preceding layers {1, ? ? ? , -1} via the operations in line 7 and line 13. Block(N (i), ?) in line 6 is an operation that uniformly samples |N (i)| ? ? elements from N (i) as blocked neighbors.</p><p>V b records the blocked nodes at the th layer, which are used in subsequent processing steps. The operations in line 10 and line 11 ensure that the blocked nodes are mapped to the same feature space as non-blocked nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reweighted Graph Convolution</head><p>We first reformulate Z ( ) i * in Equation ( <ref type="formula" target="#formula_0">1</ref>) to an expectation form:</p><p>Z</p><formula xml:id="formula_1">( ) i * = |N (i)| ? E j?p(j?N (i)|i) ?ij H ( -1) j * , (2) where p(j ? N (i)|i) is a uniform distribution over the neigh- bors of node i. |N (i)| denotes the number of elements in N (i).</formula><p>For blocked nodes, since their neighbor expansions are blocked, their estimation for Equation ( <ref type="formula">2</ref>) is less precise (having large variance) than non-blocked nodes. We can see that representations of blocked nodes carry little information about the input graph. Therefore, it is reasonable to increase the contribution of non-blocked nodes to the central nodes. We perform reweighted graph convolution to achieve this goal.</p><p>After Algorithm 1 is performed, reweighted graph convolution is formulated as follows. For readability, we denote</p><formula xml:id="formula_2">n i,1 = |N b (i)|, n i,2 = |N nb (i)| and n i = |N (i)|. ? i,1 = ? ? n i,1 + n i,2 n i,1 , ? i,2 = (1 -?) ? n i + ? i n i,2 ,<label>(3)</label></formula><formula xml:id="formula_3">? ij = ? i,1 ? n i n i,1 + n i,2 ? ?ij , ?j ? N nb (i) and i ? V nb , ? ij = ? i,2 ? n i n i,1 + n i,2 ? ?ij , ?j ? N b (i) and i ? V nb , ? ii = n i ? ?ii , i ? V b \V nb , Z ( ) i * ? j?N ( ) (i) ?ij H ( -1) j * := Z( ) i * , ?i ? V nb ? V b , (4) H ( ) i * = f ( Z( ) i * W ( ) ),<label>(5)</label></formula><p>where <ref type="formula">2</ref>), ?ij adopts a different weights, ? i,1 and ? i,2 , to adjust the contribution of non-blocked and blocked nodes to node i. In the following proposition, we prove that Z( ) is an unbiased estimation of Z ( ) i * , which makes our proposed reweighted graph convolution theoretically sound. In experiments, ? is set to 0.5 for convenience.</p><formula xml:id="formula_4">? ? [0, 1]. Compared with (|N (i)|/|N (i)|) ? ?ij in Equation (</formula><formula xml:id="formula_5">Proposition 1. Suppose H ( -1) is given. If N (i) is uni- formly sampled from N (i), N b (i) is uniformly sampled from N (i) and ? ? [0, 1], then Z( ) i * defined in Equation (4) is an unbiased estimation of Z ( ) i * .</formula><p>Proof. The proof can be found in the Appendix 1 .</p><p>1 The Appendix can be found in https://cs.nju.edu.cn/lwj/.</p><p>Algorithm 1 Sampling Algorithm Require: Mini-batch of nodes V in , the number of neighbors sampled for each node s n , ratio of blocked neighbors per node ?. Ensure:</p><formula xml:id="formula_6">{(V nb , V b , {(N nb (i), N b (i))} N i=1 )} L =1 1: V L nb = V in , V b = ? 2:</formula><p>Sample in a top-down manner: 3: for = L : 1 do 4:</p><p>for i ? V nb do 5:</p><formula xml:id="formula_7">N (i) = Sample(N (i), s n ) 6: N b (i) = Block(N (i), ?) 7: N nb (i) = N (i)\N b (i) 8: end for 9: for i ? V b do 10: N (i) = N (i) ? {i} 11: N b (i) = N b (i) ? {i} 12:</formula><p>end for 13:</p><formula xml:id="formula_8">V -1 nb = i?V nb N nb (i) 14: V -1 b = i?V nb N b (i) 15: V b = V b ? V -1 b 16: end for 3.3 Objective Function Let W = (1) , ? ? ? , W (L)</formula><p>} denote the learnable parameters defined in Equation ( <ref type="formula" target="#formula_3">5</ref>). ? = H (L) denotes the output of GNN models. For multi-class classification, f (?) in the last layer denotes the softmax function, while it denotes the sigmoid function for multi-label classification. The objective function for BNS is formulated as follows:</p><formula xml:id="formula_9">min W i?V c -Y ic log ?ic + ?/2 ? W ( ) 2 F , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where ? is a hyper-parameter for the regularization term of parameters W, V denotes the set of nodes in training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complexity Analysis</head><p>In this subsection, we compare the computation and memory complexity of different methods with those of BNS in a mini-batch training step, which is summarized in Table <ref type="table">1</ref>. For existing node-wise sampling methods NS <ref type="bibr" target="#b8">[Hamilton et al., 2017]</ref>, <ref type="bibr">VRGCN [Chen et al., 2018a]</ref> and MVS-GNN <ref type="bibr" target="#b6">[Cong et al., 2020]</ref>, they reduce the growth rate from s to s n , where s n is much smaller than s. In particular, VRGCN and MVS-GNN show that they can achieve comparable accuracy to NS with smaller s n . For layer-wise sampling method LADIES <ref type="bibr" target="#b18">[Zou et al., 2019]</ref> and subgraph sampling method <ref type="bibr">GraphSAINT [Zeng et al., 2020]</ref>, they reduce the computation and memory complexity to the level that is linear with the depth of network.</p><p>Although the above methods can achieve good performance in terms of accuracy, time cost, and memory cost on small-scale graphs (e.g., graphs with hundreds of thousands of nodes), they are not efficient or even not applicable for large-scale graphs (e.g., graphs with millions of nodes and hundreds of millions of nodes). Some problems and drawbacks existing in these methods are overlooked due to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation complexity</head><p>Memory complexity Non-sampling <ref type="bibr" target="#b12">[Kipf and Welling, 2017]</ref> O s L-1 (sBr + Br 2 ) O Lr 2 + s L ? Br NS <ref type="bibr" target="#b8">[Hamilton et al., 2017]</ref> O s L-1 n</p><formula xml:id="formula_11">? (snBr + Br 2 ) O Lr 2 + s L n ? Br VRGCN [Chen et al., 2018a] O s L-1 n ? ((sn + s) ? Br + Br 2 ) O Lr 2 + s L-1 n ? (s + sn)Br MVS-GNN [Cong et al., 2020] O s L-1 n ? ((sn + s) ? Br + Br 2 ) O Lr 2 + s L-1 n ? (s + sn)Br LADIES [Zou et al., 2019] O L ? (s l /N ) 2 ? A 0 + Ls l ? r 2 ) O Lr 2 + Ls l ? r GraphSAINT [Zeng et al., 2020] O L ? (sg/N ) 2 ? A 0 + Lsg ? r 2 ) O Lr 2 + Lsg ? r BNS (ours) O sL-1 n ? (snBr + (?/(1 -?) + 1) ? Br 2 ) O Lr 2 + sL-1 n ? snBr</formula><p>Table <ref type="table">1</ref>: Computation and memory complexity. s denotes the average number of neighbors per node in A. sn denotes the average number of neighbors sampled for each node. sn = sn ? (1 -?), where ? denotes the ratio of blocked nodes in BNS. s l denotes the average number of nodes per layer in layer-wise sampling. sg denotes the average number of nodes per subgraph in subgraph sampling. B = |Vin| denotes the mini-batch size of output layer. L is the number of layers in GNN models. r is the hidden dimension of networks.</p><p>lack of systematically experimental analysis on large-scale graphs. For example, even with low computation complexity, VRGCN, MVS-GNN and LADIES have a high time cost of preparing data (including sampling) before each round of mini-batch training. In addition, VRGCN brings a huge burden to the memory for storing all nodes' historical representations at each layer. MVS-GNN has the same complexity as the non-sampling method at the outer iteration, which might make the training infeasible on large-scale graphs because of running out of graphics memory. GraphSAINT faces the problem of sparse connection in subgraphs. Moreover, GraphSAINT adopts the non-sampling strategy at the evaluation and testing stage, which is also inefficient on large-scale graphs.</p><p>Similar to existing node-wise sampling methods, BNS reduces the growth rate from s to a small sn , where sn denotes the number of non-blocked neighbors per node. We will show that with a small sn , BNS can achieve comparable accuracy to NS with a large s n , while BNS has lower computation and memory complexity. Moreover, BNS has a low time cost of preparing data before each round of mini-batch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare BNS with other baselines on five node-classification datasets. BNS is implemented on the Pytorch platform <ref type="bibr" target="#b14">[Paszke et al., 2019]</ref> with Pytorch-Geometric Library [Fey and <ref type="bibr" target="#b7">Lenssen, 2019]</ref>. All experiments are run on a NVIDIA TitanXP GPU server with 12 GB graphics memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Ogbn-products, ogbn-papers100M and ogbn-proteins 2 are publicly available <ref type="bibr" target="#b9">[Hu et al., 2020]</ref>. Ogbn-products is a largescale dataset with millions of nodes. Ogbn-papers100M is a large-scale dataset with hundreds of millions of nodes. Ogbnproteins is a small-scale dataset with hundreds of thousands of nodes. Amazon and Yelp in GraphSAINT, are also used for evaluation. Due to space limitation, the information and results on Amazon and Yelp are moved to the Appendix. The statistics of datasets can be found in the Appendix.</p><p>2 https://ogb.stanford.edu/docs/nodeprop/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Settings</head><p>We compare BNS with <ref type="bibr">VRGCN [Chen et al., 2018a]</ref>, LADIES <ref type="bibr" target="#b18">[Zou et al., 2019]</ref> and <ref type="bibr">GraphSAINT [Zeng et al., 2020]</ref>, which are the state-of-the-art methods with node-wise sampling, layer-wise sampling and subgraph sampling, respectively. Additionally, we compare BNS with the classical node-wise sampling method NS <ref type="bibr" target="#b8">[Hamilton et al., 2017]</ref>. We do not compare BNS with MVS-GNN since MVS-GNN adopts the non-sampling strategy for training at the outer iteration, which leads to the problem of running out of graphics memory. Besides, comparisons with model simplification methods are moved to the Appendix due to space limitation. Since the original implementations of the above baselines cannot directly scale to the benchmark datasets in this paper, we re-implement them according to the corresponding authors' codes. For a fair comparison, implementations of all methods, including BNS, only differ in the sampling process. For all methods, GNN model is instantiated with GraphSAGE <ref type="bibr" target="#b8">[Hamilton et al., 2017]</ref>, since it can achieve good performance on the benchmark datasets. Note that sampling strategies and settings during inference are the same as those in the training stage for all methods except for GraphSAINT.</p><p>The hyper-parameters r, L, T (maximum epoch), ? and p (probability of dropout) are independent of sampling strategies, and hence they are set to be the same for different sampling strategies on one specific dataset. Empirically, r is set to 128 on all datasets, L is set to 5 on both ogbn-proteins and ogbn-products, and L is set to 3 on ogbn-papers100M. For T , it is set to 100 on both ogbn-products and ogbn-papers100M, and set to 1,000 on ogbn-proteins. For ? and p, the values of them are obtained by tuning with NS on the benchmark datasets. On ogbn-product, ? = 5 ? 10 -6 and p = 0.1. On ogbn-papers100M, ? = 5 ? 10 -7 and p = 0.1. On ogbnproteins, ? = 0 and p = 0. In BNS, we set ? to 0.5 for convenience and do not tune it. Adam <ref type="bibr" target="#b11">[Kingma and Ba, 2015]</ref> is used to optimize the model and the learning rate ? is set to 0.01. For all settings, experiments are run for 10 times with different initialization each time, and the mean results of 10 runs are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Criteria</head><p>The ultimate goal of sampling strategies for GNNs is to obtain high accuracy with a low time cost, not just to reduce time and memory cost to extreme cases at the expense of sac-   One reasonable way to evaluate the performance of different methods is to compare time cost when achieving the same accuracy. Since batch size has an important impact on time cost and accuracy, we design two kinds of experiments for fair comparison:</p><p>? The first experimental setting: On each dataset, for different methods, we train GNN model with the same batch size. All methods are run with the best setting that can achieve the best accuracy in this case.</p><p>? The second experimental setting: On each dataset, for different methods, we train GNN model with the maximum batch size that can achieve the best accuracy. All methods are run with the best setting that can achieve the best accuracy.</p><p>Detailed settings of each method can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Results on ogbn-products and ogbn-papers100M are summarized in Figure <ref type="figure" target="#fig_2">2</ref> and Table <ref type="table" target="#tab_1">2</ref>, from which we can draw the following conclusions. Firstly, when achieving the same accuracy under different settings, BNS is faster than all other methods. For example, from Figure <ref type="figure" target="#fig_2">2</ref> steps. We can see that the stochastically blocking policy is helpful for BNS to preserve local information around central nodes.</p><p>Results on ogbn-proteins, a relative small graph, are summarized in Figure <ref type="figure">3</ref> and Table <ref type="table" target="#tab_2">3</ref>, from which we can draw the following conclusions. Firstly, BNS is faster than all other methods when achieving the same accuracy under different settings. However, the gap of the time cost for achieving the same accuracy between BNS and other methods is small. The main reason is that neighboring expansions can easily cover the entire graph within a few layers or steps on small-scale graphs. Therefore, these methods have the same order of computation and memory complexity O(N sr + N r 2 ). Secondly, BNS can achieve the best performance in terms of accuracy with the fastest speed. This point can be drawn from Table <ref type="table" target="#tab_2">3</ref>, which is consistent with results in Figure <ref type="figure">3</ref>. Thirdly, once again, we observe that LADIES has a high time cost in preparing data. Finally, we observe that GraphSAINT (non-sampling strategy) achieves lower accuracy than NS, LADIES and BNS. This may be caused by the over-smoothing problem of GNNs <ref type="bibr">[Li et al., 2018;</ref><ref type="bibr" target="#b17">Xu et al., 2018;</ref><ref type="bibr" target="#b13">Oono and Suzuki, 2020]</ref>. This observation, in turn, shows that the stochasticity introduced by sampling can alleviate the over-smoothing problem of GNNs.</p><p>Summary. First, on large-scale graphs, BNS is 2? ? 5? faster than existing state-of-the-art methods when achieving the same accuracy. Compared with BNS, other methods have some deficiencies or limitations. For example, NS needs a large number of s n to achieve high accuracy. VRGCN and LADIES have a high time cost of preparing data, which are more expensive than performing forward and backward propagation. Second, even on the small-scale graph, BNS demonstrates the advantage of low time cost. Third, compared with other methods, BNS can achieve the best performance in accuracy with the minimum time cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We study the effectiveness of reweighted policy by setting ? i,1 = 1 and ? i,2 = 1 in Equation (3). With ? i,1 = 1 and ? i,2 = 1 in Equation (3), Equation ( <ref type="formula" target="#formula_3">5</ref>) is a plain Monte-Carlo approximation of Equation ( <ref type="formula">2</ref>). The results are presented in Table <ref type="table" target="#tab_3">4</ref>. From Table <ref type="table" target="#tab_3">4</ref>, we can conclude that reweighted policy enhances the ability of BNS in utilizing the information of blocked neighbors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>On large-scale graphs (e.g., graphs with hundreds of millions of nodes), existing sampling strategies have deficiencies or limitations in accuracy, time cost, or memory cost. Hence, designing an effective sampling strategy for efficient training of GNNs on large-scale graphs is still challenging. In this paper, we propose a novel neighbor sampling strategy, dubbed blocking-based neighbor sampling (BNS), for training GNNs on large-scale graphs. The main idea is to adopt a policy to stochastically block the ongoing expansion of neighbors, by which computation and memory complexity can be significantly reduced. Furthermore, reweighted graph convolution is proposed to adjust the contribution of blocked and nonblocked neighbors to central nodes. Extensive experiments on large-scale graphs show that, when achieving the same accuracy, BNS is 2? ? 5? faster than state-of-the-art methods. Experiments on the small-scale graph also demonstrate the advantage of BNS in terms of time cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visual illustration of BNS. Solid circles refer to nodes. The node within the inner dashed circle refers to the node of output layer. (a) We assume each node has 5 neighbors. (b) Black solid circles refer to blocked neighbors. The thickness of solid lines that connect two nodes indicates the magnitude of weights of nodes in reweighted graph convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test accuracy curves on ogbn-products and ogbn-papers100M. Methods that need more than one day to obtain the curves are omitted in the figures. b = |V |/B, where |V | denotes the number of nodes in training data and B is batch size. At each row, the first three figures present the results of the first experimental setting in Section 4.3. The last figure presents the results of the second experimental setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The ith row and the jth column of a matrix B are denoted as B i * and B * j , respectively. B ij denotes the element at the ith row and jth column in B. B 0 denotes the number of non-zero entries in B. B F denotes the Frobenius norm of B.</figDesc><table><row><cell>2.1 Notations</cell></row><row><cell>We use boldface uppercase letters, such as B, to denote matri-</cell></row><row><cell>ces.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on ogbn-products and ogbn-papers100M. Boldface letters denote the best results. Time presented in tables denotes the total training time of one run. "T1" refers to the time cost of preparing data. "T2" refers to the time cost of performing forward and backward propagation. The results in tables are obtained under the second experimental setting in the Section 4.3.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%) ?</cell><cell cols="2">ogbn-products Time (s) ?</cell><cell>T1+T2 (s)</cell><cell>Accuracy (%) ?</cell><cell cols="2">ogbn-papers100M Time (s) ?</cell><cell>T1+T2 (s)</cell></row><row><cell>NS</cell><cell>78.64 ? 0.17</cell><cell>5.5 ? 10 3</cell><cell cols="2">4.5 ? 10 3 + 9.6 ? 10 2</cell><cell>63.61 ? 0.13</cell><cell>2.5 ? 10 4</cell><cell>8.0 ? 10 3 + 1.7 ? 10 4</cell></row><row><cell>VRGCN</cell><cell>77.07 ? 0.49</cell><cell>1.2 ? 10 4</cell><cell cols="2">1.1 ? 10 4 + 1.5 ? 10 3</cell><cell>63.34 ? 0.12</cell><cell>2.2 ? 10 4</cell><cell>7.0 ? 10 3 + 1.5 ? 10 4</cell></row><row><cell>LADIES</cell><cell>78.96 ? 0.50</cell><cell>4.7 ? 10 3</cell><cell cols="2">4.5 ? 10 3 + 2.5 ? 10 2</cell><cell>63.25 ? 0.21</cell><cell>2.5 ? 10 4</cell><cell>1.2 ? 10 4 + 1.3 ? 10 4</cell></row><row><cell>GraphSAINT</cell><cell>78.95 ? 0.41</cell><cell>7.1 ? 10 3</cell><cell cols="2">4.5 ? 10 3 + 2.6 ? 10 3</cell><cell>61.60 ? 0.12</cell><cell>2.1 ? 10 4</cell><cell>8.0 ? 10 3 + 1.3 ? 10 4</cell></row><row><cell>BNS (ours)</cell><cell>80.14 ? 0.27</cell><cell cols="3">9.1 ? 10 2 7.3 ? 10 2 + 1.8 ? 10 2</cell><cell>63.88 ? 0.12</cell><cell cols="2">1.2 ? 10 4 4.3 ? 10 3 + 7.7 ? 10 3</cell></row><row><cell cols="5">rificing accuracy. In most cases, reducing memory can also</cell><cell></cell><cell></cell></row><row><cell cols="5">reduce the time cost since GNN model can perform training</cell><cell></cell><cell></cell></row><row><cell cols="5">with a larger batch size when the graphics memory cost is</cell><cell></cell><cell></cell></row><row><cell cols="5">lower. Hence, we omit the comparison of memory cost in</cell><cell></cell><cell></cell></row><row><cell cols="5">experiments. In a nutshell, the accuracy of GNN model and</cell><cell></cell><cell></cell></row><row><cell cols="5">time cost during training are presented to evaluate the perfor-</cell><cell></cell><cell></cell></row><row><cell cols="2">mance of different methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on ogbn-proteins.</figDesc><table><row><cell>(a), we can see that BNS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>o rew 79.12 ? 0.19 62.54 ? 0.14 79.40 ? 0.21 BNS 80.14 ? 0.27 63.88 ? 0.12 79.60 ? 0.29 Ablation study on reweighted policy. 'w/o rew' means BNS runs without reweighted policy.</figDesc><table><row><cell></cell><cell cols="3">Accuracy (%) or ROC-AUC (%)</cell></row><row><cell>Methods</cell><cell>ogbn-</cell><cell>ogbn-</cell><cell>ogbn-</cell></row><row><cell></cell><cell>products</cell><cell>papers100M</cell><cell>proteins</cell></row><row><cell>BNS w/</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by the <rs type="funder">NSFC-NRF Joint Research Project</rs> (No. <rs type="grantNumber">61861146001</rs>) and <rs type="funder">NSFC Project</rs> (No. <rs type="grantNumber">61921006</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DCNytzE">
					<idno type="grant-number">61861146001</idno>
				</org>
				<org type="funding" xml:id="_Qx7f8qb">
					<idno type="grant-number">61921006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Bruna</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FastGC-N: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On graph neural networks versus graph-augmented MLPs</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno>abs/2010.15116</idno>
	</analytic>
	<monogr>
		<title level="j">CoR-R</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable graph neural networks via bidirectional propagation</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-GCN: an efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Fey</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lenssen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2005">2019. 2019. 2005</date>
		</imprint>
	</monogr>
	<note>IEEE International Joint Conference on Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adavances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open graph benchmark: datasets for machine learning on graphs</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adavances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ba ; Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Klicpera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2019. 2019. 2018. 2018. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>IEEE/CVF International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Suzuki</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Velickovic et al., 2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova</title>
		<author>
			<persName><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>Adriana Romero, Pietro Li?, and Yoshua Bengio</publisher>
			<date type="published" when="2018">2019. 2019. 2018</date>
		</imprint>
	</monogr>
	<note>Adavances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Hongkuan</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ajitesh</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajgopal</forename><surname>Srivastava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Kannan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Prasanna</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adavances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
