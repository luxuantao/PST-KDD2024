<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Eliminating Cache Conflict Misses Through XOR-Based Placement Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Antonio</forename><surname>González</surname></persName>
							<email>antonio@ac.upc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat</orgName>
								<address>
									<addrLine>Politècnica de Catalunya c/ Jordi Girona 1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat</orgName>
								<address>
									<addrLine>Politècnica de Catalunya c/ Jordi Girona 1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nigel</forename><surname>Topham</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Edinburgh JCMB</orgName>
								<address>
									<addrLine>Kings Buildings</addrLine>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joan</forename><forename type="middle">M</forename><surname>Parcerisa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departament d&apos;Arquitectura de Computadors</orgName>
								<orgName type="institution">Universitat</orgName>
								<address>
									<addrLine>Politècnica de Catalunya c/ Jordi Girona 1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Eliminating Cache Conflict Misses Through XOR-Based Placement Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA703916314A8AD3227296D813C6AEFB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cache memory</term>
					<term>XOR-based placement functions</term>
					<term>conflict misses</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper makes the case for the use of XOR-based placement functions for cache memories. It shows that these XOR-mapping schemes can eliminate many conflict misses for direct-mapped and victim caches and practically all of them for (pseudo) two-way associative organizations. The paper evaluates the performance of XOR-mapping schemes for a number of different cache organizations: direct-mapped, set-associative, victim, hash-rehash, column-associative and skewed-associative. It also proposes novel replacement policies for some of these cache organizations. In particular, it presents a low-cost implementation of a pure LRU replacement policy which demonstrates a significant improvement over the pseudo-LRU replacement previously proposed. The paper shows that for a 8 Kbyte data cache, XOR-mapping schemes approximately halve the miss ratio for two-way associative and column-associative organizations. Skewed-associative caches, which already make use of XOR-mapping functions, can benefit from the LRU replacement and also from the use of more sophisticated mapping functions. For two-way associative, columnassociative and two-way skewed-associative organizations, XORmapping schemes achieve a miss ratio that is not higher than 1.10 times that of a fully-associative cache. XOR mapping schemes also provide a very significant reduction in the miss ratio for the other cache organizations, including the direct-mapped cache. Ultimately, the conclusion of this study is that XOR-based placement functions unequivocally provide highly significant performance benefits to most cache organizations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There are remarkably few papers on the use of alternative mapping schemes for cache memories. The first computers based on the HP Precision Architecture Processor <ref type="bibr" target="#b5">[6]</ref> made use of XOR-mapping functions in order to index the TLB. In these machines, the 11-bit TLB index was obtained by the exclusive OR of two 9-bit fields, one from the virtual page number and the other from the space ID, appended to two other bits of the space ID. Earlier machines that used a XOR-mapping function to index the TLB were the IBM 3033 [] and the Amdahl 470 [].</p><p>The use of XOR-mapping schemes in order to obtain a pseudorandom placement has been suggested by other authors as reported in []. In [], a comparison of a pseudo-random placemment against a set-associative one was performed. It concluded that random mapping has a small advantage in most cases, but that the advantage is not significant. We will show in this paper that for current workloads and cache organizations, this advantage can be very large.</p><p>Hashing the process ID with the address bits in order to index the cache memory was evaluated in <ref type="bibr" target="#b1">[2]</ref> for a multiprogrammed environment. Results were provided for just one trace, which shown that this scheme could reduce the miss ratio.</p><p>In practical systems, like the HP PA 7100, limited and undocumented use of XOR-mapping schemes has occurred, but there is currently no established body of published results analyzing the true benefits of alternative mapping schemes.</p><p>More recently, the use of XOR-mapping functions was proposed in skewed-associative caches <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b16">[17]</ref>. A two-way skewedassociative cache consists of two banks of the same size that are accessed simultaneously with two different hashing functions. In that paper, a family of mapping functions was defined as follows. Assume that the cache memory consists of a 2 l lines of 2 b bytes each. A memory address A=&lt;a n-1 ,a n-2 ,..., a 0 &gt; comprises the following fields: A=(A 3 , A 2 , A 1 , A 0 ) such that A 0 =&lt;a b-1 ,..., a 0 &gt;; A 1 =&lt;a l+b-2 ,..., a b &gt;; A 2 =&lt;a 2l+b-3 ,..., a l+b-1 &gt;; and A 3 =&lt;a n-1 ,..., a 2l+b-2 &gt;. Let ⊕ denote the bitwise exclusive OR; let • denote the bitwise AND operation and let be any (l-1)-bit number (a good choice for would be 1010...10); let T = 2 l -1-T. The family of twin XOR-based placement functions are defined as:</p><formula xml:id="formula_0">f 0 T : {0...2 n -1} → {0...2 l-1 -1} A=(A 3 , A 2 , A 1 , A 0 ) → ((A 2 •T)⊕A 1 , A 0 ) f 1 T : {0...2 n -1} → {0...2 l-1 -1} A=(A 3 , A 2 , A 1 , A 0 ) → ((A 2 •T)⊕A 1 , A 0 )</formula><p>In <ref type="bibr" target="#b15">[16]</ref>, it was proposed a pseudo-LRU replacement by associating a one-bit flag to each line in bank 0. If the requested data is found in bank 0, the corresponding line flag is set, whereas it reset if the data is found in bank 1. On a miss, the flag of the line selected in bank 0 is read and its value determines the bank where the missing data is to be placed.</p><p>Using a different workload from that used in this paper, it was observed that the miss ratio of the two-way skewed-associative cache was lower than that of a victim cache (with four lines in the victim buffer) and similar to the miss ratio of a four-way set associative cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Whenever a line of main memory is brought into cache a decision must be made on which line, or set of lines, in the cache will be candidates for storing that memory line. This line placement policy is one of the least researched aspects of cache design. Directmapped caches typically extract a field of bits from the address and use this to select one line from a set of . Whilst simple, and trivial to implement, this mapping function is not robust. The principal weakness of this function is its susceptibility to repetitive conflict misses. For example, if is the cache capacity and is the line size, then addresses and map to the same cache line if = . If and map to the same cache line, then addresses and are guaranteed to also map to identical cache lines, for any integer . There are two common cases when this happens:</p><p>• when accessing a stream of addresses if collides with , then there may be up to conflict misses in this stream.</p><p>• when accessing elements of two distinct arrays and , if collides with then will collide with , for any integer .</p><p>-way associativity can be used to alleviate such conflicts. However, if a working set contains conflicts on some cache line, set associativity can only eliminate at most of those conflicts. Our studies suggest that when conflict misses dominate, the critical factor is not a lack of associativity, but a defective line placement algorithm which fails to disperse data equitably between the available cache lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">XOR-mapping schemes</head><p>The use of XOR-mapping schemes has been studied extensively in the context of interleaved memories <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b20">[21]</ref> among others. In this paper we consider two types of XOR-based mapping schemes; those chosen in an ad hoc way based on common intuitive notions of how such schemes behave, and a scheme proposed by Rau <ref type="bibr" target="#b14">[15]</ref> which describes a method for constructing XOR mapping schemes based on polynomial arithmetic.</p><formula xml:id="formula_1">T T l 2 l C B a 1 a 2 a 1 B ⁄ mod C a 2 B ⁄ mod C a 1 a 2 a 1 k + a 2 k + k B ≥ A a 0 a 1 … a m , , , { } = a i a i k + m k - ( ) b 0 b 1 b 0 i [ ] b 1 j [ ] b 0 i k + [ ] b 1 j k + [ ] k B ≥ w p w &gt; w</formula><p>The former type of XOR-mapping computes a cache index by performing a bitwise XOR of two fields of the address of the requested data. We will refer to this type of schemes as bitwise XOR mapping. The family of mapping functions proposed in <ref type="bibr" target="#b15">[16]</ref> belong to this category.</p><p>In this paper we refer to Rau's scheme simply as polynomial mapping. Polynomial mapping can be understood by first considering address as a polynomial , the coefficients of which are in the Galois Field GF <ref type="bibr" target="#b1">(2)</ref>. The use of polynomial arithmetic, with coefficients restricted in this way, ensures that multiplication and addition of coefficients takes place modulo 2, and thus can be implemented as logical AND and exclusive-OR respectively. The mapping from an address to an -bit cache index is determined by the polynomial defined by , where is an irreducible polynomial of order and is such that generates all polynomials of order lower than . The polynomials that fulfill the previous requirements are called I-Poly polynomials. Rau shows how the computation of can be accomplished by the vector-matrix product of the address and an matrix of single-bit coefficients. In GF(2), this product is computed by a network of AND and XOR gates, and if the -matrix is constant the AND gates can be omitted and the mapping then requires just XOR gates with fan-in from 2 to .</p><p>The choice of an I-poly polynomial yields properties similar to prime integer modulus functions. Whereas a prime integer modulus function would be prohibitively complex, the I-poly polynomial modulus function has very low complexity; suitable even for computing a cache index.</p><p>The use of XOR-mapping schemes requires the computation of several XOR operations to obtain the cache index. Since all the XOR can be done in parallel, the delay of this computation is just one XOR gate. The XOR gates have just two inputs for the bitwise XOR scheme and a few more for the polynomial mapping scheme. However, the computation of these XOR operations can be done at the end of the address computation stage of the pipeline. In many current microprocessors, this stage is not the critical stage of the pipeline and therefore this delay may not affect the pipeline cycle time. In addition, if some kind of carry propagate adder is used, the address computation unit computes the address bits from leastsignificant to most-significant. Since the XOR-mapping schemes only use some of the least-significant bits of the address, the XOR gates can operate in parallel with the computation of the most significant-bits and their delay could be completely hidden even if the address computation stage was the critical stage of the pipeline. In other situations, the addition of this small additional delay can affect the critical path, but even in these cases, a net benefit could be obtained since the reduction in miss ratio achieved by XORmapping schemes is very high as we will show in this paper. An accurate timing evaluation will be required in these cases to consider the additional delay, which is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cache memories</head><p>This paper evaluates the performance of XOR-mapping schemes for a number of cache organizations: direct-mapped, two-way associative, victim, hash-rehash, column-associative and two-way skewed-associative. Direct-mapped and set-associative organizations <ref type="bibr" target="#b17">[18]</ref> are the most popular in current microprocessors and we assume that the reader is familiar with them. The two-way skewed-associative cache was described in section 2. Below there is a short outline of the victim, hash-rehash and column-associative caches.</p><p>The hash-rehash cache, proposed by Agarwal et al. <ref type="bibr" target="#b2">[3]</ref>, consists of a conventional direct-mapped cache for which up to two tag probes may be required to find the requested data. First, the cache is accessed with the conventional modulo function, that is using l bits of the address &lt;a b+l-1 ,a b+l-2 ,..., a b &gt; (2 l is the number of cache lines and 2 b is the line size). If the data is not found, the cache is probed again but with the most significant bit inverted. Thus, the second probe checks the tag for line &lt;a b+l-1 ,a b+l-2 ,..., a b &gt;. In case of a second probe hit, the two lines are swapped. Otherwise, the data is brought from the next memory level and it is placed in the first-probe location, whereas the data already there is moved to the second-probe location.</p><p>The column-associative cache <ref type="bibr" target="#b3">[4]</ref> improves the disappointing miss ratio of the hash-rehash cache by introducing a rehash bit associated with each line. This bit indicates whether the line contains rehashed data, that is, data that is reached in the second probe. When the first probe finds rehashed data, the corresponding line is chosen for replacement. If the rehash bit is zero, then upon a first-time miss the cache is accessed again with the second function.</p><p>In the case of a second-time hit, the lines are swapped. Otherwise, the data retrieved from memory is placed in the first line and the data already in that line is moved to the line accessed with the second function.</p><p>A victim cache <ref type="bibr" target="#b11">[12]</ref> consists of a conventional direct-mapped cache with a small fully-associative buffer in the refill path to a second-level cache or main memory. On a cache miss, the line that is evicted from the direct-mapped cache is placed in the victim cache. In the case of a miss in the direct-mapped cache that hits in the victim cache, the lines accessed in both caches are swapped. In the experiments performed in this paper, we assume a victim cache with four lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation methodology</head><p>The results presented in this paper have been obtained through simulation of various data cache organizations using the SPEC 95 floating point benchmark suite. We focus on the floating point benchmarks because they exhibit a much higher conflict miss ratio than the integer benchmarks, and thus the XOR-mapping schemes have more potential benefits for them. Integer benchmarks will also benefit from XOR-mapping schemes although to less extent. The performance metrics used for comparison of different schemes are the total miss ratio and conflict miss ratio. Since the compared schemes only differ on the placement function, a reduction in the miss ratio will result in a reduction of the average memory access time.</p><p>The programs were compiled with the maximum optimization level and instrumented with the ATOM tool <ref type="bibr" target="#b19">[20]</ref>. A data cache memory similar to the first-level cache of the Alpha 21164 microprocessor has been assumed: 8 Kilobytes capacity, 32 bytes per line, write-through and no write allocate. For each benchmark we have simulated the first billion (2 30 ) load operations. Because of the no write allocate feature, the performance metrics computed below refer only to load operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance of conventional mapping schemes</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the miss ratio for the following cache organizations: direct-mapped, two-way associative, four-way associative, hashrehash, column-associative victim and two-way skewedassociative. Of these schemes, only the two-way skewedassociative cache uses an XOR-mapping scheme, as proposed by its author. For comparison, the miss ratio of a fully-associative cache is shown in the penultimate column. For each organization, the</p><formula xml:id="formula_2">A a n 1 -… a 1 a 0 , , , 〈 〉 = A x ( ) a n 1 -x n 1 -… a 1 x 1 a 0 , , , = l R x ( ) A x ( ) V x ( )P x ( ) R x ( ) + = P x ( ) l P x ( ) x i mod P x ( ) l R x ( ) n l × H H l n</formula><p>difference between its miss ratio and that of a fully-associative cache, which is shown in brackets in Table <ref type="table" target="#tab_0">1</ref>, which represents the conflict miss ratio <ref type="bibr" target="#b10">[11]</ref>. In fact, this difference is slightly negative in the case of 104hydro2d and 141.apsi for some organizations, due to sub-optimality of LRU replacement in a fully-associative cache for these particular programs. Effectively the conflict miss ratio represents the target reduction in miss ratio that we hope to achieve through improved mapping schemes. The other type of misses, compulsory and capacity, will remain unchanged by the use of the XOR-mapping schemes.</p><p>From the results in Table <ref type="table" target="#tab_0">1</ref>, we can conclude that set associativity reduces the miss ratio, as expected, although the improvement of a two-way associative cache over a direct-mapped cache is rather low. Comparing the direct-mapped and two-way associative cache with the fully-associative cache suggests that, several benchmarks (e.g. 101.tomcatv, 102.swim, 125.turb3d, 146.wave) show significant clustering in the mapping of memory lines to cache lines under the conventional mapping scheme.</p><p>The hash-rehash cache has a miss ratio similar to that of a directmapped cache. Although both have similar access times, the hashrehash scheme requires two cache probes for some hits. Hence, the direct-mapped cache will be more effective. This poor behavior of the hash-rehash cache was also observed in <ref type="bibr" target="#b3">[4]</ref>. The columnassociative cache provides a miss ratio similar to that of a two-way associative cache. Since the former has a lower access time but requires two cache probes to satisfy some hits, the choice between these two organization should take into account the particular implementation parameters (access time and miss penalty). The victim cache removes many conflict misses and it outperforms a four-way associative cache. Finally, the two-way skewedassociative cache offers the lowest miss ratio, which is significantly lower than that of a four-way associative cache. skewed-associative cache are more positive than those observed in <ref type="bibr" target="#b15">[16]</ref>, where a miss ratio similar to a four-way associative cache was claimed, though using a different workload.</p><p>5 Bitwise XOR mapping XOR-mapping schemes exhibit a behavior which is in some way similar to full associativity but with some restrictions. For instance, in the two-way skewed-associative cache, the set of all addresses that are mapped into the same line of bank 0 are distributed over all the lines in bank 1. Thus, it is similar to having all the lines of bank 1 as alternative locations for a given line in bank 0. However, if one considers a particular memory address, it can be placed in exactly two cache locations (one in bank 0, and the other in bank 1). Below we analyze the performance of bitwise XOR mapping schemes for the other cache organizations. The mapping functions that are evaluated are based on the family of functions proposed in <ref type="bibr" target="#b15">[16]</ref>. Section 7 evaluates the performance of the polynomial mapping scheme proposed in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Direct-mapped</head><p>To describe the bitwise XOR mapping function, let us consider a memory address A=&lt;a n-1 ,a n-2 ,..., a 0 &gt; composed of the following fields: </p><formula xml:id="formula_3">A=(A 3 , A 2 , A 1 , A 0 ) such that A 0 =&lt;a b-1 ,...,</formula><formula xml:id="formula_4">f: {0...2 n -1} → {0...2 l -1} A=(A 3 , A 2 , A 1 , A 0 ) → (A 2 ⊕A 1 , A 0 )</formula><p>Figure <ref type="figure">1</ref> compares the miss ratio of a direct mapped cache with a conventional mapping function to a direct-mapped cache with the mapping function f previously defined.</p><p>It can be seen that the use of an XOR-mapping function provides a large improvement for two of the benchmarks (101 and 146). These are the two benchmarks that also most benefit from a low degree of set-associativity, as can be seen from Table <ref type="table" target="#tab_0">1</ref>. On average, the direct-mapped cache with an XOR-mapping function has a miss ratio lower than that of a column associative cache and almost equal to the miss ratio of a four-way associative cache. Notice however, that five of the ten programs exhibit slightly higher miss ratios. These are all notable for their low conflict miss ratios in a conventional direct-mapped cache. We are seeing the random introduction, with low probability, of conflicts that were not originally present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hash-rehash and column-associative</head><p>The mapping functions proposed for the skewed-associative cache <ref type="bibr" target="#b15">[16]</ref> can also be used for a hash-rehash cache and a column associative cache. For these, as for the skewed-associative cache, we define two distinct mapping functions and . The first probe uses and, if required, the second probe uses . These functions are as defined in section 2, using the address decomposition A=(A 3 , A 2 , A 1 , A 0 ) defined in section 5.1, and with a binary value of .</p><p>The cache miss ratios for hash-rehash and column associative caches using and are shown in Table <ref type="table">2</ref>. We can observe that on average the XOR-mapping functions do not provide any improvement although they are beneficial for two benchmarks (101 and 146). The net deterioration in miss ratio is due to two reasons:</p><p>• If reference A produces a cache miss, it is placed in . If the data currently in this location corresponds to memory address B, it is moved to , or discarded. The hashrehash cache always moves the data, whereas the columnassociative cache takes this decision based on the rehash bit. However, it is very likely that and . Consequently, the data from address B will be moved to a place where it will no longer be accessible and the next reference to will miss (even if the data is in cache). In addition to degrading performance, this may also cause some consistency problems.</p><p>• For a given reference, it may happen that . In this case, reference A does not have an alternative location and we loose the positive effect of pseudo-associativity caused by the use of two mapping functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Enhancing the hash-rehash and columnassociative cache</head><p>The first problem mentioned above can be solved by inhibiting the swapping of data. Of course, that will cause a significant increase in the percentage of hits that require two probes, but it will provide us with a lower bound on the miss ratio that could be obtained. Besides, swapping may significantly increase pressure on the cache ports, and may cause performance penalties as it is not always possible to hide the swapping during idle cache cycles. For instance, in an ideal out-of-order machine with two memory ports and infinite resources, we have measured that on average two memory ports are busy during 71% of cycles, and only in the 17% of cycles are both idle <ref type="bibr" target="#b7">[8]</ref>. An interesting alternative to swapping is to predict the most likely location of the two possible candidates for a given address. This has been extensively studied by Calder et al. who showed that it can be a very effective approach <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_5">f 0 f 1 f 0 f 1 T 10101010 = f 0 f 1 f 0 A ( ) f 1 A ( ) f 1 A ( ) f 0 B ( ) ≠ f 1 A ( ) f 1 B ( ) ≠</formula><p>In order to eliminate the possibility that , we propose to slightly modify the mapping functions such that they always differ in the most significant bit of the result they produce. This most significant bit will be equal to the most significant bit of A 1 for f 0 and it will be inverted for f 1 .</p><p>The proposed replacement policy is a pseudo-LRU policy inspired by the one proposed in <ref type="bibr" target="#b15">[16]</ref>. A one-bit flag is associated with each cache line. When a hit occurs, the flag of the line holding the data is reset to 0 and the flag of the alternate location is set to 1. If a miss occurs, the new line replaces the line whose flag is lower. If both flags are equal, the line at is replaced. With these changes to the mapping function and replacement policy, and the elimination of swapping, the miss ratios for a column-associative cache are as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The figure also includes the miss ratios for the conventional column-associative organization without using a XOR-mapping.</p><p>Notice that with this organization, the effect of the XORmapping scheme in the column-associative cache is very impressive, in particular for those programs with the highest miss ratio. The miss ratio of this organization is much lower than that of a four-way associative cache and somewhat lower than that of the skewed-associative cache. To isolate the effect of inverting one bit to obtain always two potential locations for each address, we have performed the simulations just with the XOR-functions (f 0 and f 1 as defined at the beginning of this section), without the bit inversion and we obtained an average miss ratio of 11.17, which is somewhat higher than that of Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Victim cache</head><p>In this case, the direct-mapped part uses the XOR-mapping function defined in section 5.1. The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. We can see that the XOR-mapping makes the average miss ratio of the victim cache to be very close to that of the two-way skewedassociative cache. Notice also that the XOR-mapping produces a slight increase in miss ratio for those benchmarks with very few conflict misses. The same behavior was observed for a directmapped cache and can be explained again by the random, but infrequent, introduction of new conflict misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Two-way associative</head><p>In the case of a two-way associative cache, consider an address A composed of four fields A=(A 3 , A 2 , A 1 , A 0 ) of n-2l-b+2, l-1, l-1 and b bits respectively. In this case, the XOR-based mapping function is defined as follows:</p><formula xml:id="formula_6">g: {0...2 n -1} → {0...2 l-1 -1} A=(A 3 , A 2 , A 1 , A 0 ) → (A 2 ⊕A 1 , A 0 )</formula><p>The same mapping function is used to access both banks, as in a conventional set-associative cache, and LRU replacement is used as in this case it can be implemented with low cost. The miss ratios corresponding to this organization are shown in Figure <ref type="figure">4</ref>.</p><p>It can be seen that the bitwise XOR mapping scheme more than halves the miss ratio. We can also see in the graphs of Figure <ref type="figure">4</ref> that the mapping function has eliminated almost all the conflict misses. In average, the total miss ratio is just 1.10 times that of a fullyassociative cache. For two programs the two-way XOR cache has lower miss ratio than a fully-associative cache. This is again due to the sub-optimality of LRU replacement in the fully-associative cache, and is a common anomaly in programs with negligible conflict misses.</p><p>When a bitwise XOR mapping is used, the average miss ratio of the two-way associative cache is slightly better than that of a column-associative cache and much better than that of the skewed associative cache. This may seem to contradict the results in <ref type="bibr" target="#b15">[16]</ref>, where Seznec observed that the two-way skewed-associative cache had a lower miss ratio than a two-way associative cache with the same mapping function for both banks. The reason for this  The table shows the total miss ratio while the depicts the conflict miss ratio.</p><formula xml:id="formula_7">f 0 A ( ) f 1 A ( ) = f 0 A ( )</formula><p>difference is twofold. Firstly, Seznec used function f 0 T described in section 2to index the two-way associative cache. This function indexes the cache using bits, whereas his twoway skewed-associative cache was indexed using bits. One of the most important benefits of XOR-mapping schemes is that they avoid conflicts among data structures that are accessed simultaneously with the same stride but whose initial addresses differ in a sum of powers of two. If these powers of two correspond to bits that are used by the mapping function, the conflicts may be avoided. Thus, to be fair, one should compare cache organizations that use the same number of bits as input to the mapping function. Both the two-way skewed-associative cache in Table <ref type="table" target="#tab_0">1</ref> and the twoway associative cache in Figure <ref type="figure">4</ref> use the same number of bits. The second reason for the difference with Seznec's results is that he used a different workload, with a much smaller working set, since his miss ratios are much lower.</p><p>The results in Figure <ref type="figure">4</ref> suggest that in the case of a two-way associative cache, it is more effective the use of more bits in each mapping function than having two different indexing functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Restricted hashing</head><p>A drawback of the XOR-mapping scheme is that it may interfere with the use of a physically tagged cache, which may be desirable for coherency reasons <ref type="bibr" target="#b10">[11]</ref>. To remove address translation from the critical path it is common to have a virtually-indexed cache with physical address tags. This typically means that the cache is indexed using only unmapped virtual address bits. This limmits the maximmum number of sets and therefore, it imposes some constraints in both the cache size and the degree of associativity.  The table shows the total miss ratio while the figure depicts the conflict miss ratio.</p><formula xml:id="formula_8">l 1 - l 1 - ( ) 2 ⁄ + 2l<label>2</label></formula><p>However, the XOR-mapping scheme requires the use of more bits of the address and therefore, heightens the constraint on the page size. One way to overcome this problem is to use fewer bits to compute the mapping. In the case of a skewed associative cache, it was shown that this produces a small reduction in performance <ref type="bibr" target="#b15">[16]</ref>. Table <ref type="table" target="#tab_5">3</ref> compares the miss ratio of a column-associative cache using the mapping functions described in section 5.2with the miss ratio obtained when using only the four least-significant bits of A 2 to perform the bitwise XOR with A 1 . It can be observed that the increase in miss ratio is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">An affordable implementation of LRU replacement</head><p>The use of two different XOR-mapping functions creates an effect similar to full associativity, as previously discussed. This suggests that an LRU replacement policy may be expensive to implement, and has motivated previous work on pseudo-LRU replacement policies <ref type="bibr" target="#b15">[16]</ref>. However, implementing LRU replacement in column-associative or skewed-associative caches is not as expensive as in the case of a fully-associative cache. One way to implement LRU for the caches that use two different mapping functions is to add a time stamp to each cache line. A count of memory references is maintained, and every time a cache line is accessed its time stamp is updated with the value of the reference counter. When a miss occurs, the candidate for replacement which has the lowest time stamp is chosen for replacement. In the case of a two-way skewed-associative or a column-associative cache this requires a single comparison between two integer fields. This replacement policy produces a noticeable benefit in the performance of the column-associative cache and the two-way skewed-associative cache, as shown in Table <ref type="table" target="#tab_6">4</ref>, especially for benchmarks 101 and 102.</p><p>The cost associated with this LRU replacement depends on the number of bits devoted to the time-stamp. The simulations reported in Table <ref type="table" target="#tab_6">4</ref> ensure that the time-stamp never overflows. A more practical scheme, that uses a small number of bits both in the counter and the time-stamp would work by shifting the counter and all the time-stamps one bit to the right whenever the reference counter overflowed. We simulated this scheme for the columnassociative cache using just 8 bits for the counter and the time stamps. The results are practically identical to those obtained with an unrestricted time stamp (the average miss ratio was 9.32).</p><p>One potential criticism of our comparison between the columnassociative and the skewed-associative caches is that the former uses one bit more of the address to compute the cache index. To isolate this effect we simulated the column-associative cache using address bits in the mapping function (the same as the skewed-associative cache), and without bit inversion. This produced an average miss ratio of 9.36, indicating no significant difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Polynomial mapping</head><p>We have investigated the performance of the XOR-mapping scheme proposed by Rau <ref type="bibr" target="#b14">[15]</ref>, which is based on polynomial arithmetic and which will be referred to as polynomial mapping. The performance of polynomial mapping has been evaluated for the column associative, the two-way associative and the two-way skewed-associative organizations. For all of them, Table <ref type="table" target="#tab_7">5</ref> compares the miss ratios of the previous XOR mapping functions based on the bitwise XOR of two bit strings (XOR) with that obtained using polynomial mapping functions (Poly). In all cases, an LRU replacement is assumed. The miss ratio of a fullyassociative cache is also shown for comparison.</p><p>To perform a fair comparison we applied the randomization scheme using the same number of bits of the original address as input to all the mapping functions; in all the cases this is 19 bits (14 without considering the bits that indicate the displacement inside the cache line). For the polynomial mapping functions, we chose the I-poly polynomials that require the fewest number of XOR entries for its implementation. The four H-matrices that represent the polynomial mappings are shown in Figure <ref type="figure">5</ref>. For the columnassociative cache, H 1 and H 2 define the mapping of the two indexing functions used by this organization. H 3 corresponds to single function utilized by the two-way associative cache. Finally, H 3 and H 4 define the two different mapping functions used by skewed-associative cache. Each mapping function requires 7 or 8 XOR gates with fan-in from 2 to 5 each. Regarding the column-associative and the two-way associative results, we can conclude from Table <ref type="table" target="#tab_7">5</ref> that the scheme based on using polynomial mapping provides a marginal advantage over the bitwise XOR scheme. However, as the former requires wider XOR gates (i.e. more inputs) the simpler XOR scheme may be preferable.</p><p>The marginal advantage of the polynomial mapping scheme can be explained in a number of ways. Firstly, both schemes are really quite similar; the principal advantage of polynomial mapping is the guarantee of optimal behavior on address patterns that lead to pathological conflict misses in a conventional mapping scheme. Such optimality may not be a feature of bitwise XOR schemes, but 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 H3= 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 H4= 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 H2= 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 H1= Figure <ref type="figure">5</ref>: H matrices used by the polynomial mapping functions of Table <ref type="table" target="#tab_7">5</ref>.</p><p>pathological cache behavior is also not a dominant feature of the SPEC95 suite. Anyway, both schemes achieve a miss ratio that is very close to that of a fully-associative cache.</p><p>On the other hand, the polynomial mapping provides a significant improvement for the skewed-associative cache. For three of the benchmarks (101, 102 and 146) this improvement is quite important. For the others, the reduction in miss ratio is very small, if any, since the miss ratio of the original mapping was already very close to that of a fully-associative cache. Overall, the skewed-associative cache using polynomial mapping and a pure LRU replacement achieves a miss ratio practically identical to that of a fully-associative cache (it is just 0.8% higher).</p><p>Figure <ref type="figure">6</ref> shows the conflict miss ratio for the columnassociative, two-way associative and skewed-associative organizations with polynomial mapping. It can be seen that the in the three cases, practically all conflict misses have been removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Swapping in the column-associative cache</head><p>In the previous sections, the column-associative cache did not incorporate the swapping feature. As a result we can expect a lower miss ratio but a higher percentage of hits requiring two probes. Table <ref type="table">6</ref> compares the performance of the column-associative cache both with and without swapping, using a bitwise XOR mapping scheme taking bits. In this case, when a reference to address A misses in cache, it is brought to f 0 (A). If B is the address of the data currently in that location, either it is moved to its alternative location (f 0 (B) or f 1 (B)) or it is discarded if its alternative location has been used more recently. In the same way, when data is found in the second probe (f 1 (A)) it is moved to f 0 (A) and the data currently in this location is moved or discarded following the same criteria as in the case of miss. In any case, data is always placed in an accessible location.</p><p>It can be seen that swapping increases the miss ratio by a factor of 1.14, but also ensures that almost all hits can be achieved with a single probe. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:Miss ratio (%) for the conventional column-associative cache and the new bitwise XOR-mapping. The table shows the total miss ratio while the depicts the conflict miss ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Miss ratio (%) for the victim cache with the conventional and the bitwise XOR mapping functions.The table shows the total miss ratio while the figure depicts the conflict miss ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Miss ratios (%) for the original schemes. The conflict miss ratio is shown in brackets.</figDesc><table><row><cell></cell><cell>direct</cell><cell>2-way</cell><cell>4-way</cell><cell>hash-rehash</cell><cell>column-assoc.</cell><cell>victim</cell><cell>2-way skewed</cell><cell>fully-assoc.</cell></row><row><cell>101.tomcatv</cell><cell>53.8 (41.3)</cell><cell>48.1 (36.4)</cell><cell>29.5 (17.0)</cell><cell>51.4 (39.1)</cell><cell>47.0 (34.5)</cell><cell>26.6 (14.1)</cell><cell>22.1 (9.6)</cell><cell>12.5</cell></row><row><cell>102.swim</cell><cell>56.2 (48.3)</cell><cell>59.1 (51.2)</cell><cell>57.1 (49.2)</cell><cell>57.6 (49.7)</cell><cell>53.7 (45.8)</cell><cell>33.7 (25.8)</cell><cell>15.1 (7.2)</cell><cell>7.9</cell></row><row><cell>103.su2cor</cell><cell>11.0 (2.1)</cell><cell>9.1 (0.2)</cell><cell>9.0 (0.1)</cell><cell>11.1 (2.2)</cell><cell>9.3 (0.4)</cell><cell>9.5 (0.6)</cell><cell>9.6 (0.7)</cell><cell>8.9</cell></row><row><cell>104.hydro2d</cell><cell>17.6 (0.1)</cell><cell>17.1 (-0.4)</cell><cell>17.3 (-0.2)</cell><cell>17.6 (0.1)</cell><cell>17.2 (-0.3)</cell><cell>17.0 (-0.5)</cell><cell>17.1 (-0.4)</cell><cell>17.5</cell></row><row><cell>107.mgrid</cell><cell>3.8 (0.3)</cell><cell>3.6 (0.1)</cell><cell>3.5 (0.0)</cell><cell>6.1 (2.6)</cell><cell>4.2 (0.7)</cell><cell>3.7 (0.2)</cell><cell>4.1 (0.6)</cell><cell>3.5</cell></row><row><cell>110.applu</cell><cell>7.6 (1.7)</cell><cell>6.4 (0.5)</cell><cell>6.0 (0.1)</cell><cell>7.8 (1.9)</cell><cell>6.5 (0.6)</cell><cell>6.9 (1.0)</cell><cell>6.7 (0.8)</cell><cell>5.9</cell></row><row><cell>125.turb3d</cell><cell>7.5 (4.7)</cell><cell>6.5 (3.7)</cell><cell>5.3 (2.5)</cell><cell>7.7 (4.9)</cell><cell>6.4 (3.6)</cell><cell>7.0 (4.2)</cell><cell>5.4 (2.6)</cell><cell>2.8</cell></row><row><cell>141.apsi</cell><cell>15.5 (3.0)</cell><cell>13.3 (0.8)</cell><cell>11.3 (-1.2)</cell><cell>18.0 (5.5)</cell><cell>13.4 (0.9)</cell><cell>10.7 (-1.8)</cell><cell>11.5 (-1.0)</cell><cell>12.5</cell></row><row><cell>145.fpppp</cell><cell>8.5 (6.8)</cell><cell>2.7 (1.0)</cell><cell>2.1 (0.4)</cell><cell>5.9 (4.2)</cell><cell>2.7 (1.0)</cell><cell>7.5 (5.8)</cell><cell>2.2 (0.5)</cell><cell>1.7</cell></row><row><cell>146.wave</cell><cell>31.8 (17.9)</cell><cell>31.7 (17.8)</cell><cell>23.0 (9.1)</cell><cell>35.4 (21.5)</cell><cell>30.7 (16.8)</cell><cell>20.1 (6.2)</cell><cell>16.8 (2.9)</cell><cell>13.9</cell></row><row><cell>Average</cell><cell>21.32 (12.61)</cell><cell>19.76 (11.05)</cell><cell>16.42 (7.71)</cell><cell>21.87 (13.16)</cell><cell>19.11 (10.40)</cell><cell>14.27 (5.56)</cell><cell>11.05 (2.34)</cell><cell>8.71</cell></row><row><cell></cell><cell></cell><cell cols="2">The results for the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Miss ratio (%) for a direct-mapped cache with the conventional and a bitwise XOR mapping. The table shows the total miss ratio while the figure depicts the conflict miss ratio.</figDesc><table><row><cell>miss ratio</cell><cell cols="2">conv. direct-mapped XOR</cell><cell></cell><cell>50.0</cell></row><row><cell>101.tomcatv</cell><cell>53.8</cell><cell>34.0</cell><cell></cell><cell>40.0</cell><cell>direct-mapped</cell></row><row><cell>102.swim</cell><cell>56.2</cell><cell>53.2</cell><cell></cell><cell></cell><cell>XOR direct-mapped</cell></row><row><cell>103.su2cor 104.hydro2d 107.mgrid 110.applu 125.turb3d</cell><cell>11.0 17.6 3.8 7.6 7.5</cell><cell>10.5 18.3 4.2 8.3 8.3</cell><cell>conflict miss ratio</cell><cell>20.0 30.0</cell></row><row><cell>141.apsi</cell><cell>15.5</cell><cell>13.7</cell><cell></cell><cell>10.0</cell></row><row><cell>145.fpppp</cell><cell>8.5</cell><cell>9.2</cell><cell></cell><cell></cell></row><row><cell>146.wave</cell><cell>31.8</cell><cell>15.5</cell><cell></cell><cell>0.0</cell></row><row><cell>Average</cell><cell>21.32</cell><cell>17.51</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tomcat swim su2cor hydro mgrid applu turb3d apsi fpppp wave average</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>miss ratio</cell><cell>hash-rehash original XOR</cell><cell>column-associative original XOR</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">101.tomcatv</cell><cell>51.4</cell><cell>38.9</cell><cell>47.0</cell><cell>37.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>102.swim</cell><cell>57.6</cell><cell>63.0</cell><cell>53.7</cell><cell>63.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>103.su2cor</cell><cell>11.1</cell><cell>13.7</cell><cell>9.3</cell><cell>12.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">104.hydro2d</cell><cell>17.6</cell><cell>19.5</cell><cell>17.2</cell><cell>18.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>107.mgrid</cell><cell>6.1</cell><cell>7.1</cell><cell>4.2</cell><cell>4.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>110.applu</cell><cell>7.8</cell><cell>12.6</cell><cell>6.5</cell><cell>11.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">125.turb3d</cell><cell>7.7</cell><cell>19.9</cell><cell>6.4</cell><cell>9.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>141.apsi</cell><cell>18.0</cell><cell>23.9</cell><cell>13.4</cell><cell>15.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>145.fpppp</cell><cell>5.9</cell><cell>11.8</cell><cell>2.7</cell><cell>5.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>146.wave</cell><cell>35.4</cell><cell>25.9</cell><cell>30.7</cell><cell>25.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell><cell>21.87</cell><cell>23.63</cell><cell>19.11</cell><cell>20.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 2 Miss ratio (%) for the hash-rehash and column-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">associative caches with the original and the bitwise XOR mapping</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">functions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f 0 A ( )</cell><cell>=</cell><cell>f 1 A ( )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>-Miss ratio (%) for the two-way associative cache with the conventional and the bitwise XOR mapping functions. The table shows the total miss ratio while the figure depicts the conflict miss ratio.</figDesc><table><row><cell>miss ratio 101.tomcatv 102.swim 103.su2cor 104.hydro2d 107.mgrid 110.applu 125.turb3d 141.apsi 145.fpppp 146.wave Average 101.tomcatv 102.swim 103.su2cor 104.hydro2d 107.mgrid 110.applu 125.turb3d 141.apsi 145.fpppp 146.wave Figure 4: miss ratio Average</cell><cell>2-way associative conv. XOR 48.1 17.0 59.1 7.9 9.1 9.6 17.1 17.2 3.6 3.7 6.4 6.9 6.5 4.6 13.3 11.4 2.7 2.7 31.7 14.4 19.76 9.54 victim cache conv. XOR 26.6 16.1 33.7 21.8 9.5 9.5 17.0 17.3 3.7 4.0 6.9 7.4 7.0 7.4 10.7 11.1 7.5 7.9 20.1 14.3 14.3 11.6</cell><cell>conflict miss ratio</cell><cell>0.0 10.0 20.0 30.0 40.0 50.0</cell><cell>tomcat swim su2cor hydro mgrid applu turb3d apsi fpppp wave average 2-way XOR 2-way tomcat swim su2cor hydro mgrid applu turb3d apsi fpppp wave average</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Miss ratio (%) for the column-associative caches with the full and partial bitwise XOR mapping</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>miss ratio</cell><cell cols="2">column-associative pseudo-LRU LRU</cell><cell cols="2">2-way skewed-associative pseudo-LRU LRU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>101.tomcatv</cell><cell>20.2</cell><cell>16.4</cell><cell>22.1</cell><cell>20.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>102.swim</cell><cell>9.7</cell><cell>8.6</cell><cell>15.1</cell><cell>12.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>103.su2cor</cell><cell>9.2</cell><cell>9.0</cell><cell>9.6</cell><cell>9.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>104.hydro2d</cell><cell>17.2</cell><cell>17.1</cell><cell>17.1</cell><cell>17.1</cell></row><row><cell>miss ratio</cell><cell cols="2">column-associative full XORing partial XORing</cell><cell>107.mgrid 110.applu</cell><cell>3.9 6.8</cell><cell>3.9 6.4</cell><cell>4.1 6.7</cell><cell>3.9 6.3</cell></row><row><cell>101.tomcatv</cell><cell>20.2</cell><cell>23.2</cell><cell>125.turb3d</cell><cell>5.1</cell><cell>4.6</cell><cell>5.4</cell><cell>4.9</cell></row><row><cell>102.swim</cell><cell>9.7</cell><cell>11.8</cell><cell>141.apsi</cell><cell>10.7</cell><cell>10.0</cell><cell>11.5</cell><cell>10.5</cell></row><row><cell>103.su2cor</cell><cell>9.2</cell><cell>9.4</cell><cell>145.fpppp</cell><cell>2.5</cell><cell>2.5</cell><cell>2.2</cell><cell>2.2</cell></row><row><cell>104.hydro2d</cell><cell>17.2</cell><cell>17.1</cell><cell>146.wave</cell><cell>15.2</cell><cell>14.6</cell><cell>16.8</cell><cell>16.3</cell></row><row><cell>107.mgrid</cell><cell>3.9</cell><cell>3.7</cell><cell>Average</cell><cell>10.04</cell><cell>9.31</cell><cell>11.05</cell><cell>10.24</cell></row><row><cell>110.applu</cell><cell>6.8</cell><cell>6.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>125.turb3d</cell><cell>5.1</cell><cell>6.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>141.apsi</cell><cell>10.7</cell><cell>12.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>145.fpppp</cell><cell>2.5</cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>146.wave</cell><cell>15.2</cell><cell>15.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell>10.04</cell><cell>10.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Miss ratios (%) for the column-associative cache and the two-way skewed-associative cache comparing pseudo-LRU with LRU replacement.</figDesc><table><row><cell>2l 2 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Miss ratios for a column associative cache, a two-way associative cache and a two-way skewed-associative cache for the two XOR-mapping schemes: bitwise XOR (XOR) and polynomial mapping (Poly).</figDesc><table><row><cell>miss ratio</cell><cell cols="6">column-associative XOR Poly XOR Poly XOR Poly 2-way associative 2-way skewed assoc.</cell><cell>fully-assoc.</cell></row><row><cell cols="5">101.tomcatv 13.8 12.8 17.0 14.8</cell><cell cols="3">20.0 12.6 12.5</cell></row><row><cell>102.swim</cell><cell>8.3</cell><cell>7.7</cell><cell>7.9</cell><cell>7.9</cell><cell>12.3</cell><cell>7.5</cell><cell>7.9</cell></row><row><cell cols="2">103.su2cor 9.1</cell><cell>9.1</cell><cell>9.6</cell><cell>9.9</cell><cell>9.1</cell><cell>9.4</cell><cell>8.9</cell></row><row><cell cols="5">104.hydro2d 17.1 17.2 17.2 17.1</cell><cell cols="3">17.1 17.1 17.5</cell></row><row><cell>107.mgrid</cell><cell>4.0</cell><cell>4.2</cell><cell>3.7</cell><cell>3.8</cell><cell>3.9</cell><cell>4.1</cell><cell>3.5</cell></row><row><cell>110.applu</cell><cell>6.6</cell><cell>6.5</cell><cell>6.9</cell><cell>6.9</cell><cell>6.3</cell><cell>6.4</cell><cell>5.9</cell></row><row><cell cols="2">125.turb3d 5.5</cell><cell>6.0</cell><cell>4.6</cell><cell>4.8</cell><cell>4.9</cell><cell>4.2</cell><cell>2.8</cell></row><row><cell>141.apsi</cell><cell cols="4">10.6 11.2 11.4 11.4</cell><cell cols="3">10.5 10.6 12.5</cell></row><row><cell>145.fpppp</cell><cell>4.0</cell><cell>2.7</cell><cell>2.7</cell><cell>2.8</cell><cell>2.2</cell><cell>2.3</cell><cell>1.7</cell></row><row><cell cols="5">146.wave 14.7 13.8 14.4 14.2</cell><cell cols="3">16.3 13.7 13.9</cell></row><row><cell>Average</cell><cell cols="7">9.36 9.12 9.54 9.37 10.24 8.78 8.71</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the Spanish Ministry of Education (grants CICYT TIC-429/95 and Acción Integrada Hispano-Británica 202B); the British Council (grant 1016); and the UK EPSRC (grant K19723).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>We have analyzed the performance of XOR-based placement functions for cache memories using the SPEC 95 floating-point benchmark suite. We have shown that XOR-mapping schemes provide a very high improvement across a broad range of different cache organizations: direct-mapped, set-associative, columnassociative and victim cache. We have also evaluated their effect on the hash-rehash cache and presented performance measures of the skewed-associative cache.</p><p>The main conclusion of this study is that XOR-based placement functions significantly reduce the number of conflict misses for all cache organizations. In particular, XOR-mapping combined with (pseudo) two-way associativity eliminates practically all the conflict misses, and obtains a miss ratio practically equal to that of a fully associative cache.</p><p>We have also presented a low-cost implementation of LRU replacement suitable for caches with two or more distinct mapping functions based on XOR-mapping schemes, and shown that it yields significant improvement over previously proposed pseudo-LRU replacement schemes.</p><p>Two class of placement functions have been considered. The first one is based on the bitwise exclusive OR of two bit strings. The second class is the polynomial mapping proposed in <ref type="bibr" target="#b14">[15]</ref> in the context of interleaved memories.</p><p>For the first class of mapping functions, among the different schemes evaluated, the lowest miss ratio is achieved by the column associative cache, closely followed by the two-way set associative cache, the two-way skewed-associative cache and the victim cache. All of them achieve a miss ratio much lower than that of a conventional four-way associative cache and close to that of a fullyassociative cache. For example, a two-way associative cache achieves an average miss ratio that is just 1.09 times that of a fullyassociative cache. Similarly, a column-associative cache can achieve a miss ratio between 1.07 and 1.23 times that of a fullyassociative cache, depending on whether swapping is implemented. For comparison, a conventional direct-mapped cache has a miss ratio that is 2.45 times that of a fully-associative cache.</p><p>Regarding polynomial mapping, we have shown that it provides a marginal advantage over the simpler bitwise XOR schemes for the two-way associative and column-associative organizations.</p><p>However, for the skewed-associative cache it achieves a significant reduction in miss ratio. Combining the effects of a pure LRU replacement and polynomial mapping, the miss ratio of the twoway skewed associative cache is reduced from 1.27 to 1.01 times that of a fully associative cache.</p><p>Comparing the three most effective organizations, i.e., skewedassociative, column-associative and set-associative, we can see that all achieve a very similar miss ratio. Each one may be preferable for different reasons: a skewed-associative has the lowest miss ratio, the column-associative has the lowest hit time and the setassociative requires less hardware to implement a LRU replacement.</p><p>In overall, we can conclude that XOR-based placement functions are an extremely powerful technique for eliminating conflict misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>conflict miss ratio</head><p>Poly column-associative Poly 2-way Poly 2-way skewed 2-way Figure <ref type="figure">6</ref>: Conflict miss ratio for the column-associative, two-way associative and skewed-associative organizations with polynomial mapping. The conflict miss ratio of a conventional 2-way associative cache is also depicted for comparison.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance Features of the PA7100 Microprocessor</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asprey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of Cache Performance for Operating Systems and Multiprogramming</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="120" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cache Performance of Operating Systems and Multiprogramming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="393" to="431" />
			<date type="published" when="1988-11">Nov. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Column-Associative Caches: A Technique for Reducing the Miss Rate of Direct-Mapped Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pudar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="179" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predictive Sequential Associative Caches</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Symp. on High Performance Computer Architecture</title>
		<meeting>Int. Symp. on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hardware Design of the First HP Precision Architecture Computeres</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Fotland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hewlet-Packard Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="1987-03">March 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XOR-Schemes: A Flexible Data Organization in Parallel Memories</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frailong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lenfant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Parallel Processing</title>
		<meeting>Int. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying Contributing Factors to ILP</title>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euromicro</title>
		<meeting>Euromicro</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing Memory Contention in Shared Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Dynamic Storage Scheme for Conflict-Free Vector Access</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harper Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Linebarger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Class of Boolean Linear Transformations for Conflict-free Power-of-two Stride Access</title>
		<author>
			<persName><forename type="first">A</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Melton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Parallel Processing</title>
		<meeting>Int. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Cydra 5 Stride-Insensitive Memory System</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlansker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Conf. on Parallel Processing</title>
		<meeting>Int. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="242" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pseudo-Randomly Interleaved Memories</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Case for Two-way Skewed-associative Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skewed-associative Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bodin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Parallel Architectures and Languages (PARLE)</title>
		<meeting>Int. Conf. on Parallel Architectures and Languages (PARLE)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache Memories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="530" />
			<date type="published" when="1982-09">Sept. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Logical Data Skewing Schemes for Interleaved Memories in Vector Processors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<idno>#753</idno>
		<imprint>
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Computer Science Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ATOM: A System for Building Customized Program Analysis Tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eustace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGPLAN Conf. on Programming Language Design and Implementation</title>
		<meeting>SIGPLAN Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Increasing the Number of Strides for Conflict-free Vector Access</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Computer Architecture</title>
		<meeting>Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
