<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiments in Learning by Imitation -Grounding and Use of Communication in Robotic Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aude</forename><surname>Billard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept, of Artificial Intelligence</orgName>
								<address>
									<country>U of Edinburgh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hertfordshire</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kerstin</forename><surname>Dautenhahn</surname></persName>
						</author>
						<title level="a" type="main">Experiments in Learning by Imitation -Grounding and Use of Communication in Robotic Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF8CAD166672FFFEF587EA62F62E80FE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social behaviour and in particular social learning are key mechanisms for the cohesion and evolu- tion of primate societies. Similarly, social skills might be desirable for artificial agents who are expected to interact with other natural or artificial agents. We view learning, communication and imitation as important capabilities to possess by social artificial agents and study how these skills can be designed and used by physically embodied autonomous robots. We study grounding and use of communication among heterogeneous agents. In particular, we investigate the role of social interactions for sharing of context and building of joint attention among communicative agents. Grounding and use of communication is investigated through simulations within a group of autonomous agents. Results show that social behaviour benefit the agents in two circumstances:</p><p>(1) agents capable of following one another, and in this way imitating each other's movements, develop faster and better a common understanding of the language; (2) furthermore, the agents' capability of communicating with one another via a common vocabulary benefits to the group and to each agent individually as it speeds up the transmission of information. We use a connection- ist model, based on Hebbian associative learning, for the learning of the word-signal pairs. This work follows robotic experiments [6, 5, 7] in which a physical autonomous robot was taught a vocabulary to describe its perceptions of objects, movement, inclination and orientation. The robot was taught either by a human instructor or by another robot. The teacher-learner robot experiments were based on an imitative strategy whereby the learner robot followed the teacher robot. The work of this paper demonstrates scaling up of this movement imitative strategy for transmitting a vocabulary across a group of robotic agents, i.e. from a teacher agent to several learner agents. In particular, it shows that imitative behaviour is necessary for the grounding of the agents' proprioceptions and speeds up the grounding of exteroceptions. These studies stress the importance of behavioural social mechanisms in addition to general cognitive abilities of associa- tivity for grounding communication in embodied agents. In particular, it shows that a simple movement imitation strategy is an interesting scenario for the transmission of a language, as it is an easy means of getting the agents to share a common context of perceptions, which is a prerequisite for a common understanding of the language to develop. It is thus suggested that a behav- iour-oriented approach might be more appropriate than a pure cognitivist one which is dominat- ing in related studies of the mechanisms involved in grounding communication.</p><p>Keywords: Social learning and behaviour; grounding communication; embodied and situated agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">THE SOCIAL ASPECT OF COMMUNICATION</head><p>Social learning theory studies the mechanisms by which an agent learns from its interactions with conspecific agents. Examples of social learning can be found in the animal kingdom, and especially in pri- mate societies, whereby, for instance, one agent learns new skills by the observation and imitation of anoth- er agent's behaviour. At present there is no widely agreed definition of imitation existing in the animal and human psychology literature. The variety of def-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>initions ranges from strong definitions, which require an aspect of novelty in the imitated behaviour to very week definitions which have no clear boundaries to forms of non-imitative social learning like contagion or social facilitation. Often researchers refer to Thorpe's definition of 'true imitation' which is the 'copying of a novel or otherwise improbable act or utterance, or some act for which there is clearly no instinctive tendency' <ref type="bibr">[53]</ref>. However, such a definition leads automatically to the question of what can be counted as a 'novel' behaviour. On the other hand of the spectrum we find definitions like one given by Davis who refers to a behavioural skill which leads to 'some sort of similarity in behaviour among two or more individuals' [15]. Matching behaviour which involves unlearned species-typical behaviour (like con- tagion : e.g. synchronised courtship, flocking and herding behaviour) is then difficult to distinguish from imitation. For following behaviour (e.g. rats learning to follow a trained rat to food in a maze) the term 'matched-dependent behaviour' is often used.</p><p>Thus, from the literature it is not quite clear when the term imitation should be used properly <ref type="bibr">(see [25]</ref> for an extensive discussion). Applying the term imitation to artificial systems like physical or simulated agents poses a number of additional problems. In <ref type="bibr">[41]</ref> ] Nehaniv and Dautenhahn discuss imitation in terms of mappings between dissimilar bodies, which can but need not involve learning. Thus, for the purpose of this paper we use imitation or imitative behaviour in the sense of having two agents, a model and an imita- tor, and both agents are able to actively maintain a fol- lowing relationship. The capacity to follow is designed and assumed to be 'instinctive'. Since our experiments address learning by imitation instead of learning to imitate, the imitative behaviour (following) is not novel.</p><p>Imitation capabilities have been studied e.g. in macaques (see the well known example of the Japanese macaques of Koshima Island whose habit of washing potatoes develop from the observation and imitation of the individual finding of one macaque <ref type="bibr">[26]</ref>), in parrots and mynah birds for reproducing songs <ref type="bibr">[42]</ref> and of course in humans (e.g <ref type="bibr">[9,</ref><ref type="bibr">37,</ref><ref type="bibr">55]</ref>). Similarly, recent work showed that artificial agents can also benefit from mutual interactions [14], e.g. for improving performance in collaborative tasks (e.g. <ref type="bibr">[27,</ref><ref type="bibr">31]</ref>), and from interactions with humans, e.g. for learning com- plex motor skills (e.g. [30, 52]). We view learning, communication' and imitation as important capabilities to possess by social robotic agents, and our previ- ous work studied how these skills can be designed and used by physical autonomous agents. In this paper, we study grounding and use of communication among simulated agents, and, in particular, we investigate the role of social interactions for the sharing of a common context and the building of joint attention between the communicative agents.</p><p>Recent studies on the development of communica- tion have addressed the problem essentially from an evolutionary perspective, (1) either through theoreti- cal models based on biological studies of brain evolu- tion (e.g. <ref type="bibr">[16,</ref><ref type="bibr">46]</ref>), or on sociological studies of pri- mates and human societies (e.g. [2, 18]), <ref type="bibr">(2)</ref> or through computer simulations (e.g. <ref type="bibr">[28,</ref><ref type="bibr">34,</ref><ref type="bibr">45,</ref><ref type="bibr">43,</ref><ref type="bibr">51 ]</ref>). For these studies, the symbol grounding problem2 [22] is solved, once the necessary cognitive abilities have evolved. However, few of these studies consider the influence of behavioural and social factors on the development of communication, exceptions are <ref type="bibr">[18,</ref><ref type="bibr">45]</ref>. A common trend among the above mentioned simulation studies is to give a very simplified physical description of the communicative agents and their environment. The communicative agents are described only in terms of their cognitive (by opposition to behavioural) abilities that enable production and reception of the communicative signals. We dis- tinguish between behavioural and cognitive capabilities. The agent's behaviour relates to regularities of the agent's dynamic interaction with its environment, as can be detected by an external observer. Behavioural capabilities are evaluated by having the agent interact- ing in an environment. Cognition generally refers to rational processes involved in the acquisition, organisation and use of knowledge, thus, cognitive capabilities refer to internal processes inside an agent.</p><p>Cognitive and behavioural capabilities in animals are closely coupled and dependent on one another. However, in artificial systems the distinction can be made much more explicit, since models which are focusing on cognitive capabilities are often neglecting or strongly simplifying agent-environment dynam- icse.g. assuming complete or global information of the world and other agents).</p><p>The cognitive functions involved in grounding of communication have been simulated e.g. as the input/output of an artificial neural network <ref type="bibr">[28]</ref>, as a matching process <ref type="bibr">[51]</ref> or as a probability function [43]. The agents in these simulations are disembod- ied, they do not occupy a physical space (they have no body, no sensors or actuators, and generally occupy not more than a single point in the space) and the result of their actions is atemporal (an action and its resu It occur in one time step). But most importantly, each agent has a perfect and identical perception of the environment features, based on an abstract model of the world.</p><p>By contrast, in our approach, grounding of com- munication is a process inherently situated <ref type="bibr">[ 10]</ref> and dependent on the agent's embodiment <ref type="bibr">[10]</ref>, as it is based on the agent's individual and subjective world representation3 which it constructs through its inter- actions with the environment [13]. We say that two agents are communicating once they have developed a similar interpretation of a set of arbitrary signals in terms of their own sensor perceptions, that is once they have achieved a similar categorisation <ref type="bibr">[22]</ref> of sen- sor perceptions and have successfully attached them with the same set of arbitrary signals. In <ref type="bibr">[5,</ref><ref type="bibr">7]</ref> we car- ried out physical experiments in which an autonomous mobile robot was taught by a human instructor and showed that it is not necessary for two agents to be of the same type to be able to communi- cate. It is not the means by which the agents perceive their environment that matters but rather that they can make similar perceptual distinctions and that these occur in close temporal relationship. In our experiments, we use an imitative strategy, namely mutual following of two agentS4, to create a perceptual context common to learner and teacher agents, upon which the learner grounds its understanding of the teacher's words. The set-up does not require that both agents use the same sensory data or even the same sensors in order to characterise a word. A con- crete situation is interpreted on the basis of the learn- er's own sensory-motor measurements.</p><p>In the computational linguistic studies we men- tioned earlier ([28, 34, 43, 51]), grounding of com- munication is regarded as a computational problem that can be solved solely by means of combinatorial analysis. For these authors, categorisation of sensor perceptions into concepts results from a process of sta- tistical elimination among all possible meaning-object pairs, where the most likely pairs, i.e. the most fre- quently observed, are chosen. However, combinatori- al analysis alone is not always sufficient to discard all irrelevant information, as it is often difficult to pres- ent a sufficiently high number of relevant pair-meaning examples compared to irrelevant ones <ref type="bibr">[16]</ref>. There are also numerous situations in which one feature does not appear (naturally) without another one, e.g. the eyes, mouth, nose and other human face features are bound to appear together with the whole face. In this case, combinatorial analysis would fail to attach two different concepts to the eyes and the face respectively, as there could be no example in which each of these features appears alone. Humans overcome this problem by using attentional mechanisms provided either by the speaker/teacher (pointing, increasing the tone of voice, linguistic deixis) or by the listener/learner (focus of gaze in the direction of the speaker's gaze or the direction pointed by the speaker's finger). Attentional mechanisms act as a cognitive process which restricts the number of observations before combinatorial analysis. However, there is more to communication than just a single cognitive process.</p><p>There is an interactive process between the two communicative agents. For those who study the development of language in children, it is clear that &amp;dquo;to be ef- fective early language learning must take place in a social setting .. <ref type="bibr">[where]</ref> .. turn taking, mutual gaze and pointing are social devices .. [used for] .. establishing a joint attention [between speaker and listener] that creates a meaningful social setting necessary for the development of language&amp;dquo; <ref type="bibr">[21 ]</ref> . Other works implemented such attentional mechanisms as processes distinct from the learning mechanisms, e.g. <ref type="bibr">Steels &amp;</ref> Vogt's pointing strategy <ref type="bibr">[50]</ref> and Yanco &amp; Stein's action-selection mechanism [57]. By contrast, we develop a single cognitive architecture which enables both associative learning, selective attention from parsing of continuous sensory information, and the creation of a mutual binding between the two agents by means of mutual phototaxis (as an underlying social relationship). In agreement with Deacon's sug- gestion [ 16] that &amp;dquo;the acquisition and use of symbols requires considerable facility for conditional associa- tive learning, including an efficient short-term mem- ory for sequences and combinations, and an ability to easily and rapidly produce new combinations&amp;dquo;, we developed a Dynamical Recurrent Associative Memory Architecture (DRAMA) [7] which provides short-term memory of events, learning of temporal sequences and fast retrieval of the learned combinations by means of Hebbian mutual associativity. Note that recent neurobiological studies support such an Hebbian approach to account for the brain processing involved in discrimination and categorisation of words (see <ref type="bibr">[48]</ref> for a good review of these studies). In addition, following Deacon's suggestion that &amp;dquo;symbolic processes and sensory-motor attentional process may utilise the same neurological computations for different purposes&amp;dquo; <ref type="bibr">[16]</ref>, in our architecture the same learning and retrieving algorithms are used for gener- ating the symbolic associations (input from a radio sensor) and the sensor-motor associations, which are then used for controlling the robot's movements (motor activity) and communication (transmission of radio signals).</p><p>This work follows previous experiments in which we studied grounding of communication in a two- agent (teacher-learner) set-up [5, 6]. A learner robot followed a teacher agent, which was in one case a robot [6] and in another case a human instructor [5], around in a given environment. The learner robot was taught to differentiate between locations in the arena <ref type="bibr">[6]</ref> (being on the plane, climbing up and climbing down a hill) and between different objects <ref type="bibr">[5]</ref> (boxes, coloured patches on the floor) by associating the cor- rect perceptual combinations with the different radio signals emitted by the teacher. Successful results demonstrated, on the one hand, the efficiency of the learning architecture at associating structurally differ- ent sensor patterns under a considerable amount of noisy data. On the other hand, these experiments showed the validity of the proposed imitative strategy, namely mutual following of the two agents, as a means of transmission of a vocabulary from one agent to a second heterogeneous agent (a human or a mor- phologically di fferent robot).</p><p>In this paper, we report on three different studies, which are all carried out in a simulated environment.</p><p>In the first part, we study how the imitative teaching scenario, used in the above described physical experiments in [5, 6], scales up to grounding communica- tion among a group of robots. In the second part, we evaluate how theimtative behaviour, namely following of the teacher agent, benefits the agent by improving its learning performance. Finally, we implement a case study in which using the communication system which was learned in the fir st set of simulation, improves the learning performance of the commu- nicative agents in another learning task. Here, the agents use the vocabulary to transmit to each other object locations, hence speeding up the learning of these locations by each agent individually. Note that we will use the term robot as well as the term agent in the rest of the paper to refer to the simulated mobile agents. The term robot is used because the agents in the simulations represent physical robotic agents, as used in previous experiments. They have similar sen- sor and actuator capabilities (sensitivity and range) as that of the physical robots and use the same control mechanism (see section 4). The aim of the simulation studies was to carry out experiments with a group of robots, what we could not do in a physical environ- ment (as we could not have more than two physical robots). We are however aware that simulations are not the same as physical experiments and have addressed this point previously in <ref type="bibr">[7,</ref><ref type="bibr">5]</ref>. Current work of one of the author investigates the implementation of the simulations reported here in a real set-up of several Khepera robots, see section 7.</p><p>The rest of this paper is divided as follows. In sec- tion 2 we explain the scenario of the experiments. In section 3 we describe the agents' control architecture and in section 4 we present the experimental set-up of the experiments. We report on the results of the exper- iments in section 5 and discuss in section 6 the contribution of these experiments to the study of the symbol grounding problem in situated robotic agents. We conclude the paper (section 7) with a short summary of the results of our studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE EXPERIMENTAL SCENARIO</head><p>We carry out two sets of simulation studies in which we investigate first grounding and then use of com- munication in a group of nine robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning the Vocabulary</head><p>In the first set (section 5.1 ), the agents learn a vocab- ulary to differentiate between coloured patches and to describe their locations in terms of distance and orientation, i.e. in polar coordinates, relative to a homing point. The vocabulary is transmitted from a teacher agent TO, which has a complete knowledge of the vocabulary from start, to eight learner agents, which have no knowledge of the vocabulary at the start of the experiments. Once a student becomes confident enough in its learning of the correct signal-meaning correlation (that is after having observed a sufficient number of times the same signal-sensor measurement pair), it becomes teacher in its turn. The teacher agent TO does not learn, that is its definition of the vocabu- lary remains unchanged during the whole experiment. The learner agents, however, carry on learning even when they begin to teach. Thus, a 'bad' teacher, that is a learner agent which has become confident in incorrect signal-meaning pairs, can become a 'good' teacher later in the experiment if it can update aga in its confidence on the correctness of its word-meaning pairing under correct external teaching (and reversely a good teacher can become a bad teacher under oppo- site circumstances). The level of confidence under which the learner becomes a teacher is varied in the experiments and its effect on the learning performance of the whole population investigated.</p><p>Transmission of the vocabulary from teacher agent to learner agent occurs as part of an imitative strategy, namely mutual following, between the two agents. Each agent follows the other by means of phototaxis, namely tracking lights which are attached to the robots. Because following is mutual, it results in a smooth binding between the two agents. The agents seldom loose sight of each other, because when the learner runs slower than the teacher waits for it, and they find each other more easily as they are both look- ing for one another. While the two agents wander ran- domly in the environment and follow each other, the teacher sends radio signals, i.e. 'words', to describe its external perceptions, i.e. observation of a coloured patch, or internal perceptions, namely odometry (rel- ative measure of distance from homing point) and ori- entation relative to the homing point. The learner attaches a meaning to the teacher's signals in terms of its own sensor-motor perceptions, that is its vision perception (coloured patches' detection) and its meas- ure of odometry and orientation. While bounded by the following process, learner and teacher agents are set in a position from which they share a common context of both external (face the same direction) and internal perceptions (perform the same movement, travel the same distance and on the same ground). This implicit similarity between the two agents' per- ceptions is what enables the learner to make sense of the teacher's words, as the teacher talks only of what it senses, unaware of the actual learner's perceptions. It is thus an unsupervised learning mechanism used in a teacher-learner set-up. We use a connectionist5 model for the learning of 'word'-observation pairs, where ' incorrect associations, due to mismatched agent's observations, are discarded compared to correct ones by a process of statistical elimination depending on their relative frequency of occurrence (cf section 3).</p><p>Further simulation studies are carried out (section 5.1.2), which investigate the importance of the fol- lowing strategy for the success of the learning by com- paring the learning curve of the agents with and with- out this following capability. This study follows previous experiments which we carried out using two teacher-learner autonomous robots. There, the learner robot was taught by the teacher robot a vocabulary to describe its external per- ceptions of objects <ref type="bibr">[7,</ref><ref type="bibr">5]</ref> and internal perceptions of The teaching scenario was based on the imitative-fol- lowing strategy, as described above. The study report-t ed here validates the imitative teaching scenario by showing that it scales up successfully to transmitting a vocabulary across a group of agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using the Vocabulary</head><p>In a second set of simulations (section 5.2) we study how the vocabulary learned in the first set of experi- ments can be used beneficially by the same group of nine agents. The experiments start with all agents knowing a vocabulary of 31 words for describing the nine different colours because it could provide fast computation and the ability for spatio-temporal asso- ciation and learning of time series, as required by the task. Note that there is no biological plausibility in the model and in its implementation. of the coloured patches present in the environment and their loca- tions in terms of of 14 distance measurements (measured by odometry) and 8 angle measurements (using its compass measurements and the odometry calcu- lus). As the agents wander randomly around the envi- ronment, they learn the locations of the coloured patches by associating the sensor perception of the particular colour input with the coordinates of the agent's own position. When one agent has learned the location of a coloured patch, it can further transmit the coordinates of the location via the communication channel (using the learned vocabulary) to all other agents. Each agent can thus learn the coloured patches' locations from listening to the speaker agent's talk- ing and without actually coming across the particular location itself This results in a speeding up of the learning of the whole population, which we study using two different scenarios for the information transmission. The transmission occurs as soon as one agent comes across a coloured patch location (one-to- many, long distance communication) or only when two agents come across one another (one-to-one, short-distance communication).</p><p>Note that in the scenarios which we used in our experiments (i.e. learning of a vocabulary for colour types and locations in polar coordinates, transmission of the vocabulary through mutual following of teacher and learner agents and use of the vocabulary for trans- mitting information about coloured patches loca- tions) was inspired by the communication system used by the Honey bees, following Moukas &amp; Hayes' robotic implementation <ref type="bibr">[40]</ref>. Honey bees communi- cate through a 'language' composed of dance patterns that form a vocabulary to describe different flower types and locations, the location being measured in terms of the distance, measure of energy required to reach it, and of the direction to follow from the nest.</p><p>New born bees 'learn' 6 to ground each dance pattern with a particular sensor perception of sugar, energy and orientation. It is suggested that the newborn bees learn the meaning of the dance by associating their memory of the dance patterns and of the scent of the flower (of which traces had been brought back by the dancer bee) with the location of the particular flower, when they later discover it <ref type="bibr">[19,</ref><ref type="bibr">36]</ref>. Note, however, that we make no claim as to our system being a valid model of the Honey bees' communication system; we only stress its analogy with this system.</p><p>3 CONTROL ARCHITECTURE Learning of the vocabulary, i.e. grounding of the teacher's signals in the learner robot's sensor-actuator state results from an association process across all the robot's sensor-actuator states, which is produced by a Dynamic Recurrent Associative Memory Architecture (DRAMA). A complete mathematical description of the architecture is given in [7]. In the following, we briefly summarise its main properties as given in <ref type="bibr">[7]</ref>, and then describe the training and retrieving algorithms which we use in the experiments reported in this paper.</p><p>The complete control system of our agents is com- posed of a set of event recognition modules (one for each sensor and actuator) and an associative module that contains the DRAMA architecture (see figure <ref type="figure">1</ref>).</p><p>Sensor and actuator information is encoded in binary (0/1) bit-strings. The event detector modules act as a selective mechanism on the sensor and actuator information. Once a variation in one sensor measurement or actuator state occurs, i.e. an event has happened, it triggers the corresponding event detector module and the new information is passed further to the associa- tive module (in the experiments a one bit variation was considered to be sufficient to trigger the module). The associative module (DRAMA architecture) con- sists of a fully recurrent network with self connections to each unit. The structure of the network is dynamically updated each time a unit has been activated by an input from the event detector. To each connection in the network are associated two parameters: a time parameter and a confidence factor. Time parameters and confidence factors are updated following Hebbian rules, providing an associative type of learning; the time parameters record the time delay between units' activation while the confidence factors keep a memo- ry of the frequency of units' coactivation. The self connections to the units provide a short-term memo- ry of the activation of the unit, by sending back the activity to the unit. The duration of the memory is limited, as the activity decreases by a fixed ratio after each passage along the recurrent connection. The unit activity stops (becomes zero) when the recurrent activity is lower than a fixed threshold. The short-term memory of a unit's activity enables associations between patterns of unit activation that have been delayed in time, a specificity required for solving our particular learning task (the following strategy implies that radio signals have to be associated with sensor measurements which are received with a varying time delay).</p><p>Figure <ref type="figure">1</ref>: Schematic representation of the control system of the learner and teacher agents with three sensor-actuator systems, ¡. e.g. compass, radio and motors. Note that in the experiments the motors do not input to the associative architecture as there is J, no external mechanisms for changing their activity, hence the dotted arrows in the figure. Only the output of the architecture to the motors is used to directing the robot's movements.</p><p>-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Behaviours</head><p>In the experiments we report here, DRAMA is used for both learner and teacher agents. Each robot starts with a set of basic behaviours for wandering, obstacle avoidance and following. The behaviours are predefined by setting in advance the values of the connec- tion parameters between infra-red sensors, bumpers, light sensors and the agents' motors. Obstacle avoid- ance uses infra-red and bumpers. Following consists of phototaxis using the light detectors (cross-connectivi- ty between light sensors and motors). Wandering is a default input unit to both motors, always active, that leads the robot to move straight when no other move- ment is produced by the two other behaviours. Similarly, the teacher robot's knowledge of the vocab- ulary (in the simulations) is predefined by setting the value of the connections between relevant sensor information and the radio emitter. Details of the implementation can be found in <ref type="bibr">[4]</ref>.</p><p>At each time step, i.e. one processing cycle, the robot's behaviour is determined by retrieving the net- work outputs to the actuators (motors and radio emit- ter) given the current sensor input. The teacher 'speaks' (emits radio signals) only when it sees the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Algorithm '</head><p>Learning is dynamical and is performed continuously by updating the connection parameters to account for variations measured in the sensor-motor state. That is, at each cycle the current sensor and actuator informa- tion is compared (in the event detectors) to the infor- mation measured in the previous cycle. If a variation is noticed, the new information is passed on to the associative memory where it is memorised for a fixed duration, during which it is associated to any incom- ing event. It is important to understand that the same network (the DRAMA architecture) is used for learn- ing (i.e. generating the symbolic associations between radio sensor and other sensors) and control of the agent's behaviours (i.e. motor activity and radio signal transmission).</p><p>The success of the learning is determined in the experiments by comparing the proportion of correct and incorrect signal-object associations. Learning is successful when correct associations have been done more often than incorrect ones. Incorrect associations result from a mismatch between teacher and learner perceptions, where the learner robot associates the teacher's radio signal with an incorrect perception (which does not correspond to that of the teacher). This mismatch is due to the imperfect following of the two agents. Note that there is no noise/imprecision in signal transmission, i.e. the signal is perfectly received. Incorrect associations can also be due to incorrect emission of the signal, as it is the case in the experiments (section 5.1) in which we vary the threshold of confidence under which the learner robots can speak. It happens that a learner robot (which has become teacher) emits incorrect signals (relative to the definition of the vocabulary), when it is allowed to speak before it had made the correct sig- nal-object correlations.</p><p>The comparison between correct and incorrect correlations consists of calculating the ratio between the values of the confidence factor parameterS7 attached to the correctly and incorrectly activated connections. The confidence factor parameter gives a measure of frequency of co-activation of the two units linked by the connection, that is, e.g., of the frequency of correlation between one signal (one activated radio unit) and an object's features (pattern of activat- ed units in colour, angle and odometry sensors). In the stage of learning the vocabulary (section 5.1), we consider only the connections between the radio sen- sor units and the units of colour (coloured patches' feature), angle and odometry sensors (location), while in the stage of learning the locations of the coloured patches (section 5.2), we consider only the connec- tions from the colour sensor to the angle and odome- try sensors. Two units are said to be correlated if the value of the confidence factor associated with their mutual connection is higher than half the maximal value of confidence factors for all connections leading to each of these units. Note that the network's connections are bidirectional and asymmetric and that we require correct correlation in both directions.</p><p>Learning of the signal-object associations is successful when all radio units (one unit stands for one signal) are correctly connected to the corresponding colour- angle-odometry unit combinations (describing the corresponding objects), that is once all the connec- tions between radio units and these other sets of units satisfy the above mentioned correlation criteria. Choosing the threshold on correlation to be half the maximal activation means that we accept a proportion of up to 50% of noisy data, where the noise consists of incorrect signal-sensor associations.</p><p>Note finally that there is no a-priori limitation on the number of words the robot could learn; this depends on the number of inputs (i.e. radio input units in the experiments) to the network, which fixes the maximal capacity of the network (i.e. the number of patterns which can be stored). This capacity has been evaluated to be of the square of the number of network units (i.e. N2, where N is the number of units, see [7]). The system could thus possibly scale up to learning a vocabulary of much bigger size. In <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>, we report on an experiment with a mini-sized hu manoid robot (doll-shaped), which is taught by a human instructor English proto-sentences to describe its interactions with the instructor. This experiment exploited DRAMA's ability for learning sequences of inputs, which allows learning of the sequence and combination of the words forming the sentences. The experiments reported in this paper exploit only DRAMA's ability at binary association for word- meaning pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retrieval</head><p>The phases in which the robot uses its understanding of the vocabulary to learn new colour patches' loca- tions (section 5.2) is obtained by retrieving the corre- lated units activity in the colour, odometry and angle sensors given the particular radio signal. The robot's emitter sends three radio signals for describing the patch type (colour) and its location in terms of angle and odometry measurements. The three radio signals retrieve a specific unit activity in colour, angle and odometry sensors. Learning of the so transmitted coloured patches' locations results from updating the connection between the co-activated units in these sensors.</p><p>A retrieval mechanism based on the notion of retrieved and real sensor states was implemented to allow the robots to use their understanding of the vocabulary to learn new colour patches' locations from hearing another robot's saying so. The robots' predefined understanding of the vocabulary is set-up by fixing from the start their networks' connections between the radio sensor and the colour, angle and odometry sensor, such as to represent a vocabulary of 31 words to label 9 colours, 14 odometry levels and 8 angle measurements. The robots communicate by exchanging information about objects' locations. One robot's message is composed of three radio signals (1 1 byte with 1 bit activated) for describing the object's colour and its location in terms of angle and distance measurements. When one agent receives a radio signal sent by a second agent (which produces a pattern of activity in the radio sensor units), it decodes it by retrieving the correlated activity in units correspon- ding to the colour, angle and odometry sensors. That is, the three radio signals retrieve a specific unit activ- ity in each of the set of units attached to colour, angle and odometry sensors. Learning of location of the coloured patches transmitted in this way results then from updating the connections between the co-acti- vated units in the colour, angle and odometry sensors (but not the radio sensor, see explanations below).</p><p>The sensors' activities which are retrieved for the colour, odometry and angle sensors are stored in a retrieved vector state (one for each sensor), which are different from the real vector states, which contains the current sensor measurement. The retrieved and real vector states of the sensors can be thought of as two sets of network units, parallel to each other and receiving outputs from all other units in the network (as shown in figure <ref type="figure" target="#fig_1">2</ref>). The retrieved vector state does not input to the network. Thus, the retrieved sensor activity in the experiment (which is placed in the retrieved sensor state) does not affect directly the robot's behaviour; that is, it does not participate in controlling the motors and radio activity (by retriev- ing the real unit activity in these sensors). However, in order to control the robots' speaking, i.e. to activate the radio emitter to transmit the objects' locations, a mechanism is implemented, which transfers the retrieved state into the real one. This transfer occurs either when the robot discovers an object or when it meets another robot and engages in an answer-ques- tion dialogue (see description of the results below).</p><p>Learning of the objects' locations results then from association among retrieved sensor states and association among real sensor states. It is important to note that association among retrieved sensor units is done separately from association among real sensor units.</p><p>Thus, in the experiment, the listener agent can either learn from the retrieved sensor units' activity (which correspond to the sensor perceptions 'transmitted' by the speaker robot) by associating mutually the active units in each sensor system or it can learn from the real sensor units' activity (its current sensor measure- ments), but it does not associate mutually its retrieved sensor state (speaker's measurement) with its real sen- sor state (its current measurement). By definition, radio measurements are never associated with other sensor measurements in this experiment. When they are produced by the speaker, they do not produce an input activity in the speaker's radio sensor but rather an output activity, which is then not forwarded to the DRAMA associative module (see beginning of section 3 and [7] for explanations); thus, no association occurs. When they are received by the learner, only the retrieved sensor states they produce are associated.</p><p>Association between real sensor states was defined so that all except the radio sensor state could be associated. Figure <ref type="figure">3</ref> illustrates the transfer from real to retrieved sensor states during a two robots' transmis- sion of an object's location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>Simulation studies are carried out in a rectangular arena measuring 700 by 700 units (1 unit = lcm), which contains nine objects, coloured patches, which can be distinguished by their different colours. Figure <ref type="figure">4</ref> shows a graphical representation of the simulated environment with the nine robots. The objects are represented as big rectangles of different colours.</p><p>Figure <ref type="figure">3</ref>: Transmission of the location of an object. The emitter robot first translates the real (as measured when it discovered the object) sensor vector states containing the description of the object features (color, angle, distance) into the three corre- sponding (retrieved) radio sensor states (signal 1-2-3). It then transfers the retrieved radio states into real ones; that is, it emits the three signals. The receiver robot picks these signals up by storing them in its real radio sensor state. It then translates these three signals by retrieving the corresponding retrieved sensor states for color, angle and distance of the object. This information is further used by the receiver robot to learn the location of the object.</p><p>These are coloured patches lying on the ground ovei which the robots can run. In the figure, we see the teacher robot (in dark grey) between five of the eight</p><formula xml:id="formula_0">1 -1 1 1 1 1-1 i</formula><p>Figure <ref type="figure">4</ref>: Simulation studies are carried out in a 2-D simula- tor, in which the robots are represented as rectangles of 30 by 20 units, a triangle determining the front. Coloured patches are represented as big rectangles whose number correspond,, to the colour type. There are nine types of coloured patches with one location for each type. In the figure, we see tht teacher robot (in dark grey) between five of the eight learnei robots (in light grey), moving between objects 5 and 8.</p><p>: learner robots (in light grey), moving between objects t 5 and 8. On top of the figure, the result of the robots' : speaking is written for each of the nine robots. The teacher robot (2nd column) outputs '128'. This refers to the activation of the 8th radio unit in the robot's network, which is the radio encoding for the label of the object, across which the three robots are currently running. None of the learner robots is speaking, i.e. they all output '0', as they have not yet seen the object and thus have not yet associated the teacher's signal with the object's features. There are nine8 robots, which are equipped with colour vision (nine colours) to distinguish between each coloured patch, infra-red vision to see the walls, light vision in front and back for mutual following by phototaxis and a radio transceiver to communicate. They also have a compass which measures bearings of 45 degrees and an odometry sensor which is incre- mented at each wheel rotation, hence giving a notion of travelled distance. Given the compass and odometry sensors information, the robots calculate their positions relative to the centre of the arena. They can _ then determine their position in the arena (the angle r <ref type="bibr">(virtual)</ref> &amp;dquo;sensor&amp;dquo;), which is divided in 8 quadrants.</p><p>&amp; d q u o ;</p><p>Infra-red and light detectors are associated with a &amp; d q u o ; cone of vision of 180 degrees which is segmented into I eight quadrants. The measurement of the sensor is ) given by an 8-bit string where each bit corresponds to the value measured in each of the eight quadrants (e.g. infra-red=(11000000) stands for an infra-red activa- tion of the first two quadrants). The range of sensitiv- ity of the sensors are given in table 1. Each coloured patch is defined by a different colour. The behaviours of all robots are calculated by the same routines, that is there is one network (DRAMA architecture) per robot and the same retrieving and training algorithms are applied sequentially to them for determining the behaviour and learning of each of the robots inde- pendently9. In order to produce a more realistic simu- lation 1°, the following behaviour is made imperfect. An agent is able to determine the position of the other agent with respect to its own by measuring the differ- ence of intensity measured by its eight frontal light detectors. It can determine the other's position with a precision of 20 degrees. In addition, noise 11 is intro- duced in the calculation of the robot's movements in order to represent the imprecision measured in the real robot's movements. The imprecise alignment and following of the robots leads to differences in their respective travelling and hence differences in their sen- sor measurements. Noise, i.e. incorrect teachingobservation associations in our experiment is due to this mismatch between teacher and learner percep- tions, which results from the imprecise robots' alignment (thus the noise is contained between 0 and 20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In this section, we report first on simulation studies in which we study transmission of a vocabulary among a group of nine agents which then use the vocabulary for transmitting information about object locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning the Vocabulary</head><p>In a first set of simulations (section 5.1.1), we study how a common understanding of the vocabulary can spread among a group of agents when starting with one teacher agent and when each successful learner agent can in turn become another teacher. In a second set of simulations (section 5.1.2), we study the influ- ence of the following strategy on the learning per- formance for different learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Social Learning in a Group of Agents</head><p>We carry out a set of 10 runs (1 run lasting for 400,000 cycles) in which we study the speed of learn- ing of a vocabulary of nine words by eight learner robots, given 1 teacher robot. The nine words refer to nine different coloured patch types, defined by nine different colours. The coloured patches are spread homogeneously in a square area as shown in figure <ref type="figure">4</ref>. The learner robots can become teachers, that is they can emit signals for 'naming' the coloured patches once they have reached a sufficient level of confidence (specified by a threshold) in their word-colour associ- ations. We ran 10 simulations with a different value of , &amp; d q u o ; the threshold (from 10 to 100) for each. Each thresh- old value corresponds to considering the agent suffi- ciently confident in a particular word-colour associa- tion when it has observed this combination for at least 10, 20, 30, ...,100 times. This refers to the actual value of the confidence factors for these word-colour connections, as the confidence factors are increased by a value of 1 at each co-activation of the corresponding units. Because the agents can make incorrect associa- tions due to mismatched observations of teacher and learner agents, correct learning of the word-colour pairs is not immediate but results from the repetition of the teaching process until incorrect associations can be discarded from the correct ones by virtue of their relative frequency of occurrence. Therefore, choosing too low a threshold on confidence factors can allow the learner agent to become a teacher before it has made a sufficient number of correct correlations compared to incorrect ones. In this case the robot would emit incorrect signals given a colour perception, hence letting the learner agent following it make an incorrect association. This would increase the global amount of noise, i.e. of incorrect association episodes, and could lead to the failure of the experiment (i.e. unsuccessful learning for all agents) if the number of incorrect asso- ciations outnumbers the number of correct ones. This Figure <ref type="figure">5</ref>: Learning curve (mean value for all agents) for four different thresholds (10,20,30 and 50) on the radio unit activation. Learning is unsuccessful as long as the threshold is lower than 50. The lines good and bad represent the proportion of correct and incorrect respectively signal-object associated pairs over the total number of taught pairs. is demonstrated by the results of the experiments, where we observe that learning is unsuccessful as long as the threshold is lower than 50.</p><p>We show in figure 5 the learning curve for four dif- ferent choices of minimal levels of confidence out of the ten investigated. We observe that the number of incorrect correlations increases in the three first examples, so as to become more frequent than correct ones, leading finally to unsuccessful learning. Note that the curves for threshold values greater than 50 have simi- lar shape to that shown in figure <ref type="figure">5</ref>. The simulation with threshold 50 was carried out for 20000 more cycles and it was shown that the curve did not decrease after having reached the maximal value of 1.</p><p>The value of 50 for the threshold under which the learning is successful reflects the proportion of noise, i.e. incorrect associations due to incorrect matchings of the teacher-learner perceptions in the particular set-up. This value would then vary from one experiment to another. In the present case the increase of the noise which leads to unsuccessful learning is due to incorrect teaching provided by the learner robots.</p><p>The more robots are speaking, the more noise is con- tained in the teaching. By definition, each robot can hear only other robots which are in a distance of 1.5 its body size. This means that each robot can be taught by at most four other robots (when placed in each quadrant around it). In fact, each robot is usual- ly taught by two other learners, as the robots tend to quickly form long chains. If one robot is a bad teacher then its incorrect teaching spreads quickly to other robots. We make three observations: 1) There is in average 20% of noise, i.e. 20 % of associations are incorrect; this is due to incorrect matching of the teacher-learner perceptions (as evaluated in the two agents, teacher-learner scenario [5, 7]); 2) Each object's description (angle and distance measure- ments) overlap with at least two other objects' descrip- tions (which makes the association more difficult, as the correct set of features is less distinct, see discussion of the architecture's properties in <ref type="bibr">[7]</ref>); 3) Each robot can be taught at most by two other learners. Given the three above facts, the noise increase due to one bad learner is enormous and chances are small that correct Figure <ref type="figure">6</ref>: Learning curves for follower and non follower learner agents in three different learning tasks: learning a vocabulary for objects, for scaling values of polar coordinates and for orientation relative to a compass. , -learning would result in this case. Correct learning is thus ensured when all learners speak only when they have correctly learned the vocabulary. Each robot is taught each signal about 5 times during its passage across one object (the teacher repeats the signal sever- al times). Correct learning of the vocabulary requires a statistics of at least 10 passages over each object (i.e. 10 different teaching episodes in which the robot approaches the object from a different direction). Therefore, correct learning should occur after about 10 . 5 = 50 associations, i.e. a value of 50 for the con- fidence factors associated with the signal-object fea- tures.</p><p>The simulations in which learning was unsuccess- ful were stopped, while the curve of bad association was still increasing. If we had let the simulations run longer we could have checked whether the curve would converge to the maximal value of 1. In this case, this would have meant that all robots had finally converged to a common definition of the vocabu- lary, while this definition would have been different from that initially taught by the teacher robot. The observation of a shift in the meaning of the words of the vocabulary for the agent is similar to the 'emer- gence of a dialect' as in the studies of [ 1, 51 in their simulations of the development of language. We did not make this analysis mainly because the aim of our experiment was to study correct transmission of a fixed pre-defined vocabulary (and also because these simulations were extremely long to run), as opposed to [ 1, 51 who studied the emergence and variation of a lexicon as an effect of its transmission. It would of course be very interesting to carry out similar studies in the future, using the physical simulation and the DRAMA architecture. In particular, it would be valu- able to compare our results with results obtained in [1, 51] where no physical or behavioural description of the agents were given. We could then determine the role played by these physical factors in the transmis- sion of a language. This point will be further discussed in section 6. We carried out a set of ten runs (1 run lasting for 100000 cycles) in which we study the importance of the imitative teaching scenario, namely mutual fol- lowing, for a good transmission of the vocabulary. For these studies we use only three agents, 1 teacher and two learners. One of the learners does not possess the predefined capability of following another agent by means of phototaxis. Instead, when it meets one of the two other agents it simply avoids it as an obstacle and carries on its random wandering. It can however learn, similarly to the other learner agent. If it is close enough to the speaking agent, it can receive its signals.</p><p>It can then associate them with all other sensor measurements it made during a short time delay window (plus minus the short-term memory duration, c.f. sec- tion 3) before and after the signals' <ref type="bibr">reception.</ref> Figure <ref type="figure">6</ref> shows the learning progress of the two learner agents (with and without phototaxis capabilities) when learning three different types of vocabular- ies : 1) a vocabulary to describe the nine objects (coloured patches) of the environment, 2) a vocabu- lary to label scaled values of polar coordinates refer- ring to the objects' locations and 3) a vocabulary for the four quadrants of a compass which measures the robot's individual orientation. Learning each of these vocabularies corresponds to associating radio signals, i.e. the teacher's words, with respectively 1) nine dif- ferent colour measurements (objects' features), 2) five different measurements along the odometry scale and eight different measurements of angle, position in the arena with respect to 8 quadrants (objects' polar coor- dinates), and 3) four different compass measure- ments. The Y-axis represents the ratio between cor- rectly and incorrectly learned words. In table 2, we show the mean values of this ratio over the 10 runs for each experiment and for each learner robot.</p><p>Results show that the non-follower agent is less successful on average and slower at learning the vocabulary concerning the coloured patches and the polar coordinates, and that it was always unsuccessful at learning the vocabulary concerning the orientation relative to a compass. These results imply that the ability of following improves the grounding of extero- ceptions, as done when naming the coloured patches detected by different colour perceptions and when naming its position relative to global polar coordi- nates. But it is especially important for grounding proprioceptions, as done when naming its relative ori- entation. Being close enough spatially is often suffi- cient for the agents to share a common context of external perceptions and then to successfully ground the vocabulary onto the same sensor perceptions. In the experiments the non-follower agent learns the vocabulary concerning exteroceptions correctly because when it receives the teacher's signal, it is often close enough to get a similar measurement of polar coordinates (the spatial scaling of the environment it learns to name is wide enough to allow two agents to share the same set of coordinates at one time) and to make a measure of colour (detection of coloured patch) shortly before or after meeting the teacher.</p><p>The follower agent is, however, faster and more suc- cessful at learning because of its constant spatial close- ness to the teacher agent. By contrast, it is not suffi- cient for the agents to be spatially close to one anoth- er for them to share a common set of internal percep- tions which would allow them to successfully ground these proprioceptions onto a common vocabulary. The imitator replicating the model's actions by using the following strategy results in both agents perform- ing the same movements relative to their body axes. It also allows it to make other similar internal percep- tions which are a consequence of its actions, e.g. its orientation, inclination (see experiments reported in [6]), and odometry. In the experiments the follower learner agent implicitly imitates or replicates the teacher's movements while following it, and conse- quently orients its body towards the same direction as the one pointed to by the teacher. In contrast, the non-follower agent which tries to avoid the teacher agent is not or seldom (and then only for a short period) oriented similarly to the teacher and is thus less likely to correctly associate the teacher's signals. This accounts for the failing of its learning observed in all 10 simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Simulations: Using the Vocabulary</head><p>We carried out 3 sets of simulation studies (10 runs each) in which the nine robots use the vocabu- lary, learned in the experiment of the previous section, to transmit to each other the location (polar coordi- nates) of the nine objects (coloured patches) in their environment. The e xperiment starts with all agents knowing a vocabulary of 31 words for describing the 9 different colours of the coloured patches present in the environment and their locations in terms of 14 distance measurements (odometry levels) and 8 angle measurements. As the agents wander randomly around the environment, they learn the locations of the coloured patches by associating the sensor perception of the particular colour input with the coordi- nates of their own position. When one agent has learned the location of a coloured patch it can further transmit it via the communication channel (using the learned vocabulary) to all other agents. Each agent can thus learn the coloured patches' locations from listening to the speaker agent's talk and without actu- ally coming across the particular location itself. This results in speeding up the learning of the whole population.</p><p>We study two different scenarios for the informa- tion transmission. The transmission occurs as soon as one agent comes across a coloured patch location (one-to-many, long distance communication) or only when two agents come across one another (one-toone, short-distance communication).</p><p>In figure <ref type="figure">7</ref> we compare the results for three differ- ent learning scenarios (called Experiments 1, 2 and 3).</p><p>In the first case the agents learn the locations of the coloured patches by making the associations when they travel over a coloured patch; in this case learning of the patches' locations results from each agent's indi- vidual search. In Experiments 1 and 3 the agents use Figure <ref type="figure">7</ref>: Speed of learning of the locations of the colour patches in three cases (mean value over all nine robots): as a result of each robot's individual exploration <ref type="bibr">(-)</ref>, by listening to another robot's speech which talks as soon as it finds a new location (.-.-), through dialogue conversation when meeting another robot (....). Data are mean values over 10 runs. Error bars are added which show the standard deviation of the values.</p><p>their knowledge of the vocabulary to transmit to each other information about each patch location. In these two experiments learning results from both individual search and from social learning (interaction with other agents). An agent informs the other agent about a patch location by emitting a set of three signals, one corresponding to the colour type, and two correspon- ding to the 'words' for its location in terms of distance and orientation. The other listener agents learn the new 1 ocation by associating together the corresponding three sensor stimuli which have been activated by the reception of the signals (as explained in the previous subsection). In Experiment 2 the agents trans- mit the location of the coloured patch as soon as they discover it, that is when they travel on it. The speaker robot's signal can be received by all robots in the whole arena (long distance communication). In Experiment 3 the robots' transmissions of informa- tion occur only when two robots meet (short distance communication). That is, when two robots are close enough to 'see' each other, they engage in a conversa- tion ; each robot speaks in turn, the robot with lowest numbering (robots are numbered arbitrarily as 1,2,..to 9) first, and asks the other for all coloured patches whose location it does not know yet. That is, one robot sends a signal for the colour type (question) which activates in the receiver agent's network the corresponding patch location (if known). The retrieval process proceeds in two stages: first the cor- responding sensor stimuli for angle and odometry are retrieved, and then by transitivity of the associations the corresponding radio signals are retrieved which are further emitted by the robot as its 'answer' to the other robot's query. The dialogue ends as soon as the agents become separated after 20 cm (e.g. when one agent turns in an opposite direction for avoiding an obstacle or another agent), or when they have enu- merated all the objects.</p><p>We observe that learning of the whole group is faster when the robots can transmit to each other their current knowledge, that is the learning curve con- verges faster in the second and third experiments than in the first one. Learning is also faster when the trans- mission can go from one robot to all robots (second experiment), rather than from one to one (third experiment). This is not a surprising result. Surely, learning with one-to-many communication is faster than with one-to-one communication. One-to-one communication of the objects' locations speeds up learning of the locations compared to an individual search; this is especially due to the fact that the agents do not stop moving to speak to each other; that is, they engage in a question-answer dialogue whenever moving close to one another. Therefore, the dialogue does not interrupt their random search behaviour and, thus, communicating with one another does not delay their individual search behaviour. Dialogues can therefore only enhance the robot's knowledge about locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION OF RESULTS AND THEORETICAL IMPLICATIONS</head><p>This paper reported on simulated experiments in which we demonstrated the validity of a connectioist learning architecture and a teacher-learner scenario based on imitative skills for transmittinga vocabulary among heterogeneous agents. In the first set of stud- ies, we showed that the scenario scales up successfully for transmitting the vocabulary among a group of robots. Further, the importance of the imitative strat- egy for a successful transmission of the vocabul ary was pointed out, as it was shown that agents lacking this capability would be slower at learning or would simply fail. The capacity of imitating the other is particularly important when the word to be learned con- cerns the agent's proprioceptions, e.g. orientation, as it allows the two agents to share similar internal per- ceptions, e.g. pointing in the same direction. An experiment was implemented to show an example of a situation in which it is advantageous and useful for the agents to communicate. Symbolic communica- tion is used by a group of robots for transmitting to each other the coordinates of objects locations. This speeds up the robots' learning of these locations, as compared to the robots learning the locations only when finding one by chance during the robots' ran- dom walk. The rest of this section discusses the contribution of these studies and their results on our understanding of the symbol grounding problem as faced by embodied situated robotic agents. We built our approach on the assumption that grounding of communication creates constraints not only on the cognitive capabilities of the agent but also and especially on its behavioural capacities. In the introduction, we distinguished between behavioural and cognitive capabilities, where the former do not result only from the outcome of the agent's cognitive processes, but from the interaction of these processes' outputs, i.e. the agent's actions, with the dynamics of the agent's environment. In <ref type="bibr">[12]</ref> we discuss the cou- pling of internal and external dynamics (i.e. behav- ioural and cognitive processes) and their roles in social understanding. Agents with poor cognitive capabilities cannot be expected to exhibit overwhelming behavioural complexity, and vice versa. A balanced design of cognitive and behavioural capabilities of the agent on the one hand, and environmental complexity on the other hand, is hard to implement but seems to be most promising. This is increasingly important in the case of social agents, where the environment is not static and fully predictable but consists of other agents.</p><p>With respect to cognitive capacities we defined key features for the learning capacities, such as a selective mechanism for the discretisation of sensor perception and a short-term memory of perceptual events and associative capacities. We implemented these in our experiments by using the DRAMA architecture. In the introduction, we pointed out the importance of a basic behavioural social relationship between the two communicative agents to act as an attentional mechanisms for eliminating irrelevant information which can not always be discarded by means of combinator- ial analysis only. We thus provided our agents with a basic ability of mutual following which creates a spa- tial and temporal binding between the two agents necessary for their sharing of a similar set of percep- tions. However, although we used a simple protocol, namely mutual phototaxis, for achieving this basic 'social' interaction, this proved to be a powerful 'exter- nal' (behavioural instead of cognitive) attentional mechanism.</p><p>Similar experiments on grounding radio signals in robots' sensor capabilities were carried out previously by <ref type="bibr">Yanco &amp; Stein [57]</ref> and Steels &amp; Vogt <ref type="bibr">[50]</ref> who used respectively reinforcement learning and evolu- tionary techniques. We showed in <ref type="bibr">[7]</ref> that our model is more efficient and faster than the above models to learn a vocabulary of the same size and type. Moreover, our model (the associative memory and the following scenario) has the advantage, as compared to these methods, to be less restricted in the sensor stim- uli the robots could talk about. In Yanco &amp; Stein's work [57], the vocabulary consisted only of the robot's actions because the learning algorithm was based on an action-selection mechanism. In <ref type="bibr">Steels &amp; Vogt [50]</ref>, the vocabulary concerned only the robots' external perceptions as these were the only perceptions they could share. By contrast, the mutual fol- lowing strategy we use in our work allows the two agents to share a common context, namely sharing external (facing the same direction) and internal perceptions (performing the same movement, travelling the same distance and on the same ground). When aligning itself behind the teacher the learner agent naturally points in the direction the teacher 'looks' at. While we avoid the problem of the mirror effectl2 occurring in pointing, we however introduce another problem lying in the fact that the learner cannot see over the teacher's body. In our experiments this is partly solved by the two agents moving and the learn- er eventually reaching the teacher's previous position. This obstruction problem also occurs in <ref type="bibr">Steels &amp;</ref> Vogt's pointing model <ref type="bibr">[50]</ref>, as in their experiments the two robots spoke also about objects on their sides which one of the agent could not see. In addition, because the learning mechanism we use is based on mutual associations between inputs from any sensor or actuator systems of the agent, the vocabulary can concern any (proprio and extero) perceptions of the agent. We reported here on experiments where the agents talked about external perceptions of objects and internal perceptions of orientation. In previous work, we reported on experiments where the learner agent was taught a vocabulary concerning its inclina- tion [6] and its actions [8].</p><p>However, a disadvantage of our following strategy over a more precise pointing mechanism, is that we are restricted to speaking only about static events or events that last long enough for the learner to eventu- ally see them. In particular, a 'finger' pointing mech- anism which is more accurate than our 'body' pointing mechanism, would be more appropriate in an environment where features cannot be spatially separated (as in the eyes and face example which we described in the introduction). Another disadvantage of our mutual following strategy is that it restricts the number and type of movements that can be taught to only those of motion. This limitation could however be overcome by using a more complex imitative strat- egy, as for instance the imitative scenario developed in [17] or in [30]. Then, it should be possible to teach concepts relative to movements of many more body actuators and also of more complex sequences of movements (compare with experiments in <ref type="bibr">[3]</ref>).</p><p>We mentioned in the introduction that previous studies on the development and evolution of commu- nication differ from our work mainly by the absence or the simplification of the physical description of the agents and their environment. However, although these studies used disembodied a-behavioural13 agents, their results have sometimes been interpreted in terms of the agents' behavioural skills. E.g., Oliphant &amp; Batali <ref type="bibr">[43]</ref> show that imitation is not a factor enhancing the evolution of communication.</p><p>This result seems to contradict ours as, in our simulations, the capacity of following and so imitating another robot's movement is advantageous. The problem lies in our different definition of imitation or imitative behaviour. The agents' imitative capability in Oliphant &amp; Batali's work is not a behavioural skill which would involve at minimum 1) a definition of the agent's actuators, 2) a mechanism responsible for the agent's observation of the second agent's actions, and 3) a mapping mechanism which interprets the observed actions into the agent's own set of actions [14, 17, 38]. The same critic applies to Smajuk &amp; Zanutto's [49] neural model of conditional learning in which imitation is only feeding the observer agent's network with data given by the observed agent's net- work, without use of any transfer mechanism. In Oliphant &amp; Batali's work imitation is for an agent to match its statistical function of word-meaning associ- ations, a purely cognitive functionality, with the one of a second agent; in other words, one agent learns the language by strictly reproducing each word-pair asso- ciation as produced by the other agent. The authors conclude that this 'imitative' learning procedure is not advantageous for the development of communication as it only allows for copying a currently existing com- municative system and does not allow for its modification and further improvement. These authors' work and their results relative to the influence of imitative behaviour on the development of communication is thus not comparable to ours as we are not approaching the problem from the same angle. In our model imitative behaviour is not the learning mechanism per se but acts alongside associative learning to constrain association of relevant data.</p><p>We follow a behaviour-oriented (rather than a pure cognitivist) approach to the problem of grounding communication as we investigate the influence of social and behavioural aspects onto the development of communication. Our approach differs from previ- ous studies on the influence of sociality onto the development of communication (e.g. <ref type="bibr">[18,</ref><ref type="bibr">44,</ref><ref type="bibr">45,</ref><ref type="bibr">56]</ref>), as we give a complete spatial and temporal description of our agents' behaviour. This allowed us to point out the influence on the success of the learn- ing of environmental factors (see <ref type="bibr">[5]</ref>), such as the teaching of objects' featural description and relative dispersion in the environment in relation to two parameters of the learning architecture (short-term memory duration and long-term memory capacity). In addition, we showed the importance of having a spatial and temporal synchronisation between the two communicative agents to allow sharing of a common perceptual context (the experiments reported here and earlier in [5]). The role of context is a key issue in sociality and social understanding. In particular, stud- ies of (first or second) language acquisition demon- strated the importance of social and verbal context for inferring the correct 'meaning' of spoken words or gestural signals <ref type="bibr">([21]</ref>). Our approach to the symbol grounding problem confirmed these ideas and showed that a simple movement imitation strategy is an interesting scenario for the transmission of a lan- guage as it is an easy means of getting the agents to share a common context of perceptions. Nevertheless, however similar the two agents' perceptions could be, they can never be exactly the same and therefore the . agent's learning capacities should be sufficiently com- plex to compensate for these differences. Not only should the agent be able to associate temporally delayed patterns, but this under a great amount of noisy and spurious data, two performance criteria which have been shown to be satisfied by the learning architecture (see [7]). This brings us to stress the following points. Whether used for grooming <ref type="bibr">[18]</ref> or for transmitting information, communication is an interactive process between the two communicative agents and as such it is a social interaction. Communication does not exist without the physical means of its production and reception. Whatever the level of interpretation chosen for the communicative signals, it is about the physical perception the communicative agents have of their world. As Steven Harnad's puts it [23] : &amp;dquo;even at such abstract cognitive heights, [referring to] the highest level of abstraction of natural language when our interactions with objects are based only on the inter- actions between names and descriptions, <ref type="bibr">[...]</ref> embod- iment is never escaped, for the power of names and propositions is completely parasitic on the meanings of those names, and those must all eventually be grounded in the sensorimotor interactions with the kinds of objects they designate, and the sensorimotor invariants on the basis of which the names are assigned&amp;dquo;. This requires very complex cognitive processes of segmentation of sensor information, sequence processing and spatial and temporal mapping. A model of the evolution of communication in terms of the agents' cognitive capabilities should then encapsulate a description of all the required cognitive functions. Similarly, if we consider the progression of brain evolution (e.g. comparing fish to apes and humans), the 'brain' of an individual animal has always evolved as a whole, as a complete organ; although different areas have differentiated and spe- cialised e.g. when comparing 'primitive' with 'advanced' vertebrates (compare reptiles and mam- mals), different parts of the body plan of a species and therefore different parts of the nervous system have not evolved independently from each other and from the rest of the body. However, the computational models of the evolution of communication have usu- ally ignored most of the so called 'low-level' cognitive functions, such as sensory perception, episodic mem- ory, and focus of attention.</p><p>Note, finally, that the evolution of an animal's cog- nition cannot be separated from the agent's behaviour, as these cognitive processes have been shaped by the constraints created by the agent's interactions with its environment. The body of an animal is a functional- ly and physiologically well integrated system. Thus, perception, action, communication and cognition are intrinsically interrelated in an embodied system. Our perception of the world is linked to our way of inter- preting the world and talking about events in and actions upon this world. Such a system, or behavioral account of cognitive processes, is closely related to the enactivist position <ref type="bibr">[35,</ref><ref type="bibr">54]</ref>, which describes a living system in terms of the structural coupling of its cellu- lar and molecular compon-ents (autopoiesis) and of its interaction with its environmental medium: &amp;dquo;the behaviour of a living system is not something that the living system does, not something that the medium specifies of its own, the behaviour arises and takes place in the relation living system/medium&amp;dquo; <ref type="bibr">[35]</ref>. Similar positions are discussed in the area of embod- ied artificial intelligence (EAI), see <ref type="bibr">[47]</ref> for a discus- sion of implications of embodiment for cognitive the- ories. As Erich Prem points out, cognitive science and linguistic research has strongly focused on formal aspects and neglected e.g. the notion of time, situat- edness and interaction dynamics in animal cognition. 'Getting the interaction dynamics right' is according to Prem a key principle of EAI research. This state- ment and the position to which it relates is supported by our experimental results on robot communication grounded in social inter-action dynamics. This brings us, then, to suggest that a behaviour-oriented approach <ref type="bibr">[12]</ref>, as e.g. an enactivist position, might be more appropriate than a pure cognitivist one for describing the cognitive processes involved in address- ing the symbol grounding problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE PERSPECTIVES</head><p>We proposed a teaching scenario based on movement imitation for transmitting a vocabulary among a group of agents and implemented it in simulated experiments. Results showed that social beha-viour benefit the agents in two circumstances; 1) agents capable of following one another and so imitating each other's movements develop faster and better a common understanding of the lan-guage; 2) the agents benefit from the so developed capability of communicating with one another via a common vocabulary as it speeds up the transmission of infor- mation.</p><p>An important point to note is that the system we described in this paper, namely the learning architec- ture and the following strategy, is not restricted to the particular implementation we presen-ted here. For instance, the capacity of the connectionist architec- ture (DRAMA) allows learning of a vocabulary of bigger size (of order 2 of the memory size) and learning of words combinations (viewed as pattern sequences) [7]. In [3] we report on the implementation of DRAMA in experiments in which a small humanoid robot was taught regular combinations of words which formed 'proto' sentences to describe patterns of actuator sequences and of sensor perceptions. Note also that the architecture is not restricted to be used only with simple mobile robots, as it does not encapsulate constraints on the sensor-actuator mode and could therefore be implemented in robots provided with finer sensor capabilities and actuators with more degrees of freedom. The grounding of symbols using an imitative strategy, as described in the paper, could, for instance, be extended to 'label' behaviours defined as a sequence of sensor-actuator states [33]. Recent work has developed robotic platforms which can per- form such complex set of actions, e.g <ref type="bibr">[11,</ref><ref type="bibr">20,</ref><ref type="bibr">32]</ref>.</p><p>Current work by Aude Billard at the Laboratory of Microlnformatics (LAMI) at the Swiss Institute of Technology at Lausanne (EPFL) investigates further some aspects of the simulation studies reported here. Experiments are carried out in which a group of robots learn a vocabulary and the objects' location (similarly to the first and second set of studies report- ed here), but in a continuous fashion rather than in two separated phases. The robots interact in a bio- logically inspired environment, where robots and objects appear and disappear ('are born' and 'die') con- tinuously. The study evaluates the influence of param- eters, such as the range of communication and the rate of death and birth of robots and objects on the robots' learning performance. Further, the simulation will be implemented in a real set-up of ten Khepera</p><p>[39] robots.</p><p>Our work showed the importance of behavioural capacities alongside cognitive ones for address-ing the symbol grounding problem. Behavioural mecha- nisms which act as external attentional processes are required alongside general cognitive abilities of asso- ciativity. We proposed a connec-tionist architecture in which the behavioural and cognitive processes are produced by the same general ability of spatio-temporal associativity. We used a simple movement imitative strategy to create a spatial and temporal synchronisation between the communicative agents, which pro- vides the required attentional mechanisms. These studies and their results lead us to suggest that a behaviour-oriented approach might be more appro- priate than a pure cognitivist one which is dom-inat- ing in linguistics and cognitive science studies on the cognitive processes involved in grounding communi- cation.</p><p>NOTES 1 Communication refers here to symbolic communica- tion, characterised by the transmission of symbolic signals or symbols, that is arbitrary physical signals whose meaning follows from social convention among the communicative agents.</p><p>2 The expression symbol grounding problem was first introduced by <ref type="bibr">[22]</ref> to refer to the question of how the symbols or the representations of the world acquire their meaning for a computational agent.</p><p>8 The choice of using nine robots and nine coloured patches is purely arbitrary. The point is to demon- strate transmission of communication among a group of agents, i.e. composed of at minimum three agents. These numbers correspond in fact to the maximal number of elements for which simulation could be carried out in a reasonable amount of time, each complete set of simulations (10 runs) requiring a week of CPU time.</p><p>9 Code is written in C and is processed serially. The C programs for the simulations were run on Ultra 1 Model 140s SPARCstations and the graphics representation was made using the MATLAB environ- ment.</p><p>10 The aim of the simulation studies was to further investigate the validity of the following teaching scenario, which had been first implemented in a physical set-up. It was therefore important to make the simulation as close as possible to the physical experiments. A simulated environment had to be used as it was not possible to have a real set-up of nine mobile robots.</p><p>points 'verbally' by indicating the quadrant in which to look at relatively to itself; however, there is a left right symmetry between the two agents in their definition of quadrants for all objects lying between them; this effect was, however, not consid- ered by the authors and might explain why the agents could never agree on more than a third of the vocabulary (only a third of the same 'quadrant' space can be seen by both agents.)</p><p>13We refer here to the discussion of the introduction section where we pointed out that in these studies, the agents have no body, no sensors or actuators; they generally occupy not more than a single point in space; they have usually no other actions than that of sending and receiving a signal, the result of their action being atemporal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>movement [8], inclination [6] and orientation [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The control system with retrieved and real sensor states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sensors sensitivity</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of learning success of follower and non-follower agent 5.1.2 The Benefit of Following</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>at GEORGIAN COURT UNIV on March 13, 2015 adb.sagepub.com Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The agent's representation of the world consists of the categorisation of its perceptions, which is done through updating the connections of its neural net- work (its controller), see section 3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>4Following behaviour is an implicit means of imita- tion, as when the first robot follows the second robot, it replicates the second robot's movements (moving, stopping, turning) in the plane.5 Our choice for using a connectionist architecture was driven by consideration relative to its implementation, see explanations given in[7]. This type of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>In the experiments presented here, we did not analyse the time parameters, which are mainly use- ful for the recording of sequences, see[7]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_4"><p>This noise consists of adding or subtracting a ran- dom number to the distance travelled in one wheel rotation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5"><p>12 In Steels &amp; Vogt's experiments, the teacher robot at GEORGIAN COURT UNIV on</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>March 13, 2015 adb.sagepub.com Downloaded from</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>Many thanks to anonymous reviewers for their precise comments which helped greatly to improve the writing of this paper. Facilities for the experiments were provided by the Department of Artificial Intelligence of the University of Edinburgh. Aude Billard was supported by a personal grant from the Swiss National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Simple Model for the Evolution of Communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifth Annual Conference on Evolutionary Programming</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The seeds of speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DRAMA, a connectionist architecture for on-line learning and control of autonomous robots: Experiments on learning of a synthetic proto-language with a doll robot&apos;</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Industrial Robot Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1999-01">1999. Jan 1999</date>
		</imprint>
	</monogr>
	<note>due out on 23rd</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DRAMA, a connectionist model for robot learning: Experiments on grounding communication through imitation in autonomous robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems, special issue on</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Nehmzow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Recce</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Bisset</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="71" to="79" />
			<date type="published" when="1998">1998. 1998. 1998</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Artificial Intelligence, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Quantitative Assessment of Mobile Robot Behaviour</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grounding communication in situated, social robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<idno>Series UMCS-97- 9-1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of TIMR 97, Towards Intelligent Mobile Robots Conference</title>
		<meeting>of TIMR 97, Towards Intelligent Mobile Robots Conference</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Dept of Computer Science, Manchester University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DRAMA, a connectionist architecture for control and learning in autonomous robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behaviour journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transmitting Communication Skills through Imitation in Autonomous robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EWLR97, Sixth European Workshop on Learning Robots</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Birk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</editor>
		<meeting>EWLR97, Sixth European Workshop on Learning Robots<address><addrLine>Brighton</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997-07-97">1997. July 97</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transmission of agression through imitation of agressive models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intelligence without reason</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI-91, International Joint Conference on Artificial Intelligence</title>
		<meeting>of IJCAI-91, International Joint Conference on Artificial Intelligence<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="659" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Behavior-based humanoid robotics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1996 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>1996 IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">I could be you -the phenomenological dimension of social understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Issue on Epistemological Aspects of Embodied AI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="417" to="453" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Cybernetics and Systems Journal</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grounding Agent Sociality: The Social World is its Own Best Model&apos;, From Agent Theory to Agent Implementation (organiser Joerg Mueller and Paolo Petta) symposium</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th European Meeting on Cybernetics and Systems Research</title>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Trappl</surname></persName>
		</editor>
		<meeting>14th European Meeting on Cybernetics and Systems Research</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="779" to="784" />
		</imprint>
	</monogr>
	<note>Published in</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Getting to know each other artificial social intelligence for autonomous robots</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="333" to="356" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imitation: a review and critique</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives in Ethology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="1973">1973</date>
			<pubPlace>Plenum, Ney York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brain-Language Coevolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Deacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Evolution of Human Language, SFI Studies in the Sciences of Complexity</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="49" to="83" />
		</imprint>
	</monogr>
	<note>Proc.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Robot Controller Using Learning by Imitation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Symposium on Intelligent Robotic Systems</title>
		<meeting>the 2nd International Symposium on Intelligent Robotic Systems<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1996. July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coevolution of neocortical size, group size and language in humans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I M</forename><surname>Dunbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="681" to="735" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The dance Language and Orientation of Bees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName><surname>Von</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<pubPlace>Bellknap, Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Open Architecture for Robot Entertainment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kageyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First International Conference on Autonomous Agents</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lewis</forename><surname>Johnson</surname></persName>
		</editor>
		<meeting>of the First International Conference on Autonomous Agents<address><addrLine>Marina del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Psychology of Language: from data to theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Harley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994. 1995</date>
			<publisher>Erlbaum (UK) Taylor &amp; Francis Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond Object Constancy&apos;. Institution of Electrical Engineers (IEE) Seminar on</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Self-Learning Robots II: Bio-Robotics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Digest 98/248: 2/1-2/3</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language as part of sensorimotor behavior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Henis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levinson</surname></persName>
		</author>
		<idno>FS-95-05</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium</title>
		<meeting><address><addrLine>Menlo Park, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Social Learning in Animals: The Roots of Culture</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Heyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Galef</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The process of sub-culture propagation among Japanese Macaques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kawamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Primates Social Behaviour</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Southwick</surname></persName>
		</editor>
		<editor>
			<persName><surname>Van Nostrand</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1963">1963</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Development of a multiagent system for robot soccer game</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="page" from="626" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The evolution of incremental learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GALA&apos;97</title>
		<meeting>GALA&apos;97<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">1997. April 1997</date>
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-Robot-Communication and Machine Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Klingspor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiser</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="719" to="746" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning by Watching: Extracting Reusable Task Knowledge from Visual Observation of Human Performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental and Theoretical Artificial Intelligence, special issue on Learning in DAI Systems</title>
		<editor>
			<persName><forename type="first">Gerhard</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="357" to="369" />
			<date type="published" when="1994-12">1994. Dec. 1994. 1998. Jul-Sep, 1998</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Robotics and Automation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptuo-Motor Primitives in Imitation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes, Autonomous Agents &apos;98 Workshop on &quot;Agents in Interaction -Acquiring Competences Through Imitation</title>
		<meeting><address><addrLine>Minneapolis/St. Paul</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Mataric</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Personal communication to Aude Billard</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Synthetic Ethology and the Evolution of Cooperative Communication</title>
		<author>
			<persName><forename type="first">B</forename><surname>Maclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><surname>Burghardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Behavior</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ontology of Observing: The Biological Foundations Of Self Consciousness And the Physical Domain Of Existence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maturana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Workbook: Texts in Cybernetics, American Society For Cybernetics Conference</title>
		<meeting><address><addrLine>Felton, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-10-98">1988. October 98</date>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfarland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<publisher>Longman Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The human infant as imitative generalist: a 20-year progress report on infant imitation with implications for comparative psychology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Learning in Animals: The Roots of Culture</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Heyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Galef</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards a developmental cognitive science. The implication of cross-modal matching and imitation for the development of representation and memory in infancy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The development and Neural Basis of Higher cognitive Functions</title>
		<editor>
			<persName><forename type="first">Adele</forename><surname>Diamond</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">608</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mobile Robot Miniaturisation: a Tool for Investigation in Control Algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mondada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Franzi &amp; P Ienne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISER&apos;93</title>
		<meeting>ISER&apos;93<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synthetic Robotic Language Acquisition by Observation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fourth Conference on Simulation of Adaptive Behaviour</title>
		<meeting>of the Fourth Conference on Simulation of Adaptive Behaviour<address><addrLine>Cape Cod, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-09">1996. September 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mapping between Dissimilar Bodies: Affordances and the Algebraic Foundations of Imitation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings European Workshop on Learning Robots</title>
		<editor>
			<persName><forename type="first">John</forename><surname>Demiris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Birk</surname></persName>
		</editor>
		<meeting>European Workshop on Learning Robots<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07-20">1998. 1998. 20 July 1998</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vocal trac and brain: a search for evolutionary bottlenecks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nottebohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. N. Y. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="page" from="643" to="649" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning and the emergence of coordinated communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prediction and imitation of linguistic sounds by neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Neural Networks and Speech Processing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Paoloni</surname></persName>
		</editor>
		<meeting>the 1st Workshop on Neural Networks and Speech Processing<address><addrLine>Roma</addrLine></address></meeting>
		<imprint>
			<publisher>Fondazione Bordoni</publisher>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Assessing the role of social development in the evolution of cooperation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 5, Proceedings of to the Fifth International Conference of The Society for Adaptive Behavior (SAB98)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Pfeifer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-A</forename><surname>Meyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="453" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The implications of embodiment for cognitive theories. A systems view</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Meeting on Cybernetics and Systems Research, EMCSR&apos;98</title>
		<meeting>the 14th European Meeting on Cybernetics and Systems Research, EMCSR&apos;98</meeting>
		<imprint>
			<date type="published" when="1994">1994. 1998</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>The Language Instinct</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Words in the Brain&apos;s Language</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pulvermuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural and Brain Science Journal</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Escape, Avoidance, and Imitation: A Neural Network approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smajuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Zanutto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behaviour</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="129" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grounding adaptive language games in robotic agents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Steels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth European Conference on Artificial Life</title>
		<meeting>the Fourth European Conference on Artificial Life<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07-97">1997. July 97</date>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Language Learning and Language Contact</title>
		<author>
			<persName><forename type="first">L</forename><surname>Steels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Empirical Approaches to Language Aquisition</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the workshop on Empirical Approaches to Language Aquisition<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human friendly teaching for industrial robots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tatsuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th IEEE International Workshop on Robot and Human Communication RO-MAN&apos;96 Tsukuba</title>
		<meeting>of 5th IEEE International Workshop on Robot and Human Communication RO-MAN&apos;96 Tsukuba<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="page" from="549" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning and instinct in animals</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Thorpe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
			<publisher>Methuen</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The Embodied Mind</title>
		<author>
			<persName><forename type="first">F</forename><surname>Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="121" to="126" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">On the Nature and Evolution of Imitation in the Animal Kingdom: Reappraisal of a Century of Research in Advances in the Study of Behaviour</title>
		<author>
			<persName><forename type="first">A</forename><surname>Whiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="239" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Evolution of Language from Social Intelligence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Worden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Evolution of Phonology and Syntax</title>
		<editor>
			<persName><forename type="first">Studdert-Kennedy &amp;</forename><surname>Hurford</surname></persName>
		</editor>
		<editor>
			<persName><surname>Knight</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An adaptive communication protocol for cooperating mobile robots</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 2: Proceedings of the Second International Conference on the Simulation of Adaptive Behavior</title>
		<editor>
			<persName><forename type="first">J.-A</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Roitblat</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>The MIT Press/Bradford Books</publisher>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m">ABOUT THE AUTHORS Aude Billard</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">In October 1998, she received her PhD from the University of Edinburgh. She worked as a research assistant at the European Center of Nuclear Research (CERN) and at the laboratory of micro-informatics (LAMI) at the EPFL. Her research interests are in the development of artificial neural controllers (ANNs) with application to robotics. In particular, she designed robot controllers for complex cognitive skills such as learning by imitation and learning of a language</title>
		<imprint/>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note>She received her BSc-MSc in Physics in March 95 from the Swiss Federal Institute of Technology in Lausanne (EPFL). Kerstin Dautenhahn</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">UK. Her research interests are Socially Intelligent Agents, Social Robotics, Animal Minds and Artificial Life. Dr. Dautenhahn is a member of the Board of Governors of the Cognitive Technology Society</title>
	</analytic>
	<monogr>
		<title level="j">Associate Editor of the journal Adaptive Behavior</title>
		<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
		<respStmt>
			<orgName>Kerstin Dautenhahn is Principal Lecturer (Research) in the Department of Computer Science, Adaptive Systems Research Group at University of Hertfordshire</orgName>
		</respStmt>
	</monogr>
	<note>Human Cognition and Social Agent Technology</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
