<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embedding Uncertain Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuelu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaochen@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
							<email>zaniolo@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embedding Uncertain Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, with the purpose of capturing latent semantic relations between entities and incorporating the structured knowledge they contain into machine learning. However, there are many KGs that model uncertain knowledge, which typically model the inherent uncertainty of relations facts with a confidence score, and embedding such uncertain knowledge represents an unresolved challenge. The capturing of uncertain knowledge will benefit many knowledge-driven applications such as question answering and semantic search by providing more natural characterization of the knowledge. In this paper, we propose a novel uncertain KG embedding model UKGE, which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer confidence scores for unseen relation facts during training. We propose and evaluate two variants of UKGE based on different confidence score modeling strategies. Experiments are conducted on three real-world uncertain KGs via three tasks, i.e. confidence prediction, relation fact ranking, and relation fact classification. UKGE shows effectiveness in capturing uncertain knowledge by achieving promising results, and it consistently outperforms baselines on these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) provide structured representations of real-world entities and relations, which are categorized into the following two types: (i) Deterministic KGs, such as <ref type="bibr">YAGO (Rebele et al. 2016)</ref> and FreeBase <ref type="bibr">(Bollacker et al. 2008)</ref>, consist of deterministic relation facts that describe semantic relations between entities; (ii) Uncertain KGs including ProBase <ref type="bibr" target="#b21">(Wu et al. 2012)</ref>, ConceptNet <ref type="bibr" target="#b19">(Speer, Chin, and Havasi 2017)</ref> and NELL <ref type="bibr">(Mitchell et al. 2018</ref>) associate every relation fact with a confidence score that represents the likelihood of the relation fact to be true.</p><p>KG embedding models are essential tools for incorporating the structured knowledge representations in KGs into machine learning. These models encode entities as lowdimensional vectors and relations as algebraic operations among entity vectors. They accurately capture the similarity of entities and preserve the structure of KGs in the embedding space. Hence, they have been the crucial feature models that benefit numerous knowledge-driven tasks (Bordes, Weston, and Usunier 2014; <ref type="bibr" target="#b8">He et al. 2017;</ref><ref type="bibr" target="#b5">Das et al. 2018)</ref>. Recently, extensive efforts have been devoted into embedding deterministic KGs. Translational models, e.g., TransE <ref type="bibr" target="#b1">(Bordes et al. 2013) and</ref><ref type="bibr">TransH (Wang et al. 2014</ref>)), and bilinear models, e.g. DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, have achieved promising performance in many tasks, such as link prediction <ref type="bibr" target="#b22">(Yang et al. 2015;</ref><ref type="bibr" target="#b20">Trouillon et al. 2016)</ref>, relation extraction (Weston, <ref type="bibr">Bordes, and others 2013)</ref>, relational learning <ref type="bibr">(Nickel, Rosasco, and Poggio 2016)</ref>, and ontology population <ref type="bibr" target="#b4">(Chen et al. 2018)</ref>.</p><p>While current embedding models focus on capturing deterministic knowledge, it is critical to incorporate uncertainty information into knowledge sources for several reasons. First, uncertainty is the nature of many forms of knowledge. An example of naturally uncertain knowledge is the interactions between proteins. Since molecular reactions are random processes, biologists label the protein interactions with their probabilities of occurrence and present them as uncertain KGs called Protein-Protein Interaction (PPI) Networks. Second, uncertainty enhances inference in knowledge-driven applications. For example, short text understanding often entails interpreting real-world concepts that are ambiguous or intrinsically vague. The probabilistic KG Probase <ref type="bibr" target="#b21">(Wu et al. 2012)</ref> provides a prior probability distribution of concepts behind a term that has critically supported short text understanding tasks involving disambiguation <ref type="bibr" target="#b20">(Wang et al. 2015;</ref><ref type="bibr">Wang and Wang 2016)</ref>. Furthermore, uncertain knowledge representations have largely benefited various applications, such as question answering <ref type="bibr" target="#b23">(Yih et al. 2013</ref>) and named entity recognition <ref type="bibr">(Ratinov and Roth 2009)</ref>.</p><p>Capturing the uncertainty information with KG embeddings remains an unresolved problem. This is a non-trivial task for several reasons. First, compared to deterministic KG embeddings, uncertain KG embeddings need to encode additional confidence information to preserve uncertainty. Second, current KG embedding models cannot capture the sub-tle uncertainty of unseen relation facts, as they assume that all the unseen relation facts are false beliefs and minimize the plausibility measures of relation facts. One major challenge of learning embeddings for uncertain KGs is to properly estimate the uncertainty of unseen relation facts.</p><p>To address the above issues, we propose a new embedding model UKGE (Uncertain Knowledge Graph Embeddings), which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Embeddings of entities and relations on uncertain KGs are learned according to confidence scores. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer the confidence score for unseen relation facts during training. We propose two variants of UKGE based on different embedding-based confidence functions. We conducted extensive experiments using three real-world uncertain KGs on three tasks: (i) confidence prediction, which seeks to predict confidence scores of unseen relation facts; (ii) relation fact ranking, which focuses on retrieving tail entities for the query (h, r, ?t) and ranking these retrieved tails in the right order; and (iii) relation fact classification, which decides whether or not a given relation fact is a strong relation fact. Our models consistently outperform the baseline models in these experiments.</p><p>The rest of the paper is organized as follows. We first review the related work in Section 2, then provide the problem definition and propose our model UKGE in the two sections that follow. In section 5, we present our experiments. Then we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To the best of our knowledge, there has been no previous work on learning embeddings for uncertain KGs. We hereby discuss the following three lines of work that are closely related to this topic.</p><p>Deterministic Knowledge Graph Embeddings Deterministic KG embeddings have been extensively explored by recent work. These models encode entities as lowdimensional vectors and relations as algebraic operations among entity vectors. There are two representative families of models, i.e. translational models and bilinear models.</p><p>Translational models share a common principle h r + r ≈ t r , where h r , t r are the entity embeddings projected in a relation-specific space. The forerunner of this family, TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, lays h r and t r in a common space as h and t with regard to any relation r. <ref type="bibr">Variants of TransE, such as TransH (Wang et al. 2014</ref><ref type="bibr">), TransR (Lin et al. 2015)</ref>, TransD <ref type="bibr" target="#b11">(Ji et al. 2015), and</ref><ref type="bibr">TransA, (Jia et al. 2016)</ref> differentiate the translations of entity embeddings in different language-specific embedded spaces based on different forms of relation-specific projections. Despite its simplicity, translational models achieve promising performance on knowledge completion and relation extraction tasks.</p><p>Bilinear models <ref type="bibr" target="#b10">(Jenatton et al. 2012)</ref> model relations as the second-order correlations between entities, using the scoring function f (h, r, t) = h T W r t. This function is first adopted by RESCAL <ref type="bibr" target="#b17">(Nickel, Tresp, and Kriegel 2011)</ref>, a collective matrix factorization model. DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> constrains W r as a diagonal matrix which reduces the computing cost and also enhances the performance. Com-plEx adjusts the corresponding scoring mechanism to a complex conjugation in a complex embedding space.</p><p>There are also other models for deterministic KG embedding, such as neural models like Neural Tensor Network (NTN) <ref type="bibr" target="#b19">(Socher et al. 2013)</ref> and ConvE <ref type="bibr" target="#b6">(Dettmers et al. 2018)</ref>, and the circular-correlation-based model HolE <ref type="bibr">(Nickel, Rosasco, and Poggio 2016)</ref>.</p><p>Uncertain Knowledge Graphs Uncertain KGs provide a confidence score along with every relation fact. The development of relation extraction and crowdsourcing in recent years enabled the construction of large-scale uncertain knowledge bases. ConceptNet (Speer, Chin, and Havasi 2017) is a multilingual uncertain KG for commonsense knowledge that is collected via crowdsourcing. The confidence scores in ConceptNet mainly come from the cooccurrence frequency of the labels in crowdsourced task results. Probase <ref type="bibr" target="#b21">(Wu et al. 2012</ref>) is a universal probabilistic taxonomy built by relation extraction. Every fact in Probase is associated with a joint probability P isA (x, y). NELL <ref type="bibr">(Mitchell et al. 2018</ref>) collects relation facts by reading web pages and learns their confidence scores from semisupervised learning with the Expectation-Maximum (EM) algorithm. Aforementioned uncertain KGs have enabled numerous knowledge-driven applications. For example, Wang and Wang (2016) utilize Probase to help understand short texts.</p><p>One recent work has proposed a matrix-factorizationbased approach to embed uncertain networks <ref type="bibr" target="#b9">(Hu et al. 2017</ref>). However, it cannot be generalized to embed uncertain KGs because this model only considers the node proximity in the networks with no explicit relations and only generates node embeddings. As far as we know, we are among the first to study the uncertain KG embedding problem.</p><p>Probabilistic Soft Logic Probabilistic soft logic (PSL) <ref type="bibr">(Kimmig et al. 2012</ref>) is a framework for probabilistic reasoning. A PSL program consists of a set of first-order logic rules with conjunctive bodies and single literal heads. PSL takes the confidence from interval [0, 1] as the soft truth values for every atom. It uses Lukasiewics t-norm (Lukasiewicz and Straccia 2008) to determine to which degree a rule is satisfied. In combination with Hinge-Loss Markov Random Field (HL-MRF), PSL is widely used in probabilistic reasoning tasks, such as social-trust prediction and preference prediction <ref type="bibr" target="#b0">(Bach et al. 2013;</ref><ref type="bibr">2017)</ref>. In this paper, we adopt PSL to enhance the embedding model performance on the unseen relation facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>We define the uncertain KG embedding problem in this section by first providing the definition of uncertain KGs.</p><p>Definition 1. Uncertain Knowledge Graph. An uncertain KG represents knowledge as a set of relations (R) defined over a set of entities (E). It consists of a set of weighted triples G = {(l, s l )}. For each pair (l, s l ), l = (h, r, t) is a triple representing a relation fact where h, t ∈ E (the set of entities) and r ∈ R (the set of relations), and s l ∈ [0, 1] represents the confidence score for this relation fact to be true.</p><p>Note that we assume the confidence score s l ∈ [0, 1] and interpret it as a probability to leverage probabilistic soft logic-based inference. The range of original confidence scores for some uncertain KG (e.g., ConceptNet) may not fall in [0, 1], and normalization will be needed in these cases. Some examples of weighted triples are listed below. Notation wise, boldfaced h, r, t are used to represent the embedding vectors for head h, relation r and tail t respectively. h, r, t are assumed lie in R k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling</head><p>In this section, we propose our model for uncertain KG embeddings. The proposed model UKGE encodes the KG structure according to the confidence scores for both observed and unseen relation facts, such that the embeddings of relation facts with higher confidence scores receive higher plausibility values.</p><p>We first design relation fact confidence score modeling based on embeddings of entities and relations, then introduce how probabilistic soft logic can be used to infer confidence scores for unseen relations, and lastly describe the joint model UKGE and its two variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding-based Confidence Score Modeling for Relation Facts</head><p>Unlike deterministic KG embedding models, uncertain KG embedding models need to explicitly model the confidence score for each triple and compare the prediction with the true score. We hereby first define and model the plausibility of triples, which can be considered as a unnormalized confidence score.</p><p>Definition 3. Plausibility. Given a relation fact triple l, the plausibility g(l) ∈ R measures how likely this relation fact holds. The higher plausibility value corresponds to the higher confidence score s.</p><p>Given a triple l = (h, r, t) and their embeddings h, r, t, we model the plausibility of (h, r, t) by the following function:</p><formula xml:id="formula_0">g(l) = r • (h • t) (1)</formula><p>where • is the element-wise product, and • is the inner product. This function captures the relatedness between embeddings h and t under the condition of relation r and is first adopted by DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref>. We employ this triple modeling technique for three reasons: (i) This technique has represented the state-of-the-art performance for modeling deterministic KGs <ref type="bibr">(Kadlec, Bajgar, and</ref>  From plausibility to confidence scores In order to transform plausibility scores to confidence scores, we consider two different mapping functions and test them in the experimental section. Formally, let a triple be l and its plausibility score be g(l), a transformation function φ(•) maps g(l) to a confidence score f (l).</p><formula xml:id="formula_1">f (l) = φ(g(l)), φ : R → [0, 1]<label>(2)</label></formula><p>Two choices of mapping φ are listed below. Logistic function. One way to map plausibility values to confidence score is a logistic function as follows:</p><formula xml:id="formula_2">φ(x) = 1 1 + e −(wx+b)<label>(3)</label></formula><p>Bounded rectifier. Another mapping is a bounded rectifier <ref type="bibr" target="#b3">(Chen et al. 2015)</ref>:</p><formula xml:id="formula_3">φ(x) = min(max(wx + b, 0), 1)<label>(4)</label></formula><p>where w is a weight b is a bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PSL-based Confidence Score Reasoning for Unseen Relation Facts</head><p>In order to better estimate confidence scores, both observed and unseen relation facts in KGs should be utilized. Deterministic KG embedding methods assume that all unseen relation facts are false beliefs, and use negative sampling to add some of these false relations into training. One major challenge of learning embeddings for uncertain KGs, however, is to properly estimate the uncertainty of unseen triples, as simply treating their confidence score as 0 can no longer capture the subtle uncertainty. For example, it is common that a Protein-Protein Interaction Network KG may have no interaction records for two proteins that can be potentially binded. Ignoring such possibility will result in information loss.</p><p>We thus introduce probabilistic soft logic (PSL) <ref type="bibr">(Kimmig et al. 2012)</ref> to infer confidence scores for these unseen relation facts to further enhance the embedding performance. PSL is a framework for confidence reasoning that propagates confidence of existing knowledge to unseen triples using soft logic.</p><p>Probabilistic Soft Logic A PSL program consists of a set of first order logic rules that describe logical dependencies between facts (atoms). One example of logical rule is shown below: Different from Boolean logic, PSL associates every atom, i.e., a triple l, with a soft truth value from the interval [0, 1], which corresponds to the confidence score in our context and enables fuzzy reasoning. The assignment process of soft truth values is called an interpretation. We denote the soft truth value of an atom l assigned by the interpretation I as I(l). Naturally, for observed relation facts, their observed confidence scores are used for assignment; and for unseen triples, the embedding-based estimated confidence scores will be assigned to them:</p><formula xml:id="formula_4">I(l) = s l , l ∈ L + I(l) = f (l), l ∈ L − (5)</formula><p>where L + denotes the observed triple set, L − denotes the unseen triples, s l denotes the confidence score for observed triple l, and f (l) denotes the embedding-based confidence score function for l.</p><p>In PSL, Lukasiewicz t-norm is used to define the basic logical operations, including logical conjunction (∧), disjunction (∨), and negation (¬), as follows:</p><formula xml:id="formula_5">l1 ∧ l2 = max{0, I(l1) + I(l2) − 1} (6) l1 ∨ l2 = min{1, I(l1) + I(l2)} (7) ¬l1 = 1 − I(l1)<label>(8)</label></formula><p>For example, according to Eq. ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>), 0.8 ∧ 0.3 = 0.1 and 0.8 ∨ 0.3 = 1. For a rule γ ≡ γ body → γ head , as it can be written as ¬γ body ∨ γ head , its value p γ can be computed as</p><formula xml:id="formula_6">pγ body →γ head = min{1, 1 − I(γ body ) + I(γ head )}<label>(9)</label></formula><p>PSL regards a rule γ as satisfied when the truth value of its head I(γ head ) is the same or higher than its body I(γ body ), i.e., when its value is greater than or equal to 1.</p><formula xml:id="formula_7">dγ = 1 − pγ = max{0, I(γ body ) − I(γ head )}<label>(10)</label></formula><p>Consider Example 4.2. Let (college, synonym, university) be l 1 , (university, synonym, college) be l 2 , and (college, synonym, institute) be l 3 . Assuming that l 1 and l 2 are observed triples in KG, and l 3 is unseen, according to Equation ( <ref type="formula">5</ref>), (6), and (9), the distance to satisfaction of this ground rule is calculated as below:</p><formula xml:id="formula_8">dγ = max{0, I(l1 ∧ l2) − I(l3)} = max{0, s l 1 + s l 2 − 1 − f (l3)} = max{0, 0.85 − f (l3)}</formula><p>where s l1 and s l2 are the ground truth confidence scores of corresponding relation facts in the uncertain KG. This equation indicates that the ground rule in Example 4.2 is completely satisfied when f (l 3 ), the estimated confidence score of (college, synonym institute), is above 0.85. When f (l 3 ) is under 0.85, the smaller f (l 3 ) is, the larger loss we have. In other words, a bigger confidence score is preferable. In the above example, we can see that the embeddingbased confidence score for this unseen relation fact, f (l 3 ), will affect the loss function, and it is desirable to learn embeddings that minimize these losses. Note that if we simply treat the unseen relation l 3 as false and use MSE (Mean Squared Error) as the loss, the loss would be f (l 3 ) 2 , which is in favor of a lower confidence score mistakenly.</p><p>Moreover, we add a rule to penalize the predicted confidence scores of all unseen relation facts, which can be considered as a prior knowledge, i.e., any unseen relation fact has a low probability to be true. Formally, for an unseen relation fact l = (h, r, t) ∈ L − , we have a ground rule γ 0 :</p><formula xml:id="formula_9">γ0 : ¬l<label>(11)</label></formula><p>According to Eq. ( <ref type="formula" target="#formula_5">8</ref>) and ( <ref type="formula" target="#formula_7">10</ref>), its distance to satisfaction d γ0 is derived as:</p><formula xml:id="formula_10">dγ 0 = f (l) (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Uncertain KGs</head><p>In this subsection, we present the objective function of uncertain KG embeddings.</p><p>Loss on observed relation facts Let L + be the set of observed relation facts, the goal is to minimize the mean squared error (MSE) between the ground truth confidence score s l and our prediction f (l) for each relation l ∈ L + :</p><formula xml:id="formula_11">J + = l∈L + |f (l) − s l | 2 (13)</formula><p>Loss on unseen relation facts Let L − be the sampled set of unseen relations, and Γ l be the set of ground rules with l as the rule head, the goal is to minimize the distance to rule satisfaction for each triple l. In particular, we choose to use the square of the distance as the following loss <ref type="bibr" target="#b0">(Bach et al. 2013)</ref>:</p><formula xml:id="formula_12">J − = l∈L − γ∈Γ l |ψγ(f (l))| 2 (14)</formula><p>where ψ γ (f (l)) denotes the weighted distance to satisfaction w γ d γ of the rule γ as a function of f (l) where w γ is a hand-crafted weight for the rule γ.</p><p>Note that when l is only covered by γ 0 : ¬l, we have The Joint Objective Function Combining Eq. ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>), we obtain the following joint objective function:</p><formula xml:id="formula_13">γ∈Γ l |ψ γ (f (l))| 2 = |f (l)| 2 ,</formula><formula xml:id="formula_14">J = l∈L + |f (l) − s l | 2 + l∈L − γ∈Γ l |ψγ(f (l))| 2 (15)</formula><p>Similar to deterministic KG embedding algorithms, we sample unseen relations by corrupting the head and the tail for observed relation facts to generate L − during training. We give two model variants that differ in the choice of f (l). We refer to the variant that adopts Equation (3) as UKGE logi and name the one using Equation ( <ref type="formula" target="#formula_3">4</ref>) as UKGE rect .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our models on three tasks: confidence prediction, relation fact ranking, and relation fact classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The evaluation is conducted on three datasets named as CN15k, NL27k, and PPI5k, which are extracted from Con-ceptNet, NELL, and the Protein-Protein Interaction Knowledge Base STRING <ref type="bibr" target="#b19">(Szklarczyk et al. 2016)</ref> respectively. CN15k matches the number of nodes with FB15k <ref type="bibr" target="#b1">(Bordes et al. 2013</ref>) -the widely used benchmark dataset for deterministic KG embeddings <ref type="bibr" target="#b1">(Bordes et al. 2013;</ref><ref type="bibr" target="#b20">Wang et al. 2014;</ref><ref type="bibr" target="#b22">Yang et al. 2015)</ref>, while NL27k is a larger dataset. PPI5k is a denser graph with fewer entities but more relation facts than the other two. Table <ref type="table" target="#tab_1">1</ref> gives the statistics of the datasets, and more details are introduced below.</p><p>CN15k CN15k is a subgraph of the commonsense KG ConceptNet. This subgraph contains 15,000 entities and 241,158 uncertain relation facts in English. The original scores in ConceptNet vary from 0.1 to 22, where 99.6% are less than or equal to 3.0. For normalization, we first bound confidence scores to x ∈ [0.1, 3.0], and then applied the min-max normalization on log x to map them into [0.1, 1.0]. NL27 NL27k is extracted from NELL <ref type="bibr">(Mitchell et al. 2018)</ref>, an uncertain KG obtained from webpage reading. NL27k contains 27,221 entities, 404 relations, and 175,412 uncertain relation facts. In the process of min-max normalization, we search for the lower boundary from 0.1 to 0.9. We have found out that normalizing the confidence score to interval [0.1, 1] yields best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPI5k</head><p>The Protein-Protein Interaction Knowledge Base STRING labels the interactions between proteins with the probabilities of occurrence. PPI5k is a subset of STRING that contains 271,666 uncertain relation facts for 4,999 proteins and 7 interactions.</p><p>In an uncertain KG, a relation fact is considered strong if its confidence score s l is above a KG-specific threshold τ . Here we set τ = 0.85 for both CN15k and NL27k. We follow the instructions from <ref type="bibr" target="#b19">(Szklarczyk et al. 2016)</ref> and set τ = 0.70 for PPI5k. Under this setting, 20.4% of relation facts in CN15k, 20.1% of those in NL27k, and 12.4% of those in PPI5k are considered strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We split each dataset into three parts: 85% for training, 7% for validation, and 8% for testing. To test if our model can correctly interpret negative links, we add the same amount of negative links as existing relation facts into the test sets.</p><p>We use Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba 2014)</ref> for training, for which we set the exponential decay rates β 1 = 0.9 and β 2 = 0.99. We report results for all models respectively based on their best hyperparameter settings. For each model, the setting is identified based on the validation set performance. We select among the following sets of hyperparameter values: learning rate lr ∈ {0.001, 0.005, 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Logical Rule Generation</head><p>Our model requires additional input as logical rules for PSL reasoning. We heuristically create candidate logical rules by considering length-2 paths (i.e., (</p><formula xml:id="formula_15">E 1 ,R 1 ,E 2 ) ∧ (E 2 ,R 2 ,E 3 ) → (E 1 ,R 3 ,E 3</formula><p>)) and validate them by hit ratio, i.e. the proportion of relation facts implied by the rule to be truly existent in the KG. The higher ratio implies that the rule is more convincing. When grounding out the logical rules, to guarantee the quality of the ground rules, we only adopt observed strong relation facts in our rule body. We eventually create 3 logical rules for CN15k, 4 for NL27k, and 1 for PPI5k. Table <ref type="table" target="#tab_2">2</ref> gives some examples of the logical rules and their hit ratios. How to systematically create more promising logical rules will be considered as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines</head><p>Three types of baselines are considered in our comparison, which include (i) deterministic KG embedding models TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, DistMult <ref type="bibr" target="#b22">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding model URGE <ref type="bibr" target="#b9">(Hu et al. 2017)</ref> Here linear stands for linear gain, and exp. stands for exponential gain. have a KG-specific confidence score threshold τ to distinguish the high-confidence relation facts from the lowconfidence ones, which will be discussed later in Section 5.7. These models cannot predict confidence scores. We compare our methods to them only on the ranking and the classification tasks. For the same reason, the early stopping is based on mean reciprocal rank (MRR) on the validation set. We adopt the implementation given by <ref type="bibr" target="#b20">(Trouillon et al. 2016</ref>) and choose the best hyper-parameters following the same grid search procedure. This implementation uses <ref type="bibr">(Duchi, Hazan, and others 2011)</ref>  • Uncertain Graph Embedding Model. URGE is proposed very recently to embed uncertain graphs. However, it cannot deal with multiple types of relations in KGs, and it only produces node embeddings. We simply ignore relation types when applying URGE to our datasets. We adopt its first-order proximity version as our tasks focus on the edge relations between nodes.</p><p>• Two Simplified Versions of Our Model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Confidence Prediction</head><p>The objective of this task is to predict confidence scores of unseen relation facts.</p><p>Evaluation protocol For each uncertain relation fact (l, s l ) in the test set, we predict the confidence score of l and report the mean squared error (MSE) and mean absolute error (MAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Results are reported in We notice that all the models achieve much smaller MSE on PPI5k than CN15k and NL27k. We hypothesize that this is because the much higher density of PPI5k facilitates embedding learning (Pujara, Augustine, and Getoor 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Relation Fact Ranking</head><p>The next task focuses on ranking tail entities in the right order for the query (h, r, ?t).</p><p>Evaluation protocol For a query (h, r, ?t), we rank all the entities in the vocabulary as tail candidates and evaluate the ranking performance using the normalized Discounted Cumulative G ain (nDCG) <ref type="bibr" target="#b16">(Li, Liu, and Zhai 2009)</ref>. We define the gain in retrieving a relevant tail t 0 as the ground truth confidence score s (h,r,t0) . We take the mean nDCG over the test query set as our ranking metric. We report the two versions of nDCG that use linear gain and exponential gain respectively. The exponential gain version puts stronger emphasis on highly relevant results.</p><p>Results Table <ref type="table" target="#tab_4">4</ref>  Besides ranking the existing relation facts highly, our models also preserve the order of the observed relation facts and thus achieve higher nDCG scores. Both UKGE rect and UKGE logi outperform all the baselines under all settings, while UKGE logi yields higher nDCG on all three datasets than UKGE rect . Considering the confidence prediction results of UKGE logi in Section 5.5, we hypothesize that the easy saturation of logistic function allows UKGE logi to better distinguish negative links from true relation facts, while this feature compromises its ability to fit confidence scores more precisely.</p><p>Case study Table <ref type="table" target="#tab_7">5</ref> gives some examples of relation fact ranking results by UKGE logi . Given a query (h, r, ?t), the top 4 predicted tails and true tails are given, sorted by their scores in descending order. The predictions are consistent with our common-sense. It is worth noting that some quite reasonable unseen relation facts such as hotel is used for re-laxing, can be predicted correctly. In other words, our proposed approach can be potentially used to infer new knowledge from the observed ones with reasonable confidence scores, which may shed light on another line of future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Relation Fact Classification</head><p>This last task is a binary classification task to decide whether a given relation fact l is a strong relation fact or not. A relation fact is considered strong if its confidence score s l is above a KG-specific threshold τ . The embedding models need to distinguish relation facts in the KG from negative links and high-confidence relation facts from lowconfidence ones.</p><p>Evaluation protocol We follow a procedure that is similar to <ref type="bibr" target="#b20">(Wang et al. 2014)</ref>. Our test set consists of relation facts from the KG and randomly sampled negative links equally. We divide the test cases into two groups, strong and weak/false, by their ground truth confidence scores. A test relation fact l is strong when l is in the KG and s l &gt; τ , otherwise weak/false. We fit a logistic regression classifier as a downstream classifier on the predicted confidence scores.</p><p>Results F-1 scores and accuracies are reported in Table <ref type="table">6</ref>. These results show that our two model variants consistently outperform all baseline models. The deterministic KG models can distinguish the existing relation facts from negative links, but they do not leverage the confidence information and cannot recognize the high-confidence ones. URGE does not encode the rich relations. Although UKGE n− fits confidence scores in the KG, it cannot correctly interpret negative links as false. Consistent with the previous two tasks, the performance of UKGE p− is worse than UKGE rect .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>To the best of our knowledge, this paper is the first work on embedding uncertain knowledge graphs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Example 3.1. Weighted triples. 1. (choir, relatedto, sing): 1.00 2. (college, synonym, university): 0.99 3. (university, synonym, institute): 0.86 4. (fork, atlocation, kitchen): 0.4 Definition 2. Uncertain Knowledge Graph Embedding Problem. Given an uncertain KG G, the embedding model aims to encode each entity and relation in a low-dimensional space in which structure information and confidence scores of relation facts are preserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Example 4.1. A Logical Rule on Transitivity of Synonym Relation. (A,synonym,B) ∧ (B,synonym,C) → (A,synonym,C) This logical rule describes the transitivity of the relation synonym. In this logical rule, A, B and C are placeholders for entities, synonym is the predicate that corresponds to the relation in uncertain KGs, (A,synonym,B) ∧ (B,synonym,C) is the body of the rule, and (A,synonym,C) is the head of the rule. A logical rule serves as a template rule. By replacing the placeholders in a logical rule with concrete entities and relations, we can get rule instances, which are called ground rules. Considering Example 4.1 and uncertain relation facts from Example 3.1, we can have the following ground rule by replacing the placeholders with real relation facts in KG. Example 4.2. A Ground Rule on Transitivity of Synonym. (college, synonym, university) ∧ (university, synonym, institute) → (college, synonym, institute)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>01}, dimensionality k ∈ {64, 128, 256, 512}, batch size b ∈ {128, 256, 512, 1024}, The L 2 regularization coefficient λ is fixed as 0.005. Training was stopped using early stopping based on MSE on the validation set, computed every 10 epochs. The best hyper-parameter combinations on CN15k and NL27k are {lr = 0.001, k = 128} and b = 128 for UKGE rect , b = 512 for UKGE logi . On PPI5k they are {lr = 0.001, k = 128, b = 256} for both variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for optimization. The best hyper-parameter combinations on CN15k and NL27k are b = 1024, {lr = 0.01, k = 128} for TransE and {lr = 0.05, k = 256} for DistMult and Com-plEx. On PPI5k they are lr = 0.1, {k = 128, b = 512} for DistMult and {k = 256, b = 1024} for TransE and ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>which is essentially the MSE loss by treating unseen relation facts as false. Statistics of the extracted datasets used in this paper. Ent. denotes entities and Rel. stands for relations. Avg(s) and Std(s) are the average and standard deviation of the confidence scores.</figDesc><table><row><cell>Dataset</cell><cell>#Ent.</cell><cell cols="4">#Rel. #Rel. Facts Avg(s) Std(s)</cell></row><row><cell cols="2">CN15k 15,000</cell><cell>36</cell><cell>241,158</cell><cell>0.629</cell><cell>0.232</cell></row><row><cell cols="2">NL27k 27,221</cell><cell>404</cell><cell>175,412</cell><cell>0.797</cell><cell>0.242</cell></row><row><cell>PPI5k</cell><cell>4,999</cell><cell>7</cell><cell>271,666</cell><cell>0.415</cell><cell>0.213</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>, and (iii) UKGE n− and UKGE p− that are two simplified versions of our model. • Deterministic KG Embedding Models. TransE, DistMult, and ComplEx have demonstrated high performance on deterministic KGs. Only the high-confidence relation facts from KGs are used for training. For each KG, we Examples of logical rules. Hit ratio means the proportion of relation facts that have already existed in the KG</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Logical Rules</cell><cell>Hit Ratio</cell></row><row><cell>CN15k</cell><cell></cell><cell cols="4">(A, relatedTo, B)∧(B, relatedTo, C)→(A, relatedTo, C) (A, causes, B)∧(B, causes, C)→(A, causes, C)</cell><cell>37.0% 35.6%</cell></row><row><cell cols="6">NL27k (A, atheletePlaysForTeam,B) ∧ (A, athletePlaysSport, C) →(B, teamPlaysSport, C)</cell><cell>42.9%</cell></row><row><cell>PPI5k</cell><cell></cell><cell></cell><cell cols="3">(A, binding, B)∧(B, binding, C)→(A, binding, C)</cell><cell>80.8%</cell></row><row><cell>Dataset</cell><cell>CN15k</cell><cell cols="2">NL27k</cell><cell cols="2">PPI5k</cell></row><row><cell>Metrics</cell><cell cols="5">MSE MAE MSE MAE MSE MAE</cell></row><row><cell>URGE</cell><cell cols="4">10.32 22.72 7.48 11.35 1.44</cell><cell>6.00</cell></row><row><cell cols="6">UKGE n− 23.96 30.38 24.86 36.67 7.46 19.32</cell></row><row><cell>UKGE p−</cell><cell cols="2">9.02 20.05 2.67</cell><cell>7.03</cell><cell>0.96</cell><cell>4.09</cell></row><row><cell>UKGE rect</cell><cell cols="2">8.61 19.90 2.36</cell><cell>6.90</cell><cell>0.95</cell><cell>3.79</cell></row><row><cell>UKGE logi</cell><cell cols="2">9.86 20.74 3.43</cell><cell>7.93</cell><cell>0.96</cell><cell>4.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mean squared error (MSE) and mean absolute error (MAE) of relation fact confidence prediction (×10 −2 ).</figDesc><table><row><cell>metrics</cell><cell>CN15K</cell><cell>NL27k</cell><cell>PPI5k</cell></row><row><cell>Dataset</cell><cell>linear exp.</cell><cell>linear exp.</cell><cell>linear exp.</cell></row><row><cell>TransE</cell><cell cols="3">0.601 0.591 0.730 0.722 0.710 0.700</cell></row><row><cell cols="4">DistMult 0.689 0.677 0.911 0.897 0.894 0.880</cell></row><row><cell cols="4">ComplEx 0.723 0.712 0.921 0.913 0.896 0.881</cell></row><row><cell>URGE</cell><cell cols="3">0.572 0.570 0.593 0.593 0.726 0.723</cell></row><row><cell cols="4">UKGE n− 0.236 0.232 0.245 0.245 0.514 0.517</cell></row><row><cell>UKGE p−</cell><cell cols="3">0.769 0.768 0.933 0.929 0.940 0.944</cell></row><row><cell cols="4">UKGE rect 0.773 0.775 0.939 0.942 0.946 0.946</cell></row><row><cell cols="4">UKGE logi 0.789 0.788 0.955 0.956 0.970 0.969</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean normalized DCG for global ranking task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>To justify the use of negative links and PSL reasoning in our model, we propose two simplified versions of UKGE rect , called UKGE n− and UKGE p− . In UKGE n− , we only keep the observed relation facts and remove negative sampling, and in UKGE p− , we remove PSL reasoning and use the MSE loss for unseen relation facts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Both our variants UKGE rect and UKGE logi outperform the baselines URGE, UKGE n− , and UKGE p− , since URGE only takes node proximity information and cannot model the rich relations between entities, and UKGE n− does not adopt negative sampling and cannot recognize negative links. The better results of UKGE rect than UKGE p− demonstrate that introducing PSL into embedding learning can enhance the model performance. Between the two model variants, UKGE rect results in smaller MSE and MAE than UKGE logi .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>shows the mean nDCG over all test queries for all compared methods. Though TransE, Dist-Mult, and ComplEx do not encode the confidence score information, they maximize the plausibility of all observed relation facts and therefore rank these existing relation facts Examples of relation fact ranking (global) results using UKGE logi . Top 4 results are shown. N/A denotes relation facts that are not observed in KG. As UKGE n− removes negative sampling from the loss function, it cannot distinguish the negative links from existing relation facts and results in the worst performance. UKGE p− yields slightly worse performance than UKGE rect .</figDesc><table><row><cell>Dataset</cell><cell>head</cell><cell cols="2">relation</cell><cell></cell><cell>true tail</cell><cell></cell><cell>confidence predicted tail predicted confidence true confidence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fast</cell><cell></cell><cell>0.968</cell><cell>fast</cell><cell>0.703</cell><cell>0.968</cell></row><row><cell></cell><cell>rush</cell><cell cols="2">relatedto</cell><cell></cell><cell>motion rapid</cell><cell></cell><cell>0.709 0.709</cell><cell>move hour</cell><cell>0.623 0.603</cell><cell>0.557 0.654</cell></row><row><cell>CN15k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>urgency sleeping</cell><cell></cell><cell>0.709 1.0</cell><cell>time relaxing</cell><cell>0.601 0.858</cell><cell>0.105 N/A</cell></row><row><cell cols="2">hotel</cell><cell cols="2">usedfor</cell><cell cols="3">rest bed away from home</cell><cell>0.984 0.709</cell><cell>sleeping rest</cell><cell>0.849 0.827</cell><cell>1.0 0.984</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">stay overnight</cell><cell>0.709</cell><cell>hotel room</cell><cell>0.797</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Honda</cell><cell></cell><cell>1.0</cell><cell>Honda</cell><cell>0.942</cell><cell>1.0</cell></row><row><cell cols="4">NL27k Toyota competeswith</cell><cell></cell><cell>Ford BMW</cell><cell></cell><cell>1.0 0.964</cell><cell>Hyundai Chrysler</cell><cell>0.910 0.908</cell><cell>0.719 N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">General Motors</cell><cell>0.930</cell><cell>Nissan</cell><cell>0.896</cell><cell>0.859</cell></row><row><cell>Metrics</cell><cell cols="2">CN15k</cell><cell cols="2">NL27k</cell><cell cols="2">PPI5k</cell></row><row><cell>Dataset</cell><cell cols="6">F-1 Accu. F-1 Accu. F-1 Accu.</cell></row><row><cell>TransE</cell><cell>23.4</cell><cell>67.9</cell><cell>65.1</cell><cell>53.4</cell><cell>83.2</cell><cell>98.5</cell></row><row><cell cols="2">DistMult 27.9</cell><cell>71.1</cell><cell>72.1</cell><cell>70.1</cell><cell>86.9</cell><cell>97.1</cell></row><row><cell cols="2">ComplEx 18.9</cell><cell>73.2</cell><cell>63.3</cell><cell>53.4</cell><cell>83.2</cell><cell>98.9</cell></row><row><cell>URGE</cell><cell>21.2</cell><cell>86.0</cell><cell>83.6</cell><cell>88.7</cell><cell>85.2</cell><cell>98.6</cell></row><row><cell cols="2">UKGE n− 23.6</cell><cell>86.1</cell><cell>64.4</cell><cell>65.5</cell><cell>92.7</cell><cell>99.3</cell></row><row><cell cols="2">UKGE p− 26.2</cell><cell>88.7</cell><cell>89.7</cell><cell>93.4</cell><cell>94.2</cell><cell>99.3</cell></row><row><cell cols="2">UKGE rect 28.8</cell><cell>90.4</cell><cell>92.3</cell><cell>95.2</cell><cell>95.1</cell><cell>99.4</cell></row><row><cell cols="2">UKGE logi 25.9</cell><cell>90.1</cell><cell>88.4</cell><cell>93.0</cell><cell>94.5</cell><cell>99.5</cell></row><row><cell cols="7">Table 6: F-1 scores (%) and accuracies (%) of relation fact</cell></row><row><cell>classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">high. We observe that DistMult and ComplEx have con-</cell></row><row><cell cols="7">siderably better performance than TransE, as TransE does</cell></row><row><cell cols="7">not handle 1-to-N relations well. ComplEx embeds enti-</cell></row><row><cell cols="7">ties and relations in the complex domain and handles asym-</cell></row><row><cell cols="7">metric relations better than DistMult. It achieves the best</cell></row><row><cell cols="7">results among the deterministic KG embedding models on</cell></row><row><cell>this task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Our model UKGE effectively preserves both the relation facts and uncertainty information in the embedding space of KG. We propose two variants of our model and conduct extensive experiments on relation fact confidence score prediction, relation fact ranking, and relation fact classification. The results are verypromising. For future work, we will study how to systematically generate reasonable logical rules and test its impact on embedding quality. We are interested in extending UKGE for uncertain knowledge extraction from text.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, Snapchat gift funds, and PPDai gift fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2013. 2017. 2008</date>
		</imprint>
	</monogr>
	<note>Hinge-loss markov random fields: Convex inference for structured prediction</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<title level="m">Marginalizing stacked linear denoising autoencoders</title>
				<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On2vec: Embedding-based relation prediction for ontology population</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011-07">2011. Jul</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On embedding uncertain graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Locally adaptive translation for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="992" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A short introduction to probabilistic soft logic</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications</title>
				<meeting>the NIPS Workshop on Probabilistic Programming: Foundations and Applications</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Iclr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Aaai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2008">2014. 2009. 2015. 2008. 2018</date>
		</imprint>
	</monogr>
	<note>Communications of the ACM. Nickel, M. Rosasco, L.; and Poggio, T. 2016. Holographic embeddings of knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yago: A multilingual knowledge base from wikipedia, wordnet, and geonames</title>
				<editor>
			<persName><surname>Conll</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Rebele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Biega</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kuzey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2011. 2017. 2009. 2016</date>
		</imprint>
	</monogr>
	<note>ISWC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proje: Embedding projection for knowledge graph completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The string database in 2017: quality-controlled proteinprotein association networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013. 2017. 2016</date>
		</imprint>
	</monogr>
	<note>Reasoning with neural tensor networks for knowledge base completion. made broadly accessible</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><surname>Icml</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Weston, J; Bordes, A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2016. 2016. 2014. 2015. 2013</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>-T.; He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
