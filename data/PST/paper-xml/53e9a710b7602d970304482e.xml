<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Representations Using Convolutional Neural Networks for Web Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<email>yshen@cs.kent.e</email>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.c</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>deng@microsoft.c</email>
						</author>
						<author>
							<persName><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
							<email>gregoire.mesnil@umontreal.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kent State University</orgName>
								<address>
									<settlement>Kent</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Montréal Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Representations Using Convolutional Neural Networks for Web Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2567948.2577348</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>3</term>
					<term>3 [Information Storage and Retrieval]: Information Search and Retrieval; I</term>
					<term>2</term>
					<term>6 [Artificial Intelligence]: Learning Semantic Representation, Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn lowdimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, using stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other semantic models, which were state-of-the-art in retrieval performance prior to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Latent semantic models, such as latent semantic analysis (LSA) and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type="bibr" target="#b1">[2]</ref>). However, most latent semantic models still view a query (or a document) as a bag of words. Therefore, they are not effective in capturing fine-grained contextual structures for information retrieval.</p><p>Modeling contextual information in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type="bibr" target="#b1">[2]</ref>[4] <ref type="bibr" target="#b7">[8]</ref>. Usually, the contextual information captured by models such as TF-IDF, BM25, and topic models, is often too coarse-grained to be effective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type="bibr" target="#b4">[5]</ref> that directly model phrases (or word n-grams), but they often suffer from the data sparseness problem. In a separate line of research, deep learning based techniques have been proposed for semantic understanding <ref type="bibr" target="#b2">[3]</ref>[6][9] <ref type="bibr" target="#b9">[10]</ref>. Salakhutdinov and Hinton <ref type="bibr" target="#b8">[9]</ref> demonstrated that the semantic structures can be extracted via a semantic hashing approach using a deep auto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type="bibr" target="#b5">[6]</ref>, which is reported to outperform significantly semantic hashing and other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type="bibr" target="#b0">[1]</ref>, we present a new Convolutional Deep Structured Semantic Models (C-DSSM). Compared with DSSM, C-DSSM has a convolutional layer that projects each word within a context window to a local contextual feature vector. Semantically similar words-withincontext are projected to vectors that are close to each other in the contextual feature space. Further, since the overall semantic meaning of a sentence is often determined by a few key words in the sentence, thus, simply mixing all words together (e.g., by summing over all local feature vectors) may introduce unnecessary divergence and hurt the effectiveness of the overall semantic representation. Therefore, C-DSSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector. The global feature vector can be then fed to feedforward neural network layers, which perform affine transformations followed by non-linear functions applied element-wise over their inputs to extract highly non-linear and effective features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">C-DSSM FOR EXTRACTING CONTEX-TUAL FEATURES FOR IR</head><p>The architecture of the C-DSSM, is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The C-DSSM contains a word hashing layer that transforms each word into a letter-tri-gram input representation, a convolutional layer to extract local contextual features, a max-pooling layer to form a global feature vector, and a final semantic layer to represent the high-level semantic feature vector of the input word sequence. In what follows, we describe each layer of the C-DSSM in detail, using the annotation illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The word hashing layer transforms each word in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type="bibr" target="#b5">[6]</ref>. For example, the word is represented by a count vector of its letter-tri-grams.</p><p>The convolution operation can be viewed as sliding window based feature extraction. It is designed to capture the contextual features for a word. Consider a word at the t-th position in a word sequence. The word hashing feature vectors of all the context words within a window around are firstly concatenated to form a context window vector, and then projected to a local contextual feature vector , as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The contextual feature vectors extracted at the convolutional layer are local features, one for each word. They need to be combined to obtain a global feature vector with a fixed size. For the IR task, we want to suppress the non-significant local features and only retain the salient features that are useful for IR in the global feature vector. Therefore, we use a max operation, known as max pooling, which forces the network to retain only the most useful local features produced by the convolutional layers.</p><p>On top of the global feature vector, a semantic layer is applied to extract the high-level semantic representation, denoted by .</p><p>In this model, both the convolutional layer and the semantic layer use the function as the non-linear activation function. We further compute the relevance score between the query and each document by measuring the cosine similarity between their semantic vectors. Formally, the semantic relevance score between a query and a document is measured as:</p><formula xml:id="formula_0">, cosine ,</formula><p>where and are the semantic vectors of the query and the document, respectively. In Web search, given the query, the documents are ranked by their semantic relevance scores.</p><p>The parameters of the C-DSSM to be learned include convolution matrix and semantic projection matrix , as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Note that the word hashing matrix is fixed without need of learning. The C-DSSM is trained on clickthrough data by maximizing the conditional likelihood of the clicked documents given a query, using stochastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We have evaluated the retrieval models on a large-scale real world data set, called the evaluation data set henceforth. The evaluation data set contains 12,071 English queries sampled from one-year query log files of a commercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type="bibr" target="#b4">[5]</ref>, we only used the title field of a Web document for ranking. The results are reported by mean Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" target="#b6">[7]</ref>. In our experiments, the clickthrough data used for model training include 30 million of query/clicked-title pairs sampled from one year query log files. We then tested the models in ranking the documents in the evaluation data set. The main results of our experiments are summarized in Table <ref type="table" target="#tab_0">1</ref>, where we compared the proposed C-DSSM (Row 6) with a set of baseline models, including BM25, the unigram language model (ULM), phrase-based translation model (PTM), word-based translation model (WTM), and the DSSM. The proposed C-DSSM (Row 6) has a convolutional layer and a max-pooling layer, both having 300 neurons, and a final output layer using 128 neurons. The results show that the proposed C-DSSM outperforms all the competing methods with a significant margin. All models, except BM25 and ULM, use the same clickthrough data for learning. Superscripts , , and indicate statistically significant improvements 0.05 over BM25, PTM, and DSSM, respectively. The proposed C-DSSM outperforms all the competing methods with a significant margin. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the C-DSSM. A convolutional layer with the window size of three is illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Comparative results with the previous approaches.</head><label>1</label><figDesc>The work presented in this paper developed a novel learnable deep learning architecture based on the use of a CNN to extract both local contextual features (via the convolution layer) and global contextual features (via the max-pooling layer) from text. Then the higher layer(s) in the overall deep architecture makes effective use of the extracted context-sensitive features to perform semantic matching between documents and queries, both in the form of text, for Web search applications.</figDesc><table><row><cell># Models</cell><cell>NDCG@1</cell><cell>NDCG@3</cell><cell>NDCG@10</cell></row><row><cell>1 BM25</cell><cell>0.305</cell><cell>0.328</cell><cell>0.388</cell></row><row><cell>2 ULM</cell><cell>0.304</cell><cell>0.327</cell><cell>0.385</cell></row><row><cell>3 WTM</cell><cell>0.315 α</cell><cell>0.342 α</cell><cell>0.411 α</cell></row><row><cell>4 PTM (len ≤ 3)</cell><cell>0.319 α</cell><cell>0.347 α</cell><cell>0.413 α</cell></row><row><cell>5 DSSM</cell><cell>0.320 α</cell><cell>0.355 αβ</cell><cell>0.431 αβ</cell></row><row><cell cols="2">6 C-DSSM win =3 0.342 αβγ</cell><cell>0.374 αβγ</cell><cell>0.447 αβγ</cell></row><row><cell cols="2">4. CONCLUSION</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dependence language model for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clickthrough-based translation models for web search: from word models to phrase models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1139" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
		<title level="m">IR evaluation methods for retrieving highly relevant documents. SIGIR</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Markov random field model for term dependencies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR Workshop Information Retrieval and Applications of Graphical Models</title>
				<meeting>SIGIR Workshop Information Retrieval and Applications of Graphical Models</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards Deeper Understanding Deep Convex Networks for Semantic Utterance Classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
