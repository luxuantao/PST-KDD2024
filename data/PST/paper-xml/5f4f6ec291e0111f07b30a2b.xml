<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NATS-Bench: Benchmarking NAS algorithms for Architecture Topology and Size</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dxy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
							<email>gabrys@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NATS-Bench: Benchmarking NAS algorithms for Architecture Topology and Size</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural Architecture Search</term>
					<term>Benchmark</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. Network topology and network size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both of those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under different search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement from a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for each candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in a more comparable and computationally effective environment. All codes are publicly available at: https://xuanyidong.com/assets/projects/NATS-Bench.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE deep learning community is undergoing a transition from hand-designed neural architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to automatically designed neural architectures <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In its early stages, the great success of deep learning was promoted by the introductions of novel neural architectures, such as ResNet <ref type="bibr" target="#b0">[1]</ref>, Inception <ref type="bibr" target="#b2">[3]</ref>, VGGNet <ref type="bibr" target="#b8">[9]</ref>, and Transformer <ref type="bibr" target="#b9">[10]</ref>. However, manually designing one architecture requires human experts to frequently try and evaluate numerous different operation and connection options <ref type="bibr" target="#b3">[4]</ref>. In contrast to architectures that are manually designed, those automatically found by neural architecture search (NAS) algorithms require much less human interaction and expert effort. These NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and sequence modeling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, a variety of NAS algorithms have been increasingly proposed. While these NAS techniques are methodically designed and show promising improvements, many setups in their algorithms are different. (1) Different search space is utilized, e.g., range of macro skeletons of the whole architecture <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and a different operation set for the micro cell within the skeleton <ref type="bibr" target="#b4">[5]</ref>, etc. (2) After a good architecture is selected, various strategies can be employed to train this architecture and report the performance, e.g., different data augmentation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, different regularization <ref type="bibr" target="#b10">[11]</ref>, different scheduler <ref type="bibr" target="#b14">[15]</ref>, and different selections of hyper-parameters <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b2">(3)</ref> The validation set for testing the performance of the selected architecture is not split in the same way <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These discrepancies cause a problem when comparing the performance of various NAS algorithms, making it difficult to conclude their relative contributions.</p><p>In response to this challenge, NAS-Bench-101 <ref type="bibr" target="#b17">[18]</ref> and NAS-HPO-Bench <ref type="bibr" target="#b18">[19]</ref> were proposed. However, some NAS algorithms cannot be applied directly on NAS-Bench-101, and NAS-HPO-Bench only has 144 candidate architectures which may be insufficient to comprehensively evaluate NAS algorithms. NAS-Bench-1shot1 <ref type="bibr" target="#b19">[20]</ref> reuses the NAS-Bench-101 dataset with some modification to analyse the one-shot NAS methods. The aforementioned works have mainly focused on the architecture topology. However, the architecture size 1 , which significantly affects a model's performance, is not considered in the existing benchmarks.</p><p>To enlarge the scope of these benchmarks and towards better reproducibility of NAS methods, we propose NATS-Bench with (1) a topology search space S t to be applicable for all NAS methods and (2) a size search space S s that supplements the lack of analysis for the architecture size. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, each architecture consists of a predefined skeleton with a stack of the searched cells. Each cell is represented as a densely-connected directed acyclic graph (DAG) as shown in the bottom section of Figure <ref type="figure" target="#fig_0">1</ref>. The node represents the sum of the feature maps and each edge is associated with an operation transforming the feature maps from the source node to the target node.</p><p>In S t , we search for the operation assigned on each edge, and thus its size is related to the number of nodes defined 1. Some papers may use size to indicate the number of parameters of a neural network. In this manuscript, the terminology "architecture size" or "size" refer to the number of channels in each layer following <ref type="bibr" target="#b20">[21]</ref>. for the DAG and the size of the operation set. We choose 4 nodes and 5 representative operation candidates for the operation set, which generates a total search space of 15,625 cells/architectures. In S s , we search for the number of channels in each layer (i.e., convolution, cell, or block). We predefine 8 candidates for the number of channels, which generates a total search space of itive training procedure of each selected architecture can be avoided <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref> so that researchers can target on the essence of NAS, i.e., search algorithm. Another benefit is that the validation time for NAS largely decreases when testing in NATS-Bench, which provides a computational power friendly environment for more participation in NAS. ( <ref type="formula">4</ref>) It provides results of each architecture on multiple datasets. The model transferability can be thoroughly evaluated for most NAS algorithms. <ref type="bibr" target="#b4">(5)</ref> In NATS-Bench, we provide systematic analysis of the proposed search space. We also evaluate 10 recent advanced NAS algorithms including reinforcement learning (RL)-based methods, evolutionary strategy (ES)-based methods, differentiable-based methods, etc. Our empirical analysis can bring some insights to the future designs of NAS algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the past few years, different kinds of search spaces and search algorithms have been proposed. They brought great advancements in many applications of neural network, such as visual perception <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, language modelling <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, etc. Despite their success, many researchers have raised concerns about the reproducibility and generalization ability of the NAS algorithms <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b26">[27]</ref>. It is essentially not clear if the reported improvements have come from hyper-parameter settings, re-training pipelines, random seeds, or the improvements of the searching algorithm itself <ref type="bibr" target="#b24">[25]</ref>. Many researchers devote their effort to solve this problem, and we will introduce them in Section 2.1 and Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NAS Benchmark</head><p>To the best of our knowledge, NAS-Bench-101 <ref type="bibr" target="#b17">[18]</ref> is the only existing large-scale architecture dataset. Similar to NATS-Bench, NAS-Bench-101 also transforms the problem of architecture search into the problem of searching neural cells, represented as a DAG. Differently, NAS-Bench-101 defines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. We summarize characteristics of our NATS-Bench and NAS-Bench-101 in Table <ref type="table">1</ref>. The main highlights of our NATS-Bench are as follows. (1) NATS-Bench is algorithm-agnostic while NAS-Bench-101 without any modification is only applicable to selected algorithms <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The original complete search space, based on the nodes in NAS-Bench-101, is huge. So, it is exceedingly difficult to efficiently traverse the training of all architectures. To trade off the computational cost and the size of the search space, they constrain the maximum number of edges in the DAG. However, it is difficult to incorporate this constraint in all NAS algorithms, such as NAS algorithms based on parameter sharing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Therefore, many NAS algorithms cannot be directly evaluated on NAS-Bench-101. Our NATS-Bench solves this problem by sacrificing the number of nodes and including all possible edges so that our search space is algorithmagnostic. <ref type="bibr" target="#b1">(2)</ref> We provide extra diagnostic information, such as architecture computational cost, fine-grained training and evaluation time, etc., which we hope will give inspirations to better and most efficient designs of NAS algorithms utilizing these diagnostic information.</p><p>Despite the existence of NAS-Bench-101, other researchers have also devoted their effort to building a fair comparison and development environments for NAS. Zela et al. <ref type="bibr" target="#b19">[20]</ref> proposed a general framework for oneshot NAS methods and reused NAS-Bench-101 to benchmark different NAS algorithms. Yu et al. <ref type="bibr" target="#b28">[29]</ref> designed a novel evaluation framework to evaluate the search phase of NAS algorithms by comparing with a random search. The aforementioned works have mainly focused on the network topology but as other aspects of DNNs, such as network size and optimizer, significantly affect the network's performance there is a need for an environment and systematic studies covering these areas of NAS. Unfortunately, until now these aspects have rarely been considered w.r.t. the problem of reproducibility and generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hyper-Parameter Optimization (HPO) Benchmark</head><p>NAS-HPO-Bench <ref type="bibr" target="#b18">[19]</ref> evaluated 62208 configurations in the joint NAS and hyper-parameter space for a simple 2-layer TABLE 1: We summarize the important characteristics of NAS-Bench-101 and NATS-Bench. Our NATS-Bench provides the search space for both architecture topology and architecture size. Besides, NATS-Bench provides train/validation/test performance on three (one for NAS-Bench-101) different datasets so that the generality of NAS algorithms can be evaluated. It also provides some diagnostic information that may provide insights to design better NAS algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NATS-BENCH</head><p>Our NATS-Bench is algorithm-agnostic. Put simply, it is applicable to almost any up-to-date NAS algorithm. In this section, we will briefly introduce our NATS-Bench.</p><p>The search space of NATS-Bench is inspired by cell-based NAS algorithms (Section 3.1). NATS-Bench evaluates each architecture on three different datasets (Section 3.2). All implementation details of NATS-Bench are introduced in Section 3.3. NATS-Bench also provides some diagnostic information which can be used for potentially better designs of future NAS algorithms (discussed in Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architectures in the Search Space</head><p>Macro Skeleton. Our search space follows the design of its counterpart as used in the recent neural cell-based NAS algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. As shown in the middle part of Figure <ref type="figure" target="#fig_0">1</ref>, the skeleton is initiated with one 3-by-3 convolution with 16 output channels and a batch normalization layer <ref type="bibr" target="#b29">[30]</ref>. The main body of the skeleton includes three stacks of cells, connected by a residual block. All cells in an architecture has the same topology. The intermediate residual block is the basic residual block with a stride of 2 <ref type="bibr" target="#b0">[1]</ref>, which serves to down-sample the spatial size and double the channels of an input feature map. The shortcut path in this residual block consists of a 2-by-2 average pooling layer with stride of 2 and a 1-by-1 convolution. The skeleton ends up with a global average pooling layer to flatten the feature map into a feature vector. The classification uses a fully connected layer with a softmax layer to transform the feature vector into the final prediction.</p><p>The Topology Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Since all cells in an architecture have the same topology, an architecture candidate in S t corresponds to a different cell, which is represented as a densely connected DAG. The densely connected DAG is obtained by assigning a direction from the i-th node to the j-th node (i &lt; j) for each edge in an undirected complete graph. Each edge in this DAG is associated with an operation transforming the feature map from the source node to the target node. All possible operations are selected from a predefined operation set, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(bottom-right). In our NATS-Bench, the predefined operation set O has L = 5 representative operations: (1) zeroize, (2) skip connection, (3) 1-by-1 convolution, (4) 3-by-3 convolution, and (5) 3-by-3 average pooling layer. The convolution in this operation set is an abbreviation of an operation sequence of ReLU, convolution, and batch normalization. The DAG has V = 4 nodes, where each node represents the sum of all feature maps transformed through the associated operations of the edges pointing to this node. We choose V = 4 to allow the search space to contain basic residual block-like cells, which require 4 nodes. Densely connected DAG does not restrict the searched topology of the cell to be densely connected, since we include zeroize in the operation set, which is an operation of dropping the associated edge. We do not impose the constraint on the maximum number of edges <ref type="bibr" target="#b17">[18]</ref>, and thus S t is applicable to most NAS algorithms, including all cell-based NAS algorithms. For each architecture in S t , each cell is stacked N = 5 times, with the number of output channels set to 16, 32 and 64 for the first, second and third stages, respectively.</p><p>The Size Search Space S s . The size search space is inspired by transformable architecture search methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In the size search space, every stack in each architecture is constructed by stacking N = 1 cell. All cells in every architecture have the same topology, which is the best one in S t on the CIFAR-100 dataset. Each architecture candidate in S s has a different configuration regarding the number of channels in each layer. 2 We build the size search space S s to include the largest number of channels in S t . Therefore, the number of channels in each layer is chosen from {8, 16, 24, 32, 40, 48, 56, 64}. Therefore, the size search space S s has 8 5 = 32768 architecture candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We train and evaluate each architecture on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b32">[33]</ref>, and ImageNet-16-120 <ref type="bibr" target="#b33">[34]</ref>. We choose these three datasets because CIFAR and ImageNet <ref type="bibr" target="#b34">[35]</ref> are the most popular image classification datasets.</p><p>We split each dataset into training, validation and test sets to provide a consistent training and evaluation settings for previous NAS algorithms <ref type="bibr" target="#b7">[8]</ref>. Most NAS methods use the validation set to evaluate architectures after the architecture is optimized on the training set. The validation performance of the architectures serves as the supervision 2. A layer could be the stem 3-by-3 convolutional layer, the cell, or the residual block. ImageNet-16-120: We build ImageNet-16-120 from the down-sampled variant of ImageNet (ImageNet16×16). As indicated in <ref type="bibr" target="#b33">[34]</ref>, down-sampling images in ImageNet can largely reduce the computation costs for optimal hyperparameters of some classical models while maintaining similar searching results. <ref type="bibr" target="#b33">[34]</ref> down-sampled the original ImageNet to 16×16 pixels to form ImageNet16×16, from which we select all images with label ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">120]</ref> to construct ImageNet-16-120. In sum, ImageNet-16-120 contains 151.7K training images, 3K validation images, and 3K test images with 120 classes.</p><p>By default, in this paper, "the training set", "the validation set", "the test set" indicate the new training, validation, and test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture Performance</head><p>Training Architectures. In order to unify the performance of every architecture, we provide the performance of every architecture in our search space. In our NATS-Bench, we follow previous literature to set up the hyper-parameters and training strategies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. We train each architecture with the same strategy, which is shown in Table <ref type="table" target="#tab_1">2</ref>. For simplification, we denote all hyper-parameters for training a model as a set H. We use H 0 , H 1 , and H 2 to denote the three kinds of hyper-parameters that we use. Specifically, we train each architecture via Nesterov momentum SGD, using the cross-entropy loss. We set the weight decay to 0.0005 and decay the learning rate from 0.1 to 0 with a cosine annealing <ref type="bibr" target="#b14">[15]</ref>. We use the same H 0 on different datasets, except for the data augmentation which is slightly different due to the image resolution. On the CIFAR datasets, we use the random flip with probability of 0.5, the random crop 32×32 patch with 4 pixels padding on each border, and the normalization over RGB channels. On ImageNet-16-120, we use a similar strategy but with random crop 16×16 patch and 2 pixels padding on each border. In H 0 , we train each architecture by 12 epochs, which can be used in banditbased algorithms <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Since 12 epochs are not sufficient to evaluate the relative ranking of different architectures, we train each candidate with more epochs (H 1 and H 2 ) to obtain a more accurate ranking. H 1 and H 2 are the same as H 0 but use 200 epochs and 90 epochs, respectively. In NATS-Bench, we apply H 0 and H 1 on the topology search space S t ; and we apply H 0 and H 2 on the size search space S s .</p><p>Metrics. We train each architecture with different random seeds on different datasets. We evaluate each architecture α after every training epoch. NATS-Bench provides the training, validation, and test loss as well as accuracy. Users can easily use our API to query the results of each trial of α, which has negligible computational costs. In this way, researchers could significantly speed up their searching algorithm on these datasets and focus solely on the essence of NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diagnostic Information</head><p>Validation accuracy is a commonly used supervision signal for NAS. However, considering the expensive computational costs for evaluating the architecture, the signal is too sparse. In our NATS-Bench, we also provide some additional diagnostic information in a form of extra statistics obtained during training of each architecture. Collecting these statistics almost involves no extra computation cost but may provide insights for better designs and training strategies of different NAS algorithms, such as platformaware NAS <ref type="bibr" target="#b11">[12]</ref>, accuracy prediction <ref type="bibr" target="#b37">[38]</ref>, mutation-based NAS <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, etc.</p><p>Architecture Computational Costs: NATS-Bench provides three computation metrics for each architecturethe number of parameters, FLOPs, and latency. Algorithms that focus on searching architectures with computational constraints, such as models on edge devices, can use these metrics directly in their algorithm designs without extra calculations. We also provide the training time and evaluation time for each architecture.</p><p>Fine-grained training and evaluation information. NATS-Bench tracks the changes in loss and accuracy of every architecture after every training epoch. These finegrained training and evaluation information often shows the trends related to the architecture performance and could help with identifying some attributes of the model, such as the speed of convergence, the stability, the over-fitting or under-fitting levels, etc. These attributes may benefit the designs of NAS algorithms. Besides, some methods learn to predict the final accuracy of an architecture based on the results of few early training epochs <ref type="bibr" target="#b37">[38]</ref>. These algorithms can be trained faster and the performance of the accuracy prediction can be evaluated using the fine-grained evaluation information.</p><p>Parameters of the optimized architecture. Our NATS-Bench releases the trained parameters for each architecture. This can provide ground truth label for hypernetwork-based NAS methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which learn to generate parameters of an architecture. Other methods mutate an architecture to become another one <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b38">[39]</ref>. With NATS-Bench, researchers could directly use the off-the-shelf parameters instead of training them from scratch and analyze how to transfer parameters from one architecture to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">What/Who can Benefit from NATS-Bench?</head><p>Our NATS-Bench provides a unified NAS library for the community and can benefit NAS algorithms from the perspective of both performance and efficiency. NAS has been dominated by multi-fidelity based methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, which learn to search based on an approximation of the performance of each candidate in order to accelerate searching. Running algorithms on our NATS-Bench can reduce the approximation to an accurate performance via only querying from the database. This can avoid sub-optimal training because of the inaccurate estimation of the performance as well as accelerate the training into seconds. Meanwhile, with the provision of our diagnostic information, such as latency, algorithms trained with such extra pieces of information can directly fetch them from our codebase with negligible efforts. Meanwhile, the designs of NAS algorithms can also have more diversity with the benefit of the diagnostic information and more potential designs will be discussed in Section 6.</p><p>In the NAS community, there has been a growing attention to the field of joint searching for both topology and size <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. By benchmarking their sub-modules for either topology or size on NATS-Bench, it may help researchers to understand the effectiveness of the submodules and give inspirations for ongoing and future research which lies in this intersection.</p><p>NATS-Bench provides a unified codebase -a NAS library -to make the benchmarking as fair as possible. In this codebase, we share the code implementation for different algorithms as much as possible. For example, the super network for weight-sharing methods is reused; the data pipelines for different methods are reused; the interface of training, forwarding, optimizing for different algorithms is kept the same. We demonstrate, using 13 state-of-the-art NAS algorithms applied on NATS-Bench, how the process has been unified through an easy-to-use API. The implementation difference between DARTS <ref type="bibr" target="#b7">[8]</ref> and GDAS <ref type="bibr" target="#b6">[7]</ref> is only less than 20 lines of code. Our library reduces the effect caused by the implementation difference when comparing different methods. It is also easy to implement new NAS algorithms by reusing and extending our library. More detailed engineering designs can be found in the documentation of our released codes. As this part is beyond the scope of this manuscript, we do not introduce it here.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS OF NATS-BENCH 4.1 An Overview of Architecture Performance</head><p>The performance of each architecture in both search spaces S t and S s is shown in Figure <ref type="figure" target="#fig_5">3</ref>. The training and test accuracy with respect to the number of parameters and number of FLOPs are shown in each column, respectively. Results show that a different number of parameters or FLOPs will affect the performance of the architectures, which indicates that the choices of operations are essential in NAS. We also observe that the performance of the architecture can vary even when the number of parameters or FLOPs stays the same.</p><p>These observations indicate the importance of how the operations are connected and how the number of channels is set. We compare all architectures in S t and S s with some classical human-designed architectures (orange star marks in Figure <ref type="figure" target="#fig_5">3</ref>). (I) Compared to candidates in S t , ResNet shows competitive performance in three datasets, however, it still has room to improve, i.e., about 2% compared to the best architecture in CIFAR-100 and ImageNet-16-120, about 1% compared to the best one with the same amount of parameters in CIFAR-100 and ImageNet-16-120. (II) In many vision tasks, pyramid structure has shown a surprising robustness and accuracy <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Regarding the parameters vs. the accuracy, the candidates in S s with a pyramid structure are far from the Pareto optimality. Regarding the FLOPs vs. the    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture Ranking on Three Datasets</head><p>The ranking of every architecture in our search space is shown in Figure <ref type="figure" target="#fig_3">2</ref>, where the architectures ranked in CIFAR-10 (x-axis) are shown in relation to their respective ranks in CIFAR-100 and ImageNet-16-120 (y-axis), indicated by green and red markers respectively. The performance of the architectures in S t shows a generally consistent ranking over the three datasets with slightly different variance, which serves to test the generality of the searching algorithm. In contrast, the ranking of architecture candidates in S s is quite different. It indicates that the optimal architecture sizes on three datasets are different.</p><p>We compute the validation as well as the test accuracy after training with H 1 and H 2 on S t and S s , respectively. Figure <ref type="figure" target="#fig_7">4</ref> visualizes their correlation. It shows the relative ranking obtained from the validation accuracy is similar to that obtained using the test accuracy. Thus, it guarantees the upper bounds of the NAS algorithms as the brute-force strategy can find an architecture that can almost achieve the highest test accuracy.</p><p>We also show the correlation coefficient across different datasets in Figure <ref type="figure" target="#fig_1">5</ref>. The correlation dramatically decreases as we only pick the top performing architecture candidates. When we directly transfer the best architecture in one dataset to another (i.e. a vanilla strategy), it can not 100% secure a good performance. This phenomena is a call for better transferable NAS algorithms instead of using the vanilla strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BENCHMARK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Background</head><p>NAS aims to find architecture α among the search space S so that this found α achieves a high performance on the  validation set. This problem can be formulated as a bi-level optimization problem:</p><formula xml:id="formula_0">min α∈S L(α, ω * α , D val )<label>(1)</label></formula><p>s.t. ω * α = arg min ω L(α, ω, D train ), where L indicates the objective function (e.g., cross-entropy loss). D train and D val denote the training data and the validation data, respectively. In the typical NAS setting, after an architecture α is found, α will be evaluated on the test data D test to figure out its real performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We evaluate 13 recent, state-of-the-art searching methods on our NATS-Bench, which can serve as baselines for future NAS algorithms in our dataset. Specifically, we evaluate some typical NAS algorithms: (I) Random Search algorithms, e.g., random search (RANDOM) <ref type="bibr" target="#b46">[47]</ref>, random search with parameter sharing (RSPS) <ref type="bibr" target="#b26">[27]</ref>. (II) ES methods, e.g., REA <ref type="bibr" target="#b5">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type="bibr" target="#b47">[48]</ref>, ENAS <ref type="bibr" target="#b4">[5]</ref>. (IV) Differentiable algorithms. e.g., first order DARTS (DARTS-V1) <ref type="bibr" target="#b7">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type="bibr" target="#b6">[7]</ref>, SETN <ref type="bibr" target="#b16">[17]</ref>, TAS <ref type="bibr" target="#b20">[21]</ref>, FBNet-V2 <ref type="bibr" target="#b43">[44]</ref>, TuNAS <ref type="bibr" target="#b48">[49]</ref>. (V) HPO methods, e.g., BOHB <ref type="bibr" target="#b35">[36]</ref>.</p><p>Among them, RANDOM, REA, REINFORCE, and BOHB are multi-trial based methods. They can be used to search on both S t and S s search spaces. Especially, using our API, we can accelerate them to be executed in seconds as shown in Table <ref type="table" target="#tab_3">3</ref>.  Other methods are weight-sharing based methods, in which the evaluation procedure can be accelerated by using our API. Notably, DARTS, GDAS, SETN are specifically designed for the topology search space S t . TAS, FBNet-V2, and TuNAS can be used on the size search space S s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Multi-trial based Methods</head><p>We follow the suggested hyper-parameters in their original papers to run each method on our topology search space S t and size search space S s . We run each experiment 500 times on three datasets and setup a maximum time budget as 2e4 seconds. Every 100 seconds, each method can let us know the current searched architecture candidate. We use the hyper-parameters H 0 (12 epochs) to obtain a validation accuracy for each trial. This validation accuracy serves as the supervision/feedback signal for these multi-trial based methods. For BOHB, given its current budget for a trial, it can early stop before fully training the model using 12 epochs. We plot the averaged accuracy of this searched architecture candidate over 500 runs in Figure <ref type="figure" target="#fig_11">6</ref>. Each subfigure corresponds to one dataset and a search space. For example, in the middle of Figure <ref type="figure" target="#fig_11">6b</ref>, we search on CIFAR-100 and show the test accuracy of the searched architecture on CIFAR-100.</p><p>Observations on the topology search space S t . (1) On CIFAR-10, most methods have similar performance. (2) On CIFAR-100, REA is similar to BOHB, which outperforms REINFORCE; and RANDOM is the worst among them. (3) On ImageNet-16-120, BOHB significantly outperforms the other methods. It may be caused by the dynamic budget mechanism for each trial in BOHB, which allows to traverse more architecture candidates.</p><p>Observations on the size search space S s . (1) REA significantly outperforms the other methods on all datasets in the size search space S s . (2) On CIFAR-100 and ImageNet-16-120, results of BOHB, REINFORCE, and RANDOM are similar. (3) On CIFAR-10, REINFORCE is better than BOHB and RANDOM. (4) As the searching time goes, the searched architecture by REA gradually matches the best one, while the other methods need much more time to catch up with REA. (5) Figure <ref type="figure" target="#fig_5">3</ref> implies a simple prior for S s : without the constraint of model cost, the larger model tends to have higher accuracy. By visualising the searched architecture, REA can quickly fit this prior while the other methods do not.</p><p>Given the flexibility and robustness of REA, we would recommend choosing REA as a searching algorithm if the computational resources are sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Weight-sharing based Methods</head><p>To compare weight-sharing based methods as fairly as possible, we keep the same hyper-parameters concerned with the optimising of the shared weights for different methods. For other hyper-parameters, e.g., hyper-parameters for optimising the controller in ENAS or hyper-parameters for optimising the architectural parameters in DARTS/GDAS, we use the same values as introduced in their original papers. In this way, we can focus on evaluating the core and unique modules in each searching algorithm. We setup the total number of epochs to 100 for search, and compare results of their searched architecture candidates after each search epoch. We run each experiment three times and report the average results in Figure <ref type="figure" target="#fig_13">7a</ref> for the topology search space and in Figure <ref type="figure" target="#fig_13">7b</ref> for the size search space.</p><p>Observations on the topology search space S t . (1) On CIFAR-10, DARTS-V1 and DARTS-V2 quickly converge to find the architecture having many skip connections, which performs poorly. However, on CIFAR-100 and ImageNet-16-120, they perform relatively well. This is because the significantly increased searching data on CIFAR-100 and ImageNet-16-120 over CIFAR-10 alleviate the problem of incorrect gradient estimation in bi-level optimization. (2) RSPS, ENAS, and SETN converge quickly and are robust on three datasets. During their searching procedure, they will randomly sample some architecture candidates, evaluate them using the shared weights, and select the candidate with the highest validation accuracy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b2">(3)</ref> The searched architecture of GDAS slowly converges to the similar one as ENAS and SETN. Some observations on S t are different from those in our preliminary version. It is because some hyper-parameters changed following either suggestions from the authors or better strategies found in our experiments. Especially, we would like to highlight some useful strategies for weightsharing based methods: (1) always use batch statistics for the batch normalization layer. (2) do not learn the scale and shift parameters of the batch normalization layer. (3) during the evaluation procedure of RSPS, ENAS, and SETN, the average accuracy for a large batch of validation data is sufficient to approximate the average accuracy on the whole validation set. In our experiments, we use the batch size of 512 for evaluation.</p><p>Observations on the topology search space S s . TAS, FBNetV2, and TuNAS utilise a set of architecture parameters to enable a learnable distribution of the number of channels (#channels), while they have different mechanisms to optimise these architecture parameters. Here we briefly compare their mechanisms that are designed to search for #channels:</p><p>(1) TAS applies the channel-wise interpolation to aggregate feature tensors with different shapes with the architecture parameters <ref type="bibr" target="#b20">[21]</ref>. (2) FBNetV2 utilises a masking mechanism to represent different candidate #channels and aggregate the masks with the architecture parameters <ref type="bibr" target="#b43">[44]</ref>. (3) TuNAS samples masks based on the learnable distribution <ref type="bibr" target="#b48">[49]</ref>. TAS and FBNetV2 optimize the architecture parameters in a differentiable way, and TuNAS uses REINFORCE. As shown in Figure <ref type="figure" target="#fig_13">7b</ref>, TAS can quickly find much better model than  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Weight-sharing vs. Multi-trial based Methods</head><p>The weight-sharing based methods and multi-trial based methods have their unique advantages and disadvantages. Multi-trial based methods can theoretically find the best architecture as long as the proxy task is accurate, and the number of trials is large enough. However, their prohibitive computational cost has motivated researchers to design efficient weight-sharing based algorithms. However, sharing weights sacrifices the accuracy of each architecture candidate. As the search space increases, the shared weights are usually not able to distinguish the performance of different candidates.</p><p>Clarification. We have tried our best to implement each method using their reported best experimental set ups.</p><p>3. Since the original hyper-parameters of FBNetV2, TAS, and TuNAS are chosen based on a different setting than our benchmark, they may be sub-optimal for the small-scale datasets used in NATS-Bench. To make them align with our experimental settings, we borrow the hyper-parameters of Adam from DARTS and GDAS for these three algorithms.</p><p>However, please be aware that some algorithms might still result in sub-optimal performance since their hyperparameters might not be optimal for our NATS-Bench. We empirically found that some NAS algorithms are sensitive to some hyper-parameters, and we have tried to compare them in as fair a way as possible. If researchers can provide better results with different hyper-parameters, we are happy to update the benchmarks according to the new experimental results. We also welcome more NAS algorithms to be tested on our dataset and would be happy to include them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>How to avoid over-fitting on NATS-Bench? Our NATS-Bench provides a benchmark for NAS algorithms, aiming to provide a fair and computationally cost-friendly environment to the NAS community. The trained architecture and the easy-to-access performance of each architecture might provide some insidious ways for designing algorithms to over-fit the best architecture in our NATS-Bench. Thus, we propose some rules to follow in order to achieve the original intention of NATS-Bench, a fair and efficient benchmark.</p><p>1. No regularization for a specific operation. Since the best architecture is known in our benchmark, specific designs (b) Results of weight-sharing based methods in the size search space Ss. We do not add any #FLOPs or #parameters constraint for these searching methods. For these algorithms, we only evaluate the sub-module that is specifically designed to search for the number of channels. to fit the structural attributes of the best performing architecture constitute one of the insidious ways to fit our NATS-Bench. For example, as mentioned in Section 5, we found that the best architecture with the same number of parameters for CIFAR10 on NATS-Bench is ResNet. Restrictions on the number of residual connections is a way to over-fit the CIFAR10 benchmark. While this can give a good result on this benchmark, the searching algorithm might not generalize to other benchmarks.</p><p>2. Use the same meta hyper-parameter for different datasets and search spaces in NATS-Bench. The searching algorithm has some meta hyper-parameter that controls the behaviour of search. For example, the temperature τ in GDAS or the band width factor in BOHB. Using the same meta hyperparameter could evaluate the robustness of the searching algorithm and prevent it from over-fitting to a specific dataset.</p><p>3. Use the provided performance. The training strategy affects the performance of the architecture. We suggest to stick to the performance provided in our benchmark even if it is feasible to use other H to get a better performance. This provides a fair comparison with other algorithms.</p><p>4. Report results of multiple searching runs. Since our benchmark can help to largely decrease the computational cost for a number of algorithms, multiple searching runs, which give stable results of the searching algorithm with acceptable time cost, are strongly recommended.</p><p>Limitation with regard to hyper-parameter optimization (HPO). The performance of an architecture depends on the hyper-parameters H for its training and the optimal configuration of H may vary for different architectures. In NATS-Bench, we use the same configuration for all architectures, which may bring biases to the performance of some architectures. One related solution is HPO, which aims to search for the optimal hyper-parameter configuration. However, searching for the optimal hyper-parameter configurations and the architecture in one shot is too computationally expensive and still is an open problem <ref type="bibr" target="#b49">[50]</ref>.</p><p>Potential extension of NATS-Bench. Despite the straightforward extension by introducing HPO into NATS-Bench, there are some other interesting directions. One tendency in NAS is the cost constrained searching. For example, how to design a FLOPs constrain loss to regularize the discovered architecture to be efficient <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>?</p><p>Since the latency and FLOPs information are off-the-shelf in NATS-Bench, our NATS-Bench can also be used to benchmark NAS algorithms using different kinds of cost loss.</p><p>Potential designs using diagnostic information in NATS-Bench. As pointed in Section 3.4, different kinds of diagnostic information are provided. We hope that more insights about NAS could be found by analyzing these diagnostic information and further motivate potential solutions for NAS. For example, parameter sharing <ref type="bibr" target="#b4">[5]</ref> is the crucial technique to improve searching efficiency, but shared parameter would sacrifice the accuracy of each architecture. Could we find a better way to share parameters of each architecture from the learned thousands of models' parameters? Could we design new algorithms to take the mutual benefits of both multi-trial and weight sharing based methods?</p><p>Generalization ability of the search space. It is important to test the generalization capability of the empirical observations on this dataset. One possible strategy is to do all benchmark experiments on a much larger search space. Unfortunately, it is prohibitive regarding the expensive computational cost. We bring some results from <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b19">[20]</ref> to provide some preliminary evidence of generalization. In Figure <ref type="figure" target="#fig_11">6</ref>, we show the rankings of RANDOM, REA, and REINFORCE is (REA ≥ REINFORCE ≥ RANDOM). This is consistent with results in NAS-Bench-101, which contains more architecture candidates. For NAS methods with parameter sharing, we find that GDAS ≥ DARTS ≥ ENAS, which is also consistent with results in NAS-Bench-1SHOT1. Therefore, though it is not guaranteed, observations from our NATS-Bench have a potential to generalize to other search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX APPLICATION PROGRAMMING INTERFACE (API)</head><p>Users can easily query all information of an architecture by using our API, such as latency, training time, number of parameters, validation accuracy, etc. In this section, we show some example codes to query them. # Query r e s u l t s o f t h e 284− th a r c h i t e c t u r e on # CIFAR−100 when t r a i n i n g with 12 epochs . # The ' data ' i s a d i c t , where t h e key i s t h e random # seed and t h e value i s t h e corresponding r e s u l t . data = nats bench . query by index ( a r c h i n d e x =284 , d a t a s e t = ' c i f a r 1 0 0 ' , hp= ' 12 ' ) # &gt;&gt; [ 7 7 7 , 8 8 8 , 9 9 9 ] p r i n t ( data . keys ( ) ) # Show t h e v a l i d a t i o n performance using t h e random # seed o f 888 f o r t h e 284− th a r c h i t e c t u r e i n f o = r e s u l t s <ref type="bibr">[ 8 8 8 ]</ref> . g e t e v a l ( ' v a l i d ' )</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Middle: the macro skeleton of each architecture candidate. Top: The size search space S s in NATS-Bench. In S s , each candidate architecture has different configuration for the channel size. Bottom: The topology search space S t in NATS-Bench. In S t , each candidate architecture has different cell topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8 5 =</head><label>5</label><figDesc>32768 architectures. Each architecture in S t and S s is trained multiple times on three different datasets. The training log and performance of each architecture are provided for each run. The training accuracy/test accuracy/training loss/test loss after every training epoch for each architecture plus the number of parameters and floating point operations (FLOPs) are accessible. NATS-Bench has shown its value in the field of NAS research. (1) It provides the first benchmark to study the architecture size. (2) It provides a unified benchmark for most up-to-date NAS algorithms including all cell-based NAS methods. With NATS-Bench, researchers can focus on designing robust searching algorithm while avoiding tedious hyper-parameter tuning of the searched architecture. Thus, NATS-Bench provides a relatively fair benchmark for the comparison of different NAS algorithms. (3) It provides the full training log of each architecture. Unnecessary repet-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The relative ranking for the topology search space St. (b) The relative ranking for the size search space Ss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The ranking of each architecture on three datasets, sorted by the ranking in CIFAR-10.</figDesc><graphic url="image-2.png" coords="5,312.00,209.62,252.00,152.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Results of all architecture candidate in the topology search space St on the CIFAR-10 dataset. (b) Results of all architecture candidate in the topology search space St on the CIFAR-100 dataset. (c) Results of all architecture candidate in the topology search space St on the ImageNet-16-120 dataset. (d) Results of all architecture candidate in the size search space Ss on the CIFAR-10 dataset. (e) Results of all architecture candidate in the size search space Ss on the CIFAR-100 dataset. (f) Results of all architecture candidate in the size search space Ss on the ImageNet-16-120 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The training and test accuracy vs. the number of parameters and FLOPs for each architecture candidate.</figDesc><graphic url="image-8.png" coords="6,48.00,593.88,516.00,95.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) The relative ranking for the topology search space St. (b) The relative ranking for the size search space Ss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: The correlation between the validation accuracy and the test accuracy for all architecture candidates in S t and S s .</figDesc><graphic url="image-10.png" coords="7,48.00,146.26,252.00,87.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) The correlation coefficient for the topology search space St. (b) The correlation coefficient for the size search space Ss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: We report the correlation coefficient between the accuracy on 6 sets, i.e., CIFAR-10 validation set (C10-V), CIFAR-10 test set (C10-T), CIFAR-100 validation set (C100-V), CIFAR-100 test set (C100-T), ImageNet-16-120 validation set (I120-V), ImageNet-16-120 test set (I120-T).</figDesc><graphic url="image-12.png" coords="7,312.00,180.88,251.99,122.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Results of NAS algorithms without weight sharing in the topology search space St. (b) Results of NAS algorithms without weight sharing in the size search space Ss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: The test accuracy of the searched architecture candidate over time. We run different searching algorithms 500 times on three datasets. We plot the test accuracy of their searched model at each timestamp for the corresponding dataset. This test accuracy is evaluated after fully training the model on the corresponding dataset and averaged over 500 runs.</figDesc><graphic url="image-14.png" coords="9,48.00,218.04,516.02,158.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Results of weight-sharing based methods in the topology search space St.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The test accuracy of the searched architecture candidate after each search epoch. We run different searching algorithms 3 times on three datasets. We plot the test accuracy of their searched model after each search epoch for the corresponding dataset. This test accuracy is evaluated after fully training the model on the corresponding dataset and averaged over 3 runs.</figDesc><graphic url="image-16.png" coords="10,48.00,219.26,516.02,158.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Listing 1 :Listing 4 :Listing 5 :</head><label>145</label><figDesc>Create an instance of our benchmark. from nats bench import c r e a t e # Load t h e data o f t h e topology s e a r c h space nats bench = c r e a t e ( s e a r c h s p a c e = ' topology ' ) # Load t h e data o f t h e s i z e s e a r c h space nats bench = c r e a t e ( s e a r c h s p a c e = ' s i z e ' ) Listing 2: Show the structure of each architecture. amount = l e n ( nats bench ) f o r i , a r c h s t r i n enumerate ( nats bench ) : p r i n t ( ' { : } / { : } : { : } ' . format ( i , amount , a r c h s t r ) )Listing 3: Query the data of 115 th architecture when training with 90 epochs (H 1 ); find the architecture with the highest accuracy on the validation set of CIFAR-100. i n f o = nats bench . query meta info by index ( arch index =115 , hp= ' 90 ' ) index , a c c u r a c y = nats bench . f i n d b e s t ( d a t a s e t = ' c i f a r 1 0 0 ' , m e t r i c o n s e t = ' v a l i d ' ) Query the configuration of 12 th architecture, its cost information, and its performance on different datasets. c o n f i g = nats bench . g e t n e t c o n f i g ( a r c h i n d e x =12 , d a t a s e t = ' c i f a r 1 0 ' ) i n f o = nats bench . g e t c o s t i n f o ( a r c h i n d e x =12 , d a t a s e t = ' c i f a r 1 0 ' ) # The i n f o i s a d i c t , where key i s t r a i n −l o s s , # t r a i n −accuracy , t r a i n −a l l −time , t e s t −l o s s , e t c . # The corresponding value i s i n f o [ key ] . i n f o = nats bench . get more info ( a r c h i n d e x =12 , d a t a s e t = ' c i f a r 1 0 ' ) i n f o = nats bench . get more info ( a r c h i n d e x =12 , d a t a s e t = ' c i f a r 1 0 0 ' ) i n f o = nats bench . get more info ( a r c h i n d e x =12 , d a t a s e t = ' ImageNet16 −120 ' ) More advanced features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>The training hyper-parameters H 0 for all candidate architectures in S s and S t . It is a standard image classification dataset and consists of 60K 32×32 colour images in 10 classes. The original training set contains 50K images, with 5K images per class. The original test set contains 10K images, with 1K images per class. Due to the need of validation set, we split all 50K training images in CIFAR-10 into two groups. Each group contains 25K images with 10 classes. We regard the first group as the new training set and the second group as the validation set. CIFAR-100: This dataset is just like CIFAR-10. It has the same images as CIFAR-10 but categorizes each image into 100 fine-grained classes. The original training set on CIFAR-100 has 50K images, and the original test set has 10K images. We randomly split the original test set into two groups of equal size -5K images per group. One group is regarded as the validation set, and another one is regarded as the new test set.</figDesc><table><row><cell></cell><cell>optimizer Nesterov</cell><cell>learning rate (LR)</cell><cell cols="5">momentum weight decay batch size norm random flip random crop epoch</cell></row><row><cell>value</cell><cell>SGD</cell><cell>cosine decay LR from 0.1 to 0</cell><cell>0.9</cell><cell>0.0005</cell><cell>256</cell><cell>p=0.5</cell><cell>12</cell></row><row><cell cols="4">signals to update the searching algorithm. The test set is</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to evaluate the performance of each searching algorithm</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">by comparing the indicators (e.g., accuracy, #parameters,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">speed) of their selected architectures. Previous methods use</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">different splitting strategies, which may result in various</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">searching costs and unfair comparisons. We hope to use the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">proposed splits to unify the training, validation and test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sets for a fairer comparison.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CIFAR-10:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>The utility of our NATS-Bench for different NAS algorithms. We show whether a NAS algorithm can use our NATS-Bench to accelerate the searching and evaluation procedure.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
				<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
				<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
				<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
				<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">737</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via selfevaluated template network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
				<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tabular benchmarks for joint architecture and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04970</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nas-bench-1shot1: Benchmarking and dissecting one shot neural architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
				<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="760" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conf. on Uncertainty in Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1803" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AMC: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BOHB: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
				<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations Workshop</title>
				<meeting>Int. Conf. Learn. Representations Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2787" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FBNetV2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">974</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of Machine Learning Research (JMLR)</title>
				<imprint>
			<date type="published" when="2012-02">Feb. 2012</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? an investigation with tunas</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">332</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">AutoHAS: Differentiable hyper-parameter and architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03656</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
