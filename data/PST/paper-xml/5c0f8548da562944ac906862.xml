<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Video Object Segmentation with Super-Trajectories</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Semi-Supervised Video Object Segmentation with Super-Trajectories</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B88A90D9068CE1BD30196DE1FC1B3773</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2819173</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2819173, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video segmentation</term>
					<term>trajectory extraction</term>
					<term>density peaks clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a semi-supervised video segmentation approach based on an efficient video representation, called as "super-trajectory". A super-trajectory corresponds to a group of compact point trajectories that exhibit consistent motion patterns, similar appearances, and close spatiotemporal relationships. We generate the compact trajectories using a probabilistic model, which enables handling of occlusions and drifts effectively. To reliably group point trajectories, we adopt a modified version of the density peaks based clustering algorithm that allows capturing rich spatiotemporal relations among trajectories in the clustering process. We incorporate two intuitive mechanisms for segmentation, called as reverse-tracking and object re-occurrence, for robustness and boosting the performance. Building on the proposed video representation, our segmentation method is discriminative enough to accurately propagate the initial annotations in the first frame onto the remaining frames. Our extensive experimental analyses on three challenging benchmarks demonstrate that, given the annotation in the first frame, our method is capable of extracting the target objects from complex backgrounds, and even reidentifying them after prolonged occlusions, producing high-quality video object segments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S Emi-supervised video segmentation refers to the par- titioning of objects in a given video sequence with available annotations in the first frame. A pixel-accurate, spatiotemporal bipartition of the video is an essential building block for a wide spectrum of applications, such as action recognition <ref type="bibr" target="#b0">[1]</ref>, object tracking <ref type="bibr" target="#b1">[2]</ref>, semantic labeling <ref type="bibr" target="#b2">[3]</ref>, to name a few. Semi-supervised techniques also provide proper initializations for further video editing and analysis tasks (e.g., interactive video cutout, dataset annotation) since they allow a tradeoff between accuracy and human interaction.</p><p>Aiming for this task, we incorporate a comprehensive video representation, super-trajectory, to capture the underlying spatiotemporal structure information that is intrinsic to real-world scenes. Each super-trajectory is composed of a group of trajectories that are similar in nature and have common characteristics. A point trajectory, e.g., the tracked positions of an individual point across multiple frames, is a constituent of the super-trajectory. This representation portrays several properties of a video:</p><p>• Long-term motion information is explicitly modeled as it consists of trajectories over extended periods;</p><p>• Spatiotemporal location information is implicitly interpreted by clustering nearby trajectories; and</p><p>• Compact features, such as color and motion pattern, are described in a conveniently compact form.</p><p>With above convenient qualities, super-trajectory simplifies and reduces the complexity of propagating labels in the • W. Wang and J. Shen are with Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, (email: wenguanwang@bit.edu.cn, shenjianbing@ucla.edu) • F. Porikli is with the Research School of Engineering, the Australian National University. (email: fatih.porikli@anu.edu.au) • R. Yang is with the University of Kentucky, Lexington, KY 40507. (email: ryang@cs.uky.edu) Fig. <ref type="figure">1</ref>. Our video segmentation method takes the first frame annotation as initialization (left). Leveraging on super-trajectories, the segmentation process achieves superior results even for challenging scenarios including heavy occlusions, complex appearance variations, and large shape deformations (middle, right).</p><p>segmentation process. We first generate point trajectories based on a probabilistic model, which handles occlusions and drifts naturally. Then, we apply the density peaks based clustering (DPC) algorithm <ref type="bibr" target="#b3">[4]</ref> that is modified to attain a proper split of videos in space and time by grouping these trajectories.</p><p>Our approach to the design of the super-trajectory is inspired by the following two motivations. Firstly, for the task of video segmentation, it is desirable to have a powerful abstraction of videos that is robust to spatiotemporal structure variations and deformations. As demonstrated in the recently released DAVIS dataset <ref type="bibr" target="#b4">[5]</ref>, most of the previous heuristic approaches exhibit serious limitations for cases with occlusions, motion blur and appearance changes. The proposed super-trajectory, on the other hand, is able to encode video competently to handle these challenges (see Fig. <ref type="figure">1</ref>).</p><p>Secondly, from the perspective of feature generation, merging and splitting video segments (through the corresponding point trajectories) into atomic spatiotemporal components is essential for handling occlusions and temporal discontinuities. However, it is known that the classical clustering methods, e.g., k-means and spectral clustering, which are widely adopted by the existing trajectory methods, cannot reach a consensus on the definition of a cluster. To address this, we modify the DPC algorithm for grouping the point trajectories, leveraging on its traits of choosing cluster centers based on a more suitable criterion.</p><p>We also introduce a reverse-tracking strategy by excluding objects that originate outside the frame to eliminate the adverse effects of camera motion. To reidentify objects after occlusions, we exploit object re-occurrence information, which reflects the spatiotemporal relations between objects across the entire video sequence.</p><p>To summarize, our method has the following contributions:</p><p>• A semi-supervised video segmentation algorithm based on super-trajectories that capture spatiotemporal relations among point trajectories (Sections 3 and 4).</p><p>• A novel super-trajectory generation method based on a modified version of the DPC algorithm to reliably determine cluster centers that represent spatiotemporal structure variations (Section 3.2).</p><p>• A reverse-tracking strategy for identifying objects with long durations of propagation, and an object re-occurrence scheme for recovering objects after occlusion (Sections 4.2 and 4.3).</p><p>We evaluate our method on three publicly acceptable datasets, namely DAVIS <ref type="bibr" target="#b4">[5]</ref>, Youtube-Object <ref type="bibr" target="#b5">[6]</ref> and SegTrack-V2 <ref type="bibr" target="#b6">[7]</ref> benchmarks and compare with the state-ofthe-art both qualitatively and quantitatively. Furthermore, to gain a complete and comprehensive understanding of its various aspects, we implement three variants of our method and conduct multiple ablative studies. We also run two groups of experiments to assess the impacts of its different components. We observe that our method compares favorably with previous state-of-the-art semi-supervised methods that do not use deep-learning. Compared the deeplearning based methods, our approach has the advantage of not requiring any training dataset.</p><p>This paper builds upon our conference paper <ref type="bibr" target="#b7">[8]</ref> and significantly extends it with in-depth discussions on the algorithm providing more details of the formulation, its implementation, and its multiple variants. It dives deeper into the two important assumptions for video segmentation, reverse-tracking and object re-occurrence, and quantitatively demonstrates their effectiveness. It also offers a more inclusive and insightful overview of the recent work of video segmentation and trajectory extraction. Last but not least, it reports extensive experimental results with an additional large-scale dataset, Youtube-Object <ref type="bibr" target="#b5">[6]</ref> for further validation.</p><p>The remainder of the paper is organized as follows. An overview of the related work is presented in Section 2. The proposed super-trajectory is described in detail in Section 3. The novel video segmentation method is explained in Sec-tion 4 followed by the experimental analyses on robustness, effectiveness, and efficiency in Section 5. Finally, we draw conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We provide a brief overview of recent works in two relevant fields: video segmentation and point trajectory extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Segmentation</head><p>According to the level of supervision required, we first broadly categorize the conventional video segmentation techniques into unsupervised, semi-supervised and supervised methods. Then, we specifically discuss the recently proposed deep-learning based approaches, due to their astonishing performance improvement.</p><p>Unsupervised algorithms ordinarily aim for oversegmentation, including solutions for hierarchical segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, temporal superpixel <ref type="bibr" target="#b10">[11]</ref>, and super-voxels <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The key assumption behind these methods is to group pixels that have consistent appearance and motion properties since other types of prior knowledge on image content and object type are absent. Similarly, motion segmentation approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> extract moving object regions from the scene background presuming the motion information is a reliable indicator of the objects. For instance, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> are specifically based on the analysis of long-term motion information, represented as trajectories, posing the segmentation task as a trajectory clustering problem. More recent works, e.g. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, propose automatic motion segmentation for foreground-background separation. While they do not require manual annotations, they rely on restrictive constraints on the application scenario. For identifying objects, many techniques employ saliency and objectness cues, which are bootstrapped from research efforts in salient object detection and generic object proposals. To this end, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> introduce saliency information as prior knowledge to infer the object, and <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> generate object segments via ranking hundreds of object candidates <ref type="bibr" target="#b26">[27]</ref>. Object proposal based approaches are usually time-consuming due to the high computational load of generating object hypotheses and complicated ranking processes. As stated in <ref type="bibr" target="#b4">[5]</ref>, unsupervised methods are well-suited for parsing large-scale databases, but they fail in case of their underlying assumptions do not hold.</p><p>Semi-supervised methods propagate the given labels in one or more key-frames to the entire video sequence <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. They are also referred as label propagation. As argued earlier, unsupervised approaches are bound by their underlying assumptions, therefore incorporating human provided annotations is considered a reliable solution for object segmentation. Semi-supervised video segmentation methods often rely on optical flow <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and share similar spirit with video tracking <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Among many variants, these methods use flow-based random field propagation models <ref type="bibr" target="#b35">[36]</ref>, patch-seam based propagation strategies <ref type="bibr" target="#b36">[37]</ref>, energy optimizations over graph models <ref type="bibr" target="#b37">[38]</ref>, joint segmentation and detection frameworks <ref type="bibr" target="#b38">[39]</ref>, and pixel segmentation on bilateral spaces <ref type="bibr" target="#b39">[40]</ref>. Semi-supervised approaches, compared to their unsupervised competitors, are more practical and would provide more accurate partitions. At the same time, they may suffer from drift issues during the propagation process.</p><p>Supervised methods <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> require tedious user interaction and iterative human corrections. They are often referred as interactive video segmentation or video cut-out in computer graphics. In general, supervised approaches can attain high-quality boundaries even though they demand extensive and time-consuming human supervision. Compared with unsupervised or semi-supervised methods, supervised approaches are capable of producing more accurate partitions. However, the labor-intensive process is unfeasible at large scale. Thus, supervised methods are more suitable for specific scenarios such as video post-production.</p><p>Deep Learning based video segmentation methods have become popular in recent years following the success of deep learning in many computer vision applications. The work of Fragkiadaki et al. <ref type="bibr" target="#b23">[24]</ref> can be viewed as an early attempt to introduce deep learning into video segmentation. In this work, a CNN based Moving Objectness Detector is trained on image and motion fields for detecting moving objects. Another option to the use deep learning is the shallow combination with the features of the pretrained neural networks, such as <ref type="bibr" target="#b33">[34]</ref>. Very recently, efforts have been devoted to explore a more integrated use of neural networks in video segmentation. These deep learning based approaches can be also classified as either unsupervised <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> or semi-supervised <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> architectures. Thanks to the strong learning capability of neural networks, deep learning based video segmentation methods achieved higher performance over the traditional heuristic methods.</p><p>In this paper, we introduce a model for semi-supervised video segmentation based on the super-trajectory concept, which is a compact and convenient abstraction of trajectories. The success of the proposed segmentation method demonstrates the potential of super-trajectories for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Point Trajectory</head><p>Point trajectories are generated through tracking points over multiple frames. They have the advantage of representing long-term motion information. Historically, Kanade-Lucas-Tomasi (KLT) <ref type="bibr" target="#b56">[57]</ref> is one of the earlier attempts to track a small set of feature points. Inspiring several follow-up studies in video segmentation and action recognition, optical flow based dense trajectories <ref type="bibr" target="#b57">[58]</ref> improve over sparse interest point tracking. Some representative studies <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> address the problem of motion segmentation of all moving objects in video, in contrast to traditional unsupervised methods that aim to extract a single primary object. These trajectory segmentation methods usually track points via dense optical flow and perform segmentation via clustering trajectories. The trajectories are directly grouped into a few clusters as object segments, using spectral clustering <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, energy-based clustering <ref type="bibr" target="#b59">[60]</ref>, or minimum cost multi-cut <ref type="bibr" target="#b64">[65]</ref>. The final partitioning is obtained via employing graphcuts on the rough clustering results, usually assuming the number of objects with different motion patterns is known.</p><p>Trajectories are also used for action recognition <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, where the trajectory aligned descriptor acts as a powerful representation for describing actions in video sequences. Wang et al. <ref type="bibr" target="#b65">[66]</ref> first used trajectory-aligned descriptors for action classification and demonstrated the state-of-the-art performance. Later, an improved version <ref type="bibr" target="#b66">[67]</ref> was developed for removing noisy trajectories from the camera motion. With the success of deep learning in computer vision tasks, Wang et al. <ref type="bibr" target="#b67">[68]</ref> encoded deep-learned features into trajectories, reporting improved performance. Generally, trajectory based action recognition methods <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> put more focus on the combination of trajectory and other descriptors (e.g., HOG) due to the pursuit of describing whole video sequences and capturing most informative parts for classifying the whole action sequence. Thus, the computation of trajectories is relatively straightforward and fast, and the trajectories have some uniform and specified lengths. However, motion segmentation models place more emphasis on the point tracking accuracy and clustering performance as they aim for generating per-frame pixel-wise segmentation. Therefore, the trajectories are usually computed using more accurate but more time-consuming optical flow, inferred via more complicated strategies and with varying lengths.</p><p>Existing approaches mostly handle trajectories in pairs or individually, and directly group all trajectories into a few clusters (as segments), ignoring the inner coherence in a group of similar trajectories. Instead, we go one step beyond the conventional trajectory methods by putting trajectories in operation as united super-trajectory groups instead of individual entities. The proposed super-trajectory idea inherits the representability of trajectory for modeling long-term motion information, while further exploiting spatiotemporal relations among trajectories, thus offering compact and atomic video representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUPER-TRAJECTORY</head><p>We first introduce the super-trajectory in this section and then describe our segmentation approach in Section 4. In Section 3.1, we present our trajectory generation method based on a probabilistic model. Then, in Section 3.2, we introduce our super-trajectory generation method using density peaks based clustering (DPC) algorithm <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trajectory Generation</head><p>Given a sequence of video frames</p><formula xml:id="formula_0">I 1:T = {I 1 , • • •, I T } within time range [1, T ],</formula><p>each pixel point can be tracked to the next frame using optical flow. This tracking process can be executed frame-by-frame until some termination conditions (e.g., occlusion, incorrect motion estimates, etc.) are reached. The tracked points are composed into a trajectory and a new tracker is initialized where prior tracker finished. Contrast to previous trajectory methods that design many hard thresholds <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b14">[15]</ref> for detecting occlusion, or unreliable motion estimates, we build our trajectory generation on a more interpretable and reasonable probabilistic model.</p><p>Let w denote a flow field indexed by pixel positions that returns a 2D flow vector at a given point. Using LDOF <ref type="bibr" target="#b68">[69]</ref>, we compute forward-flow field w t from frame I t to I t+1 , and the backward-flow field ŵt from I t to I t-1 . We track pixel potion x = (x, y, t) to the consecutive frames in both directions. The tracked points of consecutive frames are concatenated to form a trajectory τ :</p><formula xml:id="formula_1">τ = {x n } L n=1 = {(x n , y n , t n )} L n=1 , t n ∈ [1, T ],<label>(1)</label></formula><p>where L indicates the length of trajectory τ and point x n = (x n , y n , t n ) is tracked via:</p><formula xml:id="formula_2">(x n , y n ) = (x n-1 , y n-1 )+w tn-1 (x n-1 , y n-1 ).<label>(2)</label></formula><p>As the optical flow is subpixel accurate, x and y will usually end up between grid points. We use bilinear interpolation to infer the flow at these points.</p><p>We model point tracking process as a first order Markovian process, and denote the probability that n-th point x n of trajectory τ is correctly tracked from frame I t1 as p(x n |I t1:tn ). The prediction model is defined by:</p><formula xml:id="formula_3">p(x n |I t1:tn ) = p(x n |x n-1 , I tn )p(x n-1 |I t1:tn-1 ),<label>(3)</label></formula><p>where p(x 1 |I t1 ) = 1 and p(x n |x n-1 , I tn ) is formulated as:</p><formula xml:id="formula_4">p(x n |x n-1 , I tn ) ∝ exp{-(E app + E occ )}. (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The energy functions E penalize various potential tracking error. The former energy E app is expressed as:</p><formula xml:id="formula_6">E app (x n , x n-1 ) = ||I tn (x n , y n ) -I tn-1 (x n-1 , y n-1 )||,<label>(5)</label></formula><p>which penalizes the appearance variations between corresponding points. Obvious, a tracked point would be consistent in appearance over time. The latter energy E occ is included to penalize occlusions. It uses the consistency of the forward and backward flows:</p><formula xml:id="formula_7">E occ (x n , x n-1 ) = || ŵtn (x n , y n )+w tn-1 (x n-1 , y n-1 )|| || ŵtn (x n , y n )||+||w tn-1 (x n-1 , y n-1 )|| .<label>(6)</label></formula><p>Ideally, when a point is successfully tracked without occlusion, we expect an one-to-one correlation between corresponding points: x n and x n-1 . Thus the backward flow vector ŵtn (x n , y n ) should be opposite in direction of the forward flow vector w tn (x n-1 , y n-1 ): ŵtn (x n , y n ) = -w tn (x n-1 , y n-1 ), which makes the numerator close to 0. When this consistency constraint is violated, occlusions or unreliable optical flow estimates might occur.</p><p>It is important to notice that the proposed tracking model performs accurately yet our model is not limited to the above constraints. We terminate the tracking process when p(x n |I t1:tn ) &lt; 0.5, and then we start a new tracker at x n . In our implementation, we discard the trajectories shorter than four frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Super-Trajectory Generation</head><p>Previous studies indicate the value of trajectory based representations for long-term motion information. Here, our intuition is that neighboring trajectories exhibit compact spatiotemporal relationships and they have similar characteristics in appearance and motion patterns. This motives us operating on trajectories as united groups.</p><p>We generate super-trajectory by clustering trajectories adopting the recently proposed density peaks based clustering (DPC) <ref type="bibr" target="#b3">[4]</ref>. Before introducing our super-trajectory generation method, we first describe DPC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Density Peaks based Clustering (DPC)</head><p>DPC clusters the data by finding its density peaks. It provides a unique solution of fast clustering based on the idea that cluster centers are characterized by a higher density than their neighbors and by a relatively large distance from points with higher densities.</p><p>Given the distances d ij between data points, for each data point i, DPC calculates two quantities: local density ρ i and its distance δ i from points of higher density. The local density ρ i of data point i is defined as <ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_8">ρ i = j d ij .<label>(7)</label></formula><p>Here, δ i is measured by computing the minimum distance between the point i and any other point with higher density:</p><formula xml:id="formula_9">δ i = min j:ρj &gt;ρi (d ij ).<label>(8)</label></formula><p>For the point with highest density, it takes</p><formula xml:id="formula_10">δ i = max j (d ij ).</formula><p>Since δ i is much larger than the typical nearest neighbor distance only for points that are local or global maxima in the density, cluster centers are recognized as points for which the value of δ i is anomalously large.</p><p>Cluster centers are the points with high local density (ρ ↑) and large distance (δ ↑) from other points with higher local density. This core observation is illustrated by the simple example in Fig. <ref type="figure" target="#fig_0">2 (a)</ref>, where the bigger circle indicates higher local density ρ and two clusters, centered as point1 and point3, are colored blue and red, respectively. We can find that cluster centers are surrounded by neighbors with lower local density and they are at a relatively large distance from any points with a higher local density. point2 also has a relatively large local density. However, since it is closer to point1 of higher density, it has less distance δ. point3 is more favored to be a cluster center due to both high local density and large distance. The data points can be ranked by:</p><formula xml:id="formula_11">γ i = ρ i δ i ,<label>(9)</label></formula><p>where the cluster centers are recognized as points for which the values of ρ i and δ i are both large. Then the top ranking points are selected as centers. After successfully declaring cluster centers, each remaining data points is assigned to the cluster center as its nearest neighbor of higher density. This cluster assignment is performed in a single step, in contrast with other clustering algorithms (e.g., k-means) which iteratively update cluster centers with certain objective function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Grouping Trajectories via DPC</head><p>Given a trajectory τ : {(x n , y n , t n )} n spans L frames, we define three features: spatial location (l τ ), color (c τ ), and velocity (v τ ), for describing τ :</p><formula xml:id="formula_12">l τ = 1 L L n=1 (x n , y n ), c τ = 1 L L n=1 I tn (x n , y n ), v τ = 1 L L n=1 1 ∆t (x n+∆t -x n , y n+∆t -y n ) ,<label>(10)</label></formula><p>where we set ∆t = 3. We tested ∆t = {5, 7, 9} and did not observe any obvious effect on the results. Between each pair of trajectories τ i and τ j that share some frames, we define their distance d ij via measuring descriptor similarity:</p><formula xml:id="formula_13">d ij = f ∈{l,c,v} ||f τi -f τj ||. (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>We normalize color distance on max intensity, location distance on sampling step R (detailed below), motion distance on the mean motion magnitude of all the trajectories, which makes above distance measures to have similar scales. In case there is no temporal overlap, we set d ij = H, where H has a very large value. We first roughly partition trajectories into several nonoverlap clusters, and then iteratively update each partition to get the optimized trajectory clusters. The only parameter of our super-trajectory algorithm is number of spatial grids K, as the degree of spatial subdivision. The spatial sampling step becomes R = S/K, where S refers to the product of Then we need to find a proper cluster number of each trajectory group, thereby further offering a reasonable temporal split of video, and generating more accurate spatiotemporal clustering.</p><p>For each trajectory group, we initially estimate the cluster number as C = T /L, where L indicates the average length of all trajectories. Then we apply a modified DPC algorithm for generating trajectory clusters, as described in Alg. 1. In Alg. 1-3, in case δ i = H the trajectory τ i does not Select {τ i } i as cluster centers; 6: else</p><formula xml:id="formula_15">7: Compute {γ i } i via γ i = ρ i δ i ; 8:</formula><p>Select the trajectories with C highest γ values as cluster centers; 9: end if 10: Assign each remaining trajectories to cluster center as its nearest neighbor of higher density ρ.</p><p>have any temporal overlap with those trajectories that have higher local densities. This means the trajectory τ i is the center of an isolated group. If |{τ i } i | &gt; C in Alg. 1-4, there exist more than C unconnected trajectory groups. Accordingly, we select the trajectories with the highest densities of those unconnected trajectory groups as the centers (Alg. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Otherwise, as in Alg. 1-7 and 8, the trajectories with the C highest γ values are selected as the cluster centers. The whole initialization process is described in Alg. 2-1,2,3.</p><p>With above initialization process, trajectories are grouped according to their spatiotemporal relationships and similarities (see Fig. <ref type="figure">4 (b)</ref>). Next, in Alg. 2, we iteratively refine our super-trajectory assignments. In this process, each trajectory is classified into the nearest cluster center (Alg. 2-10). For reducing the searching space, we only search the trajectories fall into a 2R×2R×T space-time volume around the cluster center τ j (Alg. 2-7). This results in a signifi- cant speed advantage by limiting the size of search space to reduce the number of distance calculations. Once each trajectory has been associated to the nearest cluster center, an update step adjusts the center of each trajectory cluster via Alg. 1 with C=1 (Alg. <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">15)</ref>. We drop very small trajectory clusters and combine those trajectories to other nearest trajectory clusters. In practice, we find 4∼5 iterations for above refining process are enough for obtaining satisfactory performance. Visualization results of super-trajectory generation with different iterations are presented in Fig. <ref type="figure">4</ref>.</p><p>With DPC in Alg. 1, we group all trajectories T = {τ i } i into m non-overlapping clusters, represented as super-trajectories X = {χ j } m j=1 , where χ j = {τ i |τ i is classified into j-th cluster via Alg. 2}. Note that, m (the number of super-trajectories) is varied at each iteration in Alg. Set label l i = -1 and distance κ i = H for each trajectory τ i ;</p><p>6:</p><p>for each trajectory cluster center τ j do 7:</p><p>for each trajectory τ j falls in a 2R ×2R ×T spacetime volume around τ j do 8:</p><p>Compute distance d jj between τ j and τ j via Eq. 11;</p><p>9:</p><p>if d jj &lt; κ j then 10:</p><p>Set κ j = d jj , l j = j ; Update {τ j } j for each cluster via Alg. 1. 16: end loop clusters. Additionally, m for different videos is different even with same input parameter K. That is important, since different videos have different temporal characteristics, thus we only constrain their spatial shape via K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SUPER-TRAJECTORY FOR SEGMENTATION</head><p>In Section 3, we cluster a set of compact trajectories into super-trajectory. In this section, we describe our video segmentation approach that leverages on super-trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Super-Trajectory based Propagation</head><p>Given the mask M of the first frame, we seek a binary partitioning of pixels into foreground and background classes. Clearly, the annotation can be propagated to the rest of the video, using the trajectories that start at the first frame. However, only a few of points can be successfully tracked across the whole scene, due to occlusion, drift or unreliable motion estimation. Benefiting from our efficient trajectory clustering approach, super-trajectories are able to spread more annotation information over longer periods. This inspires us to base our label propagation process on super-trajectory.</p><p>For inferring the foreground probability of supertrajectories X , we first divide all the trajectories T into three categories: foreground trajectories T f , background trajectories T b and unlabeled trajectories T u , where T = T f ∪ T b ∪ T u . The T f and T b are the trajectories which start at the first frame and are labeled by the annotation mask M, while the T u are the trajectories start at any frames except the first frame, thus cannot be labeled via M. Accordingly, super-trajectories X are classified into two categories: labeled ones X l and unlabeled ones X u . A labeled super-trajectory χ l j ∈ X l contains at least one labeled trajectory from T f or T b , and its foreground probability can be computed as the ratio between the included foreground trajectories and the labeled ones it contains:</p><formula xml:id="formula_16">p f (χ l j ) = |χ l j ∩ T f | |χ l j ∩ T f | + |χ l j ∩ T b | . (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>For the points belonging to the labeled super-trajectory χ l j , their foreground probabilities are set as p f (χ l j ). Then we build an appearance model for estimating the foreground probabilities of unlabeled pixels. The appearance model is built upon the labeled super-trajectories X l , consists of two weighted Gaussian Mixture Models over RGB color values, one for the foreground and one for the background. The foreground GMM is estimated form all labeled super-trajectories X l , weighted by their foreground probabilities {p f (χ l j )} j . The estimation of background GM-M is analogous, with the weight replaced by the background probabilities {1-p f (χ l j )} j . The foreground GMM is initialized with 3 components, while the background GMM has 5 components, following general settings. The appearance models leverage the foreground and background supertrajectories over many frames, instead of using only the first frame or labeled trajectories, therefore they can robustly estimate appearance information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reverse Tracking</head><p>Although above model successfully propagates more annotation information across the whole video sequence, it still suffers from some difficulties: the model will be confused when a new object comes into view (see Fig. <ref type="figure" target="#fig_6">5 (b</ref>)). To this, we propose to reversely track points for excluding new incoming objects. We compute the 'source' of unlabeled trajectory τ u i ∈ T u :</p><formula xml:id="formula_18">(x 0 , y 0 ) = (x 1 , y 1 ) -v τ u i ,<label>(13)</label></formula><p>where (x 1 , y 1 ) indicates starting position and v τ u i refers to velocity via Eq. 10. It is clear that, if the virtual position (x 0 , y 0 ) is out of image frame domain, trajectory τ u i is a latecomer. For those trajectories T o ⊂ T u start outside view, we treat them as background. Labeled super-trajectory χ l j ∈ X l is redefined as the one contains at least one trajectory from T f , T b or T o , and Eq. 12 is updated as</p><formula xml:id="formula_19">p f (χ l j ) = |χ l j ∩ T f | |χ l j ∩ T f | + |χ l j ∩ T b | + |χ l j ∩ T o | . (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>Those outside trajectories T o are also adopted for training appearance model in prior step. According to our experiment in Section 5.3, this assumption offers about 6% performance improvement. Foreground estimation results via our reverse tracking strategy are presented in Fig. <ref type="figure" target="#fig_6">5 (c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Backward Re-occurrence</head><p>Most video segmentation methods assume objects are consistent between successive frames. However, it is also very common that objects move into or leave view, which poses great challenge for existing approaches. Instead of constraining local object consistency, we pursue global consistency via exploring re-occurrence of objects across the whole scene. As suggested by <ref type="bibr" target="#b18">[19]</ref>, objects, or regions, often re-occur both in space and in time. Here, we build correspondences among re-occurring regions across distant frames and transport foreground estimates globally. This process is based on super-pixel level, since super-trajectories cannot cover all of pixels. Let {r i } i be the superpixel set of input video. For each region, we search its N Nearest Neighbors (NNs) as its re-occurring regions using KD-tree search. For region r i of frame I t , we only search its NNs in previous frames</p><formula xml:id="formula_21">{I 1 , • • •, I t }.</formula><p>Such backward search strategy is for biasing the segmentation results of prior frames as the propagation accuracy degrades over time. Following <ref type="bibr" target="#b18">[19]</ref>, each region r i is represented as a concatenation of several descriptors f ri : RGB and LAB color histograms (6 channels×20 bins), HOG descriptor (9 cells×6 orientation bins) computed over a 15 × 15 patch around superpixel center, and spatial coordinate of superpixel center. The spatial coordinate is with respect to image center and normalized into [0, 1], which implicitly incorporates spatial consistency in NN-search.</p><p>After NN-search in the feature space, we construct a weight matrix W for all the regions {r i } i :</p><formula xml:id="formula_22">W ij =      e -||fr i -fr j || if r j is one of NNs of r i 1 if i = j 0 otherwise (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>Then a probability transition matrix P is built via rowwise normalization of W . We define a column vector v that gathers all the foreground probabilities of {r i } i . The foreground probability of a superpixel is assigned as the average foreground probabilities of its pixels.</p><p>Then we iteratively update v via the probability transition matrix P . In each iteration k, we update v (k) via:</p><formula xml:id="formula_24">v (k) = P v (k-1) ,<label>(16)</label></formula><p>which equivalents to updating foreground probability of a region with the weighted average of its NNs. In each iteration, we keep the foreground probabilities of those points belonging to labeled trajectories unchanged. Then we recompute v (k) and update it in next iteration. In this way, the relatively accurate annotation information of the labeled trajectories is preserved. Additionally, the annotation information is progressively propagated in a forward way and the super-trajectories based foreground estimates are consistent even across many distant frames (see Fig. <ref type="figure" target="#fig_6">5  (d)</ref>). After 10 iterations, the pixels (regions) with foreground probabilities lager than 0.5 are classified as foreground, thus obtaining final binary segments. In Section 5.3, we test N = {4, 6, • • •, 20} and only observe ±0.3% performance variation. We set N = 8 for obtaining best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>The performance evaluation and analysis of the proposed approach are reported in this section. Two groups of experiments are conducted. First, our approach is compared with some of the state-of-the-art video segmentation approaches on three universally acceptable benchmarks. Second, a few important issues regarding our approach are discussed, such as dissecting various components and variants, and the running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Parameter Settings In Section 3.2, we set number of spatial grids K = 1200. In Section 4.3, we over-segment each frame into about 2000 superpixels via SLIC <ref type="bibr" target="#b69">[70]</ref> for good boundary adherence. For each superpixel, we set the number of NNs N = 8. In our experiments, all the parameters of our algorithm are fixed to unity.</p><p>Datasets We evaluate our method on three public video segmentation benchmarks, e.g., DAVIS <ref type="bibr" target="#b4">[5]</ref>, YouTube-Objects <ref type="bibr" target="#b5">[6]</ref>, and Segtrack-V2 <ref type="bibr" target="#b6">[7]</ref>. Some statistics and features of these datasets are summarized in Table <ref type="table" target="#tab_2">1</ref>.</p><p>The newly released DAVIS <ref type="bibr" target="#b4">[5]</ref> contains 50 video sequences (3, 455 frames in total) and pixel-accurate manual ground-truth for the foreground object in every frame. Those videos span a wide range of typical challenges encountered in video object segmentation, such as occlusions, fast-motion, appearance changes and motion blur. The videos in DAVIS are split into train set <ref type="bibr" target="#b29">(30)</ref> and validation set <ref type="bibr" target="#b19">(20)</ref>.</p><p>Youtube-Objects <ref type="bibr" target="#b5">[6]</ref> is a large dataset of 1, 407 videos collected from 155 web videos. This dataset includes videos with 10 object categories. With the setting and ground-truth masks of <ref type="bibr" target="#b12">[13]</ref>, we consider totally 126 videos with more than 20, 000 frames. The annotations are roughly pixel-level and provided on every 10 th frame of downsampled videos.</p><p>SegTrack-V2 <ref type="bibr" target="#b6">[7]</ref> extends the SegTrack dataset <ref type="bibr" target="#b28">[29]</ref> to contain 8 additional videos, in which totally 14 low-resolution video sequences with 24 instance objects and 947 frames. SegTrack-V2 is a relatively small but is the most widely adopted dataset for video segmentation. It is originally proposed for joint segmentation and tracking and is designed to be challenging with respect to background-foreground color similarity, fast motion and complex shape deformation. Pixel-level annotation on the objects is offered for every frame. Since instance-level masks are provided for sequences with multiple objects, in our experiments, we treat each specific instance segmentation as separate problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head><p>To evaluate the quality of the proposed Super-Trajectory based Video segmentation (STV), we provide in this section both qualitative as well as quantitative comparison on DAVIS dataset <ref type="bibr" target="#b4">[5]</ref>, Youtube-Object <ref type="bibr" target="#b5">[6]</ref> and SegTrack-V2 <ref type="bibr" target="#b6">[7]</ref> datasets. We compare the proposed STV against nine classical state-of-the-art alternatives: BVS <ref type="bibr" target="#b39">[40]</ref>, FCP <ref type="bibr" target="#b37">[38]</ref>, JMP <ref type="bibr" target="#b43">[44]</ref>, SEA <ref type="bibr" target="#b36">[37]</ref>, TSP <ref type="bibr" target="#b10">[11]</ref>, HVS <ref type="bibr" target="#b8">[9]</ref>, VSF <ref type="bibr" target="#b32">[33]</ref>, SCF <ref type="bibr" target="#b12">[13]</ref>, and OFL <ref type="bibr" target="#b33">[34]</ref>. BVS, FCP, JMP, SEA, VSF, SCF, and OFL are semi-supervised video segmentation approaches. TSP and HVS are for unsupervised over-segmentation, but they can also accept initial annotation in the first frame, following the settings in <ref type="bibr" target="#b4">[5]</ref>. For completeness, we also report six deep learning based semi-supervised video segmentations models: VPN <ref type="bibr" target="#b49">[50]</ref>, CTN <ref type="bibr" target="#b52">[53]</ref>, SFL <ref type="bibr" target="#b53">[54]</ref>, MSK <ref type="bibr" target="#b50">[51]</ref>, OSVOS <ref type="bibr" target="#b51">[52]</ref>, OnAVOS <ref type="bibr" target="#b54">[55]</ref>, eight non-deep learning unsupervised methods: ARP <ref type="bibr" target="#b25">[26]</ref>, FST <ref type="bibr" target="#b15">[16]</ref>, CUT <ref type="bibr" target="#b64">[65]</ref>, NLC <ref type="bibr" target="#b18">[19]</ref>, MSG <ref type="bibr" target="#b60">[61]</ref>, KEY <ref type="bibr" target="#b20">[21]</ref>, CVOS <ref type="bibr" target="#b70">[71]</ref>, SAL <ref type="bibr" target="#b71">[72]</ref>, and five deep unsupervised video segmentation methods: LOV <ref type="bibr" target="#b46">[47]</ref>, FSEG <ref type="bibr" target="#b47">[48]</ref>, LMP <ref type="bibr" target="#b44">[45]</ref>, SOF <ref type="bibr" target="#b48">[49]</ref>, LOV <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation on DAVIS Dataset Evaluation Metrics</head><p>We evaluate the effectiveness of our approach on DAVIS dataset with three accompanied evaluation tools: intersection-over-union metric (J ) for measuring the region-based segmentation similarity, F-measure (F ) for measuring the contour accuracy, temporal stability (T ) for measuring the temporal consistency of segments.</p><p>Intersection-over-union metric is one of the most widely adopted metric to evaluate the performance of image/video segmentation methods. Given a segmentation mask M and ground-truth G, intersection-over-union score is defined as</p><formula xml:id="formula_25">J = M G M G .</formula><p>Contour accuracy (F ) is for measuring how well the segment contours c(M ) match the ground-truth contour c(G). Contour-based precision P c and recall R c between c(M ) and c(G) can be computed via bipartite graph matching. Given P c and R c , contour accuracy F is defined as</p><formula xml:id="formula_26">F = 2P c R c P c + R c .</formula><p>Temporal stability (T ) is used for penalizing inconsistent segments. It is computed as the per-pixel cost of matching two successive segmentation contours. The match is achieved by minimizing the Shape Context Descriptor (SCD) <ref type="bibr" target="#b72">[73]</ref> distances between the matched points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>In Table <ref type="table">2</ref>, IoU score, contour accuracy, and temporal stability (averaged on the validation set of the DAVIS dataset) are reported. It can be observed that, among the heuristic video segmentation models, our method (STV) achieves the highest average IoU score (0.689) over the 20 validation video sequences. Our results also achieve a significant improvement over the second best algorithm BVS (0.600) and the third best algorithm FCP (0.584). In addition, STV achieves the best overall contour accuracy (0.670) over other non-deep learning based algorithms and gains a competitive temporal stability score (0.185). This demonstrates that our segments align better with the ground-truth object boundaries and they are temporally consistent. We IoU score (J ), contour accuracy (F) and temporal stability (T ) averaged on the validation set of DAVIS <ref type="bibr" target="#b4">[5]</ref>. For IoU score and contour accuracy, higher values are better. For temporal stability, lower values are better. The best results of non-deep learning and deep learning models are boldfaced, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Metric</head><p>Semi It can be observed that the proposed algorithm is applicable to a large set of scenarios and robust to motion blur, occlusions and background appearance similarities. also observe that, compared with the heuristic methods, deep learning based semi-supervised video segmentation algorithms generate more accurate segmentation results. When we compare the results of unsupervised segmentation methods, we find heuristic method: ARP <ref type="bibr" target="#b25">[26]</ref> achieves the best performance. Our performance is also worse than ARP. But we find that ARP unfortunately suffers heavy computation burden due to the computation of object proposals and complex inference. This also indicates there's still large room for improvement of video segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>Qualitative video segmentation results for three video sequences from the DAVIS dataset <ref type="bibr" target="#b4">[5]</ref> are presented in Fig. <ref type="figure" target="#fig_8">6</ref>. With the first frame as initialization, the proposed algorithm has the ability to segment the objects with fast motion patterns (breakdance-flare) or large shape deformation (dog-agility). It also produces accurate segmentation maps even when the foreground suffers occlusions (libby). In Section 5.3.3, we provide a more detailed analysis of the performance of our method for typical video object segmentation challenges.  We report our performance on the Youtube-Object <ref type="bibr" target="#b5">[6]</ref> and SegTrack-V2 <ref type="bibr" target="#b6">[7]</ref> datasets, which are widely used for semi-supervised segmentation. The IoU scores of our and various state-of-the-art methods on Youtube-Object and SegTrack-V2 are presented in Table <ref type="table" target="#tab_3">3</ref> and Table <ref type="table" target="#tab_4">4</ref>, respectively. As can be seen, our method outperforms other methods overall, achieving the best IoU score on most of the videos with the average score up to 0.756 (Youtube-Object) and 0.781 (SegTrack-V2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>Representative pixel labeling results on Youtube-Object and SegTrack-V2 datasets are shown in Fig. <ref type="figure" target="#fig_9">7</ref> and Fig. <ref type="figure" target="#fig_10">8</ref>. We can observe that the target foreground objects in challenging scenarios (such as existence of similar objects, deformations, color changes, motion and image blur, scale variations, etc.) can be accurately segmented out by our algorithm. Our quantitative and qualitative results demonstrate the power of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">In-Depth Validation Experiments</head><p>In this section, we offer more detailed exploration for the proposed approach in several aspects with DAVIS dataset  <ref type="bibr" target="#b4">[5]</ref>. We test the values of important parameters, verify basic assumptions of the proposed algorithm, evaluate the contributions from each part of our approach, perform attributebased study and conduct runtime comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Parameter Verification</head><p>With the train set of the DAVIS dataset, we first study the influence of the needed input parameter: number of spatial grids K, of our super-trajectory algorithm in Section 3.2. We report the performance by plotting the IoU value of the segmentation results as functions of a variety of Ks, where we vary K = {800, 900, • • •, 1500}. As shown in Fig. <ref type="figure" target="#fig_11">9</ref> (a), the performance increases with finer super-trajectory clustering in spatial domain (K↑). However, when we further increase </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ablation Study</head><p>To quantify the improvement obtained with our proposed trajectories in Section 3.1, we compare to two baseline trajectories: LTM <ref type="bibr" target="#b14">[15]</ref> and DAD <ref type="bibr" target="#b65">[66]</ref> in our experimental results. LTM is widely used for motion segmentation and DAD shows promising performance for action detection. To be fair, we only replace our trajectory generation part with above two methods, estimate optical flow via LDOF <ref type="bibr" target="#b68">[69]</ref> and keep all other parameters fixed.</p><p>To further dissect various parts of our method, we offer five variants of the proposed algorithm STV, list as follows:</p><p>• STV-s: For demonstrating the effectiveness of the our super-trajectory based label propagation in Section 4.1, we offer baseline STV-s by performing label propagation on trajectory level and only use labeledtrajectories for establishing appearance model.</p><p>• STV-r: For evaluating the effectiveness of the proposed reverse tracking strategy in Section 4.2, we offer baseline by performing segmentation without considering outside trajectories T o .</p><p>• STV-b: For evaluating the effectiveness of the backward re-occurrence assumption, we offer baseline STV-b via performing segmentation without Eq.16.</p><p>• STV-KM: For accessing the influence of the DPC algorithm, we offer baseline STV-KM via replacing DPC with K-means clustering algorithm.</p><p>• STV-SC: For accessing the influence of the DPC algorithm, we offer baseline STV-KM via replacing DPC with spectral clustering algorithm. Fig. <ref type="figure">10</ref>. Average IoU score over the validation set of DAVIS dataset. We compare our method (STV) with two trajectory based methods: LTM <ref type="bibr" target="#b14">[15]</ref> and DAD <ref type="bibr" target="#b65">[66]</ref>, and five variations of our algorithm: STV-s, STV-r, STV-b, STV-KM and STV-SC. See Section 5.3.2 for more details.</p><p>The comparison results with above baselines are summarized in Fig. <ref type="figure">10</ref>. Four important conclusions can be drawn:</p><p>(1) compared with classical trajectory methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b65">[66]</ref>, the proposed trajectory generation approach is preferable;</p><p>(2) significant improvement over STV-s (0.689 vs 0.558) clearly demonstrates the advantage of super-trajectory for capturing rich structure information of video; (3) the improvement over STV-r and STV-b verifies the effectiveness of our reverse tracking strategy and global optimization via backward re-occurrence; (4) the DPC algorithm is more favored compared with K-means (0.689 vs 0.641) or spectral clustering algorithm (0.689 vs 0.653).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Attribute-Based Analysis</head><p>The videos in the DAVIS dataset are also categorized according to their various attributes such as appearance change (AC), background clutter (BC), camera shake (CS), dynamic background (DB), deformation (DEF), edge ambiguity (EA), fast motion (FM), heterogeneous objects (HO), interesting objects (IO), low resolution (LR), motion blur (MB), occlusion (OCC), out-of-view (OV), shape complexity (SC), scalevariation (SV). With these attribute annotations, we present a more detailed evaluation in Table <ref type="table" target="#tab_6">5</ref>. Three variations of the proposed STV method: STV-s, STV-r, and STV-b, described in Section 5.3.2, are also included for a thorough analysis of the effectiveness of our super-trajectory representation, the influence of the proposed reverse tracking scheme and the backward re-occurrence strategy.</p><p>The attribute based analysis shows that the proposed video segmentation model, STC, is robust to various challenges presented in the DAVIS dataset. Specifically, it compares favorably on any subset of videos sharing the same attribute. Due to the representation power of the supertrajectory and the efficient DPC algorithm, STC handles the dynamic background (DB) and motion blur (MB) well. With the reverse tracking strategy, STC is able to discriminate the cases involving the background clutter (BC), and occlusion (OCC). By leveraging on the backward re-occurrence, STV recovers from the out-of-view scenarios (OV) and attains an increased robustness to the deformation (DEF), shape complexity (SC), and scale-variation (SV), which are typically failure cases for methods that strongly rely on propagation of segmentations on spatiotemporal connections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Performance over Time</head><p>For semi-supervised video segmentation, as the number of frames increases, the performance will decrease since errors accumulate over time. In Fig. <ref type="figure">11</ref>, we plot the IoU scores of different approaches BVS <ref type="bibr" target="#b39">[40]</ref>, FCP <ref type="bibr" target="#b37">[38]</ref>, JMP <ref type="bibr" target="#b43">[44]</ref>, SEA <ref type="bibr" target="#b36">[37]</ref>, TSP <ref type="bibr" target="#b10">[11]</ref> and HVS <ref type="bibr" target="#b8">[9]</ref>, with the initial annotation propagated over time. It can be observed that, the IoU score of our method drops more slowly and consistently gains better performance over different numbers of propagation frames, compared with other methods. This demonstrates the results of our method experience less drift of the object regions over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Runtime Comparison</head><p>We conduct running-time comparisons on the DAVIS dataset <ref type="bibr" target="#b4">[5]</ref> with 480p image frames. We include six semisupervised video segmentation methods BVS <ref type="bibr" target="#b39">[40]</ref>, FCP <ref type="bibr" target="#b37">[38]</ref>, HVS <ref type="bibr" target="#b8">[9]</ref>, JMP <ref type="bibr" target="#b43">[44]</ref>, SEA <ref type="bibr" target="#b36">[37]</ref>, TSP <ref type="bibr" target="#b10">[11]</ref> and OFL <ref type="bibr" target="#b33">[34]</ref> for providing a comprehensive view of execution times of existing approaches. The time comparison results (excluding optical computation time) are listed in Table <ref type="table" target="#tab_7">6</ref>. Although we do not have the code of FCP <ref type="bibr" target="#b37">[38]</ref>, it's computation time must be more than 50 seconds per frame. Since FCP is based on object proposal, which takes more than 50 seconds for processing each frame. As seen, our method achieves a better tradeoff between performance and computation efficiency.</p><p>All the tests are performed on a Dell T5610 workstation with an Intel Xeon E5 CPU of 2.50 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We introduced a super-trajectory representation based semisupervised video segmentation approach. We demonstrated that, based on the density peaks based clustering, compact trajectories can be efficiently grouped into super-trajectories. Super trajectory possesses various desired properties and capable of capturing: i) long-term motion information, ii) local spatiotemporal information, and iii) diverse and compact features of video. We showed that, in our context, occlusion and drift are naturally handled by our trajectory generation method using the probabilistic model. Our solution for reverse tracking points and our approach to leverage the property of region re-occurrence both lead an improved robustness for many segmentation challenges such as occlusions and move-in/-out. By extensive experimental evaluations on three large video segmentation datasets <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we verified that our approach outperforms the current nondeep learning based and heuristic methods. We also analyzed several variants and components for a comprehensive assessment of various aspects of our method. One potential direction for future work is to combine our super-trajectory with deep learning descriptors, as the work of <ref type="bibr" target="#b67">[68]</ref>, for a more powerful representation of video sequences. Additionally, our work provides valuable evidence toward combining compact spatiotemporal representation with certain priors (e.g., saliency, objectness) for other computer vision applications such as action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the density peaks based clustering (DPC) algorithm. (a) A schematic diagram where the bigger circles indicate higher local densities ρ. (b) Sample point distributions in two dimensions. (c) Clustering results with DPC, where different colors represent different clusters. (d) Local density ρ and distance δ distributions for the data points of (b). See Section 3.2.1 for detailed explanations.</figDesc><graphic coords="4,435.37,148.01,125.34,92.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of initial super-trajectory generation process, described in Section 3.2.2. (a) The arrows indicate trajectories and the dots indicate the initial location of the trajectories. (b) We cluster trajectories into K groups on the spatial grid (in this case, K = 4).</figDesc><graphic coords="5,312.99,124.03,123.19,69.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2 (b)-(d) gives another case for DPC with two-dimensional data. As seen, in Fig. 2 (c), the cluster centers correspond to the points (represented as large solid circles) in Fig. 2 (d) with large values of δ and sizeable densities ρ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4. Super-trajectory generation via iterative trajectory clustering. (a) Frame It. (b)-(f) Visualization of super-trajectory in the time slice It at different iterations. Each pixel is assigned to the average color of all points within its super-trajectory. The blank areas are the discarded trajectories, which are shorter than four frames. The areas with obvious changes are highlighted in red circles. For clarity, we set the number of initial spatial grid to K=500. See Section 3.2.2 for more details.</figDesc><graphic coords="5,312.99,205.14,123.19,69.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 since we merge small clusters into other Algorithm 2 Super-Trajectory Generation Input: All the trajectories T ={τ i } i , spatial sampling step R; Output: Super-trajectory assignments; /* Initialization */ 1: Obtain K trajectory groups via spatial sampling step R; 2: Set initial cluster number C = T /L for each group; 3: Obtain initial cluster centers {τ j } j from each trajectory group by DPC in Alg. 1, where |{τ j } j | = m; 4: loop /* Iterative Assignment */ 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Input frames. (b) Estimated foregrounds via Eq. 12 and the appearance model in Section 4.1. (c) Estimated foregrounds via our reverse tracking strategy (Eq. 14) and the updated appearance model in Section 4.2. (d) Estimated foregrounds via backward re-occurrence based optimization (Eq. 16, Section 4.3). (e) Final segmentation results.</figDesc><graphic coords="7,461.43,100.87,96.58,54.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Qualitative segmentation results on three video sequences from DAVIS<ref type="bibr" target="#b4">[5]</ref> (from top to bottom: breakdance-flare, dog-agility and libby ). It can be observed that the proposed algorithm is applicable to a large set of scenarios and robust to motion blur, occlusions and background appearance similarities.</figDesc><graphic coords="9,60.41,338.74,95.19,53.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Qualitative segmentation results on representative video sequences from Youtube-Objects dataset [6] (from top to bottom: aero02, bird12, and dog10). The initial masks are presented in the first row.</figDesc><graphic coords="9,61.23,526.55,78.26,58.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Qualitative segmentation results on representative video sequences from SegTrack-V2 [7] (from top to bottom: cheetah1, drift1, and penguin3). The initial masks are presented in the first row.</figDesc><graphic coords="10,59.70,176.26,95.38,52.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The IoU scores for parameter selection for number of spatial grids K (a) and the number of the NNs N (b). See Section 5.3.1 for more detailed discussion. K, the final performance does not change obviously. In our experiments, we set K = 1200 where the maximum performance is obtained over the train set of DAVIS. Later, we investigate the influence of parameter N , which indicates the number of the NNs of a region in Section 4.3. We plot IoU score with varying N = {2, 4, • • •, 20} in Fig. 9 (b), and set N = 8 for achieving best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 DPC for Generating Super-Trajectory Centers Input: A group of trajectories {τ i } i and cluster number C; Output: Organized trajectory clusters; 1: Compute local densities {ρ i } i via Eq. 7; 2: Compute distance {δ i } i via Eq. 8; 3: Find trajectories {τ i } i with δ i = H;</figDesc><table /><note><p>4: if |{τ i } i | &gt; C then 5:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Characteristics of three video segmentation datasets used in evaluation.</figDesc><table><row><cell>Dataset</cell><cell>Ref Year</cell><cell>#Videos</cell><cell cols="2">#Frames #Objects</cell></row><row><cell>DAVIS</cell><cell>[5] 2016</cell><cell>50 (train:30,val:20)</cell><cell>3, 455</cell><cell>50</cell></row><row><cell cols="2">YouTube-Objects [6] 2012</cell><cell>126</cell><cell>20, 000</cell><cell>126</cell></row><row><cell>Segtrack-V2</cell><cell>[7] 2013</cell><cell>14</cell><cell>947</cell><cell>24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>IoU score (J ) on the Youtube-Objects dataset<ref type="bibr" target="#b5">[6]</ref>. The average is computed over all 126 video sequences. Higher values are better. The best results are boldfaced.</figDesc><table><row><cell cols="2">Dataset Category</cell><cell>BVS</cell><cell>OFL</cell><cell>Method VSF</cell><cell>SCF</cell><cell>STV</cell></row><row><cell></cell><cell>aeroplane</cell><cell>0.808</cell><cell>0.853</cell><cell>0.890</cell><cell>0.862</cell><cell>0.811</cell></row><row><cell></cell><cell>bird</cell><cell>0.764</cell><cell>0.831</cell><cell>0.816</cell><cell>0.810</cell><cell>0.813</cell></row><row><cell></cell><cell>boat</cell><cell>0.601</cell><cell>0.706</cell><cell>0.742</cell><cell>0.685</cell><cell>0.791</cell></row><row><cell></cell><cell>car</cell><cell>0.567</cell><cell>0.688</cell><cell>0.709</cell><cell>0.693</cell><cell>0.754</cell></row><row><cell></cell><cell>cat</cell><cell>0.527</cell><cell>0.606</cell><cell>0.677</cell><cell>0.588</cell><cell>0.780</cell></row><row><cell>Yotube</cell><cell>cow</cell><cell>0.648</cell><cell>0.715</cell><cell>0.791</cell><cell>0.685</cell><cell>0.722</cell></row><row><cell>-Object</cell><cell>dog</cell><cell>0.616</cell><cell>0.716</cell><cell>0.703</cell><cell>0.617</cell><cell>0.739</cell></row><row><cell></cell><cell>horse</cell><cell>0.531</cell><cell>0.623</cell><cell>0.678</cell><cell>0.539</cell><cell>0.716</cell></row><row><cell></cell><cell>motorbike</cell><cell>0.416</cell><cell>0.599</cell><cell>0.615</cell><cell>0.608</cell><cell>0.680</cell></row><row><cell></cell><cell>train</cell><cell>0.621</cell><cell>0.747</cell><cell>0.782</cell><cell>0.663</cell><cell>0.761</cell></row><row><cell></cell><cell>Avg.</cell><cell>0.597</cell><cell>0.701</cell><cell>0.740</cell><cell>0.649</cell><cell>0.756</cell></row><row><cell cols="7">5.2.2 Evaluation on Youtube-Object and SegTrack-V2</cell></row><row><cell cols="2">Quantitative Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>IoU score (J ) on the SegTrack-V2 dataset<ref type="bibr" target="#b6">[7]</ref>. The average is computed over all 24 object instances. Higher values are better. The best results are boldfaced.</figDesc><table><row><cell>Dataset</cell><cell>Object</cell><cell>BVS</cell><cell>OFL</cell><cell>Method SEA</cell><cell>HVS</cell><cell>STV</cell></row><row><cell></cell><cell>bird of paradise</cell><cell>0.897</cell><cell>0.871</cell><cell>0.823</cell><cell>0.868</cell><cell>0.901</cell></row><row><cell></cell><cell>birdfall</cell><cell>0.653</cell><cell>0.529</cell><cell>0.093</cell><cell>0.574</cell><cell>0.461</cell></row><row><cell></cell><cell>bmx1</cell><cell>0.671</cell><cell>0.879</cell><cell>0.445</cell><cell>0.392</cell><cell>0.922</cell></row><row><cell></cell><cell>bmx2</cell><cell>0.032</cell><cell>0.040</cell><cell>0.000</cell><cell>0.325</cell><cell>0.401</cell></row><row><cell></cell><cell>cheetah1</cell><cell>0.054</cell><cell>0.259</cell><cell>0.177</cell><cell>0.188</cell><cell>0.666</cell></row><row><cell></cell><cell>cheetah2</cell><cell>0.092</cell><cell>0.372</cell><cell>0.006</cell><cell>0.244</cell><cell>0.467</cell></row><row><cell></cell><cell>drift1</cell><cell>0.685</cell><cell>0.779</cell><cell>0.429</cell><cell>0.552</cell><cell>0.934</cell></row><row><cell></cell><cell>drift2</cell><cell>0.327</cell><cell>0.274</cell><cell>0.111</cell><cell>0.272</cell><cell>0.509</cell></row><row><cell></cell><cell>frog</cell><cell>0.761</cell><cell>0.784</cell><cell>0.634</cell><cell>0.671</cell><cell>0.812</cell></row><row><cell></cell><cell>girl</cell><cell>0.865</cell><cell>0.842</cell><cell>0.624</cell><cell>0.319</cell><cell>0.916</cell></row><row><cell></cell><cell>hummingbird1</cell><cell>0.532</cell><cell>0.672</cell><cell>0.140</cell><cell>0.137</cell><cell>0.762</cell></row><row><cell>SegTrack</cell><cell>hummingbird2</cell><cell>0.287</cell><cell>0.685</cell><cell>0.368</cell><cell>0.252</cell><cell>0.675</cell></row><row><cell>-V2</cell><cell>monkey</cell><cell>0.875</cell><cell>0.878</cell><cell>0.761</cell><cell>0.619</cell><cell>0.925</cell></row><row><cell></cell><cell>monkeydog1</cell><cell>0.405</cell><cell>0.471</cell><cell>0.049</cell><cell>0.683</cell><cell>0.432</cell></row><row><cell></cell><cell>monkeydog2</cell><cell>0.171</cell><cell>0.210</cell><cell>0.090</cell><cell>0.188</cell><cell>0.874</cell></row><row><cell></cell><cell>parachute</cell><cell>0.937</cell><cell>0.933</cell><cell>0.925</cell><cell>0.691</cell><cell>0.942</cell></row><row><cell></cell><cell>penguin1</cell><cell>0.816</cell><cell>0.804</cell><cell>0.802</cell><cell>0.720</cell><cell>0.970</cell></row><row><cell></cell><cell>penguin2</cell><cell>0.820</cell><cell>0.835</cell><cell>0.731</cell><cell>0.807</cell><cell>0.928</cell></row><row><cell></cell><cell>penguin3</cell><cell>0.785</cell><cell>0.839</cell><cell>0.463</cell><cell>0.752</cell><cell>0.951</cell></row><row><cell></cell><cell>penguin4</cell><cell>0.764</cell><cell>0.862</cell><cell>0.516</cell><cell>0.806</cell><cell>0.920</cell></row><row><cell></cell><cell>penguin5</cell><cell>0.478</cell><cell>0.823</cell><cell>0.537</cell><cell>0.627</cell><cell>0.863</cell></row><row><cell></cell><cell>penguin6</cell><cell>0.843</cell><cell>0.873</cell><cell>0.701</cell><cell>0.755</cell><cell>0.952</cell></row><row><cell></cell><cell>solider</cell><cell>0.553</cell><cell>0.868</cell><cell>0.719</cell><cell>0.665</cell><cell>0.908</cell></row><row><cell></cell><cell>worm</cell><cell>0.654</cell><cell>0.832</cell><cell>0.724</cell><cell>0.347</cell><cell>0.650</cell></row><row><cell></cell><cell>Avg.</cell><cell>0.584</cell><cell>0.675</cell><cell>0.453</cell><cell>0.518</cell><cell>0.781</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Attribute-based aggregate performance on the DAVIS dataset with IoU score (J ). Higher values are better. The best performing method of each category is highlighted in boldfaced. Fig.11. Segmentation performance with IoU score over time, reported on DAVIS dataset. It can be observed that the IoU decreases during propagation of the initial mask over the consecutive video frames. Our method STV has consistently better performance compared to others.</figDesc><table><row><cell>Dataset Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Runtime comparison (seconds/frame) on the DAVIS dataset<ref type="bibr" target="#b4">[5]</ref>. The runtime reported in<ref type="bibr" target="#b39">[40]</ref> is offered for reference, since the code released in https://github.com/owang/ BilateralVideoSegmentation is much slower.</figDesc><table><row><cell>Method</cell><cell>BVS [40]</cell><cell>FCP [38]</cell><cell>HVS [9]</cell><cell>OFL [34]</cell></row><row><cell>Time (s)</cell><cell>16 ( 0.37 * )</cell><cell>&gt;50</cell><cell>16</cell><cell>137</cell></row><row><cell cols="2">Code Type Matlab&amp;C++</cell><cell>Python</cell><cell>C++</cell><cell>Matlab&amp;C++</cell></row><row><cell>Method</cell><cell>SEA [37]</cell><cell>TSP [11]</cell><cell>JMP [44]</cell><cell>STV</cell></row><row><cell>Time (s)</cell><cell>6</cell><cell>63</cell><cell>14</cell><cell>9</cell></row><row><cell cols="3">Code Type Matlab&amp;C++ Matlab&amp;C++</cell><cell>C++</cell><cell>Matlab&amp;C++</cell></row></table><note><p>* </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We do not use a cut-off kernel or Gaussian kernel adopted in<ref type="bibr" target="#b3">[4]</ref> due to the small amount of data.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint segmentation and classification of human actions in video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking-by-segmentation with online gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3056" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised multiclass video segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3282" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fatih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="626" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2051" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple hypothesis video segmentation from superpixel flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vazquez-Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="268" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal object detection proposals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="737" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4083" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7417" to="7425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motion coherent tracking using multi-label MRF optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised video segmentation using tree structured graphical models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2257" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective video object cutout</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5645" to="5655" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="496" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SeamSeg: Video object segmentation using patch seams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">JOTS: Joint online tracking and segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video SnapCut: robust video object cutout using localized classifiers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geodesic image and video editing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="134" to="135" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discontinuity-aware video object cutout</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">JumpCut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="195" to="196" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5849" to="5858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video object segmentation with re-identification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation-CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dense point trajectories by GPU-accelerated large displacement optical flow</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="438" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Detection free tracking: Exploiting motion and topology for segmenting and tracking under entanglement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2073" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3369" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Two-granularity tracking: Mediating trajectory and detection graphs for tracking under occlusions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="552" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
