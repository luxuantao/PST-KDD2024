<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Fingertip and Palm Tracking in Depth Image Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hui</forename><surname>Liang</surname></persName>
							<email>hliang1@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Media Innovation</orgName>
								<orgName type="department" key="dep2">School of EEE</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of EEE Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
							<email>danielthalmann@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Institute for Media Innovation</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Drive</addrLine>
									<postCode>637553</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Fingertip and Palm Tracking in Depth Image Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E1E8560F676E6776F5A5AFC508827E7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.1.2 [Models and Principles]: User/Machine Systems-Human information processing; I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Depth cues</term>
					<term>Tracking Fingertip Tracking</term>
					<term>Human-Computer Interaction</term>
					<term>Kinect Sensor</term>
					<term>Geodesic Distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a vision-based approach for robust 3D fingertip and palm tracking on depth images using a single Kinect sensor. First the hand is segmented in the depth images by applying depth and morphological constraints. The palm is located by performing distance transform to the hand contour and tracked with a Kalman filter. The fingertips are detected by combining three depth-based features and tracked with a particle filter over successive frames. Quantitative results on synthetic depth sequences show the proposed scheme can track the fingertips quite accurately. Besides, its capabilities are further demonstrated through a real-life human-computer interaction application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Human hand is an essential body part for human-computer interaction due to its various usages in gesture recognition, animation synthesis and virtual object manipulation <ref type="bibr">[7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>. As important features of the hand, the positions of tracked fingertips have a variety of applications. They can be used in combination with inverse kinematics solver for hand pose estimation <ref type="bibr" target="#b1">[1]</ref>. Their trajectories can be used for gesture recognition <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b4">4]</ref> or manipulative purpose in multitouch systems <ref type="bibr" target="#b5">[5]</ref>. A lot of work have been done for visionbased fingertip tracking, while many previous methods only focus on extracting 2D fingertips and cannot track fingertips robustly for a freely moving hand <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b9">9]</ref>. Research in 3D fingertip localization and tracking is still very limited, and their performances are far from satisfactory for real-life applications <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Difficulty in accurate 3D fingertip tracking mainly lies in three aspects. First, multiple fingertips of several side-by-side fingers are hard to be distinguished. Second, traditional contour-based methods cannot locate the fingertips for bending fingers. Third, it is a challenging problem to label each detected fingertip correctly. Fig. <ref type="figure" target="#fig_0">1 (a-c</ref>) illustrates the problems.</p><p>Most previous fingertip tracking schemes are based on contour analysis of the extracted hand region <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b2">2]</ref> and usually can track the fingertips for only stretched fingers. In <ref type="bibr" target="#b6">[6]</ref> fingertips are tracked for infrared image sequences. It utilizes a template matching strategy to detect the fingertip locations. The correspondence of the fingertips between successive frames is built by minimizing the sum of distances between the predicted locations given by a Kalman filter and the detected fingertips. In <ref type="bibr" target="#b4">[4]</ref> fingertips are located within the hand region by first propagating a set of particles from the hand center to the hand contour and then choosing the particles where the transitions between skin and non-skin areas meet certain requirement. Stereoscopic vision is adopted in <ref type="bibr" target="#b2">[2]</ref> to track the 3D position of the fingertip of a single pointing finger. The fingertip is located by finding the two points which maximize the distance to the center of gravity of the hand region and the boundary curvature on the silhouette of the hand in both input images. The 3D position of the fingertip is then found using stereovision and tracked with a Kalman filter. In <ref type="bibr" target="#b9">[9]</ref> the Kinect sensor is utilized for 3D fingertip and palm center detection for two hands. The palm center is detected by applying distance transform to the inverted binary image of the hand regions. The finger regions are segmented from the palm region and fingertip locations are found by assuming they are the closest to the camera in each finger region. In <ref type="bibr" target="#b3">[3]</ref> a more discriminative circular image feature is adopted for fingertip detection, which can tackle more complex hand motion such as grasping. The fingertips are tracked by combining particle filtering and mean-shift tracking. However, none of these methods is capable of extracting the 3D positions for all the five fingertips during natural hand motios, such as in Fig. <ref type="figure">(b-c</ref>).</p><p>In this paper we present a robust fingertip and palm tracking scheme with the input of depth images captured by a single Kinect sensor. The hand region is segmented from the depth frame by applying depth and morphological constraints and the palm circle is then identified. The 3D positions of the fingertips are tracked using a particle filter through successive frames. We rely on three depthbased features to differentiate the fingertip and non-fingertip points. Quantitative test results on six synthetic sequences show that the proposed fingertip tracking scheme tracks the 3D fingertip positions quite accurately. In addition, we develop an applications based on the fingertip tracking results, in which the 3D fingertip positions are used with an inverse kinematic solver to drive a hand model to manipulate virtual objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">HAND AND PALM DETECTION</head><p>We utilize the morphology of the hand for hand segmentation in the depth image and make several assumptions on hand motion. First, we assume the hand is the nearest object to the camera and constrain global hand rotation by:</p><formula xml:id="formula_0">-15 • ≤ θx ≤ 15 • , -15 • ≤ θy ≤ 15 • , -90 • ≤ θz ≤ 90 • ,</formula><p>(2.1) where (θx, θy, θz) is the global rotation angle of the hand. Second, the depth value differences within the forearm and hand region are less than a threshold zD, e.g. zD = 0.2m. Third, based on morphology of hand, we assume that hand palm forms a globally largest blob in the hand and forearm region in the depth image when θx ≈ θy ≈ 0 • , and forms a locally largest blob when the hand rotates within ranges defined in (2.1). The palm region can thus be approximated with a circle Cp = (pp, rp), where pp is the palm center and rp is the radius. The proposed hand and palm detection scheme consists of three steps: foreground segmentation, palm localization and hand segmentation. It starts with thresholding the depth frame to obtain the foreground F . F is given by:</p><formula xml:id="formula_1">F = {(p|z(p) &lt; z0 + zD} , (<label>2.2)</label></formula><p>where (p, z(p)) denotes a pixel in the depth image at coordinate p and with depth value z(p); z0 is the minimum depth value. This ensures that both hand and forearm regions are extracted from the depth frame. Cp then equals to the largest inscribed circle of the contour of F . To reduce the computational complexity of palm localization, the center of Cp is tracked with a 2D Kalman filter. Finally the hand and forearm regions are separated by a line which is both tangent to Cp and perpendicular to the orientation of the forearm. We approximate the orientation of the forearm using the Eigenvector that corresponds to the largest Eigenvalue of the covariance matrix of the contour pixel coordinates of F . Let the extracted hand regions in the depth frame be FD. We further process FD to get a 3D point cloud FV by calculating the 3D world position for each point in FD using the projection parameters of the Kinect sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FINGERTIP DETECTION &amp; TRACKING</head><p>Inspired by the concept of Accumulative Geodesic Extrema <ref type="bibr" target="#b8">[8]</ref>, we define the fingertip position as the point that maximizes the geodesic distance from the palm center within each finger. However, due to self-occlusion of the hand, the geodesic distances may not be correctly estimated for all points with the hand region. In addition, the AGEX extraction algorithm <ref type="bibr" target="#b8">[8]</ref> cannot robustly detect the fingertip positions when multiple fingers are side-by-side as it requires the AGEX interest points to be sparsely located, and its computational complexity is high since Dijkstra's algorithm needs to be performed every time when an AGEX point is to be extracted. We address these issues by imposing more constraints on possible fingertip locations. First, we assume fingertips can be only detected where depth is discontinuous in FV , e.g. Fig. <ref type="figure" target="#fig_0">1 (a-c</ref>), and denote the set of these points as the border point set UB. Second, the relative depth differences between one point and its neighborhood are important to differentiate fingertip and non-fingertip points, e.g. Fig. <ref type="figure" target="#fig_0">1 (d)</ref>, and we design a rectangle local feature to take advantage of this fact. Third, we utilize the 3D geodesic shortest path (GSP) to differentiate nearby fingertips, which is more robust than the fingertip position alone, e.gi. Fig. <ref type="figure" target="#fig_0">1</ref>. (e).</p><p>Overall, the proposed fingertip detection and tracking scheme consists of two stages, namely, the initialization and reinitialization stage, as well as the fingertip tracking stage. In the first stage, the user is requested to pose the hand so that the fingers are not side-by-side. The fingertip positions are detected using three depth-based features. Each fingertip is then given a label l ∈ L f = {T, I, M, R, P } using a GSP-based voting strategy. The labels in L f corresponds to the thumb, index, middle, ring and pinky fingers. The second stage starts only when all five fingertips are detected in the first stage. In the second stage, each of the detected fingertips in the first stage is tracked with a single particle filter. Note that the first stage can be performed not only at the first frame. When five fingertips are detected, the fingertip tracking process can also be automatically reinitialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initialization and Re-initialization</head><p>The task of this stage is to detect all five fingertips in the depth image FV based on three depth-based features: the geodesic distance, local rectangle feature and GSP points. To estimate the geodesic distance for each point, we first build a graph G h = (V h , E h ) using the point cloud FV as in <ref type="bibr" target="#b8">[8]</ref>. V h consists of all points within FV . For each pair of vertices (p, q) ∈ V h , there is an edge between p and q if and only if they are in the 8-neighbohood of each other and their 3D distance d(p, q) = pq 2 is within threshold τ . To ensure the resulting graph is connected, we search for a set of connected components in G h using the union-find algorithm. The connected component containing the palm center is identified and the remaining ones are connected to it by finding their nearest vertices and adding an edge with weights equal to the 3D Euclidean distance. We then perform Dijkstra graph search on G h to calculate the geodesic distance from the palm center pp for each vertex p ∈ V h . Let the geodesic distance of each vertex p be dg(p). The GSP point set UG(p) for p is defined as the set of vertices on the shortest path from pp to p. A rectangle local feature RL(p) is used to describe the neighborhood of a point p in FV , which is defined as a square of size S centered at p. Each pixel q within RL(p) is binarized according to the following rule:</p><formula xml:id="formula_2">I(q) = 1 if |z(p) -z(q)| ≤ zT 0 otherwise , (3.1)</formula><p>where zT is a threshold value of about 1cm. We define η(p) as the ratio of the number of points with nonzero values in RL(p) to the size of RL(p). For a stretched hand, fingertip can only locate where dg is locally maximized in FV and the points around a fingertip take much smaller values of η than other points, say, η ≤ 0.4. Based on these observations, we detect the fingertips using Algorithm 1.</p><p>Algorithm 1 Fingertip Detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>The border point set, UB; Output:</p><p>The detected fingertip positions, p If all five fingertips are detected by Algorithm 1, each of them will be given a label l ∈ L f using a GSP-based voting strategy. We do not use the positions of the fingertips for labeling as they show great uncertainty and are not robust for labeling, e.g. Fi. 1 (c). Instead, we use the GSP point set of each detected fingertip p i f to vote for its label li. This voting strategy is inspired by the fact that the 2D relative positions of points near to pp on different GSPs remain stable against finger bending and global hand transformation. Let the GSP point sets of the fingertips be U i G = UG(p i f ) = {p i,k |k = 0, 1, ..., Ni}, i = 1, 2, ..., 5. For each fingertip p i f , a five element counter array Γi,j, j = 1, 2, ..., 5 is maintained to estimate the probability that p i f has the label lj. Note that a right hand is adopted in our system. The fingertips are labeled using the Algorithm 2.</p><p>In Fig. <ref type="figure">2</ref> we present labeling results for several samples. We can see this labeling scheme is quite robust to hand articulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fingertip Tracking</head><p>After the initial positions of the five fingertips are detected, we build a particle filter for each fingertip to track their positions through successive frames. Let (x, ω) denote a particle, where the state hypothesis x is its 2D position in FV and ω is the particle weight. The basic idea is to constrain the positions of each particle to the border point set UB to reduce the search space, instead of choosing arbitrary positions within the 2D space. Let f (y k |x k ) be the likelihood function with y k representing the current observation.</p><p>Algorithm 2 GSP-based Voting for Fingertip Labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>The GSP point sets of the five fingertips, U i G ; Output:</p><p>The label for each fingertip, li; 1: Γi,j = {0}, Nmax = max{Ni}, k = 0; 2: Extract five points p i,kr , where kr = k × Ni/Nmax; 3: Sort the five points by arranging the five vectors v i d = p i,krpp clockwisely; 4: Let the order number of p i,kr be j, Γi,j = Γi,j + 1; 5: k = k + 1. If k ≥ Nmax, go to 6, otherwise go to 2; 6: li = arg max j Γi,j; 7: return li; We now define the likelihood function f (y k |x k ) based on the geodesic distance dg, the rectangle local feature RL and GSP point set UG:</p><formula xml:id="formula_3">f (y k |x k ) = f (dg, RL, UG|x k ) = f (dg|x k )f (RL|x k )f (UG|x k ) , (<label>3.2)</label></formula><p>where we assume dg, RL and UG are conditionally independent. The three terms in f (y k |x k ) all take the form of an exponential function of certain distance metric. In f (dg|x k ), the distance metric is defined as the difference between D i g and dg(x k ), where D i g is a pre-defined geodesic distance value for the fingertip of label li. Temporal reference is used for estimating f (RL|x k ) and f (UG|x k ) as they change with the finger motions. <ref type="bibr">Let</ref>  </p><formula xml:id="formula_4">f = exp -λg dg -D i g -λ h DH -λ rl DRL (3.3)</formula><p>4. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fingertip Tracking Accuracy</head><p>We quantitatively evaluate the fingertip tracking accuracy on six synthetic sequences in terms of the Euclidean distance between the tracked fingertips and the ground truth. As it is difficult to define the fingertip locations on the skin surface, we define their ground truth using the phalanx end point of each finger. Table <ref type="table" target="#tab_0">1</ref> shows the average localization errors in centimeter on all six sequences with seq. 1 for grasping motion, seq. 2 for adduction/abduction motion, seq. 3 for successive single finger motion, seq. 4 for flexion motion of two fingers, seq. 5 for global rotation and seq. 6 for combination of grasping and global rotation. Note the localization error partly results from the fact that the fingertips are detected on the skin surface rather than the hand skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Virtual Object Manipulation</head><p>We combine the 3D positions of the fingertips and palm center with an inverse kinematics solver to drive a 3D hand model to manipulate virtual objects. Each finger is modeled as a kinematic chain and the cyclic coordinate descent algorithm <ref type="bibr" target="#b12">[12]</ref> is used for inverse kinematics estimation of the finger pose. Besides, we build a virtual environment using the Nvidia Physx SDK, which contains a 3D hand model and some virtual objects like boxes and spheres. Users can use their bare hands to perform some manipulative tasks such as moving, pushing and grasping. A sequence of snapshots for virtual object manipulation is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Fingertip and palm positions are important features for human-computer interaction. Most previous approaches cannot track the 3D positions of fingertips robustly due to the high flexibility of finger motion. In this paper, we address these issues by using multiple depth-based features for accurate fingertip localization and adopting a particle filter to track the fingertips over successive frames. The palm is located by performing distance transform to the segmented hand contour and tracked with a Kalman filter. Quantitative results on synthetic depth sequences and a real-life human-computer interaction application show the proposed scheme can track the fingertips accurately and has great potential for extension to other HCI applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>This research, which is carried out at BeingThere Centre, is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Difficulty in fingertip tracking and solutions. (a) Side-by-side fingers. (b) Bending fingers. (c) Nearby fingertips: fingertips of the thumb and index fingers are too close to be labeled correctly. (d)Rectangle feature. (e)Geodesic shortest path.</figDesc><graphic coords="1,346.79,207.77,179.18,111.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i f ; 1 : 6 :</head><label>16</label><figDesc>Preprocess: D C = {p|p ∈ UB, dg(p) &gt; dT , η(p) &lt; ηT }; 2: Label connected components: D C = D 1 ∪ D 2 ∪ ... ∪ D M ; 3: Sort the components according to size and ignore small ones to get D i B , i = 1, 2, ..., MB; 4: Get the number of fingertips MF : if MB &lt; 5, MF = MB, otherwise MF = 5; 5: p i f = arg max p∈D i B dg(p), i = 1, 2, ..., MF ; return p i f ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Algorithm 3 ; 5 :</head><label>235</label><figDesc>Figure 2: Fingertip Labeling. GSP Points (left). Labeling results (right).</figDesc><graphic coords="3,324.95,213.53,222.86,83.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Virtual object grasping.</figDesc><graphic coords="4,339.95,53.69,192.74,139.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Quantitative results on synthetic sequences</head><label>1</label><figDesc>the reference fingertip position be p ref . f (UG|x k ) is defined based on the Hausdorff distance DH (UG(x k ), UG(p ref )). f (RL|x k ) is defined based on the feature distance DRL between RL(x k ) and RLp ref , as the ratio of the number of points with the same values to the size of the rectangle. f (y k |x k ) is given by:</figDesc><table><row><cell>Seq.</cell><cell></cell><cell cols="3">Average Error (cm)</cell><cell></cell></row><row><cell>No.</cell><cell cols="5">Thumb Index Middle Ring Pinky</cell></row><row><cell>Seq. 1</cell><cell>2.51</cell><cell>1.53</cell><cell>1.50</cell><cell>1.27</cell><cell>0.77</cell></row><row><cell>Seq. 2</cell><cell>1.63</cell><cell>0.93</cell><cell>0.78</cell><cell>0.74</cell><cell>0.69</cell></row><row><cell>Seq. 3</cell><cell>1.34</cell><cell>0.88</cell><cell>0.65</cell><cell>0.84</cell><cell>0.89</cell></row><row><cell>Seq. 4</cell><cell>2.11</cell><cell>1.15</cell><cell>1.16</cell><cell>0.84</cell><cell>0.81</cell></row><row><cell>Seq. 5</cell><cell>1.20</cell><cell>0.82</cell><cell>0.75</cell><cell>0.52</cell><cell>0.59</cell></row><row><cell>Seq. 6</cell><cell>1.44</cell><cell>0.93</cell><cell>0.89</cell><cell>0.77</cell><cell>0.86</cell></row><row><cell cols="2">which is defined</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three dimensional fingertip tracking in stereovision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Consei1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bourennane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Int&apos;l Conf. on Advanced Concepts for Intelligent Vision Systems</title>
		<meeting>of the 7th Int&apos;l Conf. on Advanced Concepts for Intelligent Vision Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Particle filter-based fingertip tracking with circular hough transform features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th IAPR Conf. on Machine Vision Applications</title>
		<meeting>of the 12th IAPR Conf. on Machine Vision Applications</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast fingertip positioning by combining particle filtering with particle random diffusion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Multimedia and Expo</title>
		<meeting>IEEE Int&apos;l Conf. on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-touch surface using multiple cameras</title>
		<author>
			<persName><forename type="first">I</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gabayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aghajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th Int&apos;l Conf. on Advanced concepts for intelligent vision systems</title>
		<meeting>of the 9th Int&apos;l Conf. on Advanced concepts for intelligent vision systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE Int&apos;l Conf. on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual interpretation of hand gestures for human-computer interaction: A review</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="677" to="695" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time identification and localization of body parts from depth images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Robotics and Automation</title>
		<meeting>IEEE Int&apos;l Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking of fingertips and centres of palm using kinect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Raheja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Int&apos;l Conf. on Computational Intelligence, Modelling and Simulation</title>
		<meeting>of the 3rd Int&apos;l Conf. on Computational Intelligence, Modelling and Simulation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust hand gesture recognition with kinect sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust hand gesture recognition based on finger-earth mover distance with a commodity depth camera</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A combined optimization method for solving the inverse kinematics problem of mechanical manipulators</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="489" to="499" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
