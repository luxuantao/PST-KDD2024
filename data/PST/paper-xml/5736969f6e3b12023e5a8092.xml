<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Double-orientation Code and Nonlinear Matching Scheme for Palmprint Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lunke</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Wenliang</forename><surname>Tang</surname></persName>
							<email>wltang@ecjtu.jx.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">East China Jiaotong University</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
							<email>csdzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Biometrics Research Centre</orgName>
								<orgName type="department" key="dep2">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Kowloon, Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Double-orientation Code and Nonlinear Matching Scheme for Palmprint Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">598E79BB474B1C2A95A85A2F3F054747</idno>
					<idno type="DOI">10.1016/j.patcog.2015.08.001</idno>
					<note type="submission">Received date: 25 October 2014 Revised date: 19 May 2015 Accepted date: 2 August 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Biometric</term>
					<term>palmprint recognition</term>
					<term>double-orientation code</term>
					<term>nonlinear angular distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many palmprint authentication approaches have been proposed in recent years. Among them, the orientation based coding approach, in which the dominant orientation features of palmprints are extracted and encoded into bitwise codes, is one of the most promising approaches. The distance between codes created from two palmprint images is calculated in the matching stage. Reliable orientation feature extraction and efficient matching are the two most crucial problems in orientation based coding approaches. However, conventional coding based approaches usually extract only one dominant orientation feature by adopting filters with discrete orientations, which is sensitive to the noise and rotation. This paper proposed a novel double-orientation code (DOC) scheme to represent the orientation feature of palmprint and designed an effective nonlinear angular matching score to evaluate the similarity between the DOC. Extensive experiments performed on three types of palmprint databases demonstrate that the proposed approach has excellent performance in comparison with previously proposed state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Biometric authentication is becoming more and more popular because it is an important and effective technology for personal verification and identification <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. In palmprint authentication, the palmprint is defined as the inner surface of a hand. It contains many stable and discriminative features, including not only principal lines and wrinkles but also abundant ridges, minutiae, and textural features <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Thus the palmprint based authentication approach is able to achieve reliable personal verification and identification. In recent years, the palmprint recognition approach has received increasing research interests and various palmprint recognition algorithms have been presented <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> based on different kinds of palmprint features. For example, Huang et al. <ref type="bibr" target="#b12">[13]</ref> proposed a principle line based approach for palmprint verification. Dai et al. <ref type="bibr" target="#b13">[14]</ref> presented a ridge-based palmprint matching algorithm, which quantitatively investigates the ridge features of high resolution palmprint images and calculates the statistics of ridge features. Morales et al. <ref type="bibr" target="#b14">[15]</ref> introduced the scale invariant feature transform (SIFT) based approaches to perform palmprint recognition. The key points of palmprints obtained using SIFT are that they are robust to the image illumination, scaling and rotation variance. E. Liu et al. <ref type="bibr" target="#b15">[16]</ref> proposed a minutiae-based palmprint matching algorithm based on minutiae clustering and minutiae match propagation. Li et al. <ref type="bibr" target="#b16">[17]</ref> designed a palmprint recognition approach based on the fusion of 2D and 3D palmprint features. They first extracted correlated features from 2D and 3D palmprint images.</p><p>Then, these features were fused at the feature level to achieve satisfactory recognition accuracy. Zhang et al. <ref type="bibr" target="#b17">[18]</ref> supplied a multi-spectral palmprint recognition approach which captured palmprint images under red, green, blue, and near-infrared light. These spectral features were combined at the matching score level to improve the performance of palmprint identification. In addition, the subspace based approaches, such as the Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, and the Representation Based Classification (RBC) approaches, such as CRC <ref type="bibr" target="#b18">[19]</ref> and TPTSSR <ref type="bibr" target="#b19">[20]</ref>, can also be exploited for palmprint authentication <ref type="bibr" target="#b20">[21]</ref>.</p><p>Besides the above approaches, orientation based coding approaches are deemed to be the most promising palmprint recognition approaches, since the palmprint is full of line and textural features which carry rich and distinctive orientation information. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> proposed an effective Palmcode approach that applied a normalized 2-D Gabor filter to the palmprint image and encoded the filter results as code representation. Inspired by the Palmcode approach, Kong et al. <ref type="bibr" target="#b22">[23]</ref> proposed the Competitive code approach which adopted six Gabor filters to extract the dominant orientation features of palmprints based on the principle of the biggest response. Similar to Competitive code method, the Robust Line Orientation Code method (RLOC) <ref type="bibr" target="#b23">[24]</ref> extract orientation by using a Modified Finite Radon Transform (MFRAT). Based on the idea of the Competitive code, Zuo et al. <ref type="bibr" target="#b24">[25]</ref> designed a novel Sparse Multiscale Competitive Code (SMCC) approach to extract more accurate orientation features by using a bank of multiscale Gabor filters and employing a winner-take-all rule. Subsequently, Kong et al. <ref type="bibr" target="#b25">[26]</ref> proposed a fusion code approach that encoded the phase with dominant magnitude from four orientation's Gabor filter results. Sun et al. <ref type="bibr" target="#b26">[27]</ref> employed three groups of orthogonal Gaussian filters to extract three binary codes, i.e. the ordinal code, in terms of the sign of the filter results. To further extract more orientation features, Guo et al. <ref type="bibr" target="#b27">[28]</ref> proposed a Binary Orientation Co-occurrence Vector (BOCV) approach, which obtained all six orientations by convolving the palmprint image with six Gabor filters and encoded all filter results as orientation features. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> had improved the BOCV to E-BOCV by making out the fragile bits to further improve the performance of palmprint recognition.</p><p>It is well known that the winner-take-all rule, which extract the single orientation with the largest filter response <ref type="bibr" target="#b22">[23]</ref>, is usually used in the orientation based coding methods. However, in real operations, a bank of Gabor filters with discrete orientations are used to convolve with palmprint. It is possible that no any filter that has the same orientation as palmprint line and no filter can achieve the absolute maximum of filter response. Actually, the palmprint line usually coincide with two filters, which have larger responses than other filters in most conditions. So double-orientation feature with top-two largest responses is more reasonable than the single-orientation extraction, and it is robust to the noise and rotation.</p><p>In this paper, a robust double-orientation code (DOC) approach for palmprint recognition is proposed. First, the paper studies the rationale of the palmprint orientation based coding theory and concludes that the DOC is highly reliable and reasonable for palmprint orientation feature In the matching stage, the similarity is simply evaluated in terms of the number of the overlapping pixels of two palmprint principal lines. pixel-to-area <ref type="bibr" target="#b13">[14]</ref>  In the matching stage, the similarity is simply evaluated in terms of the number of the overlapping pixels of two palmprint principal lines. A recommended matching approach of principal lin approach, which calculates the principal line matching score as follows:</p><formula xml:id="formula_0">1 1 (A, B) ( , ) ( , ) / m n A i j S A i j B i j N = = = ∑∑ ∩ ,</formula><p>are two palmprint principal line images, " ∩ " represents the logical "AND"</p><p>is the number of pixel points of A , m and n are the row number and column and ( , ) B i j represents a neighbor area of ( , ) B i j . The larger the eater similarity between A and B . one of the most stable features of a palmprint. However, to represent the uniqueness of a palmprint because different Thus, the recognition accuracy may be low. Moreover, simple discriminative minutiae are discarded. In the matching stage, the similarity is simply evaluated in terms of the number of the overlapping principal lines is the principal line matching score as follows:</p><p>(1) logical "AND" number and column e larger the However, using only palmprint because different individuals simple using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Coding based approaches</head><p>In addition to the principal line based approach, the coding based approaches are the most promising methods for palmprint recognition. One or several filters are used to extract palmprint orientation features and these features are then converted into codes. The distance between codes is calculated to perform palmprint recognition. The representative coding based approaches include the Competitive code, Palmcode, Ordinal code, Fusion code, RLOC, BOCV, and E-BOCV approach, and so on.</p><p>The Competitive code approach <ref type="bibr" target="#b22">[23]</ref>  π , and the distance between perpendicular orientations is 3. For effective calculation, the index competitive code can be represented by three binary codes via the rule in <ref type="bibr" target="#b22">[23]</ref>. Then the hamming distance can be used to measure the similarity between two competitive codes:</p><formula xml:id="formula_1">3 0 0 1 2 (P (x, y) Q (x, y)) (P, Q) 3 N N i i y x i D N = = = ∩ = ∑∑∑ ,<label>(2)</label></formula><p>where P Q</p><formula xml:id="formula_2">i i</formula><p>is the i th bit binary code plane and " ∩ " is the logical "AND" operation. The value of hamming distance representatives the similarity between two code plane.</p><p>Compared with the Competitive code approach, the palmcode approach <ref type="bibr" target="#b21">[22]</ref> uses only the optimal 2D Gabor filter with orientation of / 4</p><p>π , including the real part and the imaginary part, to extract palmprint textural features. The fusion code approach <ref type="bibr" target="#b25">[26]</ref> uses four complex Gabor filters with orientations of / 4(j 0,1, 2,3) jπ = to extract palmprint orientation features. The phase with the largest response magnitude of the four filters is converted into a pair of binary codes. The similarity between palmcode and fusion codes is calculated by using the normalized hamming distance. In order to obtain more orientation information, the BOCV approach <ref type="bibr" target="#b27">[28]</ref> uses the same six Gabor filters as in the Competitive code approach to convolve with palmprint image. All six orientation features are encoded into six codes, which are joined to calculate the hamming distance between the testing image and training image. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> extended the BOCV to E-BOCV by incorporating fragile bits information. In the E-BOCV, fragile bits in BOCV are extracted and excluded from the BOCV matching. And a code map based metric is designed for the fragile bits similarity evaluation, which is fused with BOCV matching in score level fusion.</p><p>Similar to the Competitive code approach, the robust line orientation code (RLOC) approach</p><p>[24] adopts the MFRAT instead of Gabor filter to extract orientation code. The RLOC encodes a pixel as 1 when it is in a certain principal line; otherwise, the RLOC encode the pixel as 0. The pixel-to-area rule is used for the RLOC matching.</p><p>Inspired by the ordinal measurement, Sun et al. <ref type="bibr" target="#b26">[27]</ref> proposed the Ordinal code approach, which uses three groups of integrated perpendicular 2D Gaussian filters to convolve with palmprint image.</p><p>The signs of filtering results are encoded into three ordinal codes, and the sum of three bitwise hamming distances is computed for the similarity evaluation between the query and gallery palmprint.</p><p>Both Competitive code and Fusion code methods extract the single-orientation base on the rule of winner-take-all <ref type="bibr" target="#b22">[23]</ref> that extract the orientation of filter that has the largest filter response with palmprint. The rule of the orientation extraction in the RLOC method is also similar with that of the Competitive code method. It based on the theory that the filter response will reach the maximum when the filter orientation is consistent with that of the palmprint line. However, in real operations, the adopted orientations of filters are discrete. It is possible that no any filter has the orientation of the palmprint line and has the absolute largest response in most conditions. So the single-orientation extraction based on the winner-take-all rule maybe unstable. This motivated us to explore a more reasonable orientation based coding approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DOUBLE-ORIENTATION FEATURE EXTRACTION</head><p>The orientation based coding approaches usually based on the assumptions that each pixel in palmprint belongs to a line and the filter response will reach the maximum when the orientation of the filter is consistent with the line orientation <ref type="bibr" target="#b14">[15]</ref>. However, in real operations, the orientations of filters are discrete. So the orientation of filters is not exactly consistent with the line orientation in most conditions. This means that the "winner-take-all rule" may not extract the orientation feature correctly.</p><p>To investigate the stability of the "winner-take-all rule", six Gabor filters with orientations of / 6 (j 0,1,...,5) jπ = were chosen to perform convolution with palmprint images. Then the top-two responses, i.e. the largest response and the second-largest response, were compared. Palmprint images were selected from the PolyU palmprint database and the multispectral databases, which will be introduced in detailed in next section. In each database, 100 palmprint images from different palms were selected and each image was normalized to 64×64. There were 4,096 pixels in one image and 4096×100=409,600 pixels in 100 images. Each pixel was convolved with six Gabor filters and the discrepancy between top-two largest responses were calculated. The distribution of discrepancy was shown as Fig. <ref type="figure" target="#fig_9">2</ref> (a). The x axis is the pixel distribution and the y axis represents the top-two largest response discrepancy. It was found that there are many points having very similar top-two largest responses.</p><p>The discrepancy-ratio, which is the ratio of the discrepancy to the largest response, was introduced to represent the close degree between the top-two responses. A smaller discrepancy-ratio means a higher similarity between two largest responses. Fig. <ref type="figure" target="#fig_9">2</ref> (b) and (c) show the distribution of the "pixel percentage" and "percentage-summation" with the discrepancy-ratio of PolyU palmprint database.  π . Then we add only 0.02% "'salt &amp; pepper'" noise in the palmprint image. It can be found that the largest response of the pixel changes to 2.9102 with direction of 0 as shown in Fig. <ref type="figure" target="#fig_4">3</ref> (c). This indicates that the orientation of the largest response is changed after little noise is imposed.  orientation of filter that has the largest filter response will be treated as the orientation in several orientation based coding approaches. This is deemed to be reasonable because of the fact that the filter response will reach the maximum when the filter orientation is the orientation of palmprint line. However, it is possible that no any filter ha palmprint line and no filter can achieve the exact maximum . Because only limited discrete orientations (in most approaches adopted in real operations. Actually, the palmprint line usually coincide approaches by using different filters and concluded that the Gabor filter has better performance than other filters. Furthermore, the Gabor filter has good properties of the 2-D spectral specificity of texture as well as its variation with 2-D spatial position. We used the Gabor filter to extract orientation features of palmprint in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Revised Gabor filter</head><p>The Gabor filter has the following general form:</p><formula xml:id="formula_3">2 2 2 2 1 ( , , , , , ) exp[ ( )]exp(i 2 x ) 2 x y G x y θ µ σ β π πµ πσβ σ β ′ ′ ′ = - + ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">0 0 ( ) cos ( ) sin x x x y y θ θ ′ = - + - , 0 0 ( ) sin ( ) cos y x x y y θ θ ′ = - + - . 0 0 ( , )</formula><p>x y is the center of the function, µ is the radial frequency in radians per unit length, θ is the orientation of the Gabor function in radians, and σ and β are the standard deviations of the elliptical Gaussian along x and y axis, respectively. The ranges of x and y are the sizes of the filter and 1 i = -. Similar to the Competitive code approach, the real part of the Gabor filter is applied to extract the orientation feature of the palmprint. The Gabor filter response at an orientation can be treated as confident features occurring at that orientation <ref type="bibr" target="#b32">[32]</ref>. Lines are a small-scalar part of the palmprint image <ref type="bibr" target="#b21">[22]</ref>. So the real part of the Gabor filter should be transferred to "upside-down" form for more accurate orientation feature extraction. So the largest response means the lowest convolved value. The transformed Gabor filter is defined as:</p><formula xml:id="formula_5">2 2 2 2 1 {1 exp[ ( )]cos(2 x )} 2 x y G π πµ πσβ σ β ′ ′ ′ = - - + . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>A set of optimal parameters are set in the Gabor filter according to <ref type="bibr" target="#b12">[13]</ref>. These parameters are µ =0.0916, and σ = β =5.6179. θ is / ( 0,1,..., 1) j n j n θ θ π = -, where n θ , which is usually even, is the orientation number used in the adopted Gabor filters. θ n is set to 6 in this paper generally. Fig. <ref type="figure">5</ref> shows the appearance of the revised Gabor filter with orientation θ =0. The revised Gabor filters are used to perform orientation feature extraction of the palmprint image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Double-orientation extraction algorithm</head><p>The revised Gabor filters are palmprint image. Let j G be the is the number of Gabor filters.</p><p>palmprint image:</p><p>where I is the palmprint image, and " ⊗ " is the convolve operation.</p><p>is the filter result of ( , ) I x y with extracted as the dominant orientation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Double-orientation nonlinear matching</head><p>In this subsection, the proposed DOC based nonlinear matching scheme is presented. The hamming distance is widely used to calculate the similarity between two palmprint images in coding based approaches. For example, the Palmcode, Fusion code, and Ordinal code approaches all use the hamming distance in the matching stage. The Competitive code approach proposes an angle distance for palmprint recognition, which is equivalent to the sum of three bitwise hamming distances. Guo et al. <ref type="bibr" target="#b33">[33]</ref> proposed the unified formula of hamming distance metric. The hamming operation result is 0 if the corresponding bits are the same, otherwise, the result is 1. If two corresponding bits of two series are different, they are referred to as a pair of different bits. The final matching result is the sum of hamming results of a series of binary codes. So the hamming distance metric is linear with respect to the number: the pairs of different bits of two series.</p><p>To increase the discrimination, a nonlinear angular matching score approach is proposed to evaluate the similarity of DOC. In the orientation matching stage, only superior similarity between orientations can acquire a high matching score. When the orientation difference reaches the maximum, the matching score should be a small enough value. The nonlinear matching approach based on "single-orientation code" is defined as:</p><formula xml:id="formula_7">* _ 1 _ ( _ ) k code dis ori score code dis e =<label>(7)</label></formula><p>and</p><formula xml:id="formula_8">( ) _ min , d t d t code dis O O n O O θ = - - - ,<label>(8)</label></formula><p>where d O and t O are two "single-orientation code", and k is the parameter. The perfect matching score is 1 when two "single-orientations code" are the same (The code distance _ code dis = 0). The _ ori score should be smaller than ξ , which is a small enough value, when distance of two "single-orientations code" reach the maximum / 2 n θ . In other words,</p><formula xml:id="formula_9">*( / 2) 1 k n e θ ξ &lt; ,<label>(9)</label></formula><p>So</p><formula xml:id="formula_10">2 1 ln k n θ ξ &gt; , (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where ξ is empirically set as 0.01 in this paper. k =1.6 is acceptable when n θ =6, and k =1 is accredited when n θ =12. Fig. <ref type="figure" target="#fig_11">7</ref> (a) shows nonlinear matching scores against the code distance with n θ =12 and n θ =6, respectively. Comparatively, Fig. <ref type="figure" target="#fig_11">7</ref> (b) depicts the variation of the hamming distance with the number of bits. For double-orientation code matching score calculation, two crossing matching scores based on code difference are defined as: </p><p>where</p><formula xml:id="formula_13">_dis min(| O O |, | O O |) ( , p,s) i j i j code n α β θ α β α β = - - - = . (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>In particular, </p><formula xml:id="formula_15">p score i j = . (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>The corresponding DOC based crossing nonlinear matching scores are shown in Table <ref type="table" target="#tab_3">I</ref>. The perfect matching score is 1 when two DOCs are same. If only single sub-orientation-codes of two DOCs are the same, the final matching score will be larger than 0.5, otherwise (two DOC are absolutely different), the final matching score will be equal to or smaller than 0.2019.  p score and 2 _ p score obtained using (16). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>this section, a series of experiments was performed to estimate the performance of the proposed approach on three types of popular palmprint databases: including the left and right palmprint database, the multispectral palmprint database <ref type="bibr" target="#b34">[34]</ref>, and the IITD database <ref type="bibr" target="#b35">[35]</ref>. Several state-of-the-art coding based approaches were implemented to compare with the DOC approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Palmprint databases</head><p>The left and right palmprint database was provided by the HongKong Polytechnic University (PolyU) <ref type="bibr" target="#b34">[34]</ref>. It contained 3740 palmprint images collected from 187 different volunteers, where 10 right palmprint images and 10 left palmprint images were captured for each subject. Thus the palmprint database used contained 374 classes and each class had 10 palmprint images. Hereafter the left and right palmprint database is referred to as the PolyU database.</p><p>The multispectral palmprint database contained four independent spectral palmprint databases, including the Red spectrum, Green spectrum, Blue spectrum, and Near Infrared (NIR) spectrum palmprint databases <ref type="bibr" target="#b34">[34]</ref>. Each of them was collected by PolyU from 500 palms of 250 subjects, including 195 males and 55 The age distribution was from 20 to 60 years old. The palmprint images were collected in two separate sessions with a time interval about 9 days. In each session, the subject was asked to provide 6 images for each palm. Therefore, 24 images of each illumination from two palms were collected for each subject. In total, the database contained 6,000 images from 500 different palms for one illumination. Thus, each spectral database had 500 classes and each class had 12 palmprint images.</p><p>The public IITD palmprint database <ref type="bibr" target="#b35">[35]</ref> is a contactless based palmprint database. Images in the IITD database were captured in the indoor environment, and contactless hand images were acquired by a camera with variations in pose, projection, rotation and translation. The main problem of contactless databases lies in the significant intra-class variations resulting from the absence of any contact or guiding surface to restrict such variations.</p><p>subjects. Five hand images were captured from each of the left and rig every session. So there were 460 different images. In addition to the original hand images, the database. Fig. <ref type="figure">9</ref> shows some palmprint images In the palmprint verification, FRR (False Reject Rate), FAR (False Accept Rate) and EER (Equal Error Rate) <ref type="bibr" target="#b1">[2]</ref> were used to evaluate the performance of the proposed approach. A matching was counted as correct if the crossing matching score was larger than the threshold, otherwise, the matching was treated as incorrect. For the purpose of analysis, this threshold was considered to be the operating point of FRR, FAR and EER. The Receiver Operating Characteristic (ROC) curve, which is a graph of false reject rate versus false acceptance rate all possible operating points, was introduced to describe the performance of the palmprint recognition approach. The ROC curves of the DOC approach on three types of palmprint databases are shown in Fig. <ref type="figure" target="#fig_15">11</ref>. The ROC curve produced by other approaches, including the Competitive code (Compcode), Ordinal code (Ordicode), Fusion code (Fusncode), RLOC, BOCV, and E-BOCV approaches, are also shown in Fig. <ref type="figure" target="#fig_15">11</ref>. The ROC curve of the Palmcode is not plotted in the figure for its FAR and FRR are obviously higher than other approaches.</p><formula xml:id="formula_17">(a) (b) (c) (d)<label>(e) (f) (g)</label></formula><p>It can be seen that our approach can achieves the lowest FRR against the same FAR on all databases.</p><p>The corresponding EERs are presented in Table <ref type="table" target="#tab_4">II</ref>. One can see that the DOC approach achieves the smallest EER among all coding approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EERs Comp code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordi code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusn code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Palm code BOCV EBOCV RLOC DOC</head><p>PolyU 0.0122 0.0150 0.0155 0.0432 0.0149 0.0203 0.0180 0.0092 Red 0.0145 0.0161 0.0179 0.0297 0.0186 0. 0313 0.0223 0.0119 Green 0.0168 0.0202 0.0216 0.0507 0.0232 0.0303 0.0249 0.0146 Blue 0.0170 0.0202 0.0212 0.0463 0.0207 0.0225 0.0203 0.0146 NIR 0.0137 0.0180 0.0213 0.0332 0.0284 0.0510 0.0208 0.0121 IITD 0.0696 0.0744 0.0878 0.0933 0.0708 0.0671 0.0826 0.0622</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Palmprint identification</head><p>Identification is a one-against-many comparison process which answers the question of which approaches are also implemented to compare with the DOC approach. In the Competitive code approach, the smaller matching score between two competitive code means the more similarity between two samples. So the class of training sample that produce the smallest competitive code matching score will be treated as the class of the testing sample. Other coding based approaches also adopt the same rule as the Competitive code approach. The experimental results are shown in Fig. <ref type="figure" target="#fig_17">12</ref>, where the error rate is the rate of the number of testing samples that are classified to incorrect class by the number of all testing samples. For the clarity of the presentations, the comparative experimental results with TRAIN=1 and 2 are summarized in Table <ref type="table" target="#tab_5">III</ref> and Table <ref type="table" target="#tab_6">IV</ref>, respectively. In addition, the DOC approach using twelve Gabor filters is also implemented.   It can be seen that the proposed DOC approach achieves the lowest identification error rate among all coding approaches on each palmprint database. Generally, the DOC with n θ =12 usually performs better than that with n θ =6. The main reason is that using more filters should extract more accurate orientation feature of the palmprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the hamming distance metric</head><p>To evaluate the efficiency of the nonlinear matching score scheme, the hamming distance metric is also used to perform double-orientation code matching. The identification error rates obtained by "DOC with hamming metric" approach (TRAIN=1) are listed in Table <ref type="table" target="#tab_7">V</ref>. Compared with the results presented in Table <ref type="table" target="#tab_5">III</ref>, it can be seen that the using of nonlinear matching scheme performs much better than using the hamming distance metric with both n θ =6 and n θ =12. Thus, it can be concluded that the nonlinear matching scheme is suitable for the double-orientation code matching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Computational complexity</head><p>In this section, we compare the computational complexity of DOC approach with conventional coding based approaches. Since the number of Gabor filters used in DOC( n θ =6) is same as other coding based approaches that use six filters, such as the Competitive code, BOCV and E-BOCV approaches, the convolution computation of DOC is also same as these approaches. Thus, the time cost of code extraction in the DOC approach should be similar with those of the Competitive code, BOCV, and E-BOCV approaches. In the code matching processing of the DOC approach, it should be noted that the distance between two DOC is numerated. So the matching score between two "single-orientation code" just need to be calculated only once, which were listed in  and proposes a double-orientation code (DOC) extraction approach. The DOC can correctly and robustly represent the palmprint orientation feature. The distance between DOCs is evaluated by using a nonlinear matching score approach. It has been verified that the nonlinear angular matching score approach is more effective than the conventional hamming distance metric in evaluating the similarity of DOC. The proposed DOC approach can achieve significantly higher palmprint verification and identification accuracy than previous state-of-the-art coding approaches.</p><p>Proposed a novel DOC based method for palmprint identification.</p><p>A nonlinear matching scheme is used in the coding based method. Double orientations with top-two responses is more robust. DOC is reasonable and reliable for palmprint feature extraction. Three different types of databases are employed in experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Palmprint images and theirs principal line images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>matching approach where A and B are two palmprint principal line images, operation, A N is the number of pixel points of number of the palmprint image, matching score means the greater similarity between The principal lines are one of principal lines is not adequate to represent may have similar principal lines. Thus, the recognition accuracy may be low. principal line means that many discr Finally, extensive experiments on three types of palmprint databases effectiveness of the proposed approach. The extensive experimental results approach can achieve higher verification and identification accuracy than coding algorithms. paper is organized as follows: Section 2 briefly describes approaches. Section 3 presents the analysis of the double the double-orientation code based nonlinear matching 5, experiments of the proposed approach are supplied and analy conclusion of this paper. approach the basic feature of a palmprint, and line based recognition approaches play an important role in palmprint authentication. The principal line based approaches use a line or edge detector to extract the palmprint lines and then use them to perform palmprint recognition. In general, which are the most evident lines in the palmprint image Thus the principal lines are highly robust to noise and illumination Palmprint principal lines can be extracted by using the Gobal filter, Radon filter, Sobel operation. extracted by using MFRAT approach [13]. theirs principal line images. (a)-(b) are two palmprint images from two subjects. ( palmprint principal line images of (a)-(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>scheme for d and analyzed.</figDesc><graphic coords="4,308.16,354.72,66.72,66.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 (Fig. 2 .</head><label>22</label><figDesc>Fig.2(d)  shows the "percentage-summation" distributions on four spectral palmprint databases. The curve in Fig.2 (d)shows that the discrepancy-ratios are smaller than 0.1 for about 50% of pixels and smaller than 0.05 for about 30% of pixels. This means that a large number of pixels have very close top-two responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 (</head><label>3</label><figDesc>Fig.3 (a) shows a palmprint image and (b) shows the distribution of the pixel whose "discrepancy-ratio" is smaller than 0.02. One pixel is selected to convolve with six Gabor filters and the results of six filters are also shown in the figure. The largest responses is 2.9018 corresponding to an direction of / 6</figDesc><graphic coords="7,116.76,231.72,188.76,141.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Furthermore, Fig. 3 ( 6 π 3 π</head><label>363</label><figDesc>d) shows that one area of the palmprint image (a) is rotated by 3 o of counter-clock. The largest response of the pixel marked in the figure is 4.2523 with a direction of / before the rotation, and it becomes 4.2525 with a direction of / after the rotation. Thus, the single-orientation based on the rule of the largest filter response is sensitive to the noise and rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. The top row shows the effect on the response of the noise. responses of one pixel whose "discrepancy added to the palmprint image (a) The single orientation of filter feature of palmprint in several orientation based coding because of the fact that the filter response will reach consistent with the orientation of palmprint line. exactly consistent orientation with response in most conditions. Because six orientations) of filters are adopted with two filters, as shown in Fig.4 words, for orientation extraction by using discrete orientation of palmprint line usually coincide well model it. Therefore, the double should be more reasonable than feature with top-two responses, which more robust than single-orientation</figDesc><graphic coords="8,174.24,172.80,263.04,82.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 ,</head><label>4</label><figDesc>Fig.4, which usually have larger responses than other filter orientation extraction by using discrete orientations of filters, the most possible orientation with two orientations of filters and the use of a single filter cannot the double-orientation feature extracted based on the top-two response than the single-orientation extraction. Moreover, the double s, which is stable even there is little noise or rotation in the palmprint orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The appearance of the revised Gabor filter with orientation of θ =0.4.2 Palmprint preprocessingAll palmprint image are preprocessed before palmprint recognition. This step extracts the central region of a palmprint for accurate matching. In our method, the most representative method proposed</figDesc><graphic coords="9,225.36,513.60,160.44,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>[j and 2 j</head><label>2</label><figDesc>are two indices of the ( , ) p O x y and ( , ) s O x y are the is employed to extract the Region of Interest (ROI) of a palmprint. This method uses gaps between fingers as reference points to determine the ROI of a palmprint. At first, we use the original palmprint image to convert the convolved image into , we obtain boundaries of the binary image using a boundary tracking the landmarks based on the boundaries, where the landmarks are at the bottom of gaps between index and middle fingers and between ring and little fingers. Third, we perpendicular bisector of the line segment between two landmarks to determine the centroid of the palmprint region. Finally, we extract the normalized subimage of a fixed size, i.e. 64 64 × , located at a certain area of a palmprint and used for the palmprint feature extraction ROI extraction of a palmprint image. (a) (b) palmprint image. (a). The input palmprint image. (b). The extracted ROI of the palmprint image. orientation extraction algorithm s are used to extract double-orientation feature for all pixel G with orientation of / is the gray scalar of location ( , ) x y in the palmprint image convolve operation. All pixels in the image need to convolve with the filter with j G . The orientations with the most two dominant filter orientation features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>6 ROI</head><label>6</label><figDesc>indices of the two most minimum filter responses. In other words, the two indices of the two most minimum filter responses among . This method uses gaps the low-pass image into a binary boundaries of the binary image using a boundary tracking the bottom of , we locate the perpendicular bisector of the line segment between two landmarks to determine the centroid of the 64 64 , as the ROI, used for the palmprint feature extraction. Fig.the double-orientation code (DOC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The matching score obtained using the proposed approach and hamming distance metric. (a) shows the variation of the matching score with the angular distance. (b) shows the variation of the hamming distance with a number of different bits.</figDesc><graphic coords="12,132.60,74.40,177.96,133.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>score of two palmprint images is computed as: M and N are respectively the row number and column number of the palmprint image. The MN is the pixel number of the palmprint image. A and B are two palmprint images. The perfect matching score is 1 when corresponding DOCs of two palmprint images are same. The procedure of the palmprint matching score calculation is demonstrated in Fig.8. DOC is first extracted from each pixel of two palmprint images. Four _ ori scores , which are obtained from DOCs of two pixels, are used to calculate the 1 _ p score and 2 _ p score . The final _ match score is a normalized summation of the maximum of 1 _</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The procedure to calculate the matching score between DOCs.</figDesc><graphic coords="13,159.12,233.76,293.52,92.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. examples from there types of palmprint database. (a) is Red, Green, Blue, and NIR database, respectively 5.2. Palmprint verification Verification is a one-to-one comparison class or not. In palmprint verification samples in database. The matching palm, otherwise, the matching is viewed are 3740 samples. So the total matching is genuine matching (each class has 45 genuine matching) 6000 samples in each spectral database. for each spectral database, and the genuine matching and imposter matching number 17,964,000. For the IITD database, t Fig.10 shows the distributions of genuine matching score and imposter matching score Red, Green, Blue, NIR, and IITD database score and imposter matching score multispectral databases. A linear classes. The distributions of genuine matching and imposter matching separate as that on the PolyU database database are serious variations in rotation</figDesc><graphic coords="14,176.40,262.08,80.04,80.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The ROC curves of different approaches on different types of databases. (a) depicts the ROC curves on PolyU database. (b)-(e) depict ROC curves on Red, Green, Blue and NIR spectral databases, and (f) is the ROC curves on IITD database.</figDesc><graphic coords="16,118.44,394.32,184.92,138.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>class of the query sample is. In palmprint identification, the first "TRAIN" palmprint image(s) from each class is/are employed as the training sample and the remaining palmprint images form the testing set. The sample in the testing set is compared with all samples in the training set to produce the DOC matching scores. The testing sample will be classified to the class of the training sample that produces the highest matching score with the testing sample. Several state-of-the-art coding based approaches, such as the Competitive code, Ordinal code, Fusion code, Palmcode, BOCV, E-BOCV, and RLOC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Palmprint identification error rate. (a) depicts the error rate on PolyU database. (b)-(e) plots the error rate on Red, Green, Blue, and NIR spectral databases, and (f) is the error rates on IITD database.</figDesc><graphic coords="18,120.84,394.32,184.92,138.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is one of the most popular coding based approaches. Six</figDesc><table><row><cell cols="8">Gabor filters with different orientations are used to extract orientation features from a palmprint. The</cell></row><row><cell cols="5">orientations are finally determined as</cell><cell>jπ</cell><cell>/ 6</cell><cell>, where</cell><cell>j =</cell><cell>{0,1,...,5}</cell><cell>. Six orientation's Gabor templates</cell></row><row><cell cols="8">are convoluted with the palmprint image. The final orientation is the orientations with the greatest</cell></row><row><cell cols="8">response principle. It takes the orientation index (j 0,1,...,5) j =</cell><cell>as the competitive code. The angular</cell></row><row><cell cols="8">distance metric is used for comparing two competitive codes. The angular distance is based on the</cell></row><row><cell cols="8">following rules: the distance between parallel orientations is 0, the distance is 1 when the angles of the</cell></row><row><cell cols="2">two orientations are</cell><cell>π</cell><cell>/ 6</cell><cell cols="4">or 5 / 6 π , the distance is 2 when the angles of the two orientations are</cell></row><row><cell>2 / 6 π</cell><cell>or 4 / 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I .</head><label>I</label><figDesc>CROSSING NONLINEAR MATCHING SCORES OF DOC (THE MAXIMUM CODE DISTANCE IS 3).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II .</head><label>II</label><figDesc>THE EERS (%) OF DIFFERENT APPROACHES ON EACH PALMPRINT DATABASE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III .</head><label>III</label><figDesc>THE PALMPRINT IDENTIFICATION ERROR RATES (%) (TRAIN = 1).</figDesc><table><row><cell>Err rates</cell><cell>Comp code</cell><cell>Ordi code</cell><cell>Fusn code</cell><cell>Palm code</cell><cell cols="3">BOCV EBOCV RLOC</cell><cell>DOC ( n θ =6)</cell><cell>DOC ( n θ =12)</cell></row><row><cell>PolyU</cell><cell>3.57</cell><cell>4.81</cell><cell>4.10</cell><cell>3.86</cell><cell>3.09</cell><cell>2.61</cell><cell>6.83</cell><cell>2.55</cell><cell>2.47</cell></row><row><cell>Red</cell><cell>5.35</cell><cell>6.31</cell><cell>6.65</cell><cell>6.44</cell><cell>5.36</cell><cell>5.45</cell><cell>9.90</cell><cell>4.55</cell><cell>4.49</cell></row><row><cell>Green</cell><cell>5.64</cell><cell>7.49</cell><cell>8.05</cell><cell>8.62</cell><cell>6.44</cell><cell>5.85</cell><cell>10.69</cell><cell>5.07</cell><cell>4.71</cell></row><row><cell>Blue</cell><cell>5.47</cell><cell>7.29</cell><cell>7.49</cell><cell>8.73</cell><cell>5.89</cell><cell>4.69</cell><cell>9.13</cell><cell>5.02</cell><cell>4.67</cell></row><row><cell>NIR</cell><cell>4.85</cell><cell>6.00</cell><cell>7.40</cell><cell>7.53</cell><cell>8.00</cell><cell>8.98</cell><cell>9.29</cell><cell>3.91</cell><cell>3.89</cell></row><row><cell>IITD</cell><cell>34.89</cell><cell>38.86</cell><cell>40.38</cell><cell>49.40</cell><cell>32.88</cell><cell>32.65</cell><cell>45.43</cell><cell>32.23</cell><cell>31.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV .</head><label>IV</label><figDesc>THE PALMPRINT IDENTIFICATION ERROR RATES (%) (TRAIN = 2).</figDesc><table><row><cell>Err rates</cell><cell>Comp code</cell><cell>Ordi code</cell><cell>Fusn code</cell><cell>Palm code</cell><cell cols="3">BOCV EBOCV RLOC</cell><cell>DOC ( n θ =6)</cell><cell>DOC ( n θ =12)</cell></row><row><cell>PolyU</cell><cell>1.20</cell><cell>1.84</cell><cell>1.60</cell><cell>2.54</cell><cell>0.97</cell><cell>0.84</cell><cell>2.67</cell><cell>0.67</cell><cell>0.73</cell></row><row><cell>Red</cell><cell>1.82</cell><cell>2.20</cell><cell>2.38</cell><cell>3.88</cell><cell>2.24</cell><cell>2.28</cell><cell>3.74</cell><cell>1.30</cell><cell>1.36</cell></row><row><cell>Green</cell><cell>2.14</cell><cell>2.98</cell><cell>3.36</cell><cell>8.42</cell><cell>2.88</cell><cell>2.48</cell><cell>4.40</cell><cell>1.84</cell><cell>1.46</cell></row><row><cell>Blue</cell><cell>2.24</cell><cell>2.92</cell><cell>3.18</cell><cell>6.50</cell><cell>2.58</cell><cell>2.02</cell><cell>3.47</cell><cell>1.82</cell><cell>1.62</cell></row><row><cell>NIR</cell><cell>1.46</cell><cell>2.04</cell><cell>2.72</cell><cell>4.12</cell><cell>3.44</cell><cell>3.70</cell><cell>3.50</cell><cell>1.06</cell><cell>1.16</cell></row><row><cell>IITD</cell><cell>25.36</cell><cell>27.46</cell><cell>29.49</cell><cell>36.74</cell><cell>22.61</cell><cell>21.57</cell><cell>32.54</cell><cell>21.23</cell><cell>21.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V .</head><label>V</label><figDesc>THE PALMPRINT IDENTIFICATION ERROR RATES (%) OBTAINED USING THE HAMMING DISTANCE METRIC.</figDesc><table><row><cell cols="2">Err rates PolyU</cell><cell>Red</cell><cell>Green</cell><cell>Blue</cell><cell>NIR</cell><cell>IITD</cell></row><row><cell>DOC( θ n</cell><cell>2.59</cell><cell>4.61</cell><cell>5.13</cell><cell>5.05</cell><cell>3.97</cell><cell>31.95</cell></row><row><cell>=6)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DOC( θ n</cell><cell>2.79</cell><cell>4.82</cell><cell>5.24</cell><cell>4.98</cell><cell>4.25</cell><cell>32.87</cell></row><row><cell>=12)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table.I. Thus, the speed of matching score calculation for "single-orientation code" is also fast. It should be notice that the DOC consists two "single-orientation code". So the computational cost in DOC matching stage should be a little more than those of the Competitive code, BOCV, and E-BOCV approaches. To facilitate comparison with other coding approaches, several state-of-the-art coding approaches and the DOC approach are implemented by using MATLAB 8.1.0 on a PC with double-core Intel(R) i5-3470 (3.2GHz), RAM 8.00GB, and Windows 7.0 operating system. Code extraction and matching between two palmprints are performed for 10 times and the time taken in each phase are shown in Table.VI. The code extraction time taken in the DOC approach is about 410ms, which is comparable to the Competitive code, BOCV, and E-BOCV approaches. The matching time of DOC is longer than those of other coding approaches but it is still sufficient for the practical application.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE .</head><label>.</label><figDesc>VI, COMPUTATIONAL COSTS OF DIFFERENT APPROACHES.Orientation coding based palmprint recognition approaches consist of two main steps: palmprint orientation feature extraction and orientation feature matching. Using a reasonable orientation coding scheme and designing an effective coding based matching algorithm are two important issues in the coding based approaches. This paper analyzes the orientation feature extraction by using discrete filters</figDesc><table><row><cell>Approaches</cell><cell cols="2">Code ext Matching</cell><cell>Approaches</cell><cell cols="2">Code ext Matching</cell></row><row><cell>DOC</cell><cell>410ms</cell><cell>102ms</cell><cell>BOCV</cell><cell>395ms</cell><cell>11ms</cell></row><row><cell cols="2">Competitive code 383ms</cell><cell>41ms</cell><cell>E-BOCV</cell><cell>412ms</cell><cell>36ms</cell></row><row><cell>Ordinal code</cell><cell>196ms</cell><cell>67ms</cell><cell>RLOC</cell><cell>6.680s</cell><cell>437ms</cell></row><row><cell>Fusion code</cell><cell>40ms</cell><cell>5ms</cell><cell>Palmcode</cell><cell>19ms</cell><cell>3ms</cell></row><row><cell>6. CONCLUSIONS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT This paper is partially supported by the National Natural Science Foundation of China (Grant nos. 61370163, 61233011, 61332011 and 61162001), Shenzhen Municipal Science and Technology Innovation Council (Nos. JCYJ20130329151843309 and JCYJ20140904154630436), Jiangxi Provincial Science and Technology Support Project (20132BBF60083). Thanks to Dr. Edward C. Mignot, Shandong University, for linguistic advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to biometric recognition. Circuits and Systems for Video Technology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="20" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advanced pattern recognition technologies with applications to biometrics</title>
	</analytic>
	<monogr>
		<title level="j">Medical Information Science Reference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of palmprint recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1408" to="1418" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of hand vein recognition method. Mechatronics and Automation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2106" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">M-band wavelets application to palmprint recognition based on texture features</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="893" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locality preserving discriminant projections for face and palmprint recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2696" to="2707" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A biometric identification system based on eigenpalm and eigenfinger features. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ribaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fratric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1698" to="1709" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Does eigenpalm work? A system and evaluation perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2006. 18th International Conference on</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="445" to="448" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Research of Palmprint Recognition Based on 2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DPCA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>ISNN</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Palmprint identification using feature-level fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="487" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fusion of Face and Palmprint for Personal Identification Based on Ordinal</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Coarse to Fine Minutiae-Based Latent Palmprint Matching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2307" to="2322" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Palmprint verification based on principal lines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1316" to="1328" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust and Efficient Ridge-Based Palmprint Matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1618" to="1632" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards contactless palmprint authentication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ferre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Coarse to Fine Minutiae-Based Latent Palmprint Matching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2307" to="2322" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Joint 2D and 3D palmprint Matching with Alignment Refinement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="795" to="801" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Online System of Multi-spectral Palmprint Vefirication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="480" to="490" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<title level="m">Sparse Representation or Collaborative Representation: Which Helps Face Recognition, IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Two-Phase Test Sample Sparse Representation Method for Use With Face Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and system for video technology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Palmprint recognition by a two-phase test sample sparse representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hand-Based Biometrics (ICHB)</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online Palmprint Identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1041" to="1050" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Competitive Coding Scheme for Palmprint Verification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="520" to="523" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Palmprint verification based on robust line orientation code</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1504" to="1513" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The multiscale competitive code via sparse representation for palmprint verification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2265" to="2272" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Palmprint identification using feature-level fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="478" to="487" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Oridnal Palmprint Represention for Personal Identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="279" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Palmprint verification using binary orientation co-occurrence vector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1219" to="1227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fragile Bits in Palmprint Recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="663" to="666" />
			<date type="published" when="2012-10">October, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Encoding local image patterns using Riesz transforms: With application to palmprint and finger-knuckle-print recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1051</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High Confidence Visual Recognition of Persons by a Test of Statistical Independence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Gaugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneous detection of lines and edges using compound Gabor filters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Deemter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Du Buf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="757" to="777" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Performance Evaluation of Filter Design and Coding Schemes for Palmprint Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2008. 19th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A unified distance measurement for orientation coding in palmprint verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">950</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://www.comp.polyu.edu.hk/~biometrics/" />
		<title level="m">Multispectral palmprint database</title>
		<imprint/>
		<respStmt>
			<orgName>PolyU palmprint database</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">He is currently pursuing the Ph.D. degree in computer science and technology at Shenzhen Graduate School</title>
		<ptr target="http://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Palm.htm" />
	</analytic>
	<monogr>
		<title level="m">Lunke Fei received the B.S. and M.S. degree in computer science and technology from East China Jiaotong University</title>
		<meeting><address><addrLine>China; Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004 and 2007</date>
		</imprint>
		<respStmt>
			<orgName>Harbin Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include pattern recognition and biometrics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">His current research interests include pattern recognition, biometrics, bioinformatics, machine learning, image processing, and video analysis. Wenliang Tang received the M.S. degree in computer application technology from Sichuan University</title>
		<author>
			<orgName type="collaboration">Yong Xu received the B.S. and M.S.</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">degrees in 1994 and 1997, respectively, and the Ph.D. degree in pattern recognition and intelligence system from the Nanjing University of Science and Technology</title>
		<meeting><address><addrLine>Nanjing, China; Shenzhen, China; Chengdu, China; Nanchang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">2005. 1999</date>
		</imprint>
		<respStmt>
			<orgName>Shenzhen Graduate School, Harbin Institute of Technology ; East China Jiaotong University ; University of California, Los Angeles now</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include pattern recognition and WSN technology</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From 1986 to 1988, he was a postdoctoral fellow at Tsinghua University and then an associate professor at the Academia Sinca, Beijing. In 1994, he received his second Ph.D. in electrical and computer engineering from the University of Waterloo, Ontario, Canada. Currently, he is the Head, Department of Computing, and a chair professor at the Hong Kong Polytechnic University, where he is the founding director of the Biometrics Technology Centre (UGC/CRC) supported by the Hong Kong SAR government in 1998</title>
	</analytic>
	<monogr>
		<title level="m">He received his M.Sc. in computer science in 1982 and Ph</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>David Zhang graduated in computer science from Peking University ; Harbin Institute of Technology (HIT) ; Tsinghua University, and Adjunct Professor in Peking University, Shanghai Jiao Tong University, HIT and the University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>IJIG); book editor of the Springer International Series on Biometrics (SISB); organizer of the International Conference on Biometrics Authentication (ICBA</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Professor Zhang is a Croucher senior research fellow, distinguished speaker of the IEEE Computer Society, and fellow of both IEEE and IAPR. Corresponding author at: Bio-Computing Research Center</title>
	</analytic>
	<monogr>
		<title level="m">Associate Editor of more than 10 international journals, including IEEE Transactions and Pattern Recognition; and the author of more than 10 books and 200 journal papers</title>
		<imprint/>
		<respStmt>
			<orgName>Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen</orgName>
		</respStmt>
	</monogr>
	<note>China Email addresses: laterfall@hitsz.edu.cn (Yong Xu</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
