<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-04-04">4 April 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pradeep</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
							<email>p.atrey@uwinnipeg.ca</email>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Anwar</surname></persName>
							<email>anwar@mcrlab.uottawa.ca</email>
						</author>
						<author>
							<persName><forename type="first">Hossain</forename><forename type="middle">?</forename><surname>Abdulmotaleb</surname></persName>
						</author>
						<author>
							<persName><forename type="first">El</forename><surname>Saddik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">A El</forename><surname>Saddik</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Computer Science</orgName>
								<orgName type="institution">University of Winnipeg</orgName>
								<address>
									<settlement>Winnipeg</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Communications Research Laboratory</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-04-04">4 April 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s00530-010-0182-0</idno>
					<note type="submission">Received: 8 January 2009 / Accepted: 9 March 2010 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal information fusion ? Multimedia analysis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This survey aims at providing multimedia researchers with a state-of-the-art overview of fusion strategies, which are used for combining multiple modalities in order to accomplish various multimedia analysis tasks. The existing literature on multimodal fusion research is presented through several classifications based on the fusion methodology and the level of fusion (feature, decision, and hybrid). The fusion methods are described from the perspective of the basic concept, advantages, weaknesses, and their usage in various analysis tasks as reported in the literature. Moreover, several distinctive issues that influence a multimodal fusion process such as, the use of correlation and independence, confidence level, contextual information, synchronization between different modalities, and the optimal modality selection are also highlighted. Finally, we present the open issues for further research in the area of multimodal fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent times, multimodal fusion has gained much attention of many researchers due to the benefit it provides for various multimedia analysis tasks. The integration of multiple media, their associated features, or the intermediate decisions in order to perform an analysis task is referred to as multimodal fusion. A multimedia analysis task involves processing of multimodal data in order to obtain valuable insights about the data, a situation, or a higher level activity. Examples of multimedia analysis tasks include semantic concept detection, audio-visual speaker detection, human tracking, event detection, etc. Multimedia data used for these tasks could be sensory (such as audio, video, RFID) as well as non-sensory (such as WWW resources, database). These media and related features are fused together for the accomplishment of various analysis tasks. The fusion of multiple modalities can provide complementary information and increase the accuracy of the overall decision making process. For example, fusion of audio-visual features along with other textual information have become more effective in detecting events from a team sports video <ref type="bibr" target="#b149">[149]</ref>, which would otherwise not be possible by using a single medium.</p><p>The benefit of multimodal fusion comes with a certain cost and complexity in the analysis process. This is due to the different characteristics of the involved modalities, which are briefly stated in the following:</p><p>? Different media are usually captured in different formats and at different rates. For example, a video may be captured at a frame rate that could be different from the rate at which audio samples are obtained, or even two video sources could have different frame rates. Therefore, the fusion process needs to address this asynchrony to better accomplish a task. ? The processing time of different types of media streams are dissimilar, which influences the fusion strategy that needs to be adopted. ? The modalities may be correlated or independent. The correlation can be perceived at different levels, such as the correlation among low-level features that are extracted from different media streams and the correlation among semantic-level decisions that are obtained based on different streams. On the other hand, the independence among the modalities is also important as it may provide additional cues in obtaining a decision. When fusing multiple modalities, this correlation and independence may equally provide valuable insight based on a particular scenario or context. ? The different modalities usually have varying confidence levels in accomplishing different tasks. For example, for detecting the event of a human crying, we may have higher confidence in an audio modality than a video modality. ? The capturing and processing of media streams may involve certain costs, which may influence the fusion process. The cost may be incurred in units of time, money or other units of measure. For instance, the task of object localization could be accomplished cheaply by using a RFID sensor compared to using a video camera.</p><p>The above characteristics of multiple modalities influence the way the fusion process is carried out. Due to these varying characteristics and the objective tasks that need to be carried out, several challenges may appear in the multimodal fusion process as stated in the following:</p><p>? Levels of fusion. One of the earliest considerations is to decide what strategy to follow when fusing multiple modalities. The most widely used strategy is to fuse the information at the feature level, which is also known as early fusion. The other approach is decision level fusion or late fusion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b121">121]</ref> which fuses multiple modalities in the semantic space. A combination of these approaches is also practiced as the hybrid fusion approach <ref type="bibr" target="#b144">[144]</ref>. ? How to fuse? There are several methods that are used in fusing different modalities. These methods are particularly suitable under different settings and are described in this paper in greater detail. The discussion also includes how the fusion process utilizes the feature and decision level correlation among the modalities <ref type="bibr" target="#b102">[103]</ref>, and how the contextual <ref type="bibr" target="#b99">[100]</ref> and the confidence information <ref type="bibr" target="#b17">[18]</ref> influences the overall fusion process. ? When to fuse? The time when the fusion should take place is an important consideration in the multimodal fusion process. Certain characteristics of media, such as varying data capture rates and processing time of the media, poses challenges on how to synchronize the overall process of fusion. Often this has been addressed by performing the multimedia analysis tasks (such as event detection) over a timeline <ref type="bibr" target="#b28">[29]</ref>. A timeline refers to a measurable span of time with information denoted at designated points. The timeline-based accomplishment of a task requires identification of designated points at which fusion of data or information should take place. Due to the asynchrony and diversity among streams and due to the fact that different analysis tasks are performed at different granularity levels in time, the identification of these designated points, i.e. when the fusion should take place, is a challenging issue <ref type="bibr" target="#b7">[8]</ref>.</p><p>? What to fuse? The different modalities used in a fusion process may provide complementary or contradictory information and therefore knowing which modalities are contributing towards accomplishing an analysis task needs to be understood. This is also related to finding the optimal number of media streams <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b143">143]</ref> or feature sets required to accomplish an analysis task under the specified constraints. If the most suitable subset is unavailable, can one use alternate streams without much loss of cost-effectiveness and confidence?</p><p>This paper presents a survey of the research related to multimodal fusion for multimedia analysis in light of the above challenges. Existing surveys in this direction are mostly focused on a particular aspect of the analysis task, such as multimodal video indexing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b120">120]</ref>; automatic audio-visual speech recognition <ref type="bibr" target="#b105">[106]</ref>; biometric audiovisual speech synchrony <ref type="bibr" target="#b19">[20]</ref>; multi-sensor management for information fusion <ref type="bibr" target="#b146">[146]</ref>; face recognition <ref type="bibr" target="#b153">[153]</ref>; multimodal human computer interaction <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b96">97]</ref>; audio-visual biometric <ref type="bibr" target="#b4">[5]</ref>; multi-sensor fusion <ref type="bibr" target="#b78">[79]</ref> and many others. In spite of these literatures, a comprehensive survey focusing on the different methodologies and issues related to multimodal fusion for performing different multimedia analysis tasks is still missing. The presented survey aims to contribute in this direction. The fusion problems have also been addressed in other domains such as machine learning <ref type="bibr" target="#b47">[48]</ref>, data mining <ref type="bibr" target="#b23">[24]</ref> and information retrieval <ref type="bibr" target="#b133">[133]</ref>, however, the focus of this paper is restricted to the multimedia research domain.</p><p>Consequently, this work comments on the state-of-theart literature that uses different multimodal fusion strategies for various analysis tasks such as audio-visual person tracking, video summarization, multimodal dialog understanding, speech recognition and so forth. It also presents several classifications of the existing literature based on the fusion methodology and the level of fusion. Various issues such as the use of correlation, context and confidence, and the optimal modality selection that influences the performance of a multimodal fusion process is also critically discussed.</p><p>The remainder of this paper is organized as follows. In Sect. 2, we first address the issue levels of fusion and accordingly describe three levels (feature, decision and hybrid) of multimodal fusion, their characteristics, advantages and limitations. Section 3 addresses the issue how to fuse by describing the various fusion methods that have been used for multimedia analysis. These fusion methods have been elaborated under three different categories-the rule-based methods, the estimation-based methods, and the classification-based methods. In this section, we analyze various related works from the perspective of the level of fusion, the modality used, and the multimedia analysis task performed. A discussion regarding the different fusion methodologies and the works we analyzed is also presented here. Some other issues (e.g. the use of correlation, confidence, and the context), also related to how to fuse, are described in Sect. <ref type="bibr" target="#b3">4</ref>. This section further elaborates the issues when to fuse (the synchronization), and what to fuse (the optimal media selection). Section 5 provides a brief overview of the publicly available data sets and evaluation measures in multimodal fusion research. Finally, Sect. 6 concludes the paper by pointing out the open issues and possible avenues of further research in the area of multimodal fusion for multimedia analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Levels of fusion</head><p>The fusion of different modalities is generally performed at two levels: feature level or early fusion and decision level or late fusion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b121">121]</ref>. Some researchers have also followed a hybrid approach by performing fusion at the feature as well as the decision level.</p><p>Figure <ref type="figure">1</ref> shows different variants of the feature, decision, and hybrid level fusion strategies. We now describe the three levels of fusion and highlight their pros and cons. Various works that have adopted different fusion models at different levels (feature, decision and hybrid) in different scenarios will be discussed in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature level multimodal fusion</head><p>In the feature level or early fusion approach, the features extracted from input data are first combined and then sent as input to a single analysis unit (AU) that performs the analysis task. Here, features refer to some distinguishable properties of a media stream. For example, the feature fusion (FF) unit merges the multimodal features such as skin color and motion cues into a larger feature vector which is taken as the input to the face detection unit in order to detect a face. An illustration of this is provided in Fig. <ref type="figure">1</ref>. While Fig. <ref type="figure">1a</ref> shows an AU that receives a set of either features or decisions and provides a semantic-level decision, Fig. <ref type="figure">1b</ref> shows a FF unit that receives a set of features F 1 to F n and combines them into a feature vector F 1,n . Figure <ref type="figure">1d</ref> shows an instance of the feature level multimodal analysis task in which the extracted features are first fused using a FF unit and then the combined feature vector is passed to an AU for analysis.</p><p>In the feature level fusion approach, the number of features extracted from different modalities may be numerous, which may be summarized as <ref type="bibr" target="#b138">[138,</ref><ref type="bibr" target="#b150">150]</ref>:</p><p>? Visual features. It may include features based on color (e.g. color histogram), texture (e.g. measures of coarseness, directionality, contrast), shape (e.g. blobs), and so on. These features are extracted from the entire image, fixed-sized patches or blocks, segmented image blobs or automatically detected feature points. ? Text features. The textual features can be extracted from the automatic speech recognizer (ASR) transcript, video optical character recognition (OCR), video closed caption text, and production metadata. ? Audio features. The audio features may be generated based on the short time Fourier transform including the fast Fourier transform (FFT), mel-frequency cepstral coefficient (MFCC) together with other features such as zero crossing rate (ZCR), linear predictive coding (LPC), volume standard deviation, non-silence ratio, spectral centroid and pitch. ? Motion features. This can be represented in the form of kinetic energy which measures the pixel variation within a shot, motion direction and magnitude histogram, optical flows and motion patterns in specific directions. ? Metadata. The metadata features are used as supplementary information in the production process, such as the name, the time stamp, the source of an image or video as well as the duration and location of shots. They can provide extra information to text or visual features.</p><p>The feature level fusion is advantageous in that it can utilize the correlation between multiple features from different modalities at an early stage which helps in better task accomplishment. Also, it requires only one learning phase on the combined feature vector <ref type="bibr" target="#b121">[121]</ref>. However, in this approach it is hard to represent the time synchronization between the multimodal features <ref type="bibr" target="#b144">[144]</ref>. This is because the features from different but closely coupled modalities could be extracted at different times. Moreover, the features to be fused should be represented in the same format before fusion. In addition, the increase in the number of modalities makes it difficult to learn the cross-correlation among the heterogeneous features. Various approaches to resolve the synchronization problem are discussed in Sect. 4.2.</p><p>Several researchers have adopted the early fusion approach for different multimedia analysis tasks. For instance, Nefian et al. <ref type="bibr" target="#b85">[86]</ref> have adopted an early fusion approach in combining audio and visual features for speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decision level multimodal fusion</head><p>In the decision level or late fusion approach, the analysis units first provide the local decisions D 1 to D n (see Fig. <ref type="figure">1</ref>) that are obtained based on individual features F 1 to F n . The local decisions are then combined using a decision fusion (DF) unit to make a fused decision vector that is analyzed further to obtain a final decision D about the task or the hypothesis. Here, a decision is the output of an analysis unit at the semantic level. An illustration of DF unit is provided in Fig. <ref type="figure">1c</ref> whereas Fig. <ref type="figure">1e</ref> shows an instance of the decision level multimodal analysis in which the decisions obtained from various AUs are fused using a DF unit and the combined decision vector is further processed by an AU.</p><p>The decision level fusion strategy has many advantages over feature fusion. For instance, unlike feature level fusion, where the features from different modalities (e.g. audio and video) may have different representations, the decisions (at the semantic level) usually have the same representation. Therefore, the fusion of decisions becomes easier. Moreover, the decision level fusion strategy offers scalability (i.e. graceful upgradation or degradation) in terms of the modalities used in the fusion process, which is difficult to achieve in the feature level fusion <ref type="bibr" target="#b8">[9]</ref>. Another advantage of late fusion strategy is that it allows us to use</p><formula xml:id="formula_0">(a) AU D D F D 1 D 2 D n (b) (c) (d) (e) (f) F 1 D 1, n F F F 1 F 2 F n F 1,n / D 1 F 2 F F AU D F 1, n F 1 F n D F AU D AU F F F 1 F 2 F n F n-1 D F D 1,2 D n-1,n AU AU D n-1 D n F 1,2 D 1, n F 2 D F AU F 1 F n AU AU AU D 1 D 2 D n D 1, n D</formula><p>Fig. the most suitable methods for analyzing each single modality, such as hidden Markov model (HMM) for audio and support vector machine (SVM) for image. This provides much more flexibility than the early fusion.</p><p>On the other hand, the disadvantage of the late fusion approach lies in its failure to utilize the feature level correlation among modalities. Moreover, as different classifiers are used to obtain the local decisions, the learning process for them becomes tedious and time-consuming.</p><p>Several researchers have successfully adopted the decision level fusion strategy. For example, Iyenger et al. <ref type="bibr" target="#b56">[57]</ref> performed fusion of decisions obtained from a face detector and a speech recognizer along with their synchrony score by adopting two approaches-a linear weighted sum and a linear weighted product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid multimodal fusion</head><p>To exploit the advantages of both the feature level and the decision level fusion strategies, several researchers have opted to use a hybrid fusion strategy, which is a combination of both feature and decision level strategies. An illustration of the hybrid level strategy is presented in Fig. <ref type="figure">1f</ref> where the features are first fused by a FF unit and then the feature vector is analyzed by an AU. At the same time, other individual features are analyzed by different AUs and their decisions are fused using a DF unit. Finally, all the decisions obtained from the previous stages are further fused by a DF to obtain the final decision.</p><p>A hybrid fusion approach can utilize the advantages of both early and late fusion strategies. Therefore, many researchers ( <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b149">149]</ref>, etc.) have used the hybrid fusion strategy to solve various kinds of multimedia analysis problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods for multimodal fusion</head><p>In this section, we provide an overview of the different fusion methods that have been used by the multimedia researchers to perform various multimedia analysis tasks. The advantages and the drawbacks of each method are also highlighted. The fusion methods are divided into the following three categories: rule-based methods, classificationbased methods, and estimation-based methods (as shown in Fig. <ref type="figure">2</ref>). This categorization is based on the basic nature of these methods and it inherently means the classification of the problem space, such as, a problem of estimating parameters is solved by estimation-based methods. Similarly the problem of obtaining a decision based on certain observation can be solved by classification-based or rulebased methods. However, if the observation is obtained from different modalities, the method would require fusion of the observation scores before estimation or making a classification decision.</p><p>While the next three sections (Sect. 3.1-3.3) have been devoted to the above three classes of fusion methods; in the last section (Sect. 3.4), we present a comparative analysis of all the fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rule-based fusion methods</head><p>The rule-based fusion method includes a variety of basic rules of combining multimodal information. These include statistical rule-based methods such as linear weighted fusion (sum and product), MAX, MIN, AND, OR, majority voting. The work by Kittler et al. <ref type="bibr" target="#b68">[69]</ref> has provided the theoretical introduction of these rules. In addition to these rules, there are custom-defined rules that are constructed for the specific application perspective. The rule-based schemes generally perform well if the quality of temporal alignment between different modalities is good. In the following, we describe some representative works that have adopted the rule-based fusion strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Linear weighted fusion</head><p>Linear weighted fusion is one of the simplest and most widely used methods. In this method, the information obtained from different modalities is combined in a linear fashion. The information could be the low-level features (e.g. color and motion cues in video frames) <ref type="bibr" target="#b136">[136]</ref> or the semantic-level decisions (i.e. occurrence of an event) <ref type="bibr" target="#b89">[90]</ref>. To combine the information, one may assign normalized weights to different modalities. In literature, there are various methods for weight normalization such as minmax, decimal scaling, z score, tanh-estimators and sigmoid function <ref type="bibr" target="#b60">[61]</ref>. Each of these methods have pros and cons. The min-max, decimal scaling and z score methods are preferred when the matching scores (minimum and maximum values for min-max, maximum for decimal scaling and mean and standard deviation for z score) of the individual modalities can be easily computed. But these methods are sensitive to outliers. On the other hand, tanh Fig. <ref type="figure">2</ref> A categorization of the fusion methods normalization method is both robust and efficient but requires estimation of the parameters using training. Note that the absence of prior knowledge of the weights usually equals the weight assigned to them.</p><p>The general methodology of linear fusion can be described as follows. Let I i , 1 B i B n be a feature vector obtained from ith media source (e.g. audio, video etc.) or a decision obtained from a classifier. <ref type="foot" target="#foot_1">1</ref> Also, let w i , 1 B i B n be the normalized weight assigned to the ith media source or classifier. These vectors, assuming that they have the same dimensions, are combined by using sum or product operators and used by the classifiers to provide a high-level decision. This is shown in Eqs. 1 and 2, which are as follows:</p><formula xml:id="formula_1">I ? X n i?1 w i ? I i<label>?1?</label></formula><formula xml:id="formula_2">I ? Y n i?1 I i w i<label>?2?</label></formula><p>This method is computationally less expensive compared to other methods. However, a fusion system needs to determine and adjust the weights for the optimal accomplishment of a task. Several researchers have adopted the linear fusion strategy at the feature level for performing various multimedia analysis tasks. Examples include Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> for detecting and tracking people, and Wang et al. <ref type="bibr" target="#b136">[136]</ref> and Kankanhalli et al. <ref type="bibr" target="#b66">[67]</ref> for video surveillance and traffic monitoring. The linear fusion strategy has also been adopted at the decision level by several researchers. These include Neti et al. <ref type="bibr" target="#b86">[87]</ref> for speaker recognition and speech event detection, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> for monologue detection, Iyengar et al. <ref type="bibr" target="#b57">[58]</ref> for semantic concept detection and annotation in video, Lucey et al. <ref type="bibr" target="#b77">[78]</ref> for spoken word recognition, Hua and Zhang <ref type="bibr" target="#b54">[55]</ref> for image retrieval, McDonald and Smeaton <ref type="bibr" target="#b82">[83]</ref> for video shot retrieval and Jaffre and Pinquier <ref type="bibr" target="#b58">[59]</ref> for person identification. We briefly describe these works in the following.</p><p>Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref> used a linear weighted sum method to fuse trajectory information of the objects. The video data from each sensor in a distributed sensor network is processed for moving object detection (e.g. a blob). Once the blob locations are extracted from all sensors, their trajectory coordinates are averaged in a linear weighted fashion in order to estimate the correct location of the blob. The authors have also assigned weights to different sensors; however, the determination of these weights has been left to the user. Similar to <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> also performed linear weighted fusion of the location information of the objects. However, unlike Foresti and Snidaro <ref type="bibr" target="#b39">[40]</ref>, Yang et al. <ref type="bibr" target="#b152">[152]</ref> assigned equal weights to the different modalities.</p><p>The linear weighted sum strategy at the feature level has also been proposed by Wang et al. <ref type="bibr" target="#b136">[136]</ref> for human tracking. In this work, the authors have fused several spatial cues such as color, motion and texture by assigning appropriate weights to them. However, in the fusion process, the issue of how different weights should be assigned to different cues has not been discussed. This work was extended by Kankanhalli et al. <ref type="bibr" target="#b66">[67]</ref> for face detection, monologue detection, and traffic monitoring. In both works, the authors used a sigmoid function to normalize the weights of different modalities.</p><p>Neti et al. <ref type="bibr" target="#b86">[87]</ref> obtained individual decisions for speaker recognition and speech event detection from audio features (e.g. phonemes) and visual features (e.g. visemes). They adopted a linear weighted sum strategy to fuse these individual decisions. The authors used the training data to determine the relative reliability of the different modalities and accordingly adjusted their weights. Similar to this fusion approach, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> fused multiple modalities (face, speech and the synchrony score between them) by adopting two approaches at the decision level-a linear weighted sum and a linear weighted product. This methodology was applied for monologue detection. The synchrony or correlation between face and speech has been computed in terms of mutual information between them by considering the audio and video features as locally Gaussian distributed. The mutual information is a measure of the information of one modality conveyed about another. The weights of the different modalities have been determined at the training stage. While fusing different modalities, the authors have found the linear weighted sum approach to be a better option than the linear weighted product for their data set. This approach was later extended for semantic concept detection and annotation in video by Iyengar et al. <ref type="bibr" target="#b57">[58]</ref>. Similar to <ref type="bibr" target="#b56">[57]</ref>, the linear weighted product fusion strategy has also been adopted by Jaffre and Pinquier <ref type="bibr" target="#b58">[59]</ref> for fusing different modalities. In this work, the authors have proposed a multimodal person identification system by automatically associating voice and image using a standard product rule. The association is done through fusion of video and audio indexes. The proposed work used a common indexing mechanism for both audio and video based on frame-by-frame analysis. The audio and video indexes were fused using a product fusion rule at the late stage.</p><p>In another work, Lucey et al. <ref type="bibr" target="#b77">[78]</ref> performed a linear weighted fusion for the recognition of spoken words. The word recognizer modules, which work on audio and video data separately, provided decisions about a word in terms of the log likelihoods. These decisions are linearly fused by assigning weights to them. To determine the weights of the two decision components, the authors have chosen the discrete values (0, 0.5 and 1), which is a simple but nonrealistic choice.</p><p>A decision level fusion scheme proposed by Hua and Zhang <ref type="bibr" target="#b54">[55]</ref> is based on the human's psychological observations which they call ''attention''. The core idea of this approach is to fuse the decisions taken based on different cues such as the strength of a sound, the speed of a motion, the size of an object and so forth. These cues are considered as the attention properties and are measured by obtaining the set of features including color histogram, color moment, wavelet, block wavelet, correlogram, and blocked correlogram. The authors proposed a new fusion function which they call ''attention fusion function''. This new function is a variation of a linear weighted sum strategy and is derived by adding the difference of two decisions to their average (please refer to <ref type="bibr" target="#b54">[55]</ref> for formalism). The authors have demonstrated the utility of the proposed attention based fusion model for image retrieval. Experimental results have shown that the proposed approach performed better in comparison to average or maximal fusion rules.</p><p>In the context of video retrieval, McDonald and Smeaton <ref type="bibr" target="#b82">[83]</ref> have employed a decision level linear weighted fusion strategy to combine the normalized scores and ranks of the retrieval results. The normalization was performed using max-min method. The video shots were retrieved using different modalities such as text and multiple visual features (color, edge and texture). In this work, the authors found that the combining of scores with different weights has been best for combining text and visual results for TRECVid type searches, while combining scores and ranks with equal weights have been best for combining multiple features for a single query image. A similar approach was adopted by Yan et al. <ref type="bibr" target="#b151">[151]</ref> for reranking the video. In this work, the authors used a linear weighted fusion strategy at the decision level in order to combine the retrieval scores obtained based on text and other modalities such as audio, video and motion.</p><p>From the works discussed above, it is observed that the optimal weight assignment is the major drawback of the linear weighted fusion method. The issue of finding the appropriate weight (or confidence level) for different modalities is an open research issue. This issue is further elaborated in Sect. 4.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Majority voting</head><p>Majority voting is a special case of weighted combination with all weights to be equal. In majority voting based fusion, the final decision is the one where the majority of the classifiers reach a similar decision <ref type="bibr" target="#b113">[113]</ref>. For example, Radova and Psutka <ref type="bibr" target="#b107">[108]</ref> have presented a speaker identification system by employing multiple classifiers. Here, the raw speech samples from the speaker are treated as features. From the speech samples, a set of patterns are identified for each speaker. The pattern usually contains a current utterance of several vowels. Each pattern is classified by two different classifiers. The output scores of all the classifiers were fused in a late integration approach to obtain the majority decision regarding the identity of the unknown speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Custom-defined rules</head><p>Unlike the above approaches that use standard statistical rules, Pfleger <ref type="bibr" target="#b99">[100]</ref> presented a production rule-based decision level fusion approach for integrating inputs from pen and speech modality. In this approach, each input modality (e.g. pen input) is interpreted within its context of use, which is determined based on the previously recognized input events and dialog states belonging to the same user turn. The production rule consists of a weighting factor and a condition-action part. These rules are further divided into three classes that work together to contribute to the fusion process. First, the synchronization rules are applied to track the processing state of the individual recognizer (e.g. speech recognizer) and in case of pending recognition results the other classes of rules are not fired to ensure synchronization. Second, the rules for multimodal event interpretation are used to determine which of the input events has the lead and need to be integrated. Furthermore, there may be conflicting events due to the recognition or interpretation error, which are addresses by obtaining the event with highest score. Third, the rules for unimodal interpretations are adopted when one of the recognizers do not produce any meaningful result, for example a time-out by one recognizer, which will lead to a single modality based decision making. This approach is further extended <ref type="bibr" target="#b100">[101]</ref> and applied for discourse processing in a multiparty dialog scenario.</p><p>In another work, Holzapfel et al. <ref type="bibr" target="#b48">[49]</ref> showed an example of multimodal integration approach using customdefined rules. The authors combined speech and 3D pointing gestures as a means of natural interaction with a robot in a kitchen. Multimodal fusion is performed at the decision level based on the n-best lists generated by each of the event parsers. Their experiments showed that there is a close correlation in time of speech and gesture. Similarly, in <ref type="bibr" target="#b31">[32]</ref>, a rule-based system has been proposed for fusion of speech and 2D gestures in human computer interaction.</p><p>Here the audio and gesture modalities are fused at the decision level. A drawback of these approaches is the overhead to determine the best action based on n-best fused input.</p><p>In addition to the video, audio and gesture, other modalities such as closed caption text and external metadata have been used for several applications such as video indexing and content analysis for team sports videos. On this account, Babaguchi et al. <ref type="bibr" target="#b11">[12]</ref> presented a knowledgebased technique to leverage the closed caption text of broadcast video streams for indexing video shots based on the temporal correspondence between them. The closed caption text features are extracted as keywords and the video features are extracted as temporal changes of color distribution. This work presumably integrates textual and visual modalities using a late fusion strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Remarks on rule-based fusion methods</head><p>A summary of all the works (related to the rule-based fusion methods) described above is provided in Table <ref type="table" target="#tab_2">1</ref>. As can be seen from the table, in rule-based fusion category, linear weighted fusion method has been widely used by researchers. It is a simple as well as computationally less expensive approach. This method performs well if the weights of different modalities are appropriately determined, which has been a major issue in using this method. In the existing literature, this method has been used for face detection, human tracking, monologue detection, speech and speaker recognition, image and video retrieval, and person identification. On the other hand, the fusion using custom-defined rules has the flexibility of adding rules based on the requirements. However, in general, these rules are domain specific and defining the rules requires proper knowledge of the domain. This fusion method is widely used in the domain of multimodal dialog systems and sports video analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification-based fusion methods</head><p>This category of methods includes a range of classification techniques that have been used to classify the multimodal observation into one of the pre-defined classes. The methods in this category are the support vector machine, Bayesian inference, Dempster-Shafer theory, dynamic Bayesian networks, neural networks and maximum entropy model. Note that we can further classify these methods as generative and discriminative models from the machine learning perspective. For example, Bayesian inference and dynamic Bayesian networks are generative models, while support vector machine and neural networks are discriminative models. However, we skip further discussion on such classification for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Support vector machine</head><p>Support vector machine (SVM) <ref type="bibr" target="#b22">[23]</ref> has become increasingly popular for data classification and related tasks. More specifically, in the domain of multimedia, SVMs are being used for different tasks including feature categorization, concept classification, face detection, text categorization, modality fusion, etc. Basically SVM is considered as a supervised learning method and is used as an optimal binary linear classifier, where a set of input data vectors are partitioned as belonging to either one of the two learned classes. From the perspective of multimodal fusion, SVM is used to solve a pattern classification problem, where the input to this classifier is the scores given by the individual classifier. The basic SVM method is extended to create a non-linear classifier by using the kernel concept, where every dot product in the basic SVM formalism is replaced using a non-linear kernel function.</p><p>Many existing literature use the SVM-based fusion scheme. Adams et al. <ref type="bibr" target="#b2">[3]</ref> adopted a late fusion approach in order to detect semantic concepts (e.g. sky, fire-smoke) in videos using visual, audio and textual modalities. They use a discriminate learning approach while fusing different modalities at the semantic level. For example, the scores of all intermediate concept classifiers are used to construct a vector that is passed as the semantic feature in SVM as shown in Fig. <ref type="figure">3</ref>. This figure depicts that audio, video and text scores are combined in a high-dimensional vector before being classified by SVM. The black and white dots in the figure represent two semantic concepts. A similar approach has been adopted by Iyengar et al. <ref type="bibr" target="#b57">[58]</ref> for concept detection and annotation in video.</p><p>Wu et al. <ref type="bibr" target="#b141">[141]</ref> reported two approaches to study the optimal combination of multimodal information for video concept detection, which are gradient-descent-optimization linear fusion (GLF) and the super-kernel nonlinear fusion (NLF). In GLF, an individual kernel matrix is first constructed for each modality providing a partial view of the target concept. The individual kernel matrices are then fused based on a weighted linear combination scheme. Gradient-descent technique is used to find the optimal weights to combine the individual kernels. Finally, SVM is used on the fused kernel matrix to classify the target concept. Unlike GLF, the NLF method is used for nonlinear combination of multimodal information. This method is based on <ref type="bibr" target="#b2">[3]</ref>, where SVM is first used as a classifier for the individual modality and then super kernel non-linear fusion is applied for optimal combination of the individual classifier models. The experiments on the TREC-2003 Video Track benchmark showed that NLF and GLF performed 8.0 and 5.0% better than the best single modality, respectively. Furthermore, NLF had an average 3.0% better performance than GLF. The NLF fusion approach was later extended by the authors <ref type="bibr" target="#b143">[143]</ref> in order to obtain the best independent modalities (early fusion) and the strategy to fuse the best modalities (late fusion).</p><p>A hybrid fusion approach has been presented by Ayache et al. <ref type="bibr" target="#b10">[11]</ref> as normalized early fusion and contextual late fusion for semantic indexing of multimedia resources using visual and text cues. Unlike other works, in case of normalized early fusion, each entry of the concatenated vector is normalized and then fused. In the case of contextual late fusion, the second layer classifier based on SVM is used to exploit the contextual relationship between the different concepts. Here, the authors have also presented a kernelbased fusion scheme based on SVMs, where the kernel functions are chosen according to the different modalities.</p><p>In the area of image classification, Zhu et al. <ref type="bibr" target="#b156">[156]</ref> have reported a multimodal fusion framework to classify the images that have embedded text within their spatial coordinates. The fusion process followed two steps. At first, a bag-of-words model <ref type="bibr" target="#b72">[73]</ref> is applied to classify the given image that considers the low-level visual features. In parallel, the text detector finds the text existence in the image using text color, size, location, edge density, brightness, contrast, etc. In the second step, a pair-wise SVM classifier is used for fusing the visual and textual features together. This is illustrated in Fig. <ref type="figure" target="#fig_0">4</ref>.</p><p>In a recent work, Bredin and Chollet <ref type="bibr" target="#b18">[19]</ref> proposed a biometric-based identification scheme of a talking face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio scores</head><formula xml:id="formula_3">I 1,1 I 1,2 I 1,3 I 2,1 I 2,2 I 2,3 I 3,1 I 3,2 Video scores Text scores Concept space 1 Concept space 2</formula><p>High dimension vector SVM boundary Fig. <ref type="figure">3</ref> SVM based score space classification of combined information from multiple intermediate concepts <ref type="bibr" target="#b2">[3]</ref> The key idea was to utilize the synchrony measure between the talking face's voice and the corresponding video frames. Audio and visual sample rates are balanced by linear interpolation. By adopting a late fusion approach, the scores from the monomodal biometric speaker verification, face recognition, and synchrony were combined and passed to the SVM model, which provided the decision about the identity of the talking face. On another front, Aguilar et al. <ref type="bibr" target="#b3">[4]</ref> provided a comparison between the rule-based fusion and learning-based fusion (trained) strategy. The scores of face, fingerprint and online signature are combined using both the Sum rule and radial basis function SVM (RBF SVM) for comparison. The experimental results demonstrates that learning-based RBF SVM scheme outperforms the rule-based scheme based on some appropriate parameter selection.</p><p>Snoek et al. <ref type="bibr" target="#b121">[121]</ref> have compared both the early and late fusion strategies for semantic video analysis. Using the former approach, the visual vector has been concatenated with the text vector and then normalized to use as input in SVM to learn the semantic concept. In the latter approach the authors have adopted a probabilistic aggregation mechanism. Based on an experiment on 184 h of broadcast video using 20 semantic concepts, this study concluded that a late fusion strategy provided better performance for most concepts, but it bears an increased learning effort. The conclusion also suggested that when the early fusion performed better, the improvements were significant. However, which of the fusion strategies is better in which case needs further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Bayesian inference</head><p>The Bayesian inference is often referred to as the 'classical' sensor fusion method because it has been widely used and many other methods are based on it <ref type="bibr" target="#b44">[45]</ref>. In this method, the multimodal information is combined as per the rules of probability theory <ref type="bibr" target="#b78">[79]</ref>. The method can be applied at the feature level as well as at the decision level. The observations obtained from multiple modalities or the decisions obtained from different classifiers are combined, and an inference of the joint probability of an observation or a decision is derived <ref type="bibr" target="#b109">[109]</ref>.</p><p>The Bayesian inference fusion method is briefly described as follows. Let us fuse the feature vectors or the decisions ?I 1 ; I 2 ; . . .; I n ? obtained from n different modalities. Assuming that these modalities are statistically independent, the joint probability of an hypothesis H based on the fused feature vectors or the fused decisions can be computed as <ref type="bibr" target="#b101">[102]</ref>:</p><formula xml:id="formula_4">p?HjI 1 ; I 2 ; . . .; I n ? ? 1 N Y n k?1 p?I k jH? w k<label>?3?</label></formula><p>where N is used to normalize the posterior probability estimate p?HjI 1 ; I 2 ; . . .; I n ?: The term w j is the weight of the kth modality, and P n k?1 w j ? 1: This posterior probability is computed for all the possible hypotheses, E. The hypothesis that has the maximum probability is determined using the MAP rule ? ? argmax H2E p?HjI 1 ; I 2 ; . . .; I n ?:</p><p>The Bayesian inference method has various advantages. Based on the new observations, it can incrementally compute the probability of the hypothesis being true. It allows for any prior knowledge about the likelihood of the hypothesis to be utilized in the inference process. The new observation or the decision is used to update the a priori probability in order to compute the posterior probability of the hypothesis. Moreover, in the absence of empirical data, this method permits the use of a subjective probability estimate for the a priori of hypotheses <ref type="bibr" target="#b140">[140]</ref>.</p><p>These advantages of the Bayesian method are seen as its limitations in some cases. Bayesian inference method requires a priori and the conditional probabilities of the hypothesis to be well defined <ref type="bibr" target="#b110">[110]</ref>. In absence of any knowledge of suitable priors, the method does not perform well. For example, in gesture recognition scenarios, it is sometimes difficult to classify a gesture of two stretched fingers in ''V'' form. This gesture can be interpreted as either ''victory sign'' or ''sign indicating number two''. In this case, since the priori probability of both the classes would be 0.5, the Bayesian method would provide ambiguous results. Another limitation of this method is that it is often found unsuitable for handling mutually exclusive hypotheses and general uncertainty. It means that only one hypothesis can be true at any given time. For example, Bayesian inference method would consider two events of human's running and walking mutually exclusive and cannot handle a fuzzy event of human's fast walking or slow running.</p><p>Bayesian inference method has been successfully used to fuse multimodal information (at the feature level and at the decision level) for performing various multimedia analysis tasks. An example of Bayesian inference fusion at the feature level is the work by Pitsikalis et al. <ref type="bibr" target="#b101">[102]</ref> for audio-visual speech recognition. Meyer et al. <ref type="bibr" target="#b84">[85]</ref> and Xu and Chua <ref type="bibr" target="#b149">[149]</ref> have used the Bayesian inference method at the decision level for spoken digit recognition and sports video analysis, respectively; while Atrey et al. <ref type="bibr" target="#b7">[8]</ref> employed this fusion strategy at both the feature as well as the decision level for event detection in the multimedia surveillance domain. These works are described in the following. Pitsikalis et al. <ref type="bibr" target="#b101">[102]</ref> used the Bayesian inference method to combine the audio-visual feature vectors. The audio feature vector included 13 static MFCC and their derivatives, while the visual feature vector was formed by concatenating 6 shapes and 12 texture features. Based on the combined features, the joint probability of a speech segment is computed. In this work, the authors have also proposed to model the measurement of noise uncertainty.</p><p>At the decision level, Meyer et al. <ref type="bibr" target="#b84">[85]</ref> fused the decisions obtained from speech and visual modalities. The authors have first extracted the MFCC features from speech and the lip contour features from the speaker's face in the video, and then obtained individual decisions (in terms of probabilities) for both using HMM classifiers. These probability estimates are then fused using the Bayesian inference method to estimate the joint probability of a spoken digit. Similar to this work, Xu and Chua <ref type="bibr" target="#b149">[149]</ref> also used the Bayesian inference fusion method for integrating the probabilistic decisions about the offset and non-offset events detected in a sport video. These events have been detected by fusing audio-visual features with textual clues and by employing a HMM classifier. In this work, the authors have shown that the Bayesian inference has comparable accuracy to the rule-based schemes.</p><p>In another work, Atrey et al. <ref type="bibr" target="#b7">[8]</ref> adopted a Bayesian inference fusion approach at hybrid levels (feature level as well as decision level). The authors demonstrated the utility of this fusion (they call it 'assimilation') approach for event detection in a multimedia surveillance scenario. The feature level assimilation was performed at the intramedia stream level and the decision level assimilation was adopted at the inter-media stream level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Dempster-Shafer theory</head><p>Although the Bayesian inference fusion method allows for uncertainty modeling (usually by Gaussian distribution), some researchers have preferred to use the Dempster-Shafer (D-S) evidence theory since it uses belief and plausibility values to represent the evidence and their corresponding uncertainty <ref type="bibr" target="#b110">[110]</ref>. Moreover, the D-S method generalizes the Bayesian theory to relax the Bayesian inference method's restriction on mutually exclusive hypotheses, so that it is able to assign evidence to the union of hypotheses <ref type="bibr" target="#b140">[140]</ref>.</p><p>The general methodology of fusing the multimodal information using the D-S theory is as follows. The D-S reasoning system is based on a fundamental concept of ''the frame of discernment'', which consists of a set H of all the possible mutually exclusive hypotheses. An hypothesis is characterized by belief and plausibility. The degree of belief implies a lower bound of the confidence with which a hypothesis is detected as true, whereas the plausibility represents the upper bound of the possibility that the hypothesis could be true. A probability is assigned to every hypothesis H 2 P?H? using a belief mass function m : P?H? ! 0; 1 ? . The decision regarding a hypothesis is measured by a ''confidence interval'' bounded by its basic belief and plausibility values, as shown in Fig. <ref type="figure">5</ref>.</p><p>When there are multiple independent modalities, the D-S evidence combination rule is used to combine them. Precisely, the mass of a hypothesis H based on two modalities, I i and I j , is computed as:</p><formula xml:id="formula_5">?m i ? m j ??H? ? P I i \I j ?H m i ?I i ?m j ?I j ? 1 ? P I i \I j ?? m i ?I i ?m j ?I j ?<label>?4?</label></formula><p>Note that, the weights can also be assigned to different modalities that are fused. Although the Dempster-Shafer fusion method has been found more suitable for handling mutually inclusive hypotheses, this method suffers from the combinatorial explosion when the the number of frames of discernment is large <ref type="bibr" target="#b26">[27]</ref>.</p><p>Some of the representative works that have used the D-S fusion method for various multimedia analysis tasks are Bendjebbour et al. <ref type="bibr" target="#b15">[16]</ref> (at hybrid level) and Mena and Malpica <ref type="bibr" target="#b83">[84]</ref> (at the feature level) for segmentation of satellite images, Guironnet et al. <ref type="bibr" target="#b43">[44]</ref> for video classification, Singh et al. <ref type="bibr" target="#b116">[116]</ref> for finger print classification, and <ref type="bibr" target="#b110">[110]</ref> for human computer interaction (at the decision level).</p><p>Bendjebbour et al. <ref type="bibr" target="#b15">[16]</ref> proposed to use the D-S theory to fuse the mass functions of two regions (cloud and no B e l i e f P l a u s i b i l i t y Sum of all evidences in favor of the hypothesis Sum of all evidences against the hypothesis Fig. <ref type="figure">5</ref> An illustration of the belief and plausibility in the D-S theory <ref type="bibr" target="#b140">[140]</ref> cloud) of the image obtained from radar. They performed fusion at two levels the feature level and the decision level. At the feature level, the pixel intensity was used as a feature and the mass of a given pixel based on two sensors was computed and fused; while at the decision level, the decisions about a pixel obtained from the HMM classifier were used as mass and then the HMM outputs were combined. Similar to this work, Mena and Malpica <ref type="bibr" target="#b83">[84]</ref> also used the D-S fusion approach for the segmentation of color images for extracting information from terrestrial, aerial or satellite images. However, they extracted the information of the same image from three different sources: the location of an isolated pixel, a group of pixels, and a pair of pixels. The evidences obtained based on the location analysis were fused using the D-S evidence fusion strategy. Guironnet et al. <ref type="bibr" target="#b43">[44]</ref> extracted low-level (color or texture) descriptors from a TREC video and applied a SVM classifier to recognize the pre-defined concepts (e.g. 'beach' or 'road') based on each descriptor. The SVM classifier outputs are integrated using the D-S fusion approach, they call it the ''transferable belief model''. In the area of biometrics, Singh et al. <ref type="bibr" target="#b116">[116]</ref> used the D-S theory to combine the output scores of three different finger print classification algorithms based on the Minutiae, ridge and image pattern features. The authors showed that the D-S theory of fusing three independent evidences outperformed the individual approaches. Recently, Reddy <ref type="bibr" target="#b110">[110]</ref> also used the D-S theory for fusing the outputs of two sensors, the Hand Gesture sensor and the Brain Computing Interface sensor. Two concepts, ''Come'' and ''Here'' were detected using these two sensors. The fusion results showed that the D-S fusion approach helps in resolving the ambiguity in the sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Dynamic Bayesian networks</head><p>Bayesian inferencing can be extended to a network (graph) in which the nodes represent random variable (observations or states) of different types, e.g. audio and video; and the edges denote their probabilistic dependencies. For example, as shown in Fig. <ref type="figure">6</ref>, a speaker detection problem can be depicted by a Bayesian Network <ref type="bibr" target="#b29">[30]</ref>. The speaker node value is determined based on the value of three intermediate nodes 'visible', 'frontal' and 'speech', which are inferred from the measurement nodes 'skin', 'texture', 'face', 'mouth' and 'sound'. The figure shows the dependency of nodes upon each other. However, the network shown in Fig. <ref type="figure">6</ref> is a static one, meaning it depicts the state at a particular time instant.</p><p>A Bayesian Network works as a dynamic Bayesian network (DBN) when the temporal aspect being added to it as shown in Fig. <ref type="figure">7</ref> t-1 t Fig. <ref type="figure">7</ref> An example of a dynamic Bayesian networks <ref type="bibr" target="#b29">[30]</ref> these models describe the observed data in terms of the process that generate them, they are called generative. They are termed as probabilistic because they describe probabilistic distributions rather than the sensor data. Moreover, since they have useful graphical representations, they are also called graphical <ref type="bibr" target="#b14">[15]</ref>. Although the DBNs have been used with different names such as probabilistic generative models, graphical models, etc. for a variety of applications, the most popular and simplest form of a DBN is the HMM. The DBNs have a clear advantage over the other methods in two aspects. First, they are capable of modeling the multiple dependencies among the nodes. Second, by using them, the temporal dynamics of multimodal data can easily be integrated <ref type="bibr" target="#b119">[119]</ref>. These advantages make them suitable for various multimedia analysis tasks that require decisions to be performed using time-series data. Although DBNs are very beneficial and widely used, the determination of the right DBN state is often seen as its problem <ref type="bibr" target="#b80">[81]</ref>.</p><p>In the following, we briefly outline some representative works that have used DBN in one form or the other. Wang et al. <ref type="bibr" target="#b138">[138]</ref> have used HMM for video shot classification. The authors have extracted both audio (cepstral vector) and visual features (a gray-level histogram difference and two motion features) from each video frame and used them as the input data for the HMM. While this method used a single HMM that processed the joint audio-visual features, Nefian et al. <ref type="bibr" target="#b85">[86]</ref> used the coupled HMM (CHMM), which is a generalization of the HMM. The CHMM suits to multimodal scenarios where two or more streams need to be integrated. In this work, the authors have modeled the state asynchrony of the audio features (MFCC) and visual features (2D-DCT coefficients of the lips region) while preserving their correlation over time. This approach is used for speech recognition. The work by Adams et al. <ref type="bibr" target="#b2">[3]</ref> also used a Bayesian network in addition to SVM and showed the comparison of both for video shot retrieval.</p><p>Unlike Nefian et al. <ref type="bibr" target="#b85">[86]</ref> who used CHMM, Bengio <ref type="bibr" target="#b16">[17]</ref> has presented the asynchronous HMM (AHMM) at the feature level. The AHMM is a variant of HMM to deal with the asynchronous data streams. The authors modeled the joint probability distribution of asynchronous sequencesspeech (MFCC features) stream and video (shape and intensity features) stream that described the same event. This method was used for biometric identity verification.</p><p>Nock et al. <ref type="bibr" target="#b89">[90]</ref> and <ref type="bibr" target="#b90">[91]</ref> employed a set of HMMs trained on joint sequences of audio and visual data. The features used were MFCC from speech and DCT coefficients of the lip region in the face from video. The joint features were presented to the HMMs at consecutive time instances in order to locate a speaker. The authors also computed the mutual information (MI) between the two types of features and analyzed its effect on the overall speaker location results. Similar to this work, Beal et al. <ref type="bibr" target="#b14">[15]</ref> have used graphical models to fuse audio-visual observations for tracking a moving object in a cluttered, noisy environment. The authors have modeled audio and video observations jointly by computing their mutual dependencies. The expectation-maximization algorithm has been used to learn the model parameters from a sequence of audio-visual data. The results were demonstrated in a two microphones and one camera setting. Similarly, Hershey et al. <ref type="bibr" target="#b45">[46]</ref> also used a probabilistic generative model to combine audio and video by learning the dependencies between the noisy speech signal from a single microphone and the fine-scale appearance and location of the lips during speech.</p><p>It is important to note that all works described above assume multiple modalities (usually audio-visual data) that locally, as well as jointly, follow a Gaussian distribution. In contrast to these works, Fisher et al. <ref type="bibr" target="#b38">[39]</ref> have presented a non-parametric approach to learn the joint distribution of audio and visual features. They estimated a linear projection onto low-dimensional subspaces to maximize the mutual information between the mapped random variables. This approach was used for audio-video localization. Although the non-parametric approach is free from any parametric assumptions, they often suffer from implementation difficulties as the method results in a system of undetermined equations. That is why, the parametric approaches have been preferred <ref type="bibr" target="#b91">[92]</ref>. With this rationale, Noulas and Krose <ref type="bibr" target="#b91">[92]</ref> also presented a two-layer Bayesian network model for human face tracking. In the first layer, the independent modalities (audio and video) are analyzed, while the second layer performs the fusion incorporating their correlation. A similar approach was also presented by Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> for tracking humans in a cluttered environment. Recently, Town <ref type="bibr" target="#b131">[131]</ref> also used the Bayesian networks approach for multi-sensory fusion. In this work, the visual information obtained from the calibrated cameras is integrated with the ultrasonic sensor data at the decision level to track people and devices in an office building. The authors presented a large-scale sentient computing system known as ''SPIRIT''.</p><p>In the context of news video analysis, Chua et al. <ref type="bibr" target="#b30">[31]</ref> have emphasized on the need to utilize multimodal features (text with audio-visual) for segmenting news video into story units. In their other work <ref type="bibr" target="#b24">[25]</ref>, the authors presented an HMM-based multi-modal approach for news video story segmentation by using a combination of features. The feature set included visual-based features such as color, object-based features such as face, video-text, temporal features such as audio and motion, and semantic features such as cue-phrases. Note that the fundamental assumption which is often considered with the DBN methods is the independence among different observations/features. However, this assumption does not hold true in reality. To relax the assumption of independence between observations, Ding and Fan <ref type="bibr" target="#b37">[38]</ref> presented a segmental HMM approach to analyze a sports video. In segmental HMM, each hidden state emits a sequence of observations, which is called a segment. The observations within a segment are considered to be independent to the observations of other segments. The authors showed that the segmental HMM performed better than traditional HMM. In another work, the importance of combining text modality with the other modalities has been demonstrated by Xie et al. <ref type="bibr" target="#b145">[145]</ref>. The authors proposed a layered dynamic mixture model for topic clustering in video. In their layer approach, first a hierarchical HMM is used to find clusters in audio and visual streams; and then latent semantic analysis is used to cluster the text from the speech transcript stream. At the next level, a mixture model is adopted to learn the joint probability of the clusters from the HMM and latent semantic analysis. The authors have performed experiments with the TRECVID 2003 data set, which demonstrated that the multi-modal fusion resulted in a higher accuracy in topics clustering.</p><p>An interesting work was presented by Wu et al. <ref type="bibr" target="#b142">[142]</ref>. In this work, the authors used an influence diagram approach (a form of the Bayesian network) to represent the semantics of photos. The multimodal fusion framework integrated the context information (location, time and camera parameters), content information (holistic and perceptual local features) with the domain-oriented semantic ontology (represented by a directed acyclic graph). Moreover, since the conditional probabilities that are used to infer the semantics can be misleading, the authors have utilized the causal strength between the context/content and semantic ontology instead of using the correlation among features. The causal strength is based on the following idea. The two variables may co-vary with each other, however, there may be a third variable as a ''cause'' that may affect the value of these two variables. For example, two variables ''wearing a warm jacket'' and ''drinking coffee'' may have a large positive correlation; however, the cause behind both could be ''cold weather''. The authors have shown that the usage of causal strength in influence diagrams provide better results in the automatic annotation of photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Neural networks</head><p>Neural network (NN) is another approach for fusing multimodal data. Neural networks are considered a non-linear black box that can be trained to solve ill-defined and computationally expensive problems <ref type="bibr" target="#b140">[140]</ref>. The NN method consists of a network of mainly three types of nodes-input, hidden and output nodes. The input nodes accept sensor observations or decisions (based on these observations), and the output nodes provide the results of fusion of the observations or decisions. The nodes that are neither input nor output are referred to hidden nodes. The network architecture design between the input and output nodes is an important factor for the success or failure of this method. The weights along the paths, that connect the input nodes to the output nodes, decide the input-output mapping behavior. These weights can be adjusted during the training phase to obtain the optimal fusion results <ref type="bibr" target="#b21">[22]</ref>. This method can also be employed at both the feature level and the decision level.</p><p>In the following, we describe some works that illustrate the use of the NN fusion method for performing the multimedia analysis tasks. Gandetto et al. <ref type="bibr" target="#b40">[41]</ref> have used the NN fusion method to combine sensory data for detecting human activities in an environment equipped with a heterogeneous network of sensors with CCD cameras and computational units working together in a LAN. In this work, the authors considered two types of sensors-state sensors (e.g. CPU load, login process, and network load) and observation sensors (e.g. cameras). The human activities in regard to usage of laboratory resources were detected by fusing the data from these two types sensors at the decision level.</p><p>A variation of the NN fusion method is the time-delay neural network (TDNN) that has been used to handle temporal multimodal data fusion. Some researchers have adopted the TDNN approach for various multimedia analysis tasks, e.g. Cutler and Davis <ref type="bibr" target="#b33">[34]</ref>, Ni et al. <ref type="bibr" target="#b87">[88]</ref>, and Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> for speaker tracking. Cutler and Davis <ref type="bibr" target="#b33">[34]</ref> learned the correlation between audio and visual streams by using a TDNN method. The authors have used it for locating the speaking person in the scene. A similar approach was also presented by Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref>. In <ref type="bibr" target="#b158">[158]</ref>, the authors have also compared the TDNN approach with the BN approach and found that the BN approach performed better than the TDNN approach in many aspects. First, the choosing of the initial parameters does not affect the DBN approach while it does affect the TDNN approach. Second, the DBN approach was better in modeling the joint Gaussian distribution of audio-visual data compared to linear mapping between audio signals and the object position in video sequences in the TDNN method. Third, the graphical models provide an explicit and easily accessible structure compared to TDNN, in which, the inner structure and parameters are difficult to design. Finally, the DBN approach offers better tracking accuracy. Moreover, in the DBN approach, a posteriori probability of the estimates is available as the quantitative measure in the support of the decision.</p><p>While Cutler and Davis <ref type="bibr" target="#b33">[34]</ref> and Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> used the NN fusion approach at the feature level, Ni et al. <ref type="bibr" target="#b87">[88]</ref> adopted this approach at the feature level as well as at the decision level. In <ref type="bibr" target="#b87">[88]</ref>, the authors have used the NN fusion method to fuse low level features to recognize images. The decisions from multiple trained NN classifiers are further fused to come up with a final decision about an image.</p><p>Although the NN method is in general found suitable to work in a high-dimensional problem space and generate high-order nonlinear mapping, there are some familiar complexities associated with them. For instance, the selection of appropriate network architecture for a particular application is often difficult. Moreover, this method also suffers from slow training. Due to these limitations and other shortcomings (stated above as compared to the BN method), the NN method has not been often used for multimedia analysis tasks compared to other fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Maximum entropy model</head><p>In general, maximum entropy model is a statistical classifier which follows an information-theoretic approach and provides a probability of an observation belonging to a particular class based on the information content it has. This method has been used by few researchers for categorizing the fused multimedia observations into respective classes.</p><p>The maximum entropy model based fusion method is briefly described as follows. Let I i and I j are the two different types of input observations. The probability of these observations belonging to a class X can be given by an exponential function:</p><formula xml:id="formula_6">P?XjI i ; I j ? ? 1 Z?I i ; I j ? e F?I i ;I j ?<label>?5?</label></formula><p>where, F(I i , I j ) is the combined feature (or decision) vector and Z(I i , I j ) is the normalization factor to ensure a proper probability.</p><p>Recently, this fusion method has been used by Magalha ?es and Ru ?ger <ref type="bibr" target="#b79">[80]</ref> for semantic multimedia indexing. In this work, the authors combined the text and image based features to retrieve the images. The authors found that the maximum entropy model based fusion worked better than the Naive Bayes approach.</p><p>There are other works such as Jeon and Manmatha [63] and Argillander et al. <ref type="bibr" target="#b6">[7]</ref>, which have used maximum entropy model for multimedia analysis tasks, however in these works the authors used only single modality rather than multiple modalities. Therefore, the discussion of these works is out of scope for our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7">Remarks on classification-based fusion methods</head><p>All the representative works related to the classificationbased fusion methods are summarized in Table <ref type="table" target="#tab_4">2</ref>. Our observations are as follows:</p><p>? The Bayesian inference fusion method, which works on probabilistic principles, provides easy integration of new observation and the use of a priori information. However, they are not suitable for handling mutually exclusive hypotheses. Moreover, the lack of appropriate a priori information can lead to inaccurate fusion results using this method. On the other hand, Dempster-Shafer fusion methods are good at handling certainty and mutually exclusive hypotheses. However, in this method, it is hard to handle the large number of combinations of hypotheses. This method has been used for speech recognition, sports video analysis and event detection tasks. ? The dynamic Bayesian networks have been widely used to deal with time-series data. This method is a variation of the Bayesian Inference when used over time. The DBN method in its different forms (such as HMMs) has been successfully used for various multimedia analysis tasks such as speech recognition, speaker identification and tracking, video shot classification etc. However, in this method, it is often difficult to determine the right DBN states. Compared to DBN, the neural networks fusion method is generally suitable to work in a highdimensional problem space and it generates a highorder nonlinear mapping, which is required in many realistic scenarios. However, due to the complex nature of a network, this method suffers from slow training. ? As can be seen from the table, among various classification-based fusion methods, SVM and DBN have been widely used by researchers. SVMs have been preferred due to their improved classification performance while the DBNs have been found more suitable to model temporal data. ? There are various other classification methods used in multimedia research. These include decision tree <ref type="bibr" target="#b75">[76]</ref>, relevance vector machines <ref type="bibr" target="#b35">[36]</ref>, logistics regression <ref type="bibr" target="#b70">[71]</ref> and boosting <ref type="bibr" target="#b74">[75]</ref>. However, these methods have been used more for the traditional classification problems than for the fusion problems. Hence, we skip the description of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimation-based fusion methods</head><p>The estimation category includes the Kalman filter, extended Kalman filter and particle filter fusion methods. These methods have been primarily used to better estimate the state of a moving object based on multimodal data. For example, for the task of object tracking, multiple modalities such as audio and video are fused to estimate the position of the object. The details of these methods are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Kalman filter</head><p>The Kalman filter (KF) <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b112">112]</ref> allows for real-time processing of dynamic low-level data and provides state estimates of the system from the fused data with some statistical significance <ref type="bibr" target="#b78">[79]</ref>. For this filter to work, a linear dynamic system model with Gaussian noise is assumed, where at time t, the system true state, x(t) and its observation, y(t) are modeled based on the state at time t -1. Precisely, this is represented using the state-space model given by Eqs. 6 and 7 in the following:</p><formula xml:id="formula_7">x?t? ? A?t?x?t ? 1? ? B?t?I?t? ? w?t? ? 6? y?t? ? H?t?x?t? ? v?t? ?<label>7?</label></formula><p>where, A(t) is the transition model, B(t) is the control input model, I(t) is the input vector, H(t) is the observation model, w(t) * N(0, Q(t)) is the process noise as a normal distribution with zero mean and Q(t) covariance, and v(t) * N(0, R(t)) is the observation noise as a normal distribution with zero mean and R(t) covariance. Based on the above state-space model, the KF does not require to preserve the history of observation and only depends on the state estimation data from the previous timestamp. The benefit is obvious for systems with less storage capabilities. However, the use of the KF is limited to the linear system model and is not suitable for the systems with non-linear characteristics. For non-linear system models, a variant of the Kalman filter known as extended Kalman filter (EKF) <ref type="bibr" target="#b111">[111]</ref> is usually used. Some researchers also use KF as inverse Kalman filter (IKF) that reads an estimate and produce an observation as oppose to KF that reads an observation and produce an estimate <ref type="bibr" target="#b124">[124]</ref>. Therefore, a KF and its associated IKF can logically be arranged in series to generate the observation at the output. Another variant of KF has gained attention lately, which is the unscented Kalman filter (UKF) <ref type="bibr" target="#b64">[65]</ref>. The benefit of UKF is that it does not have a linearization step and the associated errors.</p><p>The KF is a popular fusion method. Loh et al. <ref type="bibr" target="#b76">[77]</ref> proposed a feature level fusion method for estimating the translational motion of a single speaker. They used different audio-visual features for estimating the position, velocity and acceleration of the single sound source. For position estimation in 3D space, the measurement of three microphones are used in conjunction with the camera image point. Given the position estimate, a KF is then  <ref type="bibr" target="#b124">[124]</ref> focused on the localization and tracking of single objects. The audio and video localization features are computed in terms of position estimates. EKF is used due to the non-linear estimates based on audio-based position. On the other hand, a basic KF is used at the video camera level. The outputs of the audio and video estimates are then fused within the fusion center, which is comprised of two single-input inverse KFs and a two-input basic KFs. This is shown in Fig. <ref type="figure">8</ref>. This work requires that the audio and video sources are in sync with each other. Likewise, Talantzis et al. <ref type="bibr" target="#b125">[125]</ref> have adopted a decentralized KF that fuses audio and video modalities for better location estimation in real time. A decision level fusion approach has been adopted in this work.</p><p>A recent work <ref type="bibr" target="#b154">[154]</ref> presents a multi-camera based tracking system, where multiple features such as spatial position, shape and color information are integrated together to track object blobs in consecutive image frames. The trajectories from multiple cameras are fused at feature level to obtain the position and velocity of the object in the real world. The fusion of trajectories from multiple cameras, which uses EKF, enables better tracking even when the object view is occluded. Gehrig et al. <ref type="bibr" target="#b42">[43]</ref> also adopted an EKF based fusion approach using audio and video features. Based on the observation of the individual audio and video sensor, the state of the KF was incrementally updated to estimate the speaker's position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Particle filter</head><p>Particle filters are a set of sophisticated simulation-based methods, which are often used to estimate the state distribution of the non-linear and non-Gaussian state-space model <ref type="bibr" target="#b5">[6]</ref>. These methods are also known as Sequential Monte Carlo (SMC) methods <ref type="bibr" target="#b32">[33]</ref>. In this approach, the particles represent the random samples of the state variable, where each particle is characterized by an associated weight. The particle filtering algorithm also consists of a prediction and update steps. The prediction step propagates each particle as per its dynamics while the update step reweighs a particle according to the latest sensory information. While the KF, EKF or IKF are optimal only for linear Gaussian processes, the particle methods can provide Bayesian optimal estimates for non-linear non-Gaussian processes when sufficiently large number of samples are taken.</p><p>The particle methods have been widely used in multimedia analysis. For instance, Vermaak et al. <ref type="bibr" target="#b132">[132]</ref> used particle filters to estimate the predictions from audio-and video-based observations. The reported system uses a single camera and a pair of microphones and were tested based on stored audio-visual sequences. The fusion of audio-visual features took place at the feature level, meaning that the individual particle coordinates from the features of both modalities were combined to track the speaker. Similar to this approach, Perez et al. <ref type="bibr" target="#b98">[99]</ref> adopted the particle filter approach to fuse 2D object shapes and audio information for speaker tracking. However, unlike Vermaak et al. <ref type="bibr" target="#b132">[132]</ref>, the latter uses the concept of importance particle filter, where audio information was specifically used to generate an importance function that influenced the computation of audio-based observation likelihood. The audio and video-based observation likelihoods are then combined as a late fusion scheme using a standard probabilistic product formula that forms the multimodal particle.</p><p>A probabilistic particle filter framework is proposed by Zotkin et al. <ref type="bibr" target="#b157">[157]</ref> that adopts a late fusion approach for tracking people in a videoconferencing environment. This framework used multiple cameras and microphones to estimate the 3D coordinates of the person using the sampled projection. Multimodal particle filters are used to approximate the posterior distribution of the system parameters and the tracking position in the audio-visual state-space model. Unlike Vermaak et al. <ref type="bibr" target="#b132">[132]</ref> or Perez et al. <ref type="bibr" target="#b98">[99]</ref>, this framework enables tracking multiple persons simultaneously.</p><p>Nickel et al. <ref type="bibr" target="#b88">[89]</ref> presented an approach for real-time tracking of the speaker using multiple cameras and microphones. This work used particle filters to estimate the location of the speaker by sampled projection as proposed 8 Extended/decentralized Kalman filter in the fusion process <ref type="bibr" target="#b124">[124]</ref> by Zotkin et al. <ref type="bibr" target="#b157">[157]</ref>, where each particle filter represented a 3D coordinate in the space. The evidence from all the camera views and microphones are adjusted to assign weights to the corresponding particle filter. Finally, the weighted mean of a particle set is considered as the speaker location. This work adopted a late fusion approach to obtain the final decision.</p><formula xml:id="formula_8">KF 1 KF 2 KF 1 -1 KF 2 -1 KF y 1 [k ] y 2 [k ] y 2 [k ] y 1 [k ] Fusion Center Local Processor x [ k|k ] x 2 [k |k ] x 1 [k |k ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Remarks on estimation-based fusion methods</head><p>The representative works in the estimation-based category are summarized in Table <ref type="table" target="#tab_6">3</ref>. The estimation-based fusion methods (Kalman filter, extended Kalman filter and particle filter) are generally used to estimate and predict the fused observations over a period. These methods are suitable for object localization and tracking tasks. While the Kalman filter is good for the systems with a linear model, the extended Kalman filter is better suited for non-linear systems. However, the particle filter method is more robust for non-linear and non-Gaussian models as they approach the Bayesian optimal estimate with sufficiently large number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Further discussion</head><p>In the following, we provide our observations based on the analysis of the fusion methods described above.</p><p>? Most used methods. From the literature, it has been observed that many fusion methods such as linear weighted fusion, SVM, and DBN have been used more often in comparison to the other methods. This is due to the fact the linear weighted fusion can be easily used to prioritize different modalities while fusing; SVM has improved classification performance in many multimedia analysis scenarios; and the DBN fusion method is capable of handling temporal dependencies among multimodal data, which is an important issue often considered in multimodal fusion. ? Fusion methods and levels of fusion. Existing literature suggest that linear weighted fusion is suitable to work at the decision level. Also, although SVM is generally used to classify individual modalities at the feature level, in the case of multimodal fusion, the outputs of individual SVM classifiers are fused and further classified using another SVM. That is why most of the reported works have been seen to fall into the late fusion category. Among others, the DBNs have been used more at the feature level due to its suitability in handling temporal dependencies. ? Modalities used. The modalities that have been used for multimodal fusion are mostly based on audio and video. Some works also considered text modality, while others have investigated gesture. ? Multimedia analysis tasks versus fusion methods. In Table <ref type="table" target="#tab_7">4</ref>, we summarize the existing literature in terms of the multimedia analysis tasks and the different fusion methods used for these tasks. This may be useful for the readers as a quick reference in order to decide which fusion method would be suitable for which task. It has been found that for a variety of tasks, various fusion methodologies have been adopted. However, based on the nature of a multimedia analysis task, some fusion methods have been preferred over the others. For  Bayesian inference Atrey et al. <ref type="bibr" target="#b7">[8]</ref> Dynamic Bayesian networks Town <ref type="bibr" target="#b131">[131]</ref>, Beal et al. <ref type="bibr" target="#b14">[15]</ref> Neural networks Gandetto et al. <ref type="bibr" target="#b40">[41]</ref>, Zou and Bhanu <ref type="bibr" target="#b158">[158]</ref> Kalman filter Talantzis et al. <ref type="bibr" target="#b125">[125]</ref>, Zhou and Aggarwal <ref type="bibr" target="#b154">[154]</ref>, Strobel et al. <ref type="bibr" target="#b124">[124]</ref> Human computer interaction and multimodal dialog system Custom-defined rules Corradini et al. <ref type="bibr" target="#b31">[32]</ref>, Pfleger <ref type="bibr" target="#b99">[100]</ref>, Holzapfel et al. <ref type="bibr" target="#b48">[49]</ref> Dempster-Shafer theory Reddy <ref type="bibr" target="#b110">[110]</ref> Image segmentation, classification, recognition, and retrieval Linear weighted fusion Hua and Zhang <ref type="bibr" target="#b54">[55]</ref> Support vector machine Zhu et al. <ref type="bibr" target="#b156">[156]</ref> Neural networks Ni et al. <ref type="bibr" target="#b87">[88]</ref> Dempster-Shafer Theory Mena and Malpica <ref type="bibr" target="#b83">[84]</ref>, Bendjebbour et al. <ref type="bibr" target="#b15">[16]</ref> Video classification and retrieval Linear weighted fusion Yan et al. <ref type="bibr" target="#b151">[151]</ref>, McDonald and Smeaton <ref type="bibr" target="#b82">[83]</ref> Bayesian inference Xu and Chua <ref type="bibr" target="#b149">[149]</ref> Dempster-Shafer Theory Singh et al. <ref type="bibr" target="#b116">[116]</ref> Dynamic Bayesian networks Wang et al. <ref type="bibr" target="#b138">[138]</ref>, Ding and Fan <ref type="bibr" target="#b37">[38]</ref>, Chaisorn et al. <ref type="bibr" target="#b24">[25]</ref>, Xie et al. <ref type="bibr" target="#b145">[145]</ref>, Adams et al. <ref type="bibr" target="#b2">[3]</ref> Photo and video annotation Linear weighted fusion Iyenger et al. <ref type="bibr" target="#b57">[58]</ref> linear weighted fusion method is applied to the applications which have lesser computational needs. On other hand, while the dynamic Bayesian networks fusion method is computationally more expensive than the others, the neural networks can be trained to computationally expensive problems. Regarding the time delay and synchronization problems, customdefined rules have been found more appropriate as they are usually application specific. These time delays may occur due to the resource constraints since the input data can be obtained from different types of multimedia sensors and the different CPU resources may be available for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distinctive issues of multimodal fusion</head><p>This section provides a critical look at the distinctive issues that should be considered in a multimodal fusion process. These issues have been identified in the light of the following three aspects of fusion: how to fuse (in continuation with the fusion methodologies as discussed in Sect. 3), when to fuse, and what to fuse. From the aspect of how to fuse, we will elaborate in Sect. 4.1 on the issues of the use of correlation, confidence and the contextual information while fusing different modalities. The when to fuse aspect is related to the synchronization between different modalities which will be discussed in Sect. 4.2. We will cover what to fuse aspect by describing the issue of optimal modality selection in Sect. 4.3. In the following, we highlight the importance of considering these distinctive issues and also describe the past works related to them.</p><p>4.1 Issues related to how to fuse</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Correlation between different modalities</head><p>The correlation among different modalities represents how they co-vary with each other. In many situations, the correlation between them provides additional cues that are very useful in fusing them. Therefore, it is important to know different methods of computing correlations and to analyze them from the perspective of how they affect fusion <ref type="bibr" target="#b102">[103]</ref>.</p><p>The correlation can be comprehended at various levels, e.g. the correlation between low level features and the correlation between semantic-level decisions. Also, there are different forms of correlation that have been utilized by the researchers in the multimodal fusion process. The correlation between features has been computed in the forms of correlation coefficient, mutual information, latent semantic analysis (also called lament semantic indexing), canonical correlation analysis, and cross-modal factor analysis. On the other hand, the decision level correlation has been exploited in the form of causal link analysis, causal strength and agreement coefficient. In the following, we describe the above eight forms of correlation and their usage for the various multimedia analysis tasks. We also cast light on the cases where independence between different modalities can be useful for multimedia analysis tasks. A summary of the representative works that have used correlation in different forms is provided in Table <ref type="table" target="#tab_8">5</ref>.</p><p>Correlation coefficient. The correlation coefficient is a measure of the strength and direction of a linear relationship between any two modalities. It has been widely used by multimedia researchers for joint modeling the audiovideo relationship <ref type="bibr" target="#b138">[138,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b14">15]</ref>. However, to jointly model the audio-video, the authors have often assumed them-(1) to be independent, and (2) to locally and jointly follow the Gaussian distribution.</p><p>One of the most simple and widely used forms of the correlation coefficient is the Pearson's product-moment coefficient <ref type="bibr" target="#b19">[20]</ref>, which is computed as follows. Assuming that I i and I j are the two modalities (of same or different types). The correlation coefficient CC(I i , I j ) between them can be computed as <ref type="bibr" target="#b138">[138]</ref>:</p><formula xml:id="formula_9">CC?I i ; I j ? ? ??I i ; I j ? ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ??I i ; I i ? q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ??I j ; I j ? q<label>?8?</label></formula><p>where ??I i ; I j ? is the (i, j)th element of the covariance matrix C, which is given as:</p><formula xml:id="formula_10">C ? X N k?1 ?I k i ? I m i ? ? ?I k j ? I m j ? ?<label>9?</label></formula><p>where I k i and I k j are the kth value in the feature vector I i and I j , respectively; and I m i and I m j are the mean values of these feature vectors. This method of computing correlation has been used by many researchers such as Wang et al. <ref type="bibr" target="#b138">[138]</ref> and Li et al. <ref type="bibr" target="#b73">[74]</ref>. In <ref type="bibr" target="#b73">[74]</ref>, based on the Correlation Coefficient between audio and face feature vectors, the authors have selected the faces having the maximum correlation with the audio.</p><p>Mutual information. The mutual information is a information theoretic measure of correlation that represents the amount of information one modality conveys about another. The mutual information MI(I i , I j ) between two modalities I i and I j , which are normally distributed with variances R I i and R I j , and jointly distributed with covariance R I i I j ; is computed as:</p><formula xml:id="formula_11">MI?I i ; I j ? ? 1 2 log jR I i jjR I j j jR I i I j j<label>?10?</label></formula><p>There are several works that have used mutual information as a measure of synchrony between audio and video. For instance, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref> computed the synchrony between face and speech using mutual information. The authors found that the face region had high mutual information with the speech data. Therefore, the mutual information score helped locate the speaker. Similarly, Fisher et al. <ref type="bibr" target="#b38">[39]</ref> also learned the linear projections from a joint audio-video subspace where the mutual information was maximized. Other works that have used mutual information as a measure of correlation are Darrell et al. <ref type="bibr" target="#b34">[35]</ref>, Hershey and Movellan <ref type="bibr" target="#b46">[47]</ref>, Nock et al. <ref type="bibr" target="#b89">[90]</ref>, Nock et al. <ref type="bibr" target="#b90">[91]</ref> and Noulas and Krose <ref type="bibr" target="#b91">[92]</ref> for different tasks as detailed in Table <ref type="table" target="#tab_8">5</ref>. Latent semantic analysis. Latent semantic analysis (LSA) is a technique often used for text information retrieval. This technique has proven useful to analyze the semantic relationships between different textual units. In the context of text information retrieval, the three primary goals that the LSA technique achieves are dimension reduction, noise removal and finding of the semantic and hidden relation between keywords and documents. The LSA technique has also been used to uncover the correlation between audio-visual modalities for talking-face detection <ref type="bibr" target="#b73">[74]</ref>. The learning correlation using LSA consists of four steps: construction of a joint multimodal feature space, normalization, singular value decomposition and measuring semantic association <ref type="bibr" target="#b27">[28]</ref>. The mathematical details can be found in Li et al. <ref type="bibr" target="#b73">[74]</ref>. In <ref type="bibr" target="#b73">[74]</ref>, the authors demonstrated the superiority of LSA over the traditional correlation coefficient.</p><p>Canonical correlation analysis. Canonical correlation analysis (CCA) is another powerful statistical technique that can be used to find linear mapping that maximizes the cross-correlation between two feature sets. Given two feature sets I i and I j , the CCA is a set of two linear projections A and B that whiten I i and I j . A and B are called canonical correlation matrices. These matrices are constructed under the constraints that their cross-correlation becomes diagonal and maximally compact in the projected space. The computation details of A and B can be found in <ref type="bibr" target="#b117">[117]</ref>. The first M vectors of A and B are used to compute the synchrony score CCA(I i , I j ) between two modalities I i and I j as:</p><formula xml:id="formula_12">CCA?I i ; I j ? ? 1 M X M m?1 jcorr?a T m I i ; b T m I j ?j<label>?11?</label></formula><p>where a T m and b T m are elements of A and B. It is important to note that finding the canonical correlations and maximizing the mutual information between the sets are considered equivalent if the underlying distributions are elliptically symmetric <ref type="bibr" target="#b27">[28]</ref>.</p><p>The canonical correlation analysis for computing the synchrony score between modalities has been explored by few researchers. For instance, Chetty and Wagner <ref type="bibr" target="#b27">[28]</ref> used the CCA score between audio and video modalities for biometric person authentication. In this work, the authors also used LSA and achieved about 42% overall improvement in error rate with CCA and 61% improvement with LSA. In another work, Bredin and Chollet <ref type="bibr" target="#b19">[20]</ref> also demonstrated the utility of considering CCA for audio-video based talking-face identity verification. Similarly, Slaney and Covell <ref type="bibr" target="#b117">[117]</ref> used CCA for talking-face detection.</p><p>Cross-modal factor analysis. The weakness of the LSA method lies in its inability to distinguish features from different modalities in the joint space. To overcome this weakness, Li et al. <ref type="bibr" target="#b71">[72]</ref> proposed the cross-modal factor analysis (CFA) method, in which, the features from different modalities are treated as two subsets and the semantic patterns between these two subsets are discovered. The method works as follows. Let the two subsets of features be I i and I j . The objective is to find the orthogonal transformation matrices A and B that can minimize the expression:</p><formula xml:id="formula_13">I i A ? I j B 2 F<label>?12?</label></formula><p>where A T A and B T B are unit matrices. denotes the Frobenius norm and is calculated for the matrix M as:</p><formula xml:id="formula_14">M k k F ? X x X y jm xy j 2 ! 1=2<label>?13?</label></formula><p>By solving the above equation for optimal transformation matrices A and B, the transformed version of I i and I j can be calculated as follows:</p><formula xml:id="formula_15">?i ? I i A; ?j ? I j B<label>?14?</label></formula><p>The optimized vectors ?i and ?j represent the coupled relationships between the two feature subsets I i and I j . Note that, unlike CCA, the CFA provides a feature selection capability in addition to feature dimension reduction and noise removal. These advantages make CFA a promising tool for many multimedia analysis tasks. The authors in <ref type="bibr" target="#b71">[72]</ref> have shown that although all three methods (LSA, CCA and CFA) achieved significant dimensionality reduction, the CFA gave the best results for talking head analysis. The CFA method achieved 91% detection accuracy as compared to the LSA (66.1%) and the CCA (73.9%).</p><p>All the methods described above have computed the correlation between the features extracted from different modalities. In the following, we describe the methods that have been used to compute the correlation at the semantic (or decision) level.</p><p>Causal link analysis. The events that happen in an environment are often correlated. For instance, the events of ''elevator pinging'', ''elevator door opening'', ''people coming out of elevator'' usually occur at relative times one after another. This temporal relationship between events has been utilized by Stauffer <ref type="bibr" target="#b123">[123]</ref> for detecting events in a surveillance environment. This kind of analysis of the events has been called the casual link analysis.</p><p>Assuming that the two events were linked, the likelihood p?c i ; c j ; dt ij jc i;j ? 1? of a pair (c i , c j ) of events and their relative times dt ij is estimated. Note that, dt ij is the time difference between the absolute times of event i and event j. The term c i,j = 1 indicates that the occurrence of the first event c i is directly responsible for the occurrence of the second event c j . Once the estimates of the posterior likelihood of c i,j = 1 for all i, j pairs of events have been computed, an optimal chaining hypothesis is iteratively determined. The authors have demonstrated that the casual link analysis significantly helps in the overall accuracy of event detection in an audio-video surveillance environment.</p><p>Causal strength. The causal strength is a measure of the cause due to which two variables may co-vary with each other. Wu et al. <ref type="bibr" target="#b142">[142]</ref> have preferred to use the causal strength between the context/content and the semantic ontology as described in Sect. 3.2.4. Here we describe how the causal strength is computed by adopting a probabilistic model. Let u and d be chance and decision variables, respectively. The chance variables imply effects (e.g. wearing warm jacket and drinking hot coffee) and the decision variables denote causes (e.g. cold weather). The causal strength CS u|d is computed by using Eq. 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS ujd ?</head><p>P?ujd; n? ? P?u; n? 1 ? P?u; n?</p><p>In the above equation, n refers to the state of the world, e.g. indoor or outdoor environment in the above mentioned example; and the terms P(u|d, n) and P(u, n) are the conditional probability assuming that d and n are independent of each other. The authors have shown that the usage of causal strength provides not only improved accuracy of photo annotation, but also better capability of assessing the annotation quality.</p><p>Agreement coefficient. Atrey et al. <ref type="bibr" target="#b7">[8]</ref> have used the correlation among streams at the intra-media stream and inter-media stream levels. At the intra-media stream level, they used the traditional correlation coefficient; however, at the inter-media stream level, they introduced the notion of the decision level ''agreement coefficient''. The agreement coefficient among streams has been computed based on how concurring or contradictory the evidence is that they provide. Intuitively, the higher the agreement among the streams, the more confidence one would have in the global decision, and vice versa <ref type="bibr" target="#b115">[115]</ref>.</p><p>The authors have modeled the agreement coefficient in the context of event detection in a multimedia surveillance scenario. The agreement coefficient c k i;j ?t? between the media streams I i and I j detects the kth event at time instant t, by iteratively averaging the past agreement coefficients with the current observation. Precisely, c k i;j ?t? is computed as:</p><formula xml:id="formula_17">c k i;j ?t? ? ?1 ? 2 ? jp i;k ?t? ? p j;k ?t?j? ? c k i;j ?t ? 1?<label>?16?</label></formula><p>where, p i,k (t) and p j,k (t) are the individual probabilities of the occurrence of kth event based on the media streams I i and I j , respectively, at time t C 1; and c k i;j ?0? ? 1 ? 2 ? jp i;k ?0? ? p j;k ?0?j . These probabilities represent decisions about the events. Exactly the same probabilities would imply full agreement (c k i;j ? 1) while detecting the kth event whereas totally dissimilar probabilities would mean that the two streams fully contradict each other (c k i;j ? ?1). The authors have shown that the usage of agreement coefficient resulted in better overall event detection accuracy in a surveillance scenario.</p><p>Independence. In should be noted that, in addition to using the correlation among modalities, the independent modalities can also be very useful in some cases to obtain a better decision. Let us consider the case of a multimodal dialog system <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b99">100]</ref>. In such systems, multiple modalities such as gesture and speech are used as a means of interaction. It is sometimes very hard to fuse these modalities at the feature level due to a lack of direct correspondence between their features and different temporal alignment. However, each modality can complement each other in obtaining a decision about the intended interaction event. To this regard, each modality can be processed separately in parallel to derive individual decisions and later fuse these individual decisions at a semantic level to obtain the final decision <ref type="bibr" target="#b95">[96]</ref>. Similarly, other cases of independence among modalities are also possible. For instance, environment context, device context, network context, task context and so forth may provide complementary information to the fusion process, thereby making the overall analysis tasks more robust and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Confidence level of different modalities</head><p>Different modalities may have varying capabilities of accomplishing a multimedia analysis task. For example, in good lighting condition, the video analysis may be more useful in detecting human than the audio analysis; while in a dark environment, the audio analysis could be more handy. Therefore, in the fusion process, it is important to assign the appropriate confidence level to the participating streams <ref type="bibr" target="#b115">[115]</ref>. The confidence in a stream is usually expressed by assigning appropriate weight to it.</p><p>Many fusion methods such as the linear weight fusion and the Bayesian inference do have a notion of specifying the weights to different modalities. However, the main question that remains to be answered is how to determine the weights of different modalities. These weights can vary based on several factors such as the context and the task performed. Therefore, the weight should be dynamically adjusted in order to obtain optimal fusion results.</p><p>While performing multimodal fusion, several researchers have adopted the strategy of weighting different modalities. However, many of them either have considered equal weights <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b152">152]</ref> or have not elaborated the issue of weight determination <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b136">136]</ref>, and have left it to the users to decide <ref type="bibr" target="#b39">[40]</ref>.</p><p>Other works, such as Neti et al. <ref type="bibr" target="#b86">[87]</ref>, Iyenger et al. <ref type="bibr" target="#b56">[57]</ref>, Tatbul et al. <ref type="bibr" target="#b126">[126]</ref>, Hsu and Chang <ref type="bibr" target="#b52">[53]</ref> and Atrey et al. <ref type="bibr" target="#b7">[8]</ref> have used pre-computed weights in the fusion process. The weights of different streams have usually been determined based on their past accuracy or any prior knowledge. The computation of past accuracy requires a significant amount of training and testing. However, since the process of computing the accuracy has to be performed in advance, the confidence value or the weight determined based on such accuracy value is considered ''static'' during the fusion process. It is obvious that a static value of confidence of a modality does not reflect its true current value especially under the changing context. On the other hand, determining the confidence level for each stream, based on its past accuracy, is difficult. This is because the system may provide dissimilar accuracies for various tasks under different contexts. Pre-computation of accuracies of all the streams for various detection tasks under varying contexts requires a significant amount of training and testing, which is often tedious and time consuming. Therefore, a mechanism that can determine the confidence levels of different modalities ''on the fly'' without pre-computation, needs to be explored.</p><p>In contrast to the above methods that used the static confidence, some efforts (e.g. Tavakoli et al. <ref type="bibr" target="#b127">[127]</ref>, Atrey et al. <ref type="bibr" target="#b9">[10]</ref> have also been performed towards the dynamic computation of the confidence levels of different modalities. Tavakoli et al. <ref type="bibr" target="#b127">[127]</ref> have used spatial and temporal information in clusters in order to determine a confidence level of sensors. The spatial information indicated that more sensors are covering a specific area; hence a higher confidence is assigned to the observation obtained from that area. The temporal information is obtained in the form of the sensors detecting the target consecutively for a number of time slots. If a target is consecutively detected, it was assumed that the sensors are reporting correctly. This method is more suited to the environment where the sensors' location changes over time. In a fixed sensor setting, the confidence value will likely remain constant.</p><p>Recently, Atrey et al. <ref type="bibr" target="#b9">[10]</ref> have also presented a method to dynamically compute the confidence level of a media stream based on its agreement coefficient with a trusted stream. The trusted stream is the one that has the confidence level above a certain threshold. The agreement coefficient between any two streams will be high when the similar decisions are obtained based on them, and vice versa. In this work, the authors have adopted the following idea. Let one follow a trusted news bulletin. He/she also starts by following an arbitrary news bulletin and compares the news content provided on both the news bulletins. Over a period of time, his/her confidence in the arbitrary bulletin will also grow if the news content of both the bulletins have been found similar, and vice versa. The authors have demonstrated that the confidence level of different media streams computed using this method when used in the fusion process provides the event detection results comparable to what is obtained using pre-computed confidence. The drawback with this method is that the assumption of having at least one trusted stream might not always be realistic.</p><p>The above discussion shows that, although there have been some attempts to address the issue of dynamic weight adjustment, this is still an open research problem, which is essential for the overall fusion process. A summarization of the representative works related to the computation of confidence provided in Table <ref type="table" target="#tab_9">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Contextual information</head><p>The context is accessory information that greatly influences the performance of a fusion process. For example, time and location information significantly improves the accuracy of automatic photo classification <ref type="bibr" target="#b142">[142]</ref>. Also, the light conditions may help in selecting the right set of sensors for detecting events in a surveillance environment. Some of the earlier works, which have emphasized the importance of using the contextual information in the fusion process, include Brmond and Thonnat <ref type="bibr" target="#b20">[21]</ref>, Teriyan and Puuronen <ref type="bibr" target="#b129">[129]</ref>, Teissier et al. <ref type="bibr" target="#b128">[128]</ref> and Westerveld <ref type="bibr" target="#b139">[139]</ref>. Later, many other researchers such as Sridharan et al. <ref type="bibr" target="#b122">[122]</ref>, Wang and Kankanhalli <ref type="bibr" target="#b135">[135]</ref>, Pfleger <ref type="bibr" target="#b99">[100]</ref> and Atrey et al. <ref type="bibr" target="#b7">[8]</ref> have demonstrated the advantages of using context in the fusion process.</p><p>Two research issues related to the context are <ref type="bibr" target="#b0">(1)</ref> what are the different forms of contextual information and how the contextual information is determined? and (2) how it is used in the fusion process? In the following, we discuss how these two issues have been addressed by the researchers.</p><p>The context has been represented in different forms for different multimedia analysis tasks. For example, for the image classification task, the context could be time, location and camera parameters <ref type="bibr" target="#b142">[142]</ref>, while for the multimedia music selection task, the mood of the requester could be considered as context. We identify two types of contextual information that have been often considered. These are environmental context and the situational context. The environmental context consists of time, the sensor's location or geographical location, weather conditions, etc. For example, if it is a dark environment, audio and IR sensor information should preferably be fused to detect a person <ref type="bibr" target="#b7">[8]</ref>. The situational context could be in the form of identity, mood, and capability of a person, etc. For example, if the person's mood is happy, a smart mirror should select and play a romantic song when s/he enters into a smart house <ref type="bibr" target="#b49">[50]</ref>.</p><p>The contextual information can be determined by explicitly processing the sensor data, e.g. a mood detection algorithm can be applied on the video data to determine the mood of a person. On the other hand, it can also be learned through other mechanisms such as the time from a system clock, location from a GPS device, and the sensors' geometry and location as a priori information from the system designer.</p><p>To integrate the contextual information in the fusion process, most researchers such as Westerveld <ref type="bibr" target="#b139">[139]</ref>, Jasinschi et al. <ref type="bibr" target="#b61">[62]</ref>, Wang and Kankanhalli <ref type="bibr" target="#b135">[135]</ref>, Pfleger <ref type="bibr" target="#b99">[100]</ref>, Wu et al. <ref type="bibr" target="#b142">[142]</ref>, Atrey et al. <ref type="bibr" target="#b7">[8]</ref> have adopted a rulebased scheme. This scheme is very straight forward as it follows the ''if-then-else'' strategy. For example, if it is day time, then the video cameras would be assigned a greater weight than the audio sensors in the fusion process for detecting the event of a ''human walking in the garden'', else otherwise. We describe some of these works in the following. In <ref type="bibr" target="#b139">[139]</ref>, the author has integrated image features (content) and the textual information that comes with an image (context) at the semantic level. Similar to this work, Jasinschi et al. <ref type="bibr" target="#b61">[62]</ref> have presented a layered probabilistic framework that integrates the multimedia content and context information. Within each layer, the representation of content and context is based on Bayesian networks, and hierarchical priors that provide the connection between the two layers. The authors have applied the framework for an end-to-end system called the video scout that selects, indexes, and stores TV program segments based on topic classification. In the context of dialog systems, Pfleger <ref type="bibr" target="#b99">[100]</ref> has presented a multimedia fusion scheme for detecting the user actions and events. While detecting these input events, the user's 'local turn context' has been considered. This local turn context comprises all previously recognized input events and the dialog states that both belong to the same user's turn. Wu et al. <ref type="bibr" target="#b142">[142]</ref> have used the context information (in form of the time and location) for photo annotation. They have adopted a Bayesian network fusion approach in which the context has been used to govern the transitions between nodes. Wang and Kankanhalli <ref type="bibr" target="#b135">[135]</ref> and Atrey et al. <ref type="bibr" target="#b7">[8]</ref> have used the context in the form of the environment and the sensor information. The environment information consisted of the geometry of the space under surveillance while the sensory information was related to their location and orientation. While the works described above have used the context in a static manner, Sridharan et al. <ref type="bibr" target="#b122">[122]</ref> have provided a computational model of context evolution. The proposed model represents the context using semantic-nets. The context has been defined as the union of semantic-nets, each of which can specify a fact about the environment. The interrelationships among the various aspects (e.g. the user, the environment, the allowable interactions, etc.) of the system are used to define the overall system context. The evolution of context has been modeled using a leaky bucket algorithm that has been widely used for traffic control in a network.</p><p>The representative works related to the use of contextual information in the fusion process have been summarized in Table <ref type="table" target="#tab_10">7</ref>. Although the rule-based strategy of integrating the context in the fusion process is appealing, the number of rules largely increases in varying context in a real world scenario. Therefore, other strategies for context determination and its integration in multimodal fusion remain to be explored in future.</p><p>4.2 Issue related to when to fuse Different modalities are usually captured in different formats and at different rates. Therefore, they need to be synchronized before fusion takes place <ref type="bibr" target="#b94">[95]</ref>. As the fusion can be performed at the feature as well as the decision level, the issue of synchronization is also considered at these two levels. In the feature level synchronization, the features obtained from different but closely coupled modalities captured during the same time period are combined together <ref type="bibr" target="#b27">[28]</ref>. On the other hand, the decision level synchronization needs to determine the designated points along the timeline at which the decisions should be fused. However, in both the levels of fusion, the problem of synchronization arises in different forms. In the following, we elaborate on these problems and also describe some works which have addressed them.</p><p>The feature level synchronization has been illustrated in Fig. <ref type="figure">9a</ref>. Assuming that the raw data from the two different types of modalities (modality 1 and modality 2, in the figure) are obtained at the same time t = 1. The feature extraction from these modalities can be from different time periods (e.g. 2 and 1.5 time units for modality 1 and modality 2, respectively in Fig. <ref type="figure">9a</ref>). Due to the different time periods of the data processing and feature extraction, when these two features should be combined, remains an issue. To resolve this issue, a simple strategy could be to fuse the features at regular intervals <ref type="bibr" target="#b7">[8]</ref>. Although this strategy may not be the best, it is computationally less expensive. An alternative strategy could be to combine all the features at the time instant they are available (e.g. at t = 3 in Fig. <ref type="figure">9a</ref>). An illustration of the synchronization at the decision level is provided in Fig. <ref type="figure">9b</ref>. In contrast to feature level synchronization, where only the feature extraction time impact asynchrony between the modalities, the additional time of obtaining decisions based on the extracted features further affect it. For example, as shown in Fig. <ref type="figure">9b</ref>, the time taken in obtaining the decisions could be 1.5 and 1.75 time units for modality 1 and modality 2, respectively. However, similar to feature level synchronization, in this case also, the decisions are fused using various strategies discussed earlier (e.g. at the time instant all the decisions are available, t = 4 in Fig. <ref type="figure">9b</ref>). Exploring the best strategy is an issue that can be considered in future.</p><p>Another important synchronization issue is to determine the amount of raw data needed from different modalities for accomplishing a task. To mark the start and end of a task (e.g. event detection) over a timeline, there is a need to obtain and process the data streams at certain time intervals. For example, from a video stream of 24 frames/s, 2-s data (48 frames) could be sufficient to determine a human walking event (by computing the blob displacement in a sequence of images); however, the same event (sound of footsteps) could be detected using one second of audio data of 44 kHz. This time period, which is basically the minimum amount of time to accomplish a task, could be different for different tasks when accomplished using various modalities. Ideally, it should be as small as possible since a smaller value allows task accomplishment at a finer granularity in time. In other words, the minimum time period for a specific task should be just large enough to capture the data to accomplish it. Determining the minimum time period to accomplish different tasks is a research issue that needs to be explored in future.</p><p>In multimedia fusion literature, the issue of synchronization has not been widely addressed. This is because many researchers focused on the accuracy aspect of the analysis tasks and performed experiments in an offline manner. In the offline mode, the synchorization has often been manually performed by aligning the modalities along a timeline. The researchers who performed analysis tasks in real time have usually adopted simple strategies such as synchronization at a regular interval. However, having a regular interval may not be optimal and may not lead to the accomplishment of the task with the highest accuracy. Therefore, the issue of synchronization still remains to be explored.</p><p>In the following, we discuss some representative works that have focused on synchronization issue in one way or the other. In the area of audio-visual speech processing, several researchers have computed the audio-visual synchrony. These works include Hershey and Movellan <ref type="bibr" target="#b46">[47]</ref>, Slaney and Covell <ref type="bibr" target="#b117">[117]</ref>, Iyengar et al. <ref type="bibr" target="#b56">[57]</ref>, Nock et al. <ref type="bibr" target="#b90">[91]</ref> and Bredin and Chollet <ref type="bibr" target="#b18">[19]</ref>. In these works, the audio-visual synchrony has been used as a measure of correlation that can be perceived as synchronization at the feature level.</p><p>The problem of synchronization at the decision level, which is more difficult than the feature level synchronization, has been addressed by few researchers including Holzapfel et al. <ref type="bibr" target="#b48">[49]</ref>, Atrey et al. <ref type="bibr" target="#b7">[8]</ref>, and [Xu and Chua <ref type="bibr" target="#b149">[149]</ref>. Holzapfel et al. <ref type="bibr" target="#b48">[49]</ref> have aligned the decisions obtained from the processing of gesture and speech modalities. To identify the instances at which these two decisions are to be along the timeline, the authors computed temporal correlation between the two modalities. Unlike Holzapfel et al. <ref type="bibr" target="#b48">[49]</ref>, Atrey et al. <ref type="bibr" target="#b7">[8]</ref> adopted a simple strategy to combine the decisions at regular intervals. These decisions were obtained from audio and video event detectors. The authors have empirically found that the time interval of one second was optimal in improving the overall accuracy of event detection.</p><p>The issue of time synchronization has also been widely addressed in news and sports video analysis. Satoh et al. <ref type="bibr" target="#b114">[114]</ref> adopted a multimodal approach for face identification and naming in news video by aligning the text, audio and video modalities. The authors proposed to detect face sequences from images and extract the name candidates from the transcripts. The transcripts were obtained from the audio tracks using speech recognition technique. Moreover, video captions were also processed to extract the name titles. Based on the audio-generated transcript and the video captions, the corresponding faces in the video were aligned. In the domain of sports event analysis, to determine the time when the event occurred in the broadcasted sports video, Babaguchi et al. <ref type="bibr" target="#b12">[13]</ref> used the textual overlays that usually appear in the sports video. Similar approach was used by Xu and Chua <ref type="bibr" target="#b149">[149]</ref>. In their work, the authors have used text modality in addition to the audio and video. The authors observed a significant asynchronism that resulted from different time granularities of audiovideo and text analysis. While the audio-video frames were available at a regular interval of seconds, the text availability was very slow (approximately every minute). This was because, the human operator usually enters texts for live matches which takes a few minutes to become available for automatic analysis. The authors have performed synchronization between text and audio-video modalities by using alignment. This alignment was performed by maximizing the number of matches between text events and audio-video events. The text and audiovideo events are considered matched when they are both within a temporal range, occur in the same sequential order, and the audio-video event adapts to the modeling of the text event. For example, an offense followed by a break conforms to goal's event modeling. Although the above mentioned synchronization method works well, it cannot be generalized since it is domain-oriented and highly specific to the user-defined rules. Note that, while Babaguchi et al. <ref type="bibr" target="#b12">[13]</ref> and Xu and Chua <ref type="bibr" target="#b149">[149]</ref> attempted to synchronize video based on the time extracted from the time overlays in the video and web-casted text, respectively; Xu et al. <ref type="bibr" target="#b147">[147]</ref> and <ref type="bibr" target="#b148">[148]</ref> adopted a different approach. In their work, the authors extracted the timing information from the broadcasted video by detecting the video event boundaries. The authors observed that as the webcasted text is usually not available in the broadcasted text, the time recognition from the broadcast sports video is a better choice to perform the alignment of the text and video.</p><p>A summarization of the above described works has been provided in Table <ref type="table" target="#tab_11">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Issue related to what to fuse</head><p>In the literature, the issue of what to fuse has been addressed at two different levels: modality selection and feature vector reduction. Modality selection refers to choosing different types of modalities. For example, one can select and fuse two video camera and one microphone data to determine the presence of a person. On the other hand, the fusion of features usually results into a large feature vector, which becomes a bottleneck for a particular analysis task. This is known as the curse of dimensionality. To overcome this problem, different data reduction techniques are applied to reduce the feature vector. In the following, we discuss various works that addressed the modality selection and feature vector reduction issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Modality selection</head><p>The modality selection problem is similar to the sensor stream selection problem that has often been considered as an optimization problem, in which, the best set of sensors is obtained satisfying some cost constraints. Some of the fundamental works on the sensor stream selection in the context of discrete-event systems and failure diagnosis include Oshman <ref type="bibr" target="#b93">[94]</ref>, Debouk et al. <ref type="bibr" target="#b36">[37]</ref> and Jiang et al. <ref type="bibr" target="#b63">[64]</ref>. Similarly, in the context of wireless sensor networks, the optimal media stream selection has been studied by Pahalawatta et al. <ref type="bibr" target="#b97">[98]</ref>, Lam et al. <ref type="bibr" target="#b69">[70]</ref>, and Isler and Bajcsy <ref type="bibr" target="#b55">[56]</ref>. The details of these works are omitted in the interest of brevity.</p><p>In the context of multimedia analysis, the problem of optimal modality selection has been targeted by Wu et al. <ref type="bibr" target="#b143">[143]</ref>, Kankanhalli et al. <ref type="bibr" target="#b67">[68]</ref>, and Atrey et al. <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b143">[143]</ref>, the authors proposed a two-step optimal fusion approach. In the first step, they find statistically independent modalities from raw features. Then, the second step involves super-kernel fusion to determine the optimal combination of individual modalities. The authors have provided a tradeoff between modality independence and the curse of dimensionality. Their idea of selecting optimal modalities is as follows. When the number of modalities is one, all the feature components were treated as a one-vector representation, suffering from the curse of dimensionality. On the other hand, the large number of modalities reduces the curse of dimensionality, but the inter modality correlation is increased. An optimal value of modalities would tend to balance between the curse of dimensionality and the inter modality correlation. The authors have demonstrated the utility of their scheme for image classification and video concept detection. Kankanhalli et al. <ref type="bibr" target="#b67">[68]</ref> have also presented an Experiential Sampling based method to find the optimal subset of streams in multimedia systems. Their method is the extension of the work by Debouk et al. <ref type="bibr" target="#b36">[37]</ref>, in the context of multimedia. This method may have a high cost of computing the optimal subset as it requires the minimum expected number of tests to be performed in order to determine the optimal subset. Recently, Atrey et al. <ref type="bibr" target="#b8">[9]</ref> have presented a dynamic programming based method to select the optimal subset of media streams. This method provides a threefold tradeoff between the extent to which the multimedia analysis task is accomplished by the selected subset of media streams, the overall confidence in this subset, and the cost of using this subset. In addition, their method also provides flexibility to the system designer to choose the next best sensor if the best sensor is not available. They have demonstrated the utility of the proposed method for event detection in an audio-video surveillance scenario.</p><p>From the above discussion, it can be observed that only a few attempts (Wu et al. <ref type="bibr" target="#b143">[143]</ref>, Kankanhalli et al. <ref type="bibr" target="#b67">[68]</ref>, and Atrey et al. <ref type="bibr" target="#b8">[9]</ref>) have been made to select the best (or optimal) subset of modalities for multimodal fusion. However, these methods have their own limitations and drawbacks. Moreover, they do not consider the different contexts under which the modalities may be selected. Therefore, a lot more can be done in this aspect of multimodal fusion. The methods for optimal subset modality selection described above are summarized in Table <ref type="table" target="#tab_12">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Feature vector reduction</head><p>It is important to mention that, besides selecting the optimal set of the modalities, the issue ''what to fuse'' for a particular multimedia analysis task involves the reduction of feature vector. The fusion of features that are obtained from different modalities usually result into a large feature vector, which becomes a bottleneck when processed to accomplish any multimedia analysis task. To handle such situations, researchers have used various data reduction techniques. Most commonly used techniques are principle component analysis (PCA), singular vector decomposition (SVD) and linear discriminant analysis (LDA).</p><p>PCA is used to project higher dimensional data into lower dimensional space while preserving as much information as possible. The projection that minimizes the squared error in reconstructing original data is chosen to represent the reduced set of features. The PCA technique often does not perform well when the dimensionality of the feature set is very large. This limitation is overcome by SVD technique, which is used to determine the eigen vectors that most represent the input feature set. While PCA and SVD are unsupervised techniques, LDA works in supervised mode. LDA is used for determining the linear combination of features, which is not only a reduced set of features but it is also used for classification. The readers may refer to <ref type="bibr" target="#b134">[134]</ref> for further details about these feature dimensionality reduction methods.</p><p>In multimodal fusion domain, many researchers have used these methods for feature vector dimension reduction. Some representative works are: Guironnet et al. <ref type="bibr" target="#b43">[44]</ref> used PCA for video classification, Chetty and Wagner <ref type="bibr" target="#b27">[28]</ref> utilized SVD for biometric person authentication, and Potamianos et al. <ref type="bibr" target="#b104">[105]</ref> adopted LDA for speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Other considerations</head><p>There are other situations when the issue of ''what to fuse'' needs special consideration. For instance, dealing with unlabeled data in fusion <ref type="bibr" target="#b130">[130]</ref> and handling noisy positive data for fusion <ref type="bibr" target="#b53">[54]</ref>.</p><p>There are three approaches used for learning with unlabeled data: semi-supervised learning, transductive learning and active learning <ref type="bibr" target="#b155">[155]</ref>. Semi-supervised learning methods automatically exploit unlabeled data to help estimate the data distribution in order to improve learning performance. Transductive learning is different from semi-supervised learning in that it selects the The gain from a modality is overlooked Kankanhalli et al. <ref type="bibr" target="#b67">[68]</ref> Information gain versus cost (time) Cost of computing the optimal subset could be high Atrey et al. <ref type="bibr" target="#b8">[9]</ref> Gain versus Confidence level versus processing cost The issue of how frequently the optimal subset should be recomputed needs a formalization unlabeled data from test data set. On the other hand, in active learning methods, the learning algorithm selects the unlabeled example and actively query the user/teacher for labels. Here the learning algorithm is supposed to be good enough to choose the least number of examples to learn a concept, otherwise there is a risk of including the unimportant and irrelevant examples.</p><p>Another important issue that needs to be resolved is how to reduce outliers or noise in the input data for fusion. In the multimodal fusion process, noisy data usually results into reduced classification accuracy and increased training time and size of the classifier. There are various solutions to deal with the noisy data. For instance, to employ some noise filter mechanisms to smooth the noisy data or to apply an appropriate sampling technique to differentiate the noisy data from the input data before the fusion takes place <ref type="bibr" target="#b137">[137]</ref>.</p><p>5 Benchmark datasets and evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Many multimodal fusion applications use several publicly available datasets. For example, the small 2k image datasets in Corel Image CDs. It contains representative images of fourteen categories that includes architecture, bears, clouds, elephants, fabrics, fireworks, flowers, food, landscape, people, textures, tigers, tools, and waves. Different features such as color and texture can be extracted from this 2k image dataset and be used for fusion as shown in <ref type="bibr" target="#b143">[143]</ref>.</p><p>Among the video-based fusion research, the most popular are the well-known TRECVID datasets <ref type="bibr" target="#b1">[2]</ref> that are available in different versions since 2001. A quick view of the high-level feature extraction from these datasets can be found in <ref type="bibr" target="#b118">[118]</ref>. Depending on their release, these datasets contain data files about broadcast news video, sound and vision video, BBC rushes video, BBC rushes video, London Gatwick surveillance video, and test dataset annotations for surveillance event detection. Features from visual, audio and caption tracks in TRACVID datasets are extracted and used in fusion for various multimedia analysis tasks, such as video shot retrieval <ref type="bibr" target="#b82">[83]</ref>, semantic video analysis <ref type="bibr" target="#b121">[121]</ref>, news video story segmentation <ref type="bibr" target="#b51">[52]</ref>, video concept detection <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b143">143]</ref> and so on.</p><p>The fusion literature related to biometric identification and verification make ongoing efforts to build multimodal biometric databases. For example, BANCA <ref type="bibr" target="#b13">[14]</ref> that contains face and speech modalities; XM2VTS <ref type="bibr" target="#b81">[82]</ref> that contains synchronized video and speech data; BIOMET <ref type="bibr" target="#b41">[42]</ref> that contains face, speech, fingerprint, hand and signature modalities; MYCT <ref type="bibr" target="#b92">[93]</ref> that contains 10-print fingerprint and signature modalities and several others as mentioned in <ref type="bibr" target="#b103">[104]</ref>.</p><p>Another popular dataset standardization effort has been the agenda of performance evaluation of tracking and surveillance (PETS) community <ref type="bibr" target="#b0">[1]</ref>. Several researchers have used PETS datasets for multimodal analysis tasks, for example, object tracking <ref type="bibr" target="#b154">[154]</ref>.</p><p>Although there are several available datasets that can be used for various analysis tasks, there lacks any standardization effort for a common dataset for multimodal fusion research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation measures</head><p>Several evaluation metrics are usually used to measure the performance of the fusion-based multimedia analysis tasks. For example, NIST average precision metric is used to determine the accuracy of semantic concept detection at the video shot level <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b121">121,</ref><ref type="bibr" target="#b143">143]</ref>. For news video story segmentation, the precision and recall metrics are widely used <ref type="bibr" target="#b51">[52]</ref>. Precision and recall measure are also commonly used for image retrieval <ref type="bibr" target="#b54">[55]</ref>. Similarly, for the video shot retrieval some researchers use mean average precision <ref type="bibr" target="#b82">[83]</ref>. While performing image categorization, the accuracy of the classification is measured in terms of image category detection rate <ref type="bibr" target="#b156">[156]</ref>.</p><p>To measure the performance of tracking related analysis tasks, the dominating evaluation metrics include mean distance from track, detection rate, false positive rate, recall and precision <ref type="bibr" target="#b131">[131]</ref>. Similarly, for speaker position estimation researchers have measured tracking error and calculated average distance between true and estimated position of speaker <ref type="bibr" target="#b125">[125]</ref>. In <ref type="bibr" target="#b154">[154]</ref>, the authors calculated variance of motion direction and variance of compactness to calculate the accuracy of object tracking.</p><p>Recently, Hossain et al. <ref type="bibr" target="#b50">[51]</ref> presented a multi-criteria evaluation metric to determine the quality of information obtained based on multimodal fusion. The evaluation metric includes certainty, accuracy and timeliness. The authors showed its applicability in the domain of multimedia monitoring.</p><p>In human computer interaction, fusion is used mostly to identify multimodal commands or input interactions of human such as gestures, speech etc. Therefore, metrics such as speech recognition accuracy and gesture recognition accuracy are used to measure the accuracy of these tasks <ref type="bibr" target="#b48">[49]</ref>.</p><p>Furthermore, to evaluate the fusion result for biometric verification, false acceptance rate (FAR) and false rejection rate (FRR) are used to identify the types of errors <ref type="bibr" target="#b103">[104]</ref>. The FAR and FRR are often used to present the half total error rate (HTER), which is a measure to assess the quality of a biometric verification system <ref type="bibr" target="#b16">[17]</ref>.</p><p>Overall, we observed that the researchers have used different evaulation criteria for different analysis tasks.</p><p>However a common evaluation framework for multimodal fusion is yet to be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future research directions</head><p>We have surveyed the state-of-the-art research related to multimodal fusion and commented on these works from the perspective of the usage of different modalities, the levels of fusion, and the methods of fusion. We have further provided a discussion to summarize our observation based on the reviewed literature, which can be useful for the readers to have an understanding of the appropriate fusion methodology and the level of fusion. Some distinctive issues (e.g. correlation, confidence) that influence the fusion process are also elaborated in greater detail.</p><p>Despite that a significant number of multimedia analysis tasks have been successfully performed using a variety of fusion methods, there are several areas of investigation that may be explored in the future. We have identified some of them as follows:</p><p>1. Multimedia researchers have mostly used the audio, video and the text modalities for various multimedia analysis tasks. However, the integration of some new modalities such as RFID for person identification, haptics for dialog systems, etc. can be explored further. 2. The appropriate synchronization of the different modalities is still a big research problem for multimodal fusion researchers. Specifically, when and how much data should be processed from different modalities to accomplish a multimedia analysis task, is an issue that has not yet been explored exhaustively. 3. The problem of the optimal weight assignment to the different modalities under a varying context is an open problem. Since we usually have different confidence levels in the different modalities for accomplishing various analysis tasks, the problem of dynamic computation of the confidence information for the different streams for various tasks, becomes challenging and worth researching in future. 4. How to integrate context in the fusion process? This question can be answered by thinking beyond the ''ifthen-else'' strategy. There is a need to formalize the concept of context. How may the changing context influence the fusion process? What model would be most suitable to simulate the varying nature of context? These questions require greater attention from multimedia researchers. 5. The feature level correlation among different modalities has been utilized in an effective way. However, it has been observed that correlation at the semantic level (decision level) has not been fully explored, although some initial attempts have been reported. 6. The optimal modality selection for fusion is emerging as an important research issue. From the available set, which modalities should be fused to accomplish a task at a particular time instant? The utility of these modalities could be changed with the varying context. Moreover, the optimality of modality selection can be determined based on various constraints such as the extent to which the task is accomplished, the confidence with which the task is accomplished, and the cost of using the modalities for performing the task. As the optimal subset changes over time, how frequently it should be computed so that the cost of re-computation can be reduced to meet the timeliness, is an open problem for multimedia researchers to consider. 7. Last but not least, there are various evaluation metrics that are used to measure the performance of different multimedia analysis tasks. However, it would be interesting to work on a common evaluation framework that can be used by multimodal fusion community.</p><p>Multimodal fusion for multimedia analysis is a promising research area. This survey has covered the existing works in this domain and identified several relevant issues that deserve further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4</head><label>4</label><figDesc>Fig.4Multimodal fusion using visual and text cues for image classification based on pair-wise SVM classifier<ref type="bibr" target="#b156">[156]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 9</head><label>29</label><figDesc>Fig. 9 Illustration of the synchronization between two modalities at the a feature level, b decision level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>A list of the representative works in the rule-based fusion methods category</figDesc><table><row><cell>Fusion</cell><cell>Level of</cell><cell>The work</cell><cell>Modalities</cell><cell>Multimedia analysis task</cell></row><row><cell>method</cell><cell>fusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear weighted</cell><cell>Feature</cell><cell cols="2">Foresti and Snidaro [40] Video (trajectory coordinates)</cell><cell>Human tracking</cell></row><row><cell>fusion</cell><cell></cell><cell>Wang et al. [136]</cell><cell>Video (color, motion and texture</cell><cell>Human tracking</cell></row><row><cell></cell><cell></cell><cell>Yang et al. [152]</cell><cell>Video (trajectory coordinates)</cell><cell>Human tracking</cell></row><row><cell></cell><cell></cell><cell>Kankanhalli et al. [67]</cell><cell>Video (color, motion and texture)</cell><cell>Face detection, monologue</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>detection and traffic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>monitoring</cell></row><row><cell></cell><cell cols="2">Decision Neti et al. [87]</cell><cell>Audio (phonemes) and visual (visemes)</cell><cell>Speaker recognition</cell></row><row><cell></cell><cell></cell><cell>Lucey et al. [78]</cell><cell>Audio (MFCC), video (Eigenlip)</cell><cell>Spoken word recognition</cell></row><row><cell></cell><cell></cell><cell>Iyenger et al. [57, 58]</cell><cell>Audio (MFCC), video (DCT of the face</cell><cell>Monologue detection, semantic</cell></row><row><cell></cell><cell></cell><cell></cell><cell>region) and the synchrony score</cell><cell>concept detection and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>annotation in video</cell></row><row><cell></cell><cell></cell><cell>Hua and Zhang [55]</cell><cell>Image (six features: color histogram,</cell><cell>Image retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell>color moment, wavelet, block wavelet,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>correlagram, blocked correlagram)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Yan et al. [151]</cell><cell>Text (closed caption, video OCR), audio,</cell><cell>Video retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell>video (color, edge and texture</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>histogram), motion</cell><cell></cell></row><row><cell></cell><cell></cell><cell>McDonald and Smeaton</cell><cell>Text and video (color, edge and texture)</cell><cell>Video retrieval</cell></row><row><cell></cell><cell></cell><cell>[83]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Jaffre and Pinquier [59]</cell><cell>Audio, video index</cell><cell>Person identification from</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>audio-visual sources</cell></row><row><cell cols="3">Majority voting rule Decision Radova and Psutka</cell><cell>Raw speech (set of patterns)</cell><cell>Speaker identification from</cell></row><row><cell></cell><cell></cell><cell>[108]</cell><cell></cell><cell>audio sources</cell></row><row><cell>Custom-defined</cell><cell cols="2">Decision Babaguchi et al. [12]</cell><cell>Visual (color), closed caption text</cell><cell>Semantic sports video indexing</cell></row><row><cell>rules</cell><cell></cell><cell></cell><cell>(keywords)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Corradini et al. [32]</cell><cell>Speech, 2D gesture</cell><cell>Human computer interaction</cell></row><row><cell></cell><cell></cell><cell>Holzapfel et al. [49]</cell><cell>Speech, 3D pointing gesture</cell><cell>Multimodal interaction with robot</cell></row><row><cell></cell><cell></cell><cell>Pfleger [100]</cell><cell>Pen gesture, speech</cell><cell>Multimodal dialog system</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. A DBN is also called a probabilistic generative model or a graphical model. Due to the fact that</figDesc><table><row><cell></cell><cell></cell><cell>Speaker</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kiosk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Visible</cell><cell>Frontal</cell><cell>Speech</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Skin</cell><cell>Texture</cell><cell>Face Det</cell><cell>Mouth</cell><cell>Sound</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Fig. 6 An example of a static Bayesian networks [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Speaker</cell><cell></cell><cell></cell><cell>Speaker</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Frontal</cell><cell></cell><cell></cell><cell>Frontal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Speech</cell><cell></cell><cell></cell><cell>Speech</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Speaker</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Speaker</cell><cell></cell><cell></cell></row><row><cell>Kiosk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kiosk</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Visible</cell><cell>Frontal</cell><cell>Speech</cell><cell></cell><cell></cell><cell>Visible</cell><cell>Frontal</cell><cell>Speech</cell><cell></cell></row><row><cell>Skin</cell><cell>Texture</cell><cell>Face Det</cell><cell>Mouth</cell><cell>Sound</cell><cell>Skin</cell><cell>Texture</cell><cell>Face Det</cell><cell>Mouth</cell><cell>Sound</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>A list of the representative works in the classification methods category used for multimodal fusion</figDesc><table><row><cell>Modalities</cell><cell></cell></row><row><cell>The work</cell><cell></cell></row><row><cell>Level of</cell><cell>fusion</cell></row><row><cell>Fusion method</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Modalities</cell></row><row><cell></cell><cell>The work</cell></row><row><cell></cell><cell>Level of</cell><cell>fusion</cell></row><row><cell>continued</cell><cell>Fusion method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>A list of the representative works in the estimation methods category used for multimodal fusion</figDesc><table><row><cell>Fusion method</cell><cell>Level of</cell><cell>The work</cell><cell>Modalities</cell><cell>Multimedia analysis task</cell></row><row><cell></cell><cell>fusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kalman filter and its</cell><cell cols="3">Feature Potamitis et al. [107] Audio (position, velocity)</cell><cell>Multiple speaker tracking</cell></row><row><cell>variants</cell><cell></cell><cell>Loh et al. [77]</cell><cell>Audio, video</cell><cell>Single speaker tracking</cell></row><row><cell></cell><cell></cell><cell>Gehrig et al. [43]</cell><cell>Audio (TDOA), video (position of the</cell><cell>Single speaker tracking</cell></row><row><cell></cell><cell></cell><cell></cell><cell>speaker)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Zhou and Aggarwal</cell><cell>Video [spatial position, shape, color (PCA),</cell><cell>Person/vehicle tracking</cell></row><row><cell></cell><cell></cell><cell>[154]</cell><cell>blob]</cell><cell></cell></row><row><cell></cell><cell cols="2">Decision Strobel et al. [124]</cell><cell>Audio, video</cell><cell>Single object localization and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tracking</cell></row><row><cell></cell><cell></cell><cell>Talantzis et al. [125]</cell><cell>Audio (DOA), video (position, velocity,</cell><cell>Person tracking</cell></row><row><cell></cell><cell></cell><cell></cell><cell>target size)</cell><cell></cell></row><row><cell>Particle filter</cell><cell cols="2">Feature Vermaak et al. [132]</cell><cell>Audio (TDOA), visual (gradient)</cell><cell>Single speaker tracking</cell></row><row><cell></cell><cell cols="2">Decision Zotkin et al. [157]</cell><cell>Audio (TDOA), video (skin color, shape</cell><cell>Multiple speaker tracking</cell></row><row><cell></cell><cell></cell><cell></cell><cell>matching and color histograms)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Perez et al. [99]</cell><cell>Audio (TDOA), video (coordinates)</cell><cell>Single speaker tracking</cell></row><row><cell></cell><cell></cell><cell>Nickel et al. [89]</cell><cell cols="2">Audio (TDOA), video (Haar-like features) Single speaker tracking</cell></row></table><note><p>? Application constraints and the fusion methods. From the perspective of application constraints such computation, delay and resources, we can analyze different fusion methods as follows. It has been observed that the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>A summary of the fusion method used for different multimedia analysis tasks</figDesc><table><row><cell>Multimedia analysis task</cell><cell>Fusion method</cell><cell>The works</cell></row><row><cell>Biometric identification and verification</cell><cell>Support vector machine</cell><cell>Bredin and Chollet [19], Aguilar et al. [4]</cell></row><row><cell></cell><cell>Dynamic Bayesian networks</cell><cell>Bengio et al. [17]</cell></row><row><cell>Face detection, human tracking and</cell><cell>Linear weighted fusion</cell><cell>Kankanhalli et al. [67], Jaffre and</cell></row><row><cell>activity/event detection</cell><cell></cell><cell>Pinquier [59]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>A list of some representative works that used the correlation information between different streams in the fusion process</figDesc><table><row><cell>Level of fusion</cell><cell>The form of correlation</cell><cell>The works</cell><cell>Multimedia analysis task</cell></row><row><cell>Feature</cell><cell>Correlation coefficient</cell><cell>Wang et al. [138]</cell><cell>Video shot classification</cell></row><row><cell></cell><cell></cell><cell>Nefian et al. [86]</cell><cell>Speech recognition</cell></row><row><cell></cell><cell></cell><cell>Beal et al. [15]</cell><cell>Object tracking</cell></row><row><cell></cell><cell></cell><cell>Li et al. [74]</cell><cell>Talking face detection</cell></row><row><cell></cell><cell>Mutual information</cell><cell>Fisher-III et al. [39]</cell><cell>Speech recognition</cell></row><row><cell></cell><cell></cell><cell>Darrell et al. [35]</cell><cell>Speech recognition</cell></row><row><cell></cell><cell></cell><cell>Hershey and Movellan [47]</cell><cell>Speaker localization</cell></row><row><cell></cell><cell></cell><cell>Nock et al. [90], Iyengar et al. [57]</cell><cell>Monologue detection</cell></row><row><cell></cell><cell></cell><cell>Nock et al. [91]</cell><cell>Speaker localization</cell></row><row><cell></cell><cell></cell><cell>Noulas and Krose [92]</cell><cell>Human tracking</cell></row><row><cell></cell><cell>Latent semantic analysis</cell><cell>Li et al. [74]</cell><cell>Talking face detection</cell></row><row><cell></cell><cell></cell><cell>Chetty and Wagner [28]</cell><cell>Biometric person authentication</cell></row><row><cell></cell><cell>Canonical correlation analysis</cell><cell>Slaney and Covell [117]</cell><cell>Talking face detection</cell></row><row><cell></cell><cell></cell><cell>Chetty and Wagner [28]</cell><cell>Biometric person authentication</cell></row><row><cell></cell><cell></cell><cell>Bredin and Chollet [20]</cell><cell>Talking face identity verification</cell></row><row><cell></cell><cell>Cross-modal factor analysis</cell><cell>Li et al. [72]</cell><cell>Talking head analysis</cell></row><row><cell>Decision</cell><cell>Casual link analysis</cell><cell>Stauffer [123]</cell><cell>Event detection for surveillance</cell></row><row><cell></cell><cell>Causal strength</cell><cell>Wu et al. [142]</cell><cell>Photo annotation</cell></row><row><cell></cell><cell>Agreement coefficient</cell><cell>Atrey et al. [8]</cell><cell>Event detection for surveillance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>A list of the representative works related to the usage of confidence level in the fusion process</figDesc><table><row><cell>The mode of</cell><cell>The works</cell><cell>Multimedia analysis task</cell><cell>The confidence is determined based on</cell></row><row><cell>computation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Static</cell><cell>Neti et al. [87]</cell><cell>Speaker recognition and speech</cell><cell>The past accuracy</cell></row><row><cell></cell><cell></cell><cell>event detection</cell><cell></cell></row><row><cell></cell><cell>Iyenger et al. [57]</cell><cell>Monologue detection</cell><cell></cell></row><row><cell></cell><cell>Tatbul et al. [126]</cell><cell>Military smart uniform</cell><cell></cell></row><row><cell></cell><cell>Hsu and Chang [53]</cell><cell>News video analysis</cell><cell></cell></row><row><cell></cell><cell>Atrey et al. [8]</cell><cell>Event detection for surveillance</cell><cell></cell></row><row><cell>Dynamic</cell><cell>Tavakoli et al. [127]</cell><cell>Event detection in undersea sensor networks</cell><cell>The spatial and the temporal information</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of sensors' observations</cell></row><row><cell></cell><cell>Atrey et al. [10]</cell><cell>Event detection for surveillance</cell><cell>The agreement/disagreement between</cell></row><row><cell></cell><cell></cell><cell></cell><cell>different streams</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>The representative works related to the use of contextual information in the fusion process</figDesc><table><row><cell>Contextual information</cell><cell>The works</cell><cell>Multimedia analysis task</cell></row><row><cell>Textual information with an image</cell><cell>Westerveld [139]</cell><cell>Image retrieval</cell></row><row><cell>Signature, pattern, or underlying structure</cell><cell>Jasinschi et al. [62]</cell><cell>TV program segmentation</cell></row><row><cell>in audio, video and transcript</cell><cell></cell><cell></cell></row><row><cell>Environment and sensor information</cell><cell>Wang and Kankanhalli [135],</cell><cell>Event detection for surveillance</cell></row><row><cell></cell><cell>Atrey et al. [8]</cell><cell></cell></row><row><cell>Word nets</cell><cell>Sridharan et al. [122]</cell><cell>Multimedia visualization and annotation</cell></row><row><cell>Past input events and the dialog state</cell><cell>Pfleger [100]</cell><cell>Detecting the user intention in multimodal</cell></row><row><cell></cell><cell></cell><cell>dialog systems</cell></row><row><cell>Time, location and camera parameters</cell><cell>Wu et al. [142]</cell><cell>Photo annotation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>A list of representative works that have addressed synchronization problem</figDesc><table><row><cell>Level of fusion</cell><cell>The work</cell></row></table><note><p><p><p><p><p><p><p>Babaguchi et al.</p><ref type="bibr" target="#b12">[13]</ref></p>, Xu and Chua</p><ref type="bibr" target="#b149">[149]</ref></p>, Xu et al.</p><ref type="bibr" target="#b147">[147,</ref><ref type="bibr" target="#b148">148]</ref> </p>Sports video analysis</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc>A summary of approaches used for optimal modality subset selection</figDesc><table><row><cell>The work</cell><cell>The optimality criteria</cell><cell>Drawback</cell></row><row><cell>Wu et al. [143]</cell><cell>Curse of dimensionality versus inter modality</cell><cell></cell></row><row><cell></cell><cell>correlation</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="346" xml:id="foot_0"><p>P. K. Atrey et al.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>To maintain consistency, we will use these notations for modalities in rest of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>P. K. Atrey et al.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments The authors would like to thank the editor and the anonymous reviewers for their valuable comments in improving the content of this paper. This work is partially supported by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC) of Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Bayesian networks</head><p>Wu et al. <ref type="bibr" target="#b142">[142]</ref> Semantic concept detection Linear weighted fusion Iyenger et al. <ref type="bibr" target="#b57">[58]</ref> Support vector machine Adams et al. <ref type="bibr" target="#b2">[3]</ref>, Iyenger et al. <ref type="bibr" target="#b57">[58]</ref>, Wu et al. <ref type="bibr" target="#b141">[141]</ref> Semantic multimedia indexing Custom-defined rules Babaguchi et al. <ref type="bibr" target="#b11">[12]</ref> Support vector machine Ayache et al. <ref type="bibr" target="#b10">[11]</ref> Maximum </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">PETS: Performance evaluation of tracking and surveillance (Last access date 31</title>
		<ptr target="http://www.cvg.rdg.ac.uk/slides/pets.html" />
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www-nlpir.nist.gov/projects/trecvid/trecvid.data.html" />
		<title level="m">TRECVID data availability</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Last access date 02 September</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic indexing of multimedia content using visual, audio, and text cues</title>
		<author>
			<persName><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="185" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparative evaluation of fusion strategies for multimodal biometric verification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Video-Based Biometrie Person Authentication</title>
		<meeting><address><addrLine>Guildford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="830" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio-visual biometrics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Aleksic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2025" to="2044" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Particle methods for change detection, system identification, and control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tadic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic annotation of multimedia using maximum entropy models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Argillander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Accoustic, Speech and Signal Processing</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Information assimilation framework for event detection in multimedia surveillance systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer/ACM Multimed. Syst. J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="253" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Goal-oriented optimal subset selection of correlated multimedia streams</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Confidence building among correlated streams in multimedia surveillance systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classifier fusion for svmbased multimedia semantic indexing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Que ?not</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gensel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 29th European Conference on Information Retrieval Research</title>
		<imprint>
			<publisher>Rome</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="494" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Event based indexing of broadcasted sports video by intermodal collaboration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Personalized abstraction of broadcasted american football video by highlight selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Babaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="586" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The BANCA database and evaluation protocol</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bailly-Baillie ?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marie ?thoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pore ?e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ru? ?z</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometrie Person Authentication</title>
		<meeting><address><addrLine>Guildford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="625" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A graphical model for audiovisual object tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="828" to="836" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multisensor image segmentation using Dempster-Shafer fusion in markov fields context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendjebbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Delignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Samson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pieczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal authentication using asynchronous hmms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 4th International Conference Audio and Video Based Biometric Person Authentication</title>
		<meeting><address><addrLine>Guildford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="770" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Confidence measures for multimodal identity verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariethoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual speech synchrony measure for talking-face identity verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audiovisual speech synchrony measure: application to biometrics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<idno>ID 70186</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A context representation of surveillance systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bre ?mond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<title level="m">Multi-sensor Fusion: Fundamentals and Applications with Software</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall PTR</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Getting the most out of ensemble selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on on Data Mining</title>
		<meeting><address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="828" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-modal approach to story segmentation for news video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chaisorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="187" to="208" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining text and audio-visual features in video indexing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1005" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anomaly detection using the dempstershafer method</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Aickelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<meeting><address><addrLine>Las Vegas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Audio-visual multimodal fusion for biometric person authentication and liveness verification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NICTA-HCSNet Multimodal User Interaction Workshop</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Query based event extraction along a timeline</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting and structure learning in dynamic bayesian networks for audiovisual speaker detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Quebec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="789" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Story boundary detection in large broadcast news video archives: techniques, experience and trends</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chaisorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="656" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal input fusion in human-computer interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Corradini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bernsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abrilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NATO-ASI Conference on Data Fusion for Situation Monitoring, Incident Detection</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Alert and Response Management. Karlsruhe University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of convergence results on particle filtering methods for practitioners</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="736" to="746" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audiovisual segmentation and &apos;&apos;the cocktail party effect</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interfaces. Bejing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facial expression recognition with relevance vector machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On an optimal problem in sensor selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Debouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafortune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teneketzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Discret. Event Dyn. Syst. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="417" to="445" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmental hidden markov models for viewbased sport video analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Learning Applications in Multimedia</title>
		<meeting><address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning joint statistical models for audio-visual fusion and segregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fisher-Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="772" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A distributed sensor network for video surveillance of outdoor environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Snidaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Rochester</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From multi-sensor surveillance towards smart interactive spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gandetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sciutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Negroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="641" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BIO-MET: A multimodal person authentication database including face, voice, fingerprint, hand and signature modalities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia Salicetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dorizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Les Jardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petrovska Delacretaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometrie Person Authentication</title>
		<meeting><address><addrLine>Guildford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kalman filters for audio-video source localization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Karlsruhe University</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="118" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video classification based on low-level feature fusion model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guironnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rombaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th European Signal Processing Conference</title>
		<meeting><address><addrLine>Antalya, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An introduction to multisensor fusion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE: Special Issues on Data Fusion</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="23" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Audio visual graphical models for speech processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krisjianson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Speech, Acoustics, and Signal Processing</title>
		<imprint>
			<publisher>Montreal</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="649" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Audio-vision: using audio-visual synchrony to locate sounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="813" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Implementation and evaluation of a constraint-based multimodal fusion system for speech and 3d pointing gestures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interfaces</title>
		<meeting><address><addrLine>State College, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Smart mirror for ambient home environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd IET International Conference on Intelligent Environments</title>
		<meeting><address><addrLine>Ulm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="589" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling and assessing quality of information in multi-sensor multimedia monitoring systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">News video story segmentation using fusion of multi-level multi-modal features in TRECVID 2003</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics Speech and Signal Processing</title>
		<imprint>
			<publisher>QC</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative, discriminative, and ensemble learning on multi-modal perceputal fusion toward news stroy segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expos</title>
		<meeting><address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1091" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Sensors and data fusion algorithms in mobile robotics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CSM-422, Department of Computer Science, University of Essex</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An attention-based decision fusion scheme for multimedia information retrieval</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th Pacific-Rim Conference on Multimedia</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The sensor selection problem for bounded uncertainty sensing models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Isler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Information Processing in Sensor Networks</title>
		<meeting><address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Audio-visual synchrony for detection of monologue in video archives</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>Hong Kong</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative model fusion for semantic concept detection and annotation in video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Audio/video fusion: a preprocessing step for multimodal person identification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jaffre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pinquier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on MultiModal User Authentification</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multimodal human computer interaction: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Workshop on Human Computer Interaction</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Score normalization in multimodal biometric systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2270" to="2285" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A probabilistic layered framework for integrating multimedia content and context information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Jasinschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="2057" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Using maximum entropy for automatic image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<meeting><address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3115</biblScope>
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimal sensor selection for discrete event systems with partial observation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="369" to="381" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">New extension of the Kalman filter to nonlinear systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Uhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing, Sensor Fusion, and Target Recognition VI</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">3068</biblScope>
			<biblScope unit="page" from="182" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. ASME J. Basic Eng. 82(series D)</title>
		<imprint>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Experiential sampling in multimedia systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Experiential sampling on multiple data streams</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="955" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sensor node selection for execution of continuous probabilistic queries in wireless sensor networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Video Surveillance and Sensor Networks</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leo ?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2621" to="2632" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multimedia content processing through cross-modal association</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Audio-visual talking face detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="473" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Boosting image classification with lda-based feature combination for digital photograph management</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="901" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Integrating semantic templates with decision tree for image semantic learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th International Multimedia Modeling Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="185" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Motion estimation using audio and video fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Control, Automation, Robotics and Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1569" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improved speech recognition using adaptive audio-visual fusion via a stochastic secondary classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Multimedia, Video and Speech Processing</title>
		<imprint>
			<publisher>Hong Kong</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="551" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multisensor fusion and integration: Approaches, applications, and future research directions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sens. J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Information-theoretic semantic multimedia indexing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Magalha ?es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ru ?ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="619" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A multimodal sensor fusion architecture for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Makkook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MS Thesis</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Smeraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Capdevielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abdeljaoued</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-Yacoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mayoraz</surname></persName>
		</author>
		<title level="m">Comparison of face verification results on the XM2VTS database</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">4858</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A comparison of score, rank and probability-based fusion methods for video shot retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Color image segmentation using the dempster-shafer theory of evidence for the fusion of texture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malpica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">XXXIV</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2003">2003</date>
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Continuous audiovisual digit recognition using N-best decision fusion</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Wuerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="91" to="101" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Dynamic bayesian networks for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nefian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURA-SIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Joint processing of audio and visual information for multimedia indexing and human-computer interaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cuetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference RIAO</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">An image recognition method based on multiple bp neural networks fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Information Acquisition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A joint particle filter for audio-visual speaker tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th International Conference on Multimodal Interfaces</title>
		<meeting><address><addrLine>Torento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Assessing face and speech consistency for monologue detection in video</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>French Riviera, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Speaker localisation using audio-visual synchrony: an empirical study</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<meeting><address><addrLine>Urbana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Em detection of common origin of multi-modal cues</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Noulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J A</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interfaces</title>
		<meeting><address><addrLine>Banff</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Biometric on the internet MCYT baseline corpus: a bimodal biometric database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez-Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Igarza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vivaracho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Escudero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">I</forename><surname>Moro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proc. Vis. Image Signal Process</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="395" to="401" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Optimal sensor selection strategy for discrete-time state estimators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="307" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Ten myths of multimodal interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Taming speech recognition errors within a multimodal interface</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="45" to="51" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NJ</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Jacko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Sears</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Lawrence Erlbaum Assoc</publisher>
		</imprint>
	</monogr>
	<note>Multimodal interfaces</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Optimal sensor selection for video-based target tracking in a wireless sensor network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pahalawatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="3073" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Audio-visual speaker tracking with importance particle filter</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Context based multimodal fusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interfaces</title>
		<imprint>
			<publisher>State College</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Fade -an integrated approach to multimodal fusion and discourse processing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dotoral Spotlight at ICMI 2005</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<meeting><address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">How do correlation and variance of baseexperts affect fusion in biometric authentication tasks?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4384" to="4396" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Database, protocols and tools for evaluating score-level fusion algorithms in biometric authentication</title>
		<author>
			<persName><forename type="first">N</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
	<note type="report_type">Pattern Recognit</note>
	<note>Part Special Issue: Complexity Reduction</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Hierarchical discriminant features for audio-visual LVSCR</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustic Speech and Signal Processing</title>
		<imprint>
			<publisher>Salt Lake City</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Tracking of multiple moving speakers with multiple microphone arrays</title>
		<author>
			<persName><forename type="first">I</forename><surname>Potamitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tremoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="520" to="529" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An approach to speaker identification using multiple classifiers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Radova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Psutka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1135" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Germany</forename><surname>Munich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Extended dempster-shafer theory for multi-system/sensor decision fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghassemian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Commission IV Joint Workshop on Challenges in Geospatial Analysis, Integration and Visualization II</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Evidential reasoning for multimodal fusion in human computer interaction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo, Canada</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MS Thesis</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Kalman and extended Kalman filters: concept, derivation and properties</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Institute for Systems and Robotics</title>
		<imprint>
			<publisher>Lisboa</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A unifying review of linear gaussian models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="345" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Identity verification using speech and face information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digit. Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="449" to="480" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Name-It: Naming and detecting faces in news video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimed</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Confidence fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Workshop on Robot Sensing</title>
		<imprint>
			<biblScope unit="page" from="96" to="99" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Dempster-shafer theory based finger print classifier fusion with update rule to minimize training time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Electron. Express</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="429" to="435" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Facesync: A linear operator for measuring synchronization of video facial images and audio tracks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Society</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">High-level feature detection from video in TRECVid: a 5-year retrospective of achievements</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Content Analysis, Theory and Applications</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="151" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A review on multimodal video indexing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Multimodal video indexing: A review of the state-of-the-art</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="35" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
	<note>Singapore</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Computational models for experiences in the arts and multimedia</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ACM Workshop on Experiential Telepresence</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Automated audio-visual activity analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<idno>MIT-CSAIL-TR-2005-057</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Joint audio-video object localization and tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Spors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rabenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Real time audio-visual person tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Talantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pnevmatikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Polymenakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 8th Workshop on Multimedia Signal Processing</title>
		<meeting><address><addrLine>Victoria, BC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="243" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Confidence-based data management for personal area sensor networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Data Management for Sensor Networks</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Group-based event detection in undersea sensor networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Workshop on Networked Sensing Systems</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Models for audiovisual fusion in a noisy-vowel recognition task</title>
		<author>
			<persName><forename type="first">P</forename><surname>Teissier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guerin-Dugue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Signal Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="25" to="44" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Multilevel context representation using semantic metanetwork</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Teriyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puuronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International and Interdisciplinary Conference on Modeling and Using Context</title>
		<meeting><address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>Rio de Janeiro</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Data modeling strategies for imbalanced learning in visual search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tesic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lexing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1990" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Multi-sensory and multi-modal fusion for sentient computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Town</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="235" to="253" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Sequential monte carlo fusion of sound and vision for speaker tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="741" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Learning collection fusion strategies</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Singular Value Decomposition and Principal Component Analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rechtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="109" />
			<pubPlace>Kluwel, Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Experience-based sampling technique for multimedia analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="319" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Experiential sampling for video surveillance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Workshop on Video Surveillance</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Efficient sampling of training set in large and noisy multimedia data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Multimedia content analysis: using both audio and visual clues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="12" to="36" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Image retrieval: content versus context</title>
		<author>
			<persName><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RIAO Content-Based Multimedia Information Access</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Sensor data fusion for context-aware computing using dempster-shafer theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Multimodal information fusion for video concept detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2391" to="2394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Multimodal metadata fusion using causal strength</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Tsengh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Optimal multimodal fusion for multimedia data analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York City, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="572" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Multi-level fusion of audio and visual features for speaker identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Biometrics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Layered dynamic mixture model for pattern discovery in asynchronous multi-modal streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1053" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Multi-sensor management for information fusion: issues and approaches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="163" to="186" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A novel framework for semantic annotation and personalized retrieval of sports video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Using webcast text for semantic event detection in broadcast sports video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Fusion of AV features and external information sources for event detection in team sports video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Probabilistic models for combining diverse knowledge sources in multimedia retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Learning query-class dependent weights in automatic video retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A multimodal fusion system for people detection and tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Object tracking in an outdoor environment using fusion of features and cameras</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1244" to="1255" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">The 9th Pacific Rim International Conference on Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5" to="10" />
			<pubPlace>Guilin</pubPlace>
		</imprint>
	</monogr>
	<note>Learning with unlabeled data and its application to image retrieval</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Multimodal fusion using learned text concepts for image categorization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Santa Barbara</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Joint audio-visual tracking using particle filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Zotkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1154" to="1164" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Tracking humans using multimodal fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
