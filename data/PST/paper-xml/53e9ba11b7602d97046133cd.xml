<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Urban 3D Semantic Modelling Using Stereo Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sunando</forename><surname>Sengupta</surname></persName>
							<email>ssengupta@brookes.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Oxford Brookes University</orgName>
								<address>
									<addrLine>2 2d3 Ltd</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Greveson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Shahrokni</surname></persName>
							<email>ali.shahrokni@2d3sensing.co.uk</email>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philiptorr@brookes.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Oxford Brookes University</orgName>
								<address>
									<addrLine>2 2d3 Ltd</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IEEE International Conference on Robotics and Automation (ICRA) Karlsruhe</orgName>
								<address>
									<addrLine>May 6-10</addrLine>
									<postCode>2013, 2013</postCode>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Urban 3D Semantic Modelling Using Stereo Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">994BFFBD2F754748C381F12644C4AD80</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a robust algorithm that generates an efficient and accurate dense 3D reconstruction with associated semantic labellings. Intelligent autonomous systems require accurate 3D reconstructions for applications such as navigation and localisation. Such systems also need to recognise their surroundings in order to identify and interact with objects of interest. Considerable emphasis has been given to generating a good reconstruction but less effort has gone into generating a 3D semantic model.</p><p>The inputs to our algorithm are street level stereo image pairs acquired from a camera mounted on a moving vehicle. The depth-maps, generated from the stereo pairs across time, are fused into a global 3D volume online in order to accommodate arbitrary long image sequences. The street level images are automatically labelled using a Conditional Random Field (CRF) framework exploiting stereo images, and label estimates are aggregated to annotate the 3D volume. We evaluate our approach on the KITTI odometry dataset and have manually generated ground truth for object class segmentation. Our qualitative evaluation is performed on various sequences of the dataset and we also quantify our results on a representative subset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In this paper we propose a robust computer vision algorithm that uses images from stereo cameras mounted on a vehicle to generate a dense 3D semantic model of an urban environment. In our 3D model, every voxel is either assigned to a particular object category like road, pavement, car, etc., free space or object's interior. We are motivated by the fact that autonomous robots navigating in an urban environment need to determine their path and recognise the objects in the scene <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b4">[5]</ref> .</p><p>Currently most autonomous vehicles rely on laser based systems which provide sparse 3D information <ref type="bibr" target="#b14">[15]</ref> or a locally metric topological representation of the scene <ref type="bibr" target="#b11">[12]</ref>. Sparse laser-based systems lack the details that are required for classifying objects of interest <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref> and for accurate boundary predictions <ref type="bibr" target="#b19">[20]</ref>. To obtain an accurate understanding of the scene, a dense metric representation is required <ref type="bibr" target="#b15">[16]</ref>. We show that such a representation can be obtained using a vision based system. Moreover, compared to normal cameras, the laser sensors are expensive and power hungry, can interfere with other sensors, and have a limited vertical resolution <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently, Newcombe et al. <ref type="bibr" target="#b15">[16]</ref> proposed a system for dense 3D reconstruction using a hand-held camera. The dense model is generated from overlapping depth-maps computed using every image pixel instead of sparse features, thus adding richness to the final model. Geiger et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method for fast dense reconstruction of road scenes from stereo images. Their method uses a point cloud representation which is updated by averaging the estimated 3D points and as a consequence, can quickly suffer from accumulated drift.</p><p>In the context of object class segmentation, computer vision algorithms have been effectively applied to the semantics of road scenes <ref type="bibr" target="#b2">[3]</ref>. These algorithms work in the image domain where every pixel in the image is classified into an object label such as car, road, pavement etc. Object class segmentation in the image domain was extended to generate a semantic overhead map of an urban scene from street level images <ref type="bibr" target="#b19">[20]</ref>, or a coarse 3D interpretation in the form of blocks in <ref type="bibr" target="#b9">[10]</ref>, and with a stixel representation in <ref type="bibr" target="#b6">[7]</ref>. The most related to our work is <ref type="bibr" target="#b13">[14]</ref>, where a joint representation of object labelling and disparity estimation is performed in the image domain. However, none of these methods deliver a dense and accurate 3D surface estimation.</p><p>In this paper we perform a 3D semantic modelling for large scale urban environments. Our approach is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> and a sample output of our system is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The input to our system is a sequence of calibrated, stereo image pairs rectified so that the image scan lines correspond to epipolar lines. We use a robust visual odometry method with effective feature matching to track the camera poses ( § II-A). The estimated camera poses are used to fuse the depth-maps generated from stereo pairs, producing a volumetric 3D representation of the scene. This is done online to enable reconstruction over long street image sequence ( § II-B). In parallel, the pixels in the input views are semantically classified using a CRF model. The label predictions are aggregated across the sequence in a robust manner to generate the final 3D semantic model ( § II-C). We evaluate both object labelling and odometry results our method on the KITTI <ref type="bibr" target="#b7">[8]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SEMANTIC 3D RECONSTRUCTION OF THE WORLD</head><p>In this section we explain the individual stages of the semantic 3D reconstruction pipeline in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Camera Pose Estimation</head><p>The camera pose estimation has two main steps, namely feature matching and bundle adjustment. We assume calibrated stereo cameras positioned on a rigid rig. Feature matching: The feature matching comprises of two stages, stereo matching and frame by frame matching. For stereo matching we try to find potential matches across the epipolar line based on the sum of squared differences score of an 8 × 8 patch surrounding the candidate pixel (for an image of resolution 1241 × 376). Each potential match is cross-checked to ensure it lies in a valid range (i.e. minimum depth/maximum disparity magnitude) and the fact that points must be in front of both cameras. The image matches are cross-checked for both left-right and right-left pairs and the agreed matches are kept. After the list of stereo matches is obtained, we perform frame to frame matching for both left and right images. The basic approach is similar to the stereo matching framework, except that we do not rely on epipolar constraints.</p><p>Once the matches are computed, the corresponding feature tracks are generated. All the stereo matches which also have corresponding frame-to-frame matches are kept in the track. Having this agreement between both the stereo and egomotion helps the bundle adjuster to estimate the camera poses and feature points more accurately by rejecting false matches, and simplifies the feature point initialisation phase in the bundle adjuster. We use a bundle method where our optimiser estimates camera poses and the associated features viewed by the last n cameras, leading to lower accumulated drift by reducing noise over n frames. In our experiments we set n = 20 which we found to be a good compromise between speed and accuracy. The example result of bundle adjustment is shown in Fig. <ref type="figure" target="#fig_2">3</ref>, where the camera track and the 3D points are overlayed manually on the Google map demonstrating a near perfect match between the tracked camera positions and the actual street layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Surface Reconstruction</head><p>For generating the surface, we first estimate the depth maps from stereo pairs. These are merged using the Truncated Signed Distance Function (TSDF) and finally a mesh is created using marching tetrahedra algorithm. The individual steps are described in detail below.</p><p>Depth Map Generation: Given a rectified stereo image pair, a depth map is computed from the disparity image as: z i = B.f /d i , where z i and d i are the depth and the disparity corresponding to the i th pixel respectively. The terms B and f are the camera baseline and the focal length, which are computed from the camera matrix. The disparity is computed using the OpenCV implementation of Semi-Global Block Matching (SGBM) method <ref type="bibr" target="#b10">[11]</ref>. The depth values are clipped based on a permissible disparity range.</p><p>TSDF Volume Estimation: Each depth map with estimated camera parameters is fused incrementally into a single 3D reconstruction using the volumetric TSDF representation <ref type="bibr" target="#b3">[4]</ref>. A signed distance function corresponds to the distance to the closest surface interface (zero crossing), with positive values corresponding to free space, and negative values corresponding to points behind the surface. The representation allows for the efficient registration of multiple surface measurements, by globally averaging the distance measures from every depth map at each point in space.</p><p>We assume that the depth of the true surface lies within ±µ of the observed values from the depth maps. So the points that lie in the visible space at a distance greater than µ are truncated to µ. The points beyond µ in the non-visible side are ignored. The TSDF values are computed for each depth map. They are merged using an approach similar to <ref type="bibr" target="#b15">[16]</ref> where an averaging of all TSDF's is performed. This smoothens out the irregularities in the surface normals of the individual depth estimate computed from a single stereo pair.</p><p>Online Volume Update: As we are reconstructing road scenes which can run from hundreds of meters to kilometers, we use an online grid update method. We consider an active 3×3×1 grid of voxel volumes at any time of the fusion. We allow for only one volume in the vertical direction assuming minor elevation changes compared to the direction of motion. For every new depth map, the grid is updated. As the vehicle track goes out of the range of current grid, the current grid blocks are written to memory and a new grid is initialised. This allows us to handle arbitrarily long sequence without losing any granularity of the surface.</p><p>Triangulated Meshing using Marching Tetrahedra: In order to obtain a complete meshed surface, we first infer an iso-surface from the TSDF field by finding all the zero crossings. Then we use the Marching Tetrahedra algorithm <ref type="bibr" target="#b16">[17]</ref> to extract a triangulated mesh of the zero valued iso-surface. Fig. <ref type="figure" target="#fig_3">4</ref> shows an example output of the surface reconstruction. The reconstructed model captures fine details which is evident with the pavements, cars, road and vegetation. Once the rendered mesh is obtained, the faces of the mesh are associated with object class labellings. This is more efficient than performing labelling of all the 3D points and still produces a dense labelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Model Generation</head><p>We use a Conditional Random Field (CRF) based approach that performs a pixel-wise classification on the street level images similar to <ref type="bibr" target="#b12">[13]</ref> which is briefly described below. Consider a set of random variables X = {X 1 , X 2 , . . . , X n }, where each variable X i ∈ X takes a value from a predefined label set L = {l 1 , l 2 , . . . , l k }. A labelling x refers to any possible assignment of labels to the random variables and takes values from the set L = L N . The random field is defined over a lattice V = {1, 2, . . . , N }, where each lattice point, or pixel, i ∈ V is associated with its corresponding random variable X i . Let N be the neighbourhood system of the random field defined by sets N i , ∀i ∈ V, where N i denotes the set of all neighbours (usually the 4 or 8 nearest pixels) of the variable X i . A clique c is defined as a set of random variables X c which are conditionally dependent on each other. The corresponding energy E(x) is given by: E(x) = c∈C ψ c (x c ), where the term ψ c (x c ) is known as the potential function of the clique c, and C is the set of all the cliques. The most probable or maximum a posteriori labelling x * of the CRF is defined as: x * = arg min x∈L E(x). The energy minimisation problem is solved using a graph-cut based Alpha Expansion algorithm <ref type="bibr" target="#b1">[2]</ref>.</p><p>Street Level Image Segmentation: For our application, the label set is L = {pavement, building, road, vehicle, vegetation, signage, pedestrian, wall/fence, sky, post/pole}. We used the associative hierarchical CRF <ref type="bibr" target="#b12">[13]</ref> which combines features and classifiers at different levels of the hierarchy (pixels and superpixels). The Gibbs energy for a street-level image is:</p><formula xml:id="formula_0">E(x) = i∈V ψ i (x i ) + i∈V,j∈Ni ψ ij (x i , x j ) + i∈V,j∈Ni ψ d ij (x i , x j ) + c∈C ψ c (x c )<label>(1)</label></formula><p>Fig. <ref type="figure">5</ref>: Label fusion scheme. The white dots Z fi are the sampled points on the face of a mesh triangle. The corresponding image points x j fi are obtained by projecting the face points onto the labelled street image. The mesh is labelled with the class label with the most votes on the mesh face.</p><p>Unary potential: The unary potential ψ i describes the cost of a single pixel taking a particular label. We have used the multi-feature variant of the TextonBoost algorithm <ref type="bibr" target="#b12">[13]</ref>.</p><p>Pairwise potentials: The pairwise term ψ ij is an edge preserving potential which induces smoothness in the solution by encouraging neighbouring pixels take the same label. It is defined over an neighbourhood of eight pixels taking the form of a contrast sensitive Potts model:</p><formula xml:id="formula_1">ψ ij (x i , x j ) = 0 if x i = x j , g(i, j) otherwise,<label>(2)</label></formula><p>, where the function g(i, j) is an edge feature based on the difference in colours of neighbouring pixels <ref type="bibr" target="#b12">[13]</ref>, defined as:</p><formula xml:id="formula_2">g(i, j) = θ p + θ v exp(-θ β ||I i -I j || 2 2 ),<label>(3)</label></formula><p>where I i and I j are the colour vectors of pixels i and j respectively. θ p , θ v , θ β ≥ 0 are model parameters set by cross validation. The disparity potential ψ d ij (x i , x j ) takes the same form as the pairwise potential but operates on the disparity image, where neighbouring pixels with similar disparity are encouraged to take same labels. Adding information from both image and disparity domain helps us to achieve more consistent results (we give equal importance to both these terms). An alternative potential based on the full depth map could be considered, however the back projected points can be sparse in the image domain, which is not suitable for the per-pixel inference used here.</p><p>Higher Order Potential: The higher order term ψ c (x c ) describes potentials defined over overlapping superpixels as described in <ref type="bibr" target="#b12">[13]</ref>. The potential encourages the pixels in a given segment to take the same label and penalises partial inconsistency of superpixels. This captures longer range contextual information.</p><p>Semantic Label Fusion : Once the street level image segmentations are obtained, the label predictions are fused as follows: for each triangulated face f in the generated mesh model, we randomly sample i points (Z fi ) on the face. The points are projected back in to K images using the estimated camera pose (P k ), resulting in a set of image points (x k fi ). The label predictions for all those image points are aggregated and the majority label is taken as the label of the face in the output model. The label histogram Z f for the face f is given as:</p><formula xml:id="formula_3">Z f = 1 + i∈Z f i k∈K δ(x k i = l) |L|<label>(4)</label></formula><p>where l is a label in our label-set L and |L| is the number of the labels in the label set. This provides a naive probability estimation for a mesh face taking label l. We set i = 10 in our experiments. The fusion step is illustrated in Fig. <ref type="figure">5</ref>.</p><p>The white dots in the model (top) are projected back to the images. The class label prediction of all those image points are aggregated to generate the histogram of labels and the final label prediction is made accordingly. Instead of considering all the points in the face triangle, sampling a few random points is fast, provides robustness to noise and avoids aliasing problems. This is better than considering only the face vertices label in the image because in the final model the face vertices' labels are likely to coincide with the object class boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>To demonstrate the effectiveness of our proposed system, we have used the publicly available KITTI dataset <ref type="bibr" target="#b7">[8]</ref> for our experiments. The images are 1241 × 376 at full resolution. They are captured using a specialised car in urban, residential and highway locations, making it a varied and challenging real world dataset. We have manually annotated a set of 45 images for training and 25 for testing with per-pixel class labels. The class labels are road, building, vehicle, pedestrian, pavement, tree, sky, signage, post/pole, wall/fence<ref type="foot" target="#foot_1">1</ref> .</p><p>We evaluate our camera pose estimation using two metrics, translation error (%) and rotation error (degrees/m) over an increasing number of frames with the ground truth provided by <ref type="bibr" target="#b7">[8]</ref> of sequence 8 (see table <ref type="table" target="#tab_0">I</ref>). We evaluate our sliding window bundle method (full ) and a fast variant of that. The fast method performs the Levenberg-Marquardt minimisation for two successive frame pairs to estimate camera pose and the feature points. As expected the average error for the full method reduces with increasing number of frames. Also the absolute magnitude of error for the fast method is larger than compared to the full method. Our full bundle method runs takes around 3.5 seconds per frame on a single core machine. However the fast method runs at approximately 4 fps. The feature extraction takes about 0.02 seconds, feature matching (both stereo and frame to frame) takes 0.2 seconds per frame. For disparity map extraction we use OpenCV implementation of semi-global block matching stereo <ref type="bibr" target="#b10">[11]</ref> which takes around 0.5 seconds for the full sized 1280 × 376 image. The TSDF stage is highly parallelisable as each voxel in the TSDF volume can be treated separately. Currently, our implementation considers around 10 million voxels per 3 × 3 × 1 grid of TSDF volume, running on a single core. All these steps can be optimised using the GPU implementation <ref type="bibr" target="#b18">[19]</ref>. Fig. <ref type="figure" target="#fig_4">6</ref> shows the qualitative results of the street level image segmentation using our CRF framework. The first column shows the street-level images captured by the vehicle. The second and the third column show the semantic image segmentation of the street images and the corresponding ground truth. A qualitative view of our 3D semantic model is shown in Fig. <ref type="figure" target="#fig_5">7</ref>. The arrows relate the positions in the 3D model and the corresponding images. We can see in  Next we describe the quantitative evaluation. For object level classification, we use an approach similar to <ref type="bibr" target="#b19">[20]</ref>. As generating ground truth data for large sequences is expensive, we evaluate our model by projecting the semantic labels of the model back into the image domain using the estimated camera poses. Points in the reconstructed model that are far away from the particular camera (&gt; 20m) are ignored. The projection is illustrated in Fig. <ref type="figure" target="#fig_7">9</ref>. We show quantitative results on two metrics, recall and intersection vs union measures, for both street image segmentation and semantic model. Our results are summarised in table II. 'Global' refers to the overall percentage of pixels correctly </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: 3D semantic reconstruction. The figure shows a sample output of our system. Dense 3D semantic reconstruction along with class labels is shown in the left and the surface reconstruction is shown in the right. Bottom right shows one of the image corresponding to the scene.</figDesc><graphic coords="1,313.20,173.55,244.80,107.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: System Overview. (a) Shows the input to our method which is a sequence of rectified image pairs. The disparity map (b) is computed from the images and (c) is the camera track estimation. The outputs of (b) and (c) are merged to obtain a volumetric representation of the scene (d). (e) shows the semantic segmentation of the street images which is then fused into a 3D semantic model of the scene (f). Best viewed in colour.</figDesc><graphic coords="2,107.58,54.00,396.82,156.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Bundle adjustment results, showing camera centres and 3D points, registered manually to the Google map.</figDesc><graphic coords="2,77.19,495.36,198.42,192.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Volumetric surface reconstruction. Top figure shows the 3D surface reconstruction over 250 frames (KITTI sequence 15, frames 1-250) with street image shown at the bottom. The arrow highlights the relief of the sidewalk which is correctly captured in the 3D model.</figDesc><graphic coords="3,343.47,54.00,184.25,229.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Semantic image segmentation: The top row shows the input street-level images and the middle row shows the output of the CRF labeller. The bottom row shows the corresponding ground truth for the images.</figDesc><graphic coords="4,313.20,54.00,244.80,122.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Closeup view of the 3D model. The arrows relate the image locations and the positions in the 3D model.</figDesc><graphic coords="5,54.00,526.35,244.80,180.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Semantic model of the reconstructed scene overlayed with the corresponding Google Earth image. The inset image shows the Google earth track of the vehicle.</figDesc><graphic coords="5,313.20,54.00,244.81,190.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: 3D semantic model evaluation. (a) shows the 3D semantic model. (b) shows the input image (top), corresponding image with labels back-projected from the 3D model (middle) and the ground truth image (bottom).</figDesc><graphic coords="5,313.20,581.30,244.80,101.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Odometry results: Translational and rotational error with increasing number of frames.</figDesc><table><row><cell></cell><cell cols="2">Trans. Error (%)</cell><cell cols="2">Rot. error (degs/m)</cell></row><row><cell>Length (frames)</cell><cell>fast</cell><cell>full</cell><cell>fast</cell><cell>full</cell></row><row><cell>5</cell><cell>12.2</cell><cell>12.15</cell><cell>0.035</cell><cell>0.032</cell></row><row><cell>10</cell><cell>11.84</cell><cell>11.82</cell><cell>0.028</cell><cell>0.026</cell></row><row><cell>50</cell><cell>8.262</cell><cell>8.343</cell><cell>0.021</cell><cell>0.018</cell></row><row><cell>100</cell><cell>4.7</cell><cell>4.711</cell><cell>0.019</cell><cell>0.013</cell></row><row><cell>150</cell><cell>3.951</cell><cell>3.736</cell><cell>0.017</cell><cell>0.01</cell></row><row><cell>200</cell><cell>3.997</cell><cell>3.409</cell><cell>0.015</cell><cell>0.009</cell></row><row><cell>250</cell><cell>4.226</cell><cell>3.209</cell><cell>0.013</cell><cell>0.007</cell></row><row><cell>300</cell><cell>4.633</cell><cell>3.06</cell><cell>0.012</cell><cell>0.007</cell></row><row><cell>350</cell><cell>5.057</cell><cell>2.939</cell><cell>0.011</cell><cell>0.006</cell></row><row><cell>400</cell><cell>5.407</cell><cell>2.854</cell><cell>0.01</cell><cell>0.004</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>2d3 Ltd. has been funded by the Defence Science and Technology Laboratory (DSTL), Futures and Innovation Domain. DSTL is part of the UK Ministry of Defence.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>available at http://cms.brookes.ac.uk/research/visiongroup/projects.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank David Capel, Paul Sturgess and Vibhav Vineet for their valuable input. P. H. S. Torr is in receipt of Royal Society Wolfson Research Merit Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this evaluation we have not considered the class 'sky' which is not captured in the model. Due to lack of test data we have also not included the classes 'pedestrian' and 'signage' in our evaluation. As expected, after backprojection the classification accuracy for the model reduces due to errors in camera estimate, when compared with street image segmentation results. This would especially affect the thin object classes like 'poles/posts' where small error in projection leads to large errors in the evaluation. Classes like 'vegetation', where the surface measurement tends to be noisy, have increased error in classification. Our system is designed to model static objects in the scene, which causes an adverse effect when considering moving objects such as cars which is reflected in the results. To evaluate the accuracy of the structure, we use the ground truth depth measurement from Velodyne lasers as provided in <ref type="bibr" target="#b7">[8]</ref>. The depth measurements from both the Velodyne lasers (δ g i ) and our generated model (δ i ) are projected back into the image and evaluated. We measure the number of pixels that satisfy |δ i -δ g i | ≥ δ, where δ is the allowed error in pixels. The results of our method are shown in Fig. <ref type="figure">10</ref>. δ ranges between 1 to 8 pixels. It can be noted that the estimated structural accuracy at δ = 5 pixels is around 88% which indicates the performance of the structure estimation.</p><p>IV. CONCLUSION</p><p>We have presented a novel computer vision-based system for 3D semantic modelling and reconstruction of urban environments. The input to our system is a stereo video feed from a moving vehicle. Our system robustly tracks the camera poses which are used to fuse the stereo depth-maps into a TSDF volume. The iso-surface in the TSDF space corresponding to the scene model is then augmented with semantic labels. This is done by fusing CRF-based semantic inference results using the input frames. We have demonstrated desirable results both qualitatively and quantitatively on a large urban sequence from the KITTI dataset <ref type="bibr" target="#b7">[8]</ref>. In future we would like to perform semantic labelling and reconstruction jointly, where we would like to exploit the depth while performing object labelling. We believe this will improve the overall performance of our system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping (slam): Part ii</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="108" to="117" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised monocular road detection in desert terrain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dahlkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification and semantic mapping of urban environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<imprint>
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stixmentation -probabilistic stixel based traffic scene labeling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Providence, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stereoscan: Dense 3d reconstruction in real-time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2011-06">june 2011</date>
			<biblScope unit="page" from="963" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blocks world revisited: image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, ECCV&apos;10</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="482" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Navigation in hybrid metric-topological maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marder-Eppstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3041" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint optimisation for object class segmentation and dense stereo reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Clocksin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards fully autonomous driving: Systems and algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Askeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sokolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-06">june 2011</date>
			<biblScope unit="page" from="163" to="168" />
			<pubPlace>IV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011-11">nov. 2011</date>
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surface mapping brain functions on 3d models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast probabilistic labeling of city maps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">May 9-13 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic dense visual semantic mapping from street-level imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012-10">oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving robot navigation in structured outdoor environments by identifying vegetation from laser data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Wurm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kümmerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1217" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
