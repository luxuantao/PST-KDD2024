<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MPNet: Masked and Permuted Pre-training for Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
							<email>kt.song@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MPNet: Masked and Permuted Pre-training for Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pretraining method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a largescale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous stateof-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-training models <ref type="bibr" target="#b14">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr">Radford et al., 2019b;</ref><ref type="bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref type="bibr">Raffel et al., 2019a)</ref> have greatly boosted * Authors contribute equally to this work. Correspondence to Tao Qin: taoqin@microsoft.com 1 https://github.com/microsoft/MPNet the accuracy of NLP tasks in the past years. One of the most successful models is BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which mainly adopts masked language modeling (MLM) for pre-training<ref type="foot" target="#foot_0">2</ref> . MLM leverages bidirectional context of masked tokens efficiently, but ignores the dependency among the masked (and to be predicted) tokens <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type="bibr" target="#b30">(Yang et al., 2019</ref>) introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens. However, PLM has its own limitation: Each token can only see its preceding tokens in a permuted sequence but does not know the position information of the full sentence (e.g., the position information of future tokens in the permuted sentence) during the autoregressive pre-training, which brings discrepancy between pre-training and fine-tuning. Note that the position information of all the tokens in a sentence is available to BERT while predicting a masked token.</p><p>In this paper, we find that MLM and PLM can be unified in one view, which splits the tokens in a sequence into non-predicted and predicted parts. Under this unified view, we propose a new pretraining method, masked and permuted language modeling (MPNet for short), which addresses the issues in both MLM and PLM while inherits their advantages: 1) It takes the dependency among the predicted tokens into consideration through permuted language modeling and thus avoids the issue of BERT; 2) It takes position information of all tokens as input to make the model see the position information of all the tokens in the sentence and thus alleviates the position discrepancy of XLNet.</p><p>We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in <ref type="bibr" target="#b30">Yang et al. (2019)</ref>; <ref type="bibr">Liu et al. (2019a)</ref>, and fine-tune on a variety of down-streaming benchmark tasks, including GLUE, SQuAD, RACE and IMDB. Experimental results show that MPNet outperforms MLM and PLM by a large margin, which demonstrates that 1) the effectiveness of modeling the dependency among the predicted tokens (MPNet vs. MLM), and 2) the importance of the position information of the full sentence (MPNet vs. PLM). Moreover, MPNet outperforms previous well-known models BERT, XLNet and RoBERTa by 4.6, 3.2 and 1.3 points respectively on GLUE tasks under the same model setting, indicating the great potential of MP-Net for language understanding<ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MPNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>The key of pre-training methods <ref type="bibr" target="#b14">(Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Song et al., 2019;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b1">Clark et al., 2020)</ref> is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation. For language understanding, masked language modeling (MLM) in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> are two representative objectives. In this section, we briefly review MLM and PLM, and discuss their pros and cons.</p><p>MLM in BERT BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> is one of the most successful pre-training models for natural language understanding. It adopts Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> as the feature extractor and introduces masked language model (MLM) and next sentence prediction as training objectives to learn bidirectional representations. Specifically, for a given sentence</p><formula xml:id="formula_0">x = (x 1 , x 2 , ? ? ? , x n ),</formula><p>MLM randomly masks 15% tokens and replace them with a special symbol [M ]. Denote K as the set of masked positions, x K as the set of masked tokens, and x \K as the sentence after masking. As shown in the example in the left side of Figure <ref type="figure" target="#fig_0">1</ref></p><formula xml:id="formula_1">(a), K = {2, 4}, x K = {x 2 , x 4 } and x \K = (x 1 , [M ], x 3 , [M ],</formula><p>x 5 ). MLM pre-trains the model ? by maximizing the following objective</p><formula xml:id="formula_2">log P (x K |x \K ; ?) ? k?K log P (x k |x \K ; ?). (1)</formula><p>PLM in XLNet Permuted language model (PLM) is proposed in XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> to retain the benefits of autoregressive modeling and also allow models to capture bidirectional context. For a given sentence x = (x 1 , x 2 , ? ? ? , x n ) with length of n, there are n! possible permutations. Denote Z n as the permutations of set {1, 2, ? ? ? , n}. For a permutation z ? Z n , denote z t as the t-th element in z and z &lt;t as the first t -1 elements in z. As shown in the example in the right side of Figure <ref type="figure" target="#fig_0">1</ref>(b), z = (1, 3, 5, 2, 4), and if t = 4, then z t = 2, x zt = x 2 and z &lt;t = {1, 3, 5}. PLM pretrains the model ? by maximizing the following objective</p><formula xml:id="formula_3">log P (x; ?) = E z?Zn n t=c+1 log P (x zt |x z&lt;t ; ?),</formula><p>(2) where c denotes the number of non-predicted tokens x z&lt;=c . In practice, only a part of last tokens x z&gt;c (usually c = 85% * n) are chosen to predict and the remaining tokens are used as condition in order to reduce the optimization difficulty <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>.</p><p>Pros and Cons of MLM and PLM We compare MLM and PLM from two perspectives: the dependency in the predicted (output) tokens and the discrepancy between pre-training and fine-tuning in the input sentence.</p><p>? Output Dependency: As shown in Equation <ref type="formula">1</ref>, MLM assumes the masked tokens are independent with each other and predicts them separately, which is not sufficient to model the complicated context dependency in natural language <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. In contrast, PLM factorizes the predicted tokens with the product rule in any permuted order, as shown in Equation <ref type="formula">2</ref>, which avoids the independence assumption in MLM and can better model dependency among predicted tokens.</p><p>? Input Discrepancy Since in fine-tuning of downstream language understanding tasks, a model can see the full input sentence, to reduce equiv.</p><p>x 1 <ref type="bibr">[M]</ref> x 3 P 1</p><p>[M] x 5</p><p>x 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>x 2 P 2 P 3 P 4 P 5</p><p>x 1 x 3 x 5</p><formula xml:id="formula_4">P 1 [M] [M]</formula><p>x 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>x 4 P 3 P 5 P 2 P 4 (a) MLM equiv.</p><p>x 1 x 2 x 3 P 1</p><p>x 4 x 5</p><p>x 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>x 2 P 2 P 3 P 4 P 5</p><p>x 1 x 3 x 5 P 1</p><p>x 2 x 4 the discrepancy between pre-training and finetuning, the model should see as much information as possible of the full sentence during pretraining. In MLM, although some tokens are masked, their position information (i.e., the position embeddings) are available to the model to (partially) represent the information of full sentence (how many tokens in a sentence, i.e., the sentence length). However, each predicted token in PLM can only see its preceding tokens in a permuted sentence but does not know the position information of the full sentence during the autoregressive pre-training, which brings discrepancy between pre-training and fine-tuning.</p><p>As can be seen, PLM is better than MLM in terms of leveraging output dependency while worse in terms of pre-training and fine-tuning discrepancy. A natural question then arises: can we address the issues in both MLM and PLM while inherit their advantages?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Unified View of MLM and PLM</head><p>To address the issues and inherit the advantages of MLM and PLM, in this section, we provide a unified view to understand MLM and PLM. Both BERT and XLNet take Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> as their backbone. Transformer takes tokens and their positions in a sentence as input, and is not sensitive to the absolute input order of those tokens, only if each token is associated with its correct position in the sentence.</p><p>This inspires us to propose a unified view for MLM and PLM, which rearranges and splits the tokens into non-predicted and predicted parts, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. For MLM in Figure <ref type="figure" target="#fig_0">1</ref>(a), the input in the left side is equal to first permuting the sequence and then masking the tokens in rightmost (x 2 and x 4 are masked in the permuted sequence (x 1 , x 3 , x 5 , x 2 , x 4 ) as shown in the right side). For PLM in Figure <ref type="figure" target="#fig_0">1</ref>(b), the sequence (x 1 , x 2 , x 3 , x 4 , x 5 ) is first permuted into (x 1 , x 3 , x 5 , x 2 , x 4 ) and then the rightmost tokens x 2 and x 4 are chosen as the predicted tokens as shown in the right side, which equals to the left side. That is, in this unified view, the non-masked tokens are put in the left side while the masked and to be predicted tokens are in the right side of the permuted sequence for both MLM and PLM.</p><p>Under this unified view, we can rewrite the objective of MLM in Equation <ref type="formula">1</ref>as</p><formula xml:id="formula_5">E z?Zn n t=c+1 log P (x zt |x z&lt;=c , M z&gt;c ; ?), (3)</formula><p>where M z&gt;c denote the mask tokens [M] in position z &gt;c . As shown in the example in Figure <ref type="figure" target="#fig_0">1</ref>(a), n = 5, c = 3, x z&lt;=c = (x 1 , x 3 , x 5 ), x z&gt;c = (x 2 , x 4 ) and M z&gt;c are two mask tokens in position z 4 = 2 and z 5 = 4. We also put the objective of PLM from Equation <ref type="formula">2</ref>here</p><formula xml:id="formula_6">E z?Zn n t=c+1 log P (x zt |x z&lt;t ; ?).<label>(4)</label></formula><p>As can be seen from Equation <ref type="formula">3</ref>and 4, under this unified view, MLM and PLM share similar mathematical formulation but just with slight difference in the conditional part in P (x zt |?; ?): MLM conditions on x z&lt;=c and M z&gt;c , and PLM conditions on x z&lt;t . In the next subsection, we describe how to modify the conditional part to address the issues and inherit the advantages of MLM and PLM.</p><p>x 1 x 3 x 5 P 1 [M] [M] Transformer P 3 P 5 P 4 P 6</p><p>[M] P 2</p><p>x 4 x 6 x 2 x 4 P 4</p><p>x 6 P 6</p><p>x 2 P 2 (a) query key/value</p><formula xml:id="formula_7">[M] [M] [M] + [M] + [M] + x 4 + x 6 + x 2 + x 5 + x 3 + x 1 + x 1 x 2</formula><p>x 6</p><p>x 4</p><p>[M]</p><p>[M]</p><p>[M]</p><p>x 5</p><p>x 3 non-predicted predicted Since some attention masks in content and query stream are overlapped, we use black lines to denote them in (a). Each row in (b) represents the attention mask for a query position and each column represents a key/value position.</p><p>The predicted part x z&gt;c = (x 4 , x 6 , x 2 ) is predicted by the query stream. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our Proposed Method</head><p>As can be seen, MPNet conditions on x z&lt;t (the tokens preceding the current predicted token x zt ) rather than only the non-predicted tokens x z&lt;=c in MLM as shown in Equation <ref type="formula">3</ref>; comparing with PLM as shwon in Equation <ref type="formula" target="#formula_6">4</ref>, MPNet takes more information (i.e., the mask symbol [M ] in position z &gt;c ) as inputs. Although the objective seems simple, it is challenging to implement the model efficiently. To this end, we describe several key designs of MPNet in the following paragraphs.</p><p>Input Tokens and Positions For a token sequence x = (x 1 , x 2 , ? ? ? , x 6 ) with length n = 6, we randomly permute the sequence and get a permuted order z = (1, 3, 5, 4, 6, 2) and a permuted sequence x z = (x 1 , x 3 , x 5 , x 4 , x 6 , x 2 ), where the length of the non-predicted part is c = 3, the non-predicted part is x z&lt;=c = (x 1 , x 3 , x 5 ), and the predicted part is x z&gt;c = (x 4 , x 6 , x 2 ). Additionally, we add mask tokens M z&gt;c right before the predicted part, and obtain the new input tokens</p><formula xml:id="formula_9">(x z&lt;=c , M z&gt;c , x z&gt;c ) = (x 1 , x 3 , x 5 , [M ], [M ], [M ],</formula><p>x 4 , x 6 , x 2 ) and the corresponding position sequence (z &lt;=c , z &gt;c , z &gt;c ) = (p 1 , p 3 , p 5 , p 4 , p 6 , p 2 , p 4 , p 6 , p 2 ), as shown in Figure <ref type="figure" target="#fig_2">2a</ref>. In MPNet, (x z&lt;=c , M z&gt;c ) = (x 1 , x 3 , x 5 , [M ], [M ], [M ]) are taken as the nonpredicted part, and x z&gt;c = (x 4 , x 6 , x 2 ) are taken as the predicted part. For the non-predicted part (x z&lt;=c , M z&gt;c ), we use bidirectional modeling <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>  Modeling Output Dependency with Two-Stream Self-Attention For the predicted part x z&gt;c , since the tokens are in permuted order, the next predicted token could occur in any position, which makes it difficult for normal autoregressive prediction. To this end, we follow PLM to adopt two-stream self-attention <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> to autoregressively predict the tokens, which is illustrated in Figure <ref type="figure" target="#fig_5">3</ref>. In two-stream self-attention, the query stream can only see the previous tokens and positions as well as current position but cannot see current token, while the content stream can see all the previous and current tokens and positions, as shown in Figure <ref type="figure" target="#fig_2">2a</ref>. For more details about two-stream self-attention, please refer to <ref type="bibr" target="#b30">Yang et al. (2019)</ref>. One drawback of two-stream self-attention in PLM is that it can only see the the previous Table <ref type="table">1</ref>: An example sentence "the task is sentence classification" to illustrate the conditional information of MLM, PLM and MPNet.</p><p>x 1 x 3 x 5 P 1</p><p>[M] [M] P 3 P 5 P 4 P 6</p><p>[M] P 2</p><p>x 4 x 6 x 2 P 4 P 6 P 2 [M] [M] tokens in the permuted sequence, but does not know the position information of the full sentence during the autoregressive pre-training, which brings discrepancy between pre-training and fine-tuning. To address this limitation, we modify it with position compensation as described next.</p><formula xml:id="formula_10">P 4 P 6 [M] P 2 Content Stream Q K, V Q K, V Query Stream ? ? Content Stream Q K, V Q K, V Query Stream</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reducing Input Discrepancy with Position Compensation</head><p>We propose position compensation to ensure the model can see the full sentence, which is more consistent with downstream tasks. As shown in Figure <ref type="figure" target="#fig_2">2b</ref>, we carefully design the attention masks for the query and content stream to ensure each step can always see n tokens, where n is the length of original sequence (in the above example, n = 6) 4 . For example, when predicting token x z 5 = x 6 , the query stream in the original two-stream attention <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> takes mask token M z 5 = [M ] and position p z 5 = p 6 as the attention query, and can only see previous tokens x z &lt;5 = (x 1 , x 3 , x 5 , x 4 ) and positions p z &lt;5 = (p 1 , p 3 , p 5 , p 4 ) in the content stream, but cannot see positions p z &gt;=5 = (p 6 , p 2 ) and thus miss the full-sentence information. Based on our position compensation, as shown in the second last line of the query stream in Figure <ref type="figure" target="#fig_2">2b</ref>  positions p z &gt;=5 = (p 6 , p 2 ). The position compensation in the content stream follows the similar idea as shown in Figure <ref type="figure" target="#fig_2">2b</ref>. In this way, we can greatly reduce the input discrepancy between pre-training and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Advantage</head><p>The main advantage of MPNet over BERT and XLNet is that it conditions on more information while predicting a masked token, which leads to better learnt representations and less discrepancy with downstream tasks.</p><p>As shown in Table <ref type="table">1</ref>, we take a sentence [The, task, is, sentence, classification] as an example to compare the condition information of MP-Net/BERT/XLNet. Suppose we mask two words [sentence, classification] for prediction. As can be seen, while predicting a masked word, MPNet conditions on all the position information to capture a global view of the sentence (e.g., the model knows that there two missing tokens to predict, which is helpful to predict two tokens "sentence classification" instead of three tokens "sentence pair classification"). Note that PLM does not have such kind of information. Furthermore, to predict a word, MPNet conditions on all preceding tokens including the masked and predicted ones for better context modeling (e.g., the model can better predict "classification" given previous token "sentence", instead of predicting "answering" as if to predict "question answering"). In contrast, MLM does not condition on other masked tokens. Based on the above example, we can derive Ta-ble 2, which shows how much conditional information is used on average to predict a masked token in each pre-training objective. We assume all the three objectives mask and predict the same amount of tokens (15%), following the common practice in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> <ref type="foot" target="#foot_2">5</ref> . As can be seen, MLM conditions on 85% tokens and 100% positions since masked tokens contains position information; PLM conditions on all the 85% unmasked tokens and positions and 15%/2 = 7.5% masked tokens and positions<ref type="foot" target="#foot_3">6</ref> , resulting in 92.5% tokens and positions in total. MP-Net conditions on 92.5% tokens similar to PLM, but 100% positions like that in MLM thanks to the position compensation.</p><p>To summarize, MPNet utilizes the most information while predicting masked tokens. On the one hand, MPNet can learn better representations with more information as input; on the other hand, MP-Net has the minimal discrepancy with downstream language understanding tasks since 100% token and position information of an input sentence is available to a model for those tasks (e.g., sentence classification tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We conduct experiments under the BERT base setting (BERT BASE ) <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads as 12, and 110M model parameters in total. For the pretraining objective of MPNet, we randomly permute the sentence following PLM <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>  <ref type="foot" target="#foot_4">7</ref> , choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. Additionally, we also apply whole word mask <ref type="bibr" target="#b2">(Cui et al., 2019)</ref> and relative positional embedding <ref type="bibr" target="#b21">(Shaw et al., 2018)</ref> <ref type="foot" target="#foot_5">8</ref> in our model pretraining since these tricks have been successfully validated in previous works <ref type="bibr" target="#b30">(Yang et al., 2019;</ref><ref type="bibr">Raffel et al., 2019b)</ref>.</p><p>For pre-training corpus, we follow the data used in RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref>, which includes: Wikipedia and BooksCorpus <ref type="bibr" target="#b31">(Zhu et al., 2015)</ref>, OpenWebText <ref type="bibr">(Radford et al., 2019a)</ref>, CC-News <ref type="bibr">(Liu et al., 2019b)</ref> and <ref type="bibr">Stories (Trinh and Le, 2018)</ref>, with 160GB data size in total. We use a subword dictionary with 30K BPE codes in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> to tokenize the sentences. We limit the length of sentences in each mini-batch up to 512 tokens following the previous practice <ref type="bibr">(Liu et al., 2019b;</ref><ref type="bibr" target="#b7">Joshi et al., 2019)</ref> and use a batch size of 8192 sentences. We use Adam (Kingma and Ba, 2014) with ? 1 = 0.9, ? 2 = 0.98 and = 1e -6. We pre-train our model for 500K steps to be comparable with state-of-the-art models like XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref> and ELECTRA <ref type="bibr" target="#b1">(Clark et al., 2020)</ref>. We use 32 NVIDIA Tesla V100 GPUs, with 32GB memory and FP16 for pre-training, which is estimated to take 35 days<ref type="foot" target="#foot_6">9</ref> .</p><p>During fine-tuning, we do not use query stream in two-stream self-attention and use the original hiddens to extract context representations following <ref type="bibr" target="#b30">Yang et al. (2019)</ref>. The fine-tuning experiments on each downstream tasks are conducted 5 times and the median value is chosen as the final result. For experimental comparison, we mainly compare MPNet with previous state-of-the-art pre-trained models using the same BERT BASE setting unless otherwise stated. We will also pre-train MPNet in BERT LARGE setting in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on GLUE Benchmark</head><p>The General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b27">(Wang et al., 2019)</ref> is a collection of 9 natural language understanding tasks, which include two single-sentence tasks (CoLA <ref type="bibr" target="#b28">(Warstadt et al., 2018)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (MRPC <ref type="bibr" target="#b5">(Dolan and Brockett, 2005)</ref>, STS-B <ref type="bibr" target="#b0">(Cer et al., 2017)</ref>, QQP), four inference tasks (MNLI <ref type="bibr" target="#b29">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b3">(Dagan et al., 2006)</ref>, WNLI <ref type="bibr" target="#b10">(Levesque et al., 2012)</ref>). We follow RoBERTa hyper-parameters for single-task finetuning, where RTE, STS-B and MRPC are started from the MNLI fine-tuned model.</p><p>We list the results of MPNet and other existing strong baselines in Table <ref type="table" target="#tab_3">3</ref>. All of the listed results are reported in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84.5 RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref> 87 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD v1.1 EM F1</head><p>BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 80.8 88.5 RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref>  single model without any data augmentation for fair comparisons. On the dev set of GLUE tasks, MPNet outperforms BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> and RoBERTa <ref type="bibr">(Liu et al., 2019a)</ref> by 4.6, 3.2, 1.3 points on average. On the test set of GLEU tasks, MPNet outperforms ELECTRA <ref type="bibr" target="#b1">(Clark et al., 2020)</ref>, which achieved previous state-of-the-art accuracy on a variety of language understanding tasks, by 0.5 point on average, demonstrating the advantages of MPNet for language understanding. Since MPNet is still under pre-training, we believe there still exists large improvement space for MPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on Question Answering (SQuAD)</head><p>The Stanford Question Answering Dataset (SQuAD) task requires to extract the answer span from the provided context based on the question. We evaluate our model on SQuAD v1.1 <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b19">(Rajpurkar et al., 2018)</ref>. SQuAD v1.1 always exists the corresponding answer for each question and the corresponding context, while some questions do not have the corresponding answer in SQuAD v2.0. For v1.1, we add a classification layer on the outputs from the pre-trained model to predict whether the token is a start or end position of the answer. For v2.0, we additionally add a binary classification layer to predict whether the answer exists.</p><p>The results of MPNet on SQuAD dev set are reported on Table <ref type="table" target="#tab_4">4</ref>. All of the listed results are reported in BERT BASE setting and from single model without any data augmentation for fair comparisons. It can be seen that MPNet outperforms BERT, XLNet and RoBERTa by a large margin, both in SQuAD v1.1 and v2.0, which are consistent with the results on GLUE tasks, demonstrating the advantages of MPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on RACE</head><p>The ReAding Comprehension from Examinations (RACE) <ref type="bibr" target="#b9">(Lai et al., 2017)</ref>  four choices. The task is to select the correct choice based on the given options. The results on RACE task are listed in Table <ref type="table" target="#tab_6">6</ref>. We can only find the results from BERT and XLNet pre-trained on Wikipedia and BooksCorpus (16GB data). For a fair comparison, we also pre-train MP-Net on 16GB data (marked as * in Table <ref type="table" target="#tab_6">6</ref>). MPNet greatly outperforms BERT and XLNet across the three metrics, demonstrating the advantages of MP-Net. When pre-training MPNet with the default 160GB data, an additional 1.6 points gain (72.0 vs. 70.4) can be further achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on IMDB</head><p>We further study MPNet on the IMDB text classification task <ref type="bibr" target="#b13">(Maas et al., 2011)</ref>, which contains over 50,000 movie reviews for binary sentiment classification. The results are reported in Table <ref type="table" target="#tab_6">6</ref>. It can be seen that MPNet trained on Wikipedia and BooksCorpus (16GB data) outperform BERT and PLM (XLNet) by 0.6 and 0.1 point. When pre-training with 160GB data, MPNet achieves additional 0.4 point gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>We further conduct ablation studies to analyze several design choices in MPNet, including introducing dependency among predicted tokens to MLM, introducing position compensation to PLM, etc. The results are shown in Table <ref type="table" target="#tab_5">5</ref>. We have several observations:</p><p>? After removing position compensation, MPNet degenerates to PLM, and its accuracy drops by 0.6-2.3 points in downstream tasks. This demonstrates the effectiveness of position compensation and the advantage of MPNet over PLM.</p><p>? After removing permutation operation but still keeping the dependency among the predicted tokens with two-stream attention (i.e., MLM + output dependency), the accuracy drops slightly by 0.5-1.7 points. This verifies the gain of permutation used in MPNet.</p><p>? If removing both permutation and output dependency, MPNet degenerates to MLM, and its accuracy drops by 0.5-3.7 points, demonstrating the advantage of MPNet over MLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed MPNet, a novel pretraining method that addresses the problems of MLM in BERT and PLM in XLNet. MPNet leverages the dependency among the predicted tokens through permuted language modeling and makes the model to see auxiliary position information to reduce the discrepancy between pre-training and fine-tuning. Experiments on various tasks demonstrate that MPNet outperforms MLM and PLM, as well as previous strong pre-trained models such as BERT, XLNet, RoBERTa by a large margin. In the future, we will pre-train MPNet with larger model setting for better performance, and apply MPNet on more diverse language understanding tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A unified view of MLM and PLM, where x i and p i represent token and position embeddings. The left side in both MLM (a) and PLM (b) are in original order, while the right side in both MLM (a) and PLM (b) are in permuted order and are regarded as the unified view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The structure of MPNet. (b) The attention mask of MPNet. The light grey lines in (a) represent the bidirectional self-attention in the non-predicted part(x z&lt;=c , M z&gt;c ) = (x 1 , x 5 , x 3 , [M ], [M ], [M ]), which correspond to the light grey attention mask in (b). The blue and green mask in (b) represent the attention mask in content and query streams in two-stream self-attention, which correspond to the blue, green and black lines in (a). Since some attention masks in content and query stream are overlapped, we use black lines to denote them in (a). Each row in (b) represents the attention mask for a query position and each column represents a key/value position. The predicted part x z&gt;c = (x 4 , x 6 , x 2 ) is predicted by the query stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2 illustrates the key idea of MPNet. The training objective of MPNet is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to extract the representations, which is illustrated as the light grey lines in Figure 2a. Next, we describe how to model the dependency among the predicted part in next paragraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) log P (sentence | the task is [M] [M]) + log P (classification | the task is [M] [M]) PLM (XLNet) log P (sentence | the task is) + log P (classification | the task is sentence) MPNet log P (sentence | the task is [M] [M]) + log P (classification | the task is sentence [M])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The two-stream self-attention mechanism used in MPNet, where the query stream re-uses the hidden from the content stream to compute key and value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>P 1 P 3 P 5 P 4 P 6 P 2 P 4 P 6 P 2</figDesc><table><row><cell></cell><cell></cell><cell>can be attended</cell></row><row><cell></cell><cell></cell><cell>cannot be attended</cell></row><row><cell></cell><cell></cell><cell>Content Stream</cell></row><row><cell>[M]</cell><cell>predicted</cell><cell>Query Stream</cell></row><row><cell>non-predicted</cell><cell>predicted</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, the query stream can see additional tokens M z&gt;=5 = ([M ], [M ]) and 4 One trivial solution is to let the model see all the input tokens, i.e., 115% * n tokens, but introduces new discrepancy as the model can only see 100% * n tokens during fine-tuning.</figDesc><table><row><cell>Objective</cell><cell cols="2"># Tokens # Positions</cell></row><row><cell>MLM (BERT)</cell><cell>85%</cell><cell>100%</cell></row><row><cell>PLM (XLNet)</cell><cell>92.5%</cell><cell>92.5%</cell></row><row><cell>MPNet</cell><cell>92.5%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The percentage of input information (tokens and positions) leveraged in MLM, PLM and MPNet, assuming they predict the same amount (15%) of tokens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between MPNet and the previous strong pre-trained models under BERT BASE setting on the dev and test set of GLUE tasks. We only list the results on each set that are available in the published papers. STS is reported by Pearman correlation, CoLA is reported by Matthew's correlation, and other tasks are reported by accuracy.</figDesc><table><row><cell></cell><cell>.6</cell><cell>92.8</cell><cell>91.9 78.7 94.8</cell><cell>90.2</cell><cell>63.6</cell><cell>91.2 86.4</cell></row><row><cell>MPNet</cell><cell>88.5</cell><cell>93.3</cell><cell>91.9 85.2 95.4</cell><cell>91.5</cell><cell>65.0</cell><cell>90.9 87.7</cell></row><row><cell>Single model on test set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT (Devlin et al., 2019)</cell><cell>84.6</cell><cell>90.5</cell><cell>89.2 66.4 93.5</cell><cell>84.8</cell><cell>52.1</cell><cell>87.1 79.9</cell></row><row><cell>ELECTRA (Clark et al., 2020)</cell><cell>88.5</cell><cell>93.1</cell><cell>89.5 75.2 96.0</cell><cell>88.1</cell><cell>64.6</cell><cell>91.0 85.8</cell></row><row><cell>MPNet</cell><cell>88.5</cell><cell>93.0</cell><cell>89.6 80.5 95.6</cell><cell>88.2</cell><cell>64.0</cell><cell>90.7 86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between MPNet and the previous strong pre-trained models under BERT BASE setting on the SQuAD dev set. We report results by exact match (EM) and F1 score.</figDesc><table><row><cell></cell><cell cols="2">84.6 91.5</cell></row><row><cell>MPNet</cell><cell cols="2">86.8 92.5</cell></row><row><cell>SQuAD v2.0</cell><cell>EM</cell><cell>F1</cell></row><row><cell>BERT (Devlin et al., 2019)</cell><cell cols="2">73.7 76.3</cell></row><row><cell>XLNet (Yang et al., 2019)</cell><cell>80.2</cell><cell>-</cell></row><row><cell cols="3">RoBERTa (Liu et al., 2019a) 80.5 83.7</cell></row><row><cell>MPNet</cell><cell cols="2">82.8 85.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>is a large-scale dataset collected from the English examinations from middle and high school students. In RACE, each passage has multiple questions and each question has Ablation study of MPNet under BERT BASE setting on the dev set of SQuAD tasks (v1.1 and v2.0) and GLUE tasks (MNLI and SST-2). The experiments in ablation study are all pre-trained on the Wikipedia and BooksCorpus (16GB size) for 1M steps, with a batch size of 256 sentences, each sentence with up to 512 tokens.</figDesc><table><row><cell>SQuAD v1.1 SQuAD v2.0</cell><cell>GLUE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on the RACE and IMDB test set under BERT BASE setting. For RACE, the results of BERT are from the RACE leaderboard 10 and the results of XL-Net are obtained from the original paper<ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. "Middle" and "High" denote the accuracy on the middle school set and high school set in RACE. For IMDB, the result of BERT is from<ref type="bibr" target="#b24">Sun et al. (2019)</ref> and the result of XLNet is ran by ourselves with only PLM pre-training objective but no long context memory<ref type="bibr" target="#b30">(Yang et al., 2019)</ref>. "*" represents pre-training only on Wikipedia and BooksCorpus (16GB size).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We do not consider next sentence prediction here since previous works<ref type="bibr" target="#b30">(Yang et al., 2019;</ref> Liu et al., 2019a;<ref type="bibr" target="#b7">Joshi et al., 2019)</ref> have achieved good results without next sentence prediction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We are still pre-training MPNet in larger model setting. We also welcome someone who has computation resources to pre-train larger MPNet models based on our released code to further advance the state of the art.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>XLNet masks and predicts 1/7 tokens, which are close to 15% predicted tokens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>PLM is actually a language model, which predicts the i-th token given the previous i -1 tokens. Therefore, the number of tokens can be leveraged on average is (n -1)/2 where n is number of predicted tokens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Note that we only improve upon PLM in XLNet, and we do not use long-term memory in XLNet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We followRaffel et al. (2019b)  to adopt a shared relative position embedding across each layer for efficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Note that MPNet is still under pre-training with only 270K steps, but has already outperformed previous models. We will update the results once pre-trained for 500K steps.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>http://www.qizhexie.com/data/RACE leaderboard.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pre-training Hyper-parameters</head><p>The pre-training hyper-parameters are reported in Table <ref type="table">7</ref>. <ref type="bibr">Hyper</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fine-tuning Hyper-parameters</head><p>The fine-tuning hyper-parameters are reported in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Ablation Studies</head><p>We further conduct more experiments to analyze the effect of whole word mask and relative positional embedding. The results are reported in Table <ref type="table">9</ref>.</p><p>Hyper-parameter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RACE SQuAD GLUE</head><p>Learning </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-training with whole word masking for chinese BERT</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08101</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Winograd Schema Challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>text transformer. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">How to fine-tune BERT for text classification?</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1905.05583</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bowman</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>CoRR, abs/1805.12471</idno>
	</analytic>
	<monogr>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
