<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion Context: A New Representation for Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Multimedia and Network Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqun</forename><surname>Hu</surname></persName>
							<email>yqhu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Multimedia and Network Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Syin</forename><surname>Chan</surname></persName>
							<email>asschan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Multimedia and Network Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang-Tien</forename><surname>Chia</surname></persName>
							<email>asltchia@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Multimedia and Network Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motion Context: A New Representation for Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7E9212E4E97819E2BEB3B4BA6F298AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the key challenges in human action recognition from video sequences is how to model an action sufficiently. Therefore, in this paper we propose a novel motion-based representation called Motion Context (MC), which is insensitive to the scale and direction of an action, by employing image representation techniques. A MC captures the distribution of the motion words (MWs) over relative locations in a local region of the motion image (MI) around a reference point and thus summarizes the local motion information in a rich 3D MC descriptor. In this way, any human action can be represented as a 3D descriptor by summing up all the MC descriptors of this action. For action recognition, we propose 4 different recognition configurations: MW+pLSA, MW+SVM, MC+w 3 -pLSA (a new direct graphical model by extending pLSA), and MC+SVM. We test our approach on two human action video datasets from KTH and Weizmann Institute of Science (WIS) and our performances are quite promising. For the KTH dataset, the proposed MC representation achieves the highest performance using the proposed w 3 -pLSA. For the WIS dataset, the best performance of the proposed MC is comparable to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of advanced security systems, human action recognition in video sequences has become an important research topic in computer vision, whose aim is to make machines recognize human actions using different types of information, especially the motion information, in the video sequences.</p><p>The basic process for this problem can be divided into three issues: First, how to detect the existence of human actions? Second, how to represent human actions? Lastly, how to recognize these actions? Many research works have been done to address these issues (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>). In this paper, we mainly focus on the second issue, that is, how to represent human actions after having detected their existence. In our approach, we model each video sequence as a collection of so-called motion images (MIs), and to model the action in each MI, we propose a novel motion-based representation called motion context <ref type="bibr">(MC)</ref>, which is insensitive to the scale and direction of an action, to capture the distribution of the motion words (MWs) over relative locations in a local region around a reference point and thus summarize the local motion information in a rich, local 3D MC descriptor. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates some MIs and their corresponding MC representations using the video clips in the KTH dataset.</p><p>To describe an action, only one 3D descriptor is generated by summing up all the MC descriptors of this action in the MIs. For action recognition, we employ 3 different approaches: pLSA <ref type="bibr" target="#b6">[7]</ref>, w 3 -pLSA (a new direct graphical model by extending pLSA) and SVM <ref type="bibr" target="#b7">[8]</ref>. Our approach is tested on two human action video datasets from KTH <ref type="bibr" target="#b1">[2]</ref> and Weizmann Institute of Science <ref type="bibr" target="#b8">[9]</ref>, and the performances are quite promising. The rest of this paper is organized as follows: Section 2 reviews some related works in human action recognition. Section 3 presents the details of our MC representation. Section 4 introduces the 3 recognition approaches. Our experimental results are shown in Section 5, and finally Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Each video sequence can be considered as a collection of consecutive images (frames), which makes it possible to model human actions using some image representation techniques. One influential model is the Bag-of-Words (BOW) model (e.g. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>). This model represents each human action as a collection of independent codewords in a pre-defined codebook generated from the training data. However, videos contain temporal information while images do not. So how to exploit this temporal information becomes a key issue for human action representation.</p><p>Based on image representation techniques, many research works have shown that temporal information can be integrated with the interesting point detectors and descriptors to locate and describe the interesting points in the videos. Laptev et al. <ref type="bibr" target="#b0">[1]</ref> proposed a 3D interesting point detector where they added the temporal constraint to the Harris interesting point detector to detect local structures in the space-time dimensions. Efros et al. <ref type="bibr" target="#b11">[12]</ref> proposed a motion descriptor using the optical flow from different frames to represent human actions. Recently, Scovanner et al. <ref type="bibr" target="#b3">[4]</ref> applied sub-histograms to encode local temporal and spatial information to generate a 3D version of SIFT <ref type="bibr" target="#b12">[13]</ref> (3D SIFT), and Savarese et al. <ref type="bibr" target="#b13">[14]</ref> proposed so-called "spatial-temporal correlograms" to encode flexible long range temporal information into the spatial-temporal motion features.</p><p>However, a common issue behind these interesting point detectors is that the detected points sometimes are too few to sufficiently characterize the human action behavior, and hence reduce the recognition performance. This issue has been avoided in <ref type="bibr" target="#b5">[6]</ref> by employing the separable linear filter method <ref type="bibr" target="#b2">[3]</ref>, rather than such space-time interesting point detectors, to obtain the motion features using a quadrature pair of 1D Gabor filters temporally.</p><p>Another way of using temporal information is to divide a video into smaller groups of consecutive frames as the basic units and represent a human action as a collection of the features extracted from these units. In <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, every three consecutive frames in each video were grouped together and integrated with their graphical models as a node to learn the spatial-temporal relations among these nodes. Also in <ref type="bibr" target="#b15">[16]</ref>, the authors took the average of a sequence of binary silhouette images of a human action to create the "Average Motion Energy" representation. Similarly, <ref type="bibr" target="#b16">[17]</ref> proposed a concept of "Motion History Volumes", an extension of "Motion History Images" <ref type="bibr" target="#b17">[18]</ref>, to capture the motion information from a sequence of video frames.</p><p>After the human action representations have been generated, both discriminative approaches (e.g. kernel approaches <ref type="bibr" target="#b1">[2]</ref>) and generative approaches (e.g. pLSA <ref type="bibr" target="#b18">[19]</ref>, MRF <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, semi-LDA <ref type="bibr" target="#b9">[10]</ref>, hierarchical graphical models <ref type="bibr" target="#b5">[6]</ref>) can be employed to recognize them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motion Context Representation</head><p>A motion context representation is generated based on the motion words which are extracted from the motion images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motion Image</head><p>We believe that effective utilization of the temporal information is crucial for human action recognition. In our approach, we adopt the strategy in <ref type="bibr" target="#b16">[17]</ref>, that is, to group the consecutive frames of each video sequence according to their temporal information.</p><p>More specifically, to generate a motion image (MI), first U * V frames of a video sequence are extracted, converted into gray scale and divided into nonoverlapping U groups, each with V consecutive frames. Then we calculate the standard deviation (stdev) among the frames within a group pixel by pixel to detect the motion information. Finally, putting the stdev values into the corresponding pixel positions, a MI is generated for each frame group. Fig. <ref type="figure">2</ref>  consecutive frames. Since stdev can measure the variances of the pixel intensity values, it can definitely detect motions.</p><p>We would like to mention that the length of each group, V , should be long enough to capture the motion information sufficiently but not too long. Fig. <ref type="figure" target="#fig_1">3</ref> illustrates the effects of different V on the MIs of human running and walking. If V = 5, the difference between the two actions is quite clear. With V increased to 60, the motion information of both actions spreads in the MIs, making it difficult to distinguish them. A further investigation of V will be essential in our MC representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motion Word</head><p>The concept of motion words (MWs) refers to that of visual words in the BOW model. After generating the MIs, some image interesting point detectors are first applied to locate the important patches in the MIs. Then image descriptors are employed to map these patches into a high dimensional feature space to generate local feature vectors for them. Next, using clustering approaches such as Kmeans, these local feature vectors in the training data are clustered to generate a so-called motion word dictionary where the centers of the clusters are treated as the MWs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Motion Context</head><p>For each MW, there is one important affiliated attribute, its location in the corresponding MI. For human action recognition, the relative movements of different parts of the body are quite useful. To capture the structures of these relative movements, we introduce the concept of motion context (MC). This concept is inspired by Shape Context (SC) <ref type="bibr" target="#b19">[20]</ref>, which has been widely used in object recognition. The basic idea of SC is to locate the distribution of other shape points over relative positions in a region around a pre-defined reference point. Subsequently, 1D descriptors are generated to represent the shapes of objects.</p><p>In our representation, we utilize the polar coordinate system to capture the relative angles and distances between the MWs and the reference point (the pole of the polar coordinate system) for each action in the MIs, similar to SC. This reference point is defined as the geometric center of the human motion, and the relative distances are normalized by the maximum distance in the support region, which makes the MC insensitive to changes in scale of the action. Here, the support region is defined as the area which covers the human action in the MI. Fig. <ref type="figure" target="#fig_2">4</ref> (left) illustrates our MC representation. Suppose that the angular coordinate is divided into M equal bins, the radial coordinate is divided into N equal bins and there are K MWs in the dictionary, then each MW can be put into one of the M *N bins to generate a 3D MC descriptor for each MC representation, as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref> (right). To represent a human action in each video sequence, we sum up all the MC descriptors of this action to generate one 3D descriptor with the same dimensions.</p><p>When generating MC representations, another factor should also be considered, that is, the direction of the action, because the same action may occur in different directions. E.g. a person may be running in one direction or the opposite direction. In such cases, the distributions of the interesting points in the two corresponding MIs should be roughly symmetric about the y-axis. Combining the two distributions for the same action will reduce the discriminability of our representation. To avoid this, we define the orientation of each MC representation as the sector where most interesting points are detected, e.g. the shaded one (blue) in Fig. <ref type="figure" target="#fig_2">4</ref> (left). This sector can be considered to represent the main characteristics of the motion in one direction. For the same action but in the opposite direction, we then align all the orientations to the pre-defined side by flipping the MC representations horizontally around the y-axis. Thus our representation is symmetry-invariant. Fig. <ref type="figure" target="#fig_3">5</ref> illustrates this process. Notice that this process is done automatically without the need to know the action direction.</p><p>The entire process of modeling human actions using the MC representation is summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. The main steps of modeling the human actions using the MC representation</head><p>Step 1 Obtain the MIs from the video sequences.</p><p>Step 2 Generate the MC representation for each human action in the MIs.</p><p>Step 3 Generate the 3D MC descriptor for each MC representation.</p><p>Step 4 Sum up all the 3D MC descriptors of an action to generate one 3D descriptor to represent this action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Action Recognition Approaches</head><p>We apply 3 different approaches to recognize the human actions based on the MWs or the 3D MC descriptors: pLSA, w 3 -pLSA and SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">pLSA</head><p>pLSA aims to introduce an aspect model, which builds an association between documents and words through the latent aspects by probability. Here, we follow the terminology of text classification where pLSA was used first. The graphical model of pLSA is illustrated in Fig. <ref type="figure" target="#fig_4">6 (a)</ref>. Suppose D = {d 1 , . . . , d I }, W = {w 1 , . . . , w J } and Z = {z 1 , . . . , z K } denote a document set, a word set and a latent topic set, respectively. pLSA models the joint probability of documents and words as: </p><formula xml:id="formula_0">P (d i , w j ) = k P (d i , w j , z k ) = k P (w j |z k )P (z k |d i )P (d i )<label>( 1 )</label></formula><formula xml:id="formula_1">L = i j n(d i , w j ) log P (d i , w j )<label>( 2 )</label></formula><p>where n(d i , w j ) denotes the document-word co-occurrence table, where the number of co-occurrences of d i and w j is recorded in each cell.</p><p>To learn the probability distributions involved, pLSA employs the Expectation Maximization (EM) algorithm shown in Table <ref type="table" target="#tab_1">2</ref> and records P (w j |z k ) for recognition, which is learned from the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">w 3 -pLSA</head><p>To bridge the gap between the human actions and our MC descriptors, we extend pLSA to develop a new graphical model, called w 3 -pLSA. See Fig. <ref type="figure" target="#fig_4">6 (b)</ref>, where d denotes human actions, z denotes latent topics, w, θ and s denote motion words, and the indexes in the angular and radial coordinates in the polar coordinate system, respectively.</p><p>Referring to pLSA, we model the joint probability of human actions, motion words and their corresponding indices in the angular and radial coordinates as</p><formula xml:id="formula_2">P (d i , w j , θ m , s r ) = k P (d i , w j , θ m , s r , z k ) = k P (d i )P (z k |d i )P (w j , θ m , s r |z k ) (3)</formula><p>and maximize the L function below.</p><formula xml:id="formula_3">L = i j m r n(d i , w j , θ m , s r ) log P (d i , w j , θ m , s r )<label>( 4 )</label></formula><p>Similarly, to learn the probability distributions involved, w 3 -pLSA employs the Expectation Maximization (EM) algorithm shown in Table <ref type="table" target="#tab_2">3</ref> and records P (w j , θ m , s r |z k ) for recognition, which is learned from the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Support Vector Machine</head><p>A support vector machine (SVM) <ref type="bibr" target="#b7">[8]</ref> is a powerful tool for binary classification tasks. First it maps the input vectors into a higher dimensional feature space, then it conducts a separating hyperplane to separate the input data, finally on each side of this hyperplane two parallel hyperplanes are conducted. SVM tries to find the separating hyperplane which maximizes the distance between the two parallel hyperplanes. Notice that in a SVM, there is an assumption that the larger the distance between the two parallel hyperplanes the smaller the generalization error of the classifier will be. Specifically, suppose the input data is {(x 1 , y 1 ), (x 2 , y 2 ), • • • , (x n , y n )} where x i (i = 1, 2, • • • , n) denotes the input vector and the corresponding y i (i = 1, 2, • • • , n) denotes the class label (positive "1" and negative "-1"). Then the separating hyperplane is defined as w • x + b = 0 and the two corresponding parallel hyperplanes are w • x + b = 1 for the positive class and w • x + b = -1 for the negative class, where w is the vector perpendicular to the separating hyperplane and b is a scalar. If a test vector x t satisfies w • x t + b &gt; 0, it will be classified as a positive instance. Otherwise, if it satisfies w • x t + b &lt; 0, it will be classified as a negative instance. A SVM tries to find the optimal w and b to maximize the distance between the two parallel hyperplanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our approach has been tested on two human action video datasets from KTH <ref type="bibr" target="#b1">[2]</ref> and Weizmann Institute of Science (WIS) <ref type="bibr" target="#b8">[9]</ref>. The KTH dataset is one of the largest datasets for human action recognition containing six types of human actions: boxing, handclapping, handwaving, jogging, running, and walking. For each type, there are 99 or 100 video sequences of 25 different persons in 4 different scenarios: outdoors (S1), outdoors with scale variation (S2), outdoors with different clothes (S3) and indoors (S4), as illustrated in Fig. <ref type="figure" target="#fig_5">7</ref> (left). In the WIS dataset, there are altogether 10 types of human actions: walk, run, jump, gallop sideways, bend, one-hand wave, two-hands wave, jump in place, jumping jack, and skip. For each type, there are 9 or 10 video sequences of 9 different persons with the similar background, as shown in Fig. <ref type="figure" target="#fig_5">7</ref> (right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation</head><p>To generate MC representations for human actions, we need to locate the reference points and the support regions first. Some techniques in body tracking (e.g. <ref type="bibr" target="#b20">[21]</ref>) can be applied to locate the areas and the geometric centers of the human bodies in each frame group of a video sequence. The integration of the areas of a person can be defined as its support region and the mean of its centers can be defined as the reference point for this action in the MI. However, this issue is beyond the purpose of this paper. So considering that in our datasets each video sequence only contains one person, we simply assume that in each MI the support region of each human action covers the whole MI, and we adopted a simple method to roughly locate the reference points. First, we generated one MI from every 5-frame group of each video sequence empirically. Then a Gaussian filter was applied to denoise these MIs so that the motion information from the background was suppressed. Next, we used the Canny edge detector to locate the edges in each MI, and finally took the geometric center of the edge points as the reference point for the action.</p><p>After locating the reference points, we followed the steps in Table <ref type="table">1</ref> to generate the MC representations for human actions. The detector and descriptor involved in Step 2 are the Harris-Hessian-Laplace detector <ref type="bibr" target="#b21">[22]</ref> and the SIFT descriptor  <ref type="bibr" target="#b12">[13]</ref>, and the clustering method used here is K-means. Then based on the MWs and the MC descriptors of the training data, we trained pLSA, w 3 -pLSA and SVM for each type of actions separately, and a test video sequence was classified to the type of actions with the maximum likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>To show the efficiency of our MC representation and the discriminability of the MWs, we designed 4 different recognition configurations: MW+pLSA, MW+SVM, MC+w 3 -pLSA, and MC+SVM. Here we used libsvm <ref type="bibr" target="#b22">[23]</ref> with the linear kernel. To utilize the MWs, we employed the BOW model to represent each human action as a histogram of the MWs without the M *N spatial bins. First, we tested our approach on the KTH dataset. We adopted two different training strategies: split-data-equally (SDE) and leave-one-out (LOO). The SDE strategy means that the video collection is divided into two equal sets randomly: one as the training data (50 video sequences) and the other as the test data for each type of actions, and we repeated this experiment for 15 times. In the LOO strategy, for each type of actions, only the video sequences of one person are selected as the test data and the rest as the training data, and when applying this strategy to the KTH dataset, for each run we randomly selected one person for each type of actions as the test data and repeated this experiment for 15 times. Empirically, in our model, the number of MWs is 100, and the numbers of the quantization bins in the angular and radial dimensions are 10 and 2, respectively. The number of latent topics in both graphical models is 40.</p><p>Table <ref type="table" target="#tab_3">4</ref> shows our average recognition rate for each type of actions and the comparison with others on the KTH dataset under different training strategies and recognition configurations. From this table, we can draw the following Table <ref type="table">5</ref>. Comparison (%) between our approach and others on the WIS dataset. Notice that "✕" denotes that this type of actions was not involved in their experiments. conclusions: (1) MWs without any spatial information are not discriminative enough to recognize the actions. MW+pLSA returns the best performance (84.65%) using MWs, which is lower than the state of the art. (2) MC representation usually achieves better performances than MWs, which demonstrates that the distributions of the MWs are quite important for action recognition. MC+w 3 -pLSA returns the best performance (91.33%) among all the approaches.</p><p>Unlike the KTH dataset, the WIS dataset only has 9 or 10 videos for each type of human actions, which may result in underfit when training the graphical models. To utilize this dataset sufficiently, we only used the LOO training strategy to learn the models for human actions and tested on all the video sequences. We compare our average recognition rates with others in Table <ref type="table">5</ref>. The experimental configuration of the MC representation is kept the same as that used on the KTH dataset, while the number of MWs used in the BOW model is modified empirically to 300. The number of latent topics is unchanged. From this table, we can see that MC+SVM still returns the best performance (92.89%) among the different configurations, which is comparable to other approaches and higher than the best performance (84.1%) using MW. These results demonstrate that our MC presentation can model the human actions properly with the distributions of the MWs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated that our Motion Context (MC) representation, which is insensitive to changes in the scales and directions of the human actions, can model the human actions in the motion images (MIs) effectively by capturing the distribution of the motion words (MWs) over relative locations in a local region around the reference point and thus summarize the local motion information in a rich 3D descriptor. To evaluate this novel representation, we adopt two training strategies (split-data-equally (SDE) and leave-one-out (LOO)), design 4 different recognition configurations (MW+pLSA, MW+SVM, MC+w 3 -pLSA, and MC+SVM) and test them on two human action video datasets from KTH and Weizmann Institute of Science (WIS). The performances are promising. For the KTH dataset, all configurations using MC outperform existing approaches where the best performances are obtained using w 3 -pLSA (88.67% for SDE and 91.33% for LOO). For the WIS dataset, our MC+SVM returns the comparable performance (92.89%) using the LOO strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustrations of the frame groups, motion images, and our motion context representations on the KTH dataset. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of effects of different lengths of frame groups on the MIs using human running and walking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of our MC representation (left) and its 3D descriptor (right). On the left, P denotes a MW at an interesting point, O denotes the reference point, Θ and S denote the relative angle and normalized distance between P and O in the support region (the black rectangle), respectively, and the shaded sector (blue) denotes the orientation of the whole representation. On the right, each MW is quantized into a point to generate a 3D MC descriptor. This figure is best viewed in color.</figDesc><graphic coords="5,90.53,57.15,112.00,111.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of aligning an inconsistent MC representation of an action in the opposite direction. The pre-defined orientation of the actions is the left side of y-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Graphical models of pLSA (a) and our w 3 -pLSA (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Some sample frames from the KTH dataset (left) and the WIS dataset (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Rec.Con. bend jack jump pjump run side skip walk wave1 wave2 ave. MW+pLSA 77.8 100.0 88.9 88.9 70.0 100.0 60.0 100.0 66.7 88.9 84.1 MW+SVM 100.0 100.0 100.0 77.8 30.0 77.8 40.0 100.0 100.0 100.0 81.44 MC+w 3 -pLSA 66.7 100.0 77.8 66.7 80.0 88.9 100.0 100.0 100.0 100.0 88.0 MC+SVM 100.0 100.0 100.0 88.9 80.0 100.0 80.0 80.0 100.0 100.0 92.89 Wang et al. [16] 100.0 100.0 89.0 100.0 100.0 100.0 89.0 100.0 89.0 100.0 96.7 Ali et al. [26] 100.0 100.0 55.6 100.0 88.9 88.9 100.0 100.0 100.0 92.6 Scovanner [4] 100.0 100.0 67.0 100.0 80.0 100.0 50.0 89.0 78.0 78.0 84.2 Niebles et al. [6] 100.0 100.0 100.0 44.0 67.0 78.0 56.0 56.0 56.0 72.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>illustrates the MI generation process for a frame group. Motions usually cause strong changes in the pixel intensity values at the corresponding positions among the Illustration of the MI generation process for a frame group. The black dots denote the pixel intensity values.</figDesc><table><row><cell>Group of Frames</cell><cell>Motion Image</cell></row><row><cell>Calucate</cell><cell></cell></row><row><cell>Standard Deviation</cell><cell></cell></row><row><cell>Fig. 2.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The EM algorithm for pLSA</figDesc><table /><note><p>E-step: P (z k |di, wj ) ∝ P (wj|z k )P (z k |di)P (di) M-step: P (wj|z k ) ∝ i n(di, wj )P (z k |di, wj) P (z k |di) ∝ j n(di, wj )P (z k |di, wj) P (di) ∝ j n(di, wj )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The EM algorithm for w3 -pLSA E-step: P (z k |di, wj , θm, sr) ∝ P (wj, θm, sr|z k )P (z k |di)P (di) M-step: P (wj, θm, sr|z k ) ∝ i n(di, wj, θm, sr)P (z k |di, wj , θm, sr)</figDesc><table /><note><p>P (z k |di) ∝ j,m,r n(di, wj, θm, sr)P (z k |di, wj , θm, sr) P (di) ∝ j,m,r n(di, wj, θm, sr)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison (%) between our approach and others on the KTH dataset</figDesc><table><row><cell>Rec.Con.</cell><cell cols="8">Tra.Str. boxing hand-c hand-w jogging running walking average</cell></row><row><cell>MW+pLSA</cell><cell>SDE</cell><cell>85.2</cell><cell>91.9</cell><cell>91.7</cell><cell>71.2</cell><cell>73.6</cell><cell>82.1</cell><cell>82.62</cell></row><row><cell></cell><cell>LOO</cell><cell>82.0</cell><cell>90.9</cell><cell>91.0</cell><cell>82.0</cell><cell>79.0</cell><cell>83.0</cell><cell>84.65</cell></row><row><cell>MW+SVM</cell><cell>SDE</cell><cell>90.4</cell><cell>84.8</cell><cell>82.8</cell><cell>65.1</cell><cell>76.1</cell><cell>82.0</cell><cell>80.20</cell></row><row><cell></cell><cell>LOO</cell><cell>85.0</cell><cell>82.8</cell><cell>82.0</cell><cell>62.0</cell><cell>70.0</cell><cell>87.0</cell><cell>78.14</cell></row><row><cell>MC+w 3 -pLSA</cell><cell>SDE</cell><cell>98.4</cell><cell>90.8</cell><cell>93.9</cell><cell>79.3</cell><cell>77.9</cell><cell>91.7</cell><cell>88.67</cell></row><row><cell></cell><cell>LOO</cell><cell>95.0</cell><cell>97.0</cell><cell>93.0</cell><cell>88.0</cell><cell>84.0</cell><cell cols="2">91.0 91.33</cell></row><row><cell>MC+SVM</cell><cell>SDE</cell><cell>91.7</cell><cell>91.6</cell><cell>88.1</cell><cell>78.0</cell><cell>84.7</cell><cell>90.4</cell><cell>87.42</cell></row><row><cell></cell><cell>LOO</cell><cell>88.0</cell><cell>93.9</cell><cell>91.0</cell><cell>77.0</cell><cell>85.0</cell><cell>90.0</cell><cell>87.49</cell></row><row><cell cols="2">Savarese et al. [14] LOO</cell><cell>97.0</cell><cell>91.0</cell><cell>93.0</cell><cell>64.0</cell><cell>83.0</cell><cell>93.0</cell><cell>86.83</cell></row><row><cell>Wang et al. [10]</cell><cell>LOO</cell><cell>96.0</cell><cell cols="2">97.0 100.0</cell><cell>54.0</cell><cell>64.0</cell><cell>99.0</cell><cell>85.00</cell></row><row><cell cols="4">Niebles et al. [19] LOO 100.0 77.0</cell><cell>93.0</cell><cell>52.0</cell><cell>88.0</cell><cell>79.0</cell><cell>81.50</cell></row><row><cell>Dollár et al. [3]</cell><cell>LOO</cell><cell>93.0</cell><cell>77.0</cell><cell>85.0</cell><cell>57.0</cell><cell>85.0</cell><cell>90.0</cell><cell>81.17</cell></row><row><cell>Schuldt et al. [2]</cell><cell>SDE</cell><cell>97.9</cell><cell>59.7</cell><cell>73.6</cell><cell>60.4</cell><cell>54.9</cell><cell>83.8</cell><cell>71.72</cell></row><row><cell>Ke et al. [24]</cell><cell>SDE</cell><cell>69.4</cell><cell>55.6</cell><cell>91.7</cell><cell>36.1</cell><cell>44.4</cell><cell>80.6</cell><cell>62.96</cell></row><row><cell>Wong et al. [25]</cell><cell>SDE</cell><cell>96.0</cell><cell>92.0</cell><cell>83.0</cell><cell>79.0</cell><cell>54.0</cell><cell cols="2">100.0 84.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>D. Forsyth, P. Torr, and A. Zisserman (Eds.): ECCV 2008, Part IV, LNCS 5305, pp. 817-829, 2008. c Springer-Verlag Berlin Heidelberg 2008</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>VS-PETS</editor>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatiotemporal video segmentation based on graphical models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Loe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. IP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="937" to="947" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hierarchical model of shape and appearance for human action classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Hingham, MA, USA; Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-latent dirichlet allocation: A hierarchical model for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sabzmeydani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="240" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Human action recognition using distribution of oriented rectangular patches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="271" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial-temporal correlations for unsupervised action classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sel Pozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Motion and Video Computing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Copper Mountain, Colorado</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video segmentation based on graphical models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Loe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Informative shape representations for human action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="1266" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page">1249</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Shape context: A new descriptor for shape matching and object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="831" to="837" />
		</imprint>
		<respStmt>
			<orgName>NIPS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast human pose estimation using appearance and motion via multi-dimensional boosting regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">166</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning motion categories using both semantic and structural information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chaotic invariants for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
