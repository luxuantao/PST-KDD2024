<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance Evaluation of NoSQL Databases: A Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Klein</surname></persName>
							<email>jklein@sei.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Software Engineering Institute</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Gorton</surname></persName>
							<email>igorton@sei.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Software Engineering Institute</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Ernst</surname></persName>
							<email>nernst@sei.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Software Engineering Institute</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Donohoe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Engineering Institute</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kim</forename><surname>Pham</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Telemedicine and Advanced Technology Research Center US Army Medical Research and Material Command Frederick</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chrisjan</forename><surname>Matser</surname></persName>
							<email>cmatser@codespinnerinc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Telemedicine and Advanced Technology Research Center US Army Medical Research and Material Command Frederick</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance Evaluation of NoSQL Databases: A Case Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B58C252C3DFC36021AEBC5720791620F</idno>
					<idno type="DOI">10.1145/2694730.2694731</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Performance</term>
					<term>Measurement Performance</term>
					<term>NoSQL</term>
					<term>Big Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The choice of a particular NoSQL database imposes a specific distributed software architecture and data model, and is a major determinant of the overall system throughput. NoSQL database performance is in turn strongly influenced by how well the data model and query capabilities fit the application use cases, and so system-specific testing and characterization is required. This paper presents a method and the results of a study that selected among three NoSQL databases for a large, distributed healthcare organization. While the method and study considered consistency, availability, and partition tolerance (CAP) tradeoffs, and other quality attributes that influence the selection decision, this paper reports on the performance evaluation method and results. In our testing, a typical workload and configuration produced throughput that varied from 225 to 3200 operations per second between database products, while read operation latency varied by a factor of 5 and write latency by a factor of 4 (with the highest throughput product delivering the highest latency). We also found that achieving strong consistency reduced throughput by 10-25% compared to eventual consistency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>COTS product selection has been extensively studied in software engineering <ref type="bibr" target="#b1">[1]</ref>[2] <ref type="bibr" target="#b3">[3]</ref>. In complex technology landscapes with multiple competing products, organizations must balance the cost and speed of the technology selection process against the fidelity of the decision <ref type="bibr" target="#b4">[4]</ref>. While there is rarely a single 'right' answer in selecting a complex component for an application, selection of inappropriate components can be costly, reduce downstream productivity due to rework, and even lead to project cancelation. This is especially true for large scale, big data systems due to their complexity and the magnitude of the investment.</p><p>In this context, COTS selection of NoSQL databases for big data applications presents several unique challenges:  This is an early architecture decision that must inevitably be made with incomplete knowledge about requirements;  The capabilities and features of NoSQL products vary widely, making generalized comparisons difficult;  Prototyping at production scale for performance analysis is usually impractical, as this would require hundreds of servers, multi-terabyte data sets, and thousands or millions of clients;  The solution space is changing rapidly, with new products constantly emerging, and existing products releasing several versions per year with ever-evolving feature sets.</p><p>We faced these challenges during a recent project for a healthcare provider seeking to adopt NoSQL technology for an Electronic Health Record (EHR) system. The system supports healthcare delivery for over nine million patients in more than 100 facilities across the globe. Data currently grows at over one terabyte per month, and all data must be retained for 99 years.</p><p>In this paper, we outline a technology evaluation and selection method we have devised for big data systems. We then describe a study we performed for the healthcare provider described above.</p><p>We introduce the study context, our evaluation approach, and the results of both extensive performance and scalability testing and a detailed feature comparison. We conclude by, describing some of the challenges for software architecture and design that NoSQL approaches bring. The specific contributions of the paper are as follows:</p><p> A rigorous method that organizations can follow to evaluate the performance and scalability of NoSQL databases.  Performance and scalability results that empirically demonstrate variability of up to 14x in throughput and 5x in latency in the capabilities of the databases we tested to support the requirements of our healthcare customer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EHR CASE STUDY</head><p>Our method is inspired by earlier work on middleware evaluation <ref type="bibr" target="#b5">[5]</ref>[6] and is customized to address the characteristics of big data systems. The basic main steps are depicted in Figure <ref type="figure" target="#fig_0">1</ref> and outlined below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Project Context</head><p>Our customer was a large healthcare provider developing a new electronic healthcare record (EHR) system to replace an existing system that utilizes thick client applications at sites around the world that access a centralized relational database. The customer decided to consider NoSQL technologies for two specific uses, namely:</p><p> the primary data store for the EHR system  a local cache at each site to improve request latency and availability</p><formula xml:id="formula_0">A c f 2 W d p N W th s m a te E in 2 O d g d o p f C th 2 A p c a c c    </formula><p>As the customer cases, but had n focus the technolo  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Create load test client</head><p>The test client was based on the YCSB framework <ref type="bibr">[7]</ref>, which provides capabilities to manage test execution and perform test measurement. For test execution, YCSB has default data models, data sets, and workloads, which we modified and replaced with implementations specific to our use case data and requests.</p><p>We were able to leverage YCSB's test execution management capabilities to specify the total number of operations to be performed, and the mix of read and write operations in the workload. The test execution capabilities also allow creation of concurrent client sessions using multiple execution threads.</p><p>The YCSB built-in measurement framework measures the latency for each operation performed, as the time from when the request is sent to the database until the response is received back from the database. The YCSB reporting framework records latency measurements separately for read and write operations. Latency distribution is a key scalability measure for big data systems <ref type="bibr" target="#b8">[8]</ref> [9], so we collected both average and 95 th percentile values.</p><p>We extended the YCSB reporting framework to report overall throughput, in operations per second. This measurement was calculated by dividing the total number of operations performed (read plus write) by the workload execution time, measured from the start of the first operation to the completion of the last operation in the workload execution, not including pre-test setup and post-test cleanup times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Define and execute test scripts</head><p>The stakeholder workshop identified a typical workload for the EHR system of 80% read and 20% write operations. For this operation mix, we defined a read operation to retrieve the five most recent observations for a single patient, and a write operation to insert a single new observation record for a single existing patient.</p><p>Our customer was also interested in using NoSQL technology for a local cache, so we defined a write-only workload to represent the daily download from a centralized primary data store of records for patients with scheduled appointments for that day. We also defined a read-only workload to represent flushing the cache back to the centralized primary data store.</p><p>Each test ran the selected workload three times, in order to minimize the impact of any transient events in the cloud infrastructure. For each of these three runs, the workload execution was repeated using a different number of client threads (1, 2, 5, 10, 25, 50, 100, 200, 500, and 1000). Results were post-processed by averaging measurements across the three runs for each thread count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PERFORMANCE AND SCALABILITY RESULTS</head><p>We report here on our results for a nine-node configuration that reflected a typical production deployment. As noted above, we also tested other configurations, ranging from a single server up to a nine-node cluster. The single-node configuration's availability and scalability limitations make it impractical for production use, however, in the discussion that follows we compare the single node configuration to distributed configurations, to provide insights into the efficiency of a database's distributed coordination mechanisms and resource usage, and guides tradeoffs between scaling by adding more nodes versus using faster nodes with more storage.</p><p>Defining a configuration required several design decisions. The first decision was how to distribute client connections across the server nodes. MongoDB uses a centralized router node with all clients connected to that single node. Cassandra's data center aware distribution feature was used to create three sub-clusters of three nodes each, and client connections were spread uniformly across the three nodes in one of the sub-clusters. In the case of Riak, the product architecture only allowed client connections to be spread uniformly across the full set of nine nodes. An alternative might have been to test Riak on three nodes with no replication, however other constraints in the Riak architecture resulted in extremely poor performance in this configuration, and so the nine-node configuration was used.</p><p>A second design decision was how to achieve the desired level of consistency, which requires coordinating write operation settings and read operation settings <ref type="bibr" target="#b10">[10]</ref>. Each of the three databases offered slightly different options, and we explored two approaches, discussed in the next two sections. The first reports results using strong consistency, and the second reports results using eventual consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Using Strong Consistency</head><p>The selected options are summarized in Table <ref type="table" target="#tab_2">1</ref>. For MongoDB, the effect is that all writes were committed on the primary server, and all reads were from the primary server. For Cassandra, the effect is that all writes were committed on a majority quorum at each of the three sub-clusters, while a read required a majority quorum only on the local sub-cluster. For Riak, the effect was to require a majority quorum on the entire nine-node cluster for both write operations and read operations. In all cases, Cassandra provided the best overall performance, with read-only workload performance roughly comparable to the single node configuration, and write-only and read/write workload performance slightly better than the single node configuration. This implies that, for Cassandra, the performance gains that accrue from decreased contention for disk I/O and other per node resources (compared to the single node configuration) are greater than the additional work of coordinating write and read quorums across replicas and data centers. Furthermore, Cassandra's "data center aware" features provide some separation of replication configuration from sharding configuration. In this test configuration, this allowed a larger portion of the read operations to be completed without requiring request coordination (i.e. peer-topeer proxying of the client request), compared to Riak. Riak performance in this representative production configuration was nearly 4x better than the single node configuration. In test runs using the write-only workload and the read/write workload, our Riak client had insufficient socket resources to execute the workload for 500 and 1000 concurrent sessions. These data points are hence reported as zero values in Figures <ref type="figure">3</ref> and<ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W a c s</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 Configurati</head><p>Figure <ref type="figure">3</ref> Co</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4 Co</head><p>We later determ ambiguous doc configuration pa session and not </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATE</head><p>Systematic evalu nsightful compar for an application providing both q qualitative under Gorton describes platforms, which ency, rather than s described abov different opti </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WORK AND</head><p>hnology offers orizontal scaling, pecific implemen is generally pe specific data set processing <ref type="bibr" target="#b11">[11]</ref> sing <ref type="bibr" target="#b12">[12]</ref>. Thes long enabled v h consumers can uration, and infra nchmarks were de vant for NoSQL s on of strong and y of strong and ev y standard for exec CSB++ <ref type="bibr" target="#b13">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USIONS</head><p>alability and nd simplified chosen early in the architecture design process. We have described a systematic method to perform this technology selection in a context where the solution space is broad and changing fast, and the system requirements may not be fully defined. Our method evaluates the products in the specific context of use, starting with elicitation of quality attribute scenarios to capture key architecture drivers and selection criteria. Next, product documentation is surveyed to identify viable candidate technologies, and finally, rigorous prototyping and measurement is performed on a small number of candidates to collect data to make the final selection.</p><p>We described the execution of this method to evaluate NoSQL technologies for an electronic healthcare system, and present the results of our measurements of performance, along with a qualitative assessment of alignment of the NoSQL data model with system-specific requirements.</p><p>There were a number of challenges in carrying out such an performance analysis on big data systems. These included:</p><p> Creating the test environment -performance analysis at this scale requires very large data sets that mirror real application data. This raw data must then be loaded into the different data models that we defined for each different NoSQL database. A minor change to the data model in order to explore performance implications required a full data set reload, which is timeconsuming.  Validating quantitative criteria -Quantitative criteria, with hard "go/no-go" thresholds, were problematic to validate through prototyping, due to the large number of tunable parameters in each database, operating system, and cloud infrastructure. Minor configuration parameter changes can cause unexpected performance effects, often due to non-obvious interactions between the different parameters. In order to avoid entering an endless test and analyze cycle, we framed the performance criteria in terms of the shape of the performance curve, and focused more on sensitivities and inflection points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -Lig</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2.2 Specifyi</head><label></label><figDesc></figDesc><table><row><cell cols="2">was familiar wit th RDMS techno</cell></row><row><cell cols="2">no experience us sing NoSQL, th</cell></row><row><cell cols="2">ogy evaluation o only on NoSQL</cell></row><row><cell cols="2">ghtweight Evalu</cell></row><row><cell></cell><cell>Data (LE</cell></row><row><cell cols="2">ing Require</cell></row><row><cell>We conducted a</cell><cell>stakeholder wo</cell></row><row><cell cols="2">drivers. A key n need that emerge</cell></row><row><cell>performance and</cell><cell>scalability that</cell></row><row><cell>NoSQL database .</cell><cell></cell></row><row><cell cols="2">We worked with h the customer to</cell></row><row><cell cols="2">he EHR system, , which formed t</cell></row><row><cell cols="2">scalability assess sment. The first</cell></row><row><cell cols="2">medical test resul lts for a particula</cell></row><row><cell>achieving strong</cell><cell>consistency for</cell></row><row><cell cols="2">est result is writ tten for a patien</cell></row><row><cell>EHR to make</cell><cell>patient care d</cell></row><row><cell cols="2">nformation abou ut that patient, reg</cell></row><row><cell cols="2">2.3 Select C Candidate N</cell></row><row><cell cols="2">Our customer w was specifically</cell></row><row><cell cols="2">different NoSQL L data models</cell></row><row><cell cols="2">graph) would sup pport their applic</cell></row><row><cell cols="2">database from ea ach category to in</cell></row><row><cell cols="2">out graph datab bases, as they</cell></row><row><cell cols="2">partitioning requi ired for the cust</cell></row><row><cell cols="2">feature assessme ent of various</cell></row><row><cell cols="2">Cassandra and M MongoDB as ou</cell></row><row><cell cols="2">hese are market l leaders in each N</cell></row><row><cell cols="2">2.4 Design a and Execute</cell></row><row><cell cols="2">A thorough eva aluation of com</cell></row><row><cell cols="2">prototyping with h each to reveal</cell></row><row><cell cols="2">capabilities and a allow compariso</cell></row><row><cell cols="2">and performed a a systematic pro</cell></row><row><cell cols="2">comparison of th he three database</cell></row><row><cell cols="2">cases defined dur ring the requirem</cell></row><row><cell cols="2"> Defined a co onsistent test e</cell></row><row><cell cols="2">database, whi ch included serv</cell></row><row><cell cols="2">and network to opology.  Mapped the lo ogical model for</cell></row><row><cell cols="2">data model an nd loaded the da</cell></row><row><cell cols="2">data set.  Created a loa d test client tha</cell></row><row><cell cols="2">write operatio ns defined for ea</cell></row><row><cell cols="2">many concurr rent requests, to</cell></row><row><cell cols="2">responds as th he request load in</cell></row><row><cell cols="2"> Defined and e xecuted test scrip</cell></row><row><cell cols="2">the database u using the test clien</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>te uation and Prot EAP4BD) ments</head><label></label><figDesc></figDesc><table><row><cell cols="3">ology for these u use execution time and the substantial costs of constructing and loading</cell><cell cols="2">Test ca ases were execu uted on several d distributed confi igurations to</cell></row><row><cell cols="2">hey directed us the data set for different data models and test configurations.</cell><cell>to</cell><cell cols="2">measur re performance a and scalability, ra anging from bas seline testing</cell></row><row><cell></cell><cell>echnology.</cell><cell></cell><cell cols="2">on a s single server to o nine server i instances that s sharded and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>replicat ted data.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Based o on this approach h, we were able e to produce test t results that</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">allow c comparison of pe erformance and</cell><cell>scalability of ea ach database</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">for this s customer's EHR R system. Our te est results were n not intended</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">to be g general-purpose</cell><cell>benchmarks, so o tests using oth her types of</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">workloa ads, server con nfigurations, and d data set size es were not</cell></row><row><cell></cell><cell></cell><cell></cell><cell>perform med.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">3. EV VALUATIO ON SETUP</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">3.1 T Test Environ nment</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">The thr ree databases we</cell><cell>tested were:</cell></row><row><cell></cell><cell>otyping for Big</cell><cell></cell><cell cols="2"> Mon ngoDB version 2 2.2, a document s store (http p://docs.mongodb b.org/v2.2/);  Cass sandra version 2. .0, a column stor re</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(http</cell><cell>x.com/document ation/cassandra/2 2.0);</cell></row><row><cell>orkshop to elicit</cell><cell cols="2">requirements an nd</cell><cell> Riak</cell><cell>key-value store</cell></row><row><cell cols="3">ed was to under rstand the inhere ent</cell><cell>(http</cell><cell>om/riak/1.4.10/).</cell></row><row><cell cols="3">is achievable w ith each candida ate</cell><cell>Tests w</cell><cell>on two databa se server config gurations: A</cell></row><row><cell></cell><cell></cell><cell></cell><cell>single</cell><cell>and a nine-nod de configuration n that was</cell></row><row><cell cols="3">o define two driv the basis for our t use case was ar patient. The se r all readers whe nt, because all cl ecisions need t gardless of locati NoSQL Data interested in un ving use cases f r performance an retrieving rece econd use case w en a new medic linicians using th to see the sam for nd ent was cal he me on. . nderstanding ho ow bases</cell><cell>represen validate node geograp The dat replicat Mongo center a data dis stored a was sha</cell><cell>roduction deploy yment. A singl le node test t environment fo or each database e. The nine-used a topol logy that rep presented a uted deployment t across three d data centers. ioned (i.e. "shard ded") across three e nodes, and tional groups of three nodes eac ch. We used secondary featu ure, and Cassa andra's data on feature. Riak k did not suppor rt this "3x3" e used a flattene nodes. ed configuration n where data nine nodes, with h three replicas o of each shard</cell></row><row><cell cols="3">(key-value, co cation, so we sel nvestigate in det did not suppo tomer's requirem products, we r three candidat NoSQL category. olumn, documen lected one NoSQ tail. We later rule ort the horizont ments. Based on settled on Ria te technologies, nt, QL ed tal n a ak, as .</cell><cell cols="2">All te (http://a sting was per rformed using aws.amazon.com m/ec2/). Databa ase servers ex the Amazon EC2 cloud xecuted on "m1.lar rge" instances, w with the database e data and log fil les stored on separate e EBS volumes attached to eac ch server instanc ce. The EBS volume es were not pro ovisioned with the EC2 IOPS feature, to minimi ize the tuning p parameters used in each test co onfiguration. Server instances r ran the Cen ntOS operatin ng system (http://w www.centos.org) ). The test cli ient also execu uted on an</cell></row><row><cell cols="3">e Performan plex database p l the performan nce Tests platforms requir ce and scalabili ity res</cell><cell cols="2">"m1.lar rge" instance, an nd also used the e CentOS operat ating system. cloud d data center). All inst tances were in th he same EC2 av vailability zone (i i.e. the same</cell></row><row><cell cols="3">ns [4]. To this e ocedure for an " es we evaluated. end, we develope "apples to apple Based on the u ed es" use ments step, we:</cell><cell cols="2">3.2 M Mapping the e data mode el We us sed the HL7 F Fast Healthcare Interoperability y Resources (FHIR) ) (http://www.h hl7.org/implemen nt/standards/fhir/ /) for our prototyp yping. The logic cal data model consisted of FH HIR Patient</cell></row><row><cell cols="3">environment for ver platform, te r evaluating ea st client platform ch m,</cell><cell cols="2">Resourc ces (e.g., demogr raphic informatio on such as name es, addresses, and tele ephone numbers s), and laborator ry test results rep presented as FHIR O Observation Res sources (e.g., tes st type, result q quantity, and</cell></row><row><cell cols="3">patient records atabase with a la to each database arge synthetic te est e's</cell><cell cols="2">record t to the associated d test result record ds. result u units). There wa as a one-to-many y relation from</cell><cell>each patient</cell></row><row><cell cols="3">at performs the d ach use case. Th o characterize h database read an his client can issu how each produ nd ue uct ncreases. pts that exerted a a specified load o on nt.</cell><cell cols="2">A synth hetic data set w was used for testi ing. This data s et contained one mil llion patient reco ords, and 10 mil llion lab result re ecords. Each patient had between 0 a and 20 test result t records, with an n average of seven. T These Patient an nd Observation R Resources were m mapped into the data a model for each h of the databases s we tested. This data set size was a t tradeoff between n fidelity (the ac ctual system cou uld be larger, depend ding on data a aging and archi iving strategy) versus test</cell></row></table><note><p>p://www.datastax k version 1.4, a k p://docs.basho.co were performed node server, a ntative of a pr ed our base test configuration phically distribu ta set was partiti ted to two addit oDB's primary/s aware distributio stribution, so we arded across all n across the nine n</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 -Settings for representative production configuration</head><label>1</label><figDesc></figDesc><table><row><cell>Database</cell><cell>Write Options</cell><cell>Read Options</cell></row><row><cell>MongoDB</cell><cell>Primary Ack'd</cell><cell>Primary Preferred</cell></row><row><cell>Cassandra</cell><cell>EACH_QUORUM</cell><cell>LOCAL_QUORUM</cell></row><row><cell>Riak</cell><cell>quorum</cell><cell>quorum</cell></row><row><cell cols="3">The throughput performance for the representative production</cell></row><row><cell cols="3">configuration for each of the workloads is shown in Figures 2, 3,</cell></row><row><cell cols="3">and 4. Cassandra performance peaked at approximately 3200</cell></row><row><cell cols="3">operations per second, with Riak peaking at approximately 480</cell></row><row><cell cols="3">operations per second, and MongoDB peaking at approximately 225</cell></row><row><cell cols="2">operations per second.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Latency, Repre ration, Read/Wr Using Event</head><label></label><figDesc></figDesc><table><row><cell cols="2">e eventual consiste</cell><cell cols="2">determi ining that this di id not impact the e results for one</cell><cell>through 250</cell></row><row><cell cols="2">u used in the tests</cell><cell cols="2">concurr rent sessions, an nd given that Ria ak had qualitativ ve capability</cell></row><row><cell cols="2">o offered slightly</cell><cell cols="2">gaps w with respect to</cell><cell>our strong co onsistency requir rements (as</cell></row><row><cell cols="2">s summarized in T</cell><cell>discuss</cell><cell>ed below), we d decided not to re e-execute the tes sts for those</cell></row><row><cell cols="2">w were committed</cell><cell cols="2">data po oints.</cell></row><row><cell cols="2">r equest acknowl o operations were e</cell><cell cols="2">Mongo node c oDB performance configuration, ac e is significantly chieving less tha y lower here tha an 10% of the an the single single node</cell></row><row><cell></cell><cell></cell><cell cols="2">through hput. Two factor rs influenced the</cell><cell>MongoDB resul lts. First, the</cell></row><row><cell></cell><cell></cell><cell cols="2">sharded d configuration n introduces th he MongoDB</cell><cell>router and</cell></row><row><cell></cell><cell></cell><cell cols="2">configu uration nodes int to the deployme ent. The router n node request</cell></row><row><cell></cell><cell></cell><cell cols="2">proxyin ng became a pe erformance bottl leneck. The rea ad and write</cell></row><row><cell></cell><cell></cell><cell cols="2">operatio on latencies show wn in Figures 5</cell><cell>and 6 have nea arly constant</cell></row><row><cell></cell><cell></cell><cell cols="2">average e latency for Mon ngoDB as the nu umber of concurr rent sessions</cell></row><row><cell></cell><cell></cell><cell cols="2">is incre eased, which we e attribute the ra apid saturation o of the router</cell></row><row><cell></cell><cell></cell><cell>node.</cell></row><row><cell></cell><cell></cell><cell cols="2">The se econd factor a affecting Mong goDB performa ance is the</cell></row><row><cell></cell><cell>-Throughput, R ion, Read-Only Representative P Workload (high Production her is better)</cell><cell cols="2">interact tion between the e sharding schem me used by Mo ongoDB and our wor rkloads. MongoD DB used a range--based sharding scheme with rebalan ncing (http://doc cs.mongodb.org/ /v2.2/core/sharde ed-clusters/).</cell></row><row><cell></cell><cell></cell><cell cols="2">Our wo orkloads generat ted a monotonic cally increasing k key for new</cell></row><row><cell></cell><cell></cell><cell cols="2">records s to be written,</cell><cell>which caused</cell><cell>all write opera ations to be</cell></row><row><cell></cell><cell></cell><cell cols="2">directed d to the same sh hard. While this</cell><cell>key generation</cell><cell>approach is</cell></row><row><cell></cell><cell></cell><cell>typical</cell><cell>(in fact, many S SQL databases p provide "autoincr rement" key</cell></row><row><cell></cell><cell></cell><cell cols="2">types th hat do this auto omatically), in t this case it conc centrates the</cell></row><row><cell></cell><cell>Figure 6 -Co</cell><cell cols="2">write lo impacts availab oad for all new r s performance. le to us, as it wo records on a sing A different i ould impact othe gle node and thu indexing schem er systems that o us negatively me was not our customer</cell></row><row><cell></cell><cell></cell><cell cols="2">operate es. (We note that t MongoDB intr roduced hash-bas sed sharding</cell></row><row><cell></cell><cell></cell><cell cols="2">in v2.4, , after our testing g had concluded. )</cell></row><row><cell></cell><cell></cell><cell cols="2">Our tes sts also measured d latency of read d and write opera ations. While</cell></row><row><cell></cell><cell></cell><cell cols="2">Cassand dra achieved the e highest overall</cell><cell>throughput, it al lso delivered</cell></row><row><cell></cell><cell></cell><cell cols="2">the high hest average late encies. For examp mple, at 32 client</cell><cell>connections,</cell></row><row><cell>F th s C c T</cell><cell>-Throughput, R nfiguration, Wr Representative P rite-Only Workl Production load</cell><cell cols="2">Riak's r Mongo faster). for each read operation la oDB's write oper Figures 5 and 6 atency was 20% ration latency w 6 show average of Cassandra (5x was 25% of Cass and 95 th percent x faster), and sandra's (4x tile latencies h test.</cell></row><row><cell>c</cell><cell></cell><cell></cell></row><row><cell>fr</cell><cell></cell><cell></cell></row><row><cell>c</cell><cell></cell><cell></cell></row><row><cell>c</cell><cell></cell><cell></cell></row><row><cell>In</cell><cell></cell><cell></cell></row><row><cell>p</cell><cell></cell><cell></cell></row><row><cell>w</cell><cell></cell><cell></cell></row><row><cell>s</cell><cell></cell><cell></cell></row><row><cell>s</cell><cell></cell><cell></cell></row><row><cell>f</cell><cell></cell><cell></cell></row><row><cell>r</cell><cell></cell><cell></cell></row><row><cell>to</cell><cell></cell><cell></cell></row><row><cell>c</cell><cell></cell><cell></cell></row><row><cell>d</cell><cell></cell><cell></cell></row><row><cell>5 S in f</cell><cell>-Throughput, R nfiguration, Rea Representative P ad/Write Work Production kload</cell><cell cols="2">F 4.2 E We als Figure 5 -Read Configur Evaluation U so report perform mance results for esentative Produ uction rite Workload tual Consist r the performan tency ce "cost" of</cell></row><row><cell>p q G p</cell><cell>mined that this umentation of arameter, which t a pool share ed by all clien resource exhau Riak's intern creates a poo stion was due nal thread po l for each clie nt sessions. Aft to ool ent ter</cell><cell cols="2">strong r data -replica consisten the performanc ncy. These result ce of MongoDB ts do not include did not warran e MongoDB nt additional charact combin terization of that nation of write a database for our and read operati r application. The ion settings that e tests used a t resulted in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 -Set</head><label>2</label><figDesc></figDesc><table><row><cell>Database</cell></row><row><cell>Cassandra</cell></row><row><cell>Riak</cell></row><row><cell>For Cassandra, a</cell></row><row><cell>hroughput movin</cell></row><row><cell>shows throughpu</cell></row><row><cell>Cassandra datab</cell></row><row><cell>configuration wit</cell></row><row><cell>The same compa</cell></row><row><cell>client sessions, th</cell></row><row><cell>from eventual to</cell></row><row><cell>configuration issu</cell></row><row><cell>concurrent sessio</cell></row><row><cell>n summary, the</cell></row><row><cell>performance, bu</cell></row><row><cell>workloads and c</cell></row><row><cell>several factors. F</cell></row><row><cell>storage load bett</cell></row><row><cell>features allowed</cell></row><row><cell>ecords, particula</cell></row><row><cell>o-peer architectu</cell></row><row><cell>coordination of b</cell></row><row><cell>data centers.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>The effe on one node (w ledgement was executed on one r</figDesc><table><row><cell>Write Latency,</cell></row><row><cell>nfiguration, Rea</cell></row><row><cell>ttings for eventu</cell></row><row><cell>Write Options</cell></row><row><cell>ONE</cell></row><row><cell>Noquorum</cell></row><row><cell>at 32 client sess</cell></row><row><cell>ng from eventua</cell></row><row><cell>ut performance f</cell></row><row><cell>base, comparing</cell></row><row><cell>th the eventual co</cell></row><row><cell>arison is shown</cell></row><row><cell>here is only a 10%</cell></row><row><cell>strong consisten</cell></row><row><cell>ues resulted in n</cell></row><row><cell>ns.)</cell></row><row><cell>Cassandra datab</cell></row><row><cell>ut with the hi</cell></row><row><cell>configurations t</cell></row><row><cell>First, hash-based</cell></row><row><cell>er than MongoD</cell></row><row><cell>efficient retriev</cell></row><row><cell>arly compared to</cell></row><row><cell>ure and data cent</cell></row><row><cell>both read and wr</cell></row><row><cell>ED WORK</cell></row><row><cell>uation methods</cell></row><row><cell>risons of the cap</cell></row><row><cell>n. Prototyping s</cell></row><row><cell>quantitative cha</cell></row><row><cell>rstanding of ot</cell></row><row><cell>s a rigorous ev</cell></row><row><cell>can be viewed a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>ure 7 -Cassandr Figure 8 -Riak -</head><label></label><figDesc></figDesc><table><row><cell cols="2">abase products</cell></row><row><cell cols="2">orkload against a</cell></row><row><cell cols="2">for general SQL</cell></row><row><cell cols="2">nsaction process</cell></row><row><cell cols="2">efinitions have</cell></row><row><cell cols="2">surements, which</cell></row><row><cell cols="2">orkload, configu</cell></row><row><cell cols="2">ducts. These ben</cell></row><row><cell cols="2">and are not relev</cell></row><row><cell cols="2">ra -Compariso</cell></row><row><cell></cell><cell>consistency</cell></row><row><cell cols="2">-Comparison o</cell></row><row><cell></cell><cell>consistency</cell></row><row><cell>[7] has emerged</cell><cell>as the de facto s</cell></row><row><cell cols="2">marks for NoSQ QL systems. YC</cell></row><row><cell cols="2">multi-phase workl load definitions a</cell></row><row><cell cols="2">tiple clients to in ncrease the load o</cell></row><row><cell cols="2">growing collectio on of published</cell></row><row><cell cols="2">CSB++, from pro oduct vendors [1</cell></row><row><cell cols="2">7]. In this proj ject, we built</cell></row><row><cell cols="2">mizing it with a a more complex</cell></row><row><cell cols="2">c workload defin nition.</cell></row><row><cell cols="2">URTHER W</cell></row><row><cell>L database tech</cell><cell></cell></row><row><cell>ility through ho</cell><cell></cell></row><row><cell>odels, but the sp</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Copyright 2014 ACM. This material is based upon work funded and supported by the Department of Defense under Contract No. FA8721-05-C-0003 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center. References herein to any specific commercial product, process, or service by trade name, trade mark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by Carnegie Mellon University or its Software Engineering Institute. This material has been approved for public release and unlimited distribution. T-Check SM . DM-0001936</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Process for COTS Software Product Evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Comella-Dorda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lewis</surname></persName>
		</author>
		<idno>/SEI-2003-TR-017</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Software Engineering Institute</publisher>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsolved Tricky Issues on COTS Selection and Evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving decision support for software component selection through systematic cross-referencing and analysis of multiple decision criteria</title>
		<author>
			<persName><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraxner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plangg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Hawaii Intl. Conf. on System Sciences (HICSS)</title>
		<meeting>46th Hawaii Intl. Conf. on System Sciences (HICSS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1193" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rigorous evaluation of COTS middleware technology</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MEMS: a method for evaluating middleware architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">S</forename><surname>Abanmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2 nd Intl. Conf. on Quality of Software Architectures (QoSA&apos;06)</title>
		<meeting>the 2 nd Intl. Conf. on Quality of Software Architectures (QoSA&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="9" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating COTS Middleware Acquisition: The i-Mate Process</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="79" />
			<date type="published" when="2003-03">2003. March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<idno type="DOI">10.1145/1807128.1807152</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ACM Symp. on Cloud Computing (SoCC &apos;10)</title>
		<meeting>1st ACM Symp. on Cloud Computing (SoCC &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s Highly Available Key-value Store</title>
		<author>
			<persName><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<idno type="DOI">10.1145/1294261.1294281</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 21 st ACM SIGOPS Symp. on Operating Systems Principles (SOSP &apos;07)</title>
		<meeting>21 st ACM SIGOPS Symp. on Operating Systems Principles (SOSP &apos;07)<address><addrLine>Stevenson, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<idno type="DOI">10.1145/2408776.2408794</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013-02">February 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distribution, Data, Deployment: Software Architecture Convergence in Big Data Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1109/MS.2014.51</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2014-03-18">18 March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking Database Systems: A Systematic Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turbyfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int&apos;l Conf. on Very Large Data Bases (VLDB &apos;83)</title>
		<meeting>9th Int&apos;l Conf. on Very Large Data Bases (VLDB &apos;83)</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="8" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Measure of Transaction Processing Power</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Datamation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="112" to="118" />
			<date type="published" when="1985-04">April 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/2038916.2038925</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd ACM Symp. on Cloud Computing (SOCC &apos;11)</title>
		<meeting>2nd ACM Symp. on Cloud Computing (SOCC &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ultra-High Performance NoSQL Benchmarking: Analyzing Durability and Performance Tradeoffs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nelubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Engber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Thumbtack Technology, Inc., White Paper</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Benchmarking Top NoSQL Databases</title>
		<author>
			<persName><surname>Datastax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Datastax Corporation, White Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NoSQL Databases: MongoDB vs Cassandra</title>
		<author>
			<persName><forename type="first">V</forename><surname>Abramova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernardino</surname></persName>
		</author>
		<idno type="DOI">10.1145/2494444.2494447</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l C* Conf. on Computer Science and Software Engineering</title>
		<meeting>Int&apos;l C* Conf. on Computer Science and Software Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="14" to="22" />
		</imprint>
	</monogr>
	<note>C3S2E &apos;13</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Can the Elephants Handle the NoSQL Onslaught?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Teletia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1712" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
