<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense 3D Object Reconstruction from a Single Depth View</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<email>bo.yang@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
							<email>stefano.rosa@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Markham</surname></persName>
							<email>andrew.markham@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
							<email>niki.trigoni@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
							<email>hongkai.wen@dcs.warwick.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense 3D Object Reconstruction from a Single Depth View</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB426DA4F8A031436FCD42D14816D67E</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2868195</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2868195, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Reconstruction</term>
					<term>Shape Completion</term>
					<term>Shape inpainting</term>
					<term>Single Depth View</term>
					<term>Adversarial Learning</term>
					<term>Conditional GAN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 256 3 by recovering the occluded/missing regions. The key idea is to combine the generative capabilities of 3D encoder-decoder and the conditional adversarial networks framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T O reconstruct the complete and precise 3D geometry of an object is essential for many graphics and robotics applications, from AR/VR <ref type="bibr" target="#b0">[1]</ref> and semantic understanding, to object deformation <ref type="bibr" target="#b1">[2]</ref>, robot grasping <ref type="bibr" target="#b2">[3]</ref> and obstacle avoidance. Classic approaches use the off-the-shelf low-cost depth sensing devices such as Kinect and RealSense cameras to recover the 3D shape of an object from captured depth images. Those approaches typically require multiple depth images from different viewing angles of an object to estimate the complete 3D structure <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b5">[6]</ref>. However, in practice it is not always feasible to scan all surfaces of an object before reconstruction, which leads to incomplete 3D shapes with occluded regions and large holes. In addition, acquiring and processing multiple depth views require more computing power, which is not ideal in many applications that require real-time performance.</p><p>We aim to tackle the problem of estimating the complete 3D structure of an object using a single depth view. This is a very challenging task, since the partial observation of the object (i.e., a depth image from one viewing angle) can be theoretically associated with an infinite number of possible 3D models. Traditional reconstruction approaches typically use interpolation techniques such as plane fitting, Laplacian hole filling <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>, or Poisson surface estimation <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b9">[10]</ref> to infer the underlying 3D structure. However, they can only recover very limited occluded or missing regions, e.g., small holes or gaps due to quantization artifacts, sensor noise and insufficient geometry information.</p><p>Interestingly, humans are surprisingly good at solving such ambiguity by implicitly leveraging prior knowledge.</p><p>For example, given a view of a chair with two rear legs occluded by front legs, humans are easily able to guess the most likely shape behind the visible parts. Recent advances in deep neural networks and data driven approaches show promising results in dealing with such a task.</p><p>In this paper, we aim to acquire the complete and highresolution 3D shape of an object given a single depth view. By leveraging the high performance of 3D convolutional neural nets and large open datasets of 3D models, our approach learns a smooth function that maps a 2.5D view to a complete and dense 3D shape. In particular, we train an endto-end model which estimates full volumetric occupancy from a single 2.5D depth view of an object.</p><p>While state-of-the-art deep learning approaches <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b2">[3]</ref> for 3D shape reconstruction from a single depth view achieve encouraging results, they are limited to very small resolutions, typically at the scale of 32 3 voxel grids. As a result, the learnt 3D structure tends to be coarse and inaccurate. In order to generate higher resolution 3D objects with efficient computation, Octree representation has been recently introduced in <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref>. However, increasing the density of output 3D shapes would also inevitably pose a great challenge to learn the geometric details for high resolution 3D structures, which has yet to be explored.</p><p>Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distributions, among which Generative Adversarial Networks (GANs) <ref type="bibr" target="#b15">[16]</ref> and Variational Autoencoders (VAEs) <ref type="bibr" target="#b16">[17]</ref> emerge as two powerful frameworks for generative learning, including image and text generation <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b18">[19]</ref>, and latent space learning <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>. In the past few years, a number of works <ref type="bibr" target="#b21">[22]</ref> [23] <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b24">[25]</ref> applied such generative models to learn latent space to represent 3D object shapes, in order to solve tasks such as new image generation, object classification, recognition and shape retrieval.</p><p>In this paper, we propose 3D-RecGAN++, a simple yet effective model that combines a skip-connected 3D encoder-decoder with adversarial learning to generate a complete and fine-grained 3D structure conditioned on a single 2.5D view. Particularly, our model firstly encodes the 2.5D view to a compressed latent representation which implicitly represents general 3D geometric structures, then decodes it back to the most likely full 3D shape. Skip-connections are applied between the encoder and decoder to preserve high frequency information. The rough 3D shape is then fed into a conditional discriminator which is adversarially trained to distinguish whether the coarse 3D structure is plausible or not. The encoder-decoder is able to approximate the corresponding shape, while the adversarial training tends to add fine details to the estimated shape. To ensure the final generated 3D shape corresponds to the input single partial 2.5D view, adversarial training of our model is based on a conditional GAN <ref type="bibr" target="#b25">[26]</ref> instead of random guessing. The above network excels the competing approaches <ref type="bibr" target="#b2">[3]</ref> [12] <ref type="bibr" target="#b26">[27]</ref>, which either use a single fully connected layer <ref type="bibr" target="#b2">[3]</ref>, a low capacity decoder without adversarial learning <ref type="bibr" target="#b11">[12]</ref>, or the multi-stage and ineffective LSTMs <ref type="bibr" target="#b26">[27]</ref> to estimate the full 3D shapes.</p><p>Our contributions are as follows:</p><p>(1) We propose a simple yet effective model to reconstruct the complete and accurate 3D structure using a single arbitrary depth view. Particularly, our model takes a simple occupancy grid map as input without requiring object class labels or any annotations, while predicting a compelling shape within a high resolution of 256 3 voxel grid. By drawing on both 3D encoder-decoder and adversarial learning, our approach is end-to-end trainable with high level of generality.</p><p>(2) We exploit conditional adversarial training to refine the 3D shape estimated by the encoder-decoder. Our contribution here is that we use the mean value of a latent vector feature, instead of a single scalar, as the output of the discriminator to stabilize GAN training.</p><p>(3) We conduct extensive experiments for single category and multi-category object reconstruction, outperforming the state of the art. Importantly, our approach is also able to generalize to previously unseen object categories. At last, our model also performances robustly on real-world data, after being trained purely on synthetic datasets.</p><p>(4) To the best of our knowledge, there are no good open datasets which have the ground truth for occluded/missing parts and holes for each 2.5D view in real-world scenarios. We therefore collect and release our real-world testing dataset to the community.</p><p>A preliminary version of this work has been published in ICCV 2017 workshops <ref type="bibr" target="#b27">[28]</ref>. Our code and data are available at: https://github.com/Yang7879/3D-RecGAN-extended</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review different pipelines for 3D reconstruction or shape completion. Both conventional geometry based techniques and the state of the art deep learning approaches are covered.</p><p>(1) 3D Model/Shape Completion. Monszpart et al. use plane fitting to complete small missing regions in <ref type="bibr" target="#b28">[29]</ref>, while shape symmetry is applied in <ref type="bibr" target="#b29">[30]</ref> [31] <ref type="bibr" target="#b31">[32]</ref> [33] <ref type="bibr" target="#b33">[34]</ref> to fill in holes. Although these methods show good results, relying on predefined geometric regularities fundamentally limits the structure space to hand-crafted shapes. Besides, these approaches are likely to fail when missing or occluded regions are relatively big. Another similar fitting pipeline is to leverage database priors. Given a partial shape input, an identical or most likely 3D model is retrieved and aligned with the partial scan in <ref type="bibr" target="#b34">[35]</ref>  <ref type="bibr" target="#b35">[36]</ref> [37] <ref type="bibr" target="#b37">[38]</ref> [39] <ref type="bibr" target="#b39">[40]</ref>. However, these approaches explicitly assume the database contains identical or very similar shapes, thus being unable to generalize to novel objects or categories.</p><p>(2) Multiple RGB/Depth Images Reconstruction. Traditionally, 3D dense reconstruction in SfM and visual SLAM requires a collection of RGB images <ref type="bibr" target="#b40">[41]</ref>. Geometric shape is recovered by dense feature extraction and matching <ref type="bibr" target="#b41">[42]</ref>, or by directly minimizing reprojection errors <ref type="bibr" target="#b42">[43]</ref> from color images. Shape priors are also concurrently leveraged with the traditional multi-view reconstruction for dense object shape estimation in <ref type="bibr" target="#b43">[44]</ref> [45] <ref type="bibr" target="#b45">[46]</ref>. Recently, deep neural nets are designed to learn the 3D shape from multiple RGB images in <ref type="bibr" target="#b46">[47]</ref> [48] <ref type="bibr" target="#b48">[49]</ref> [50] [51] <ref type="bibr" target="#b51">[52]</ref>. However, resolution of the recovered occupancy shape is usually up to a small scale of 32 3 . With the advancement of depth sensors, depth images are also used to recover the object shape. Classic approaches usually fuse multiple depth images through iterative closest point (ICP) algorithms <ref type="bibr" target="#b3">[4]</ref> [53] <ref type="bibr" target="#b53">[54]</ref>, while recent work <ref type="bibr" target="#b13">[14]</ref> learns the 3D shape using deep neural nets from multiple depth views.</p><p>(3) Single RGB Image Reconstruction. Predicting a complete 3D object model from a single view is a longstanding and extremely challenging task. When reconstructing a specific object category, model templates can be used. For example, morphable 3D models are exploited for face recovery <ref type="bibr" target="#b54">[55]</ref>  <ref type="bibr" target="#b55">[56]</ref>. This concept was extended to reconstruct simple objects in <ref type="bibr" target="#b56">[57]</ref>. For general and complex object reconstruction from a single RGB image, recent works <ref type="bibr" target="#b57">[58]</ref> [59] <ref type="bibr" target="#b59">[60]</ref> aim to infer 3D shapes using multiple RGB images for weak supervision. Shape prior knowledge is utilized in <ref type="bibr" target="#b60">[61]</ref> [62] <ref type="bibr" target="#b62">[63]</ref> for shape estimation. To recover high resolution 3D shapes, Octree representation is introduced in <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> [15] to save computation, while an inverse discrete cosine transform (IDCT) technique is proposed in <ref type="bibr" target="#b63">[64]</ref>. Lin et al. <ref type="bibr" target="#b64">[65]</ref> designed a pseudo-renderer to predict dense 3D shapes, while 2.5D sketches and dense 3D shapes are sequentially estimated from a single RGB image in <ref type="bibr" target="#b65">[66]</ref>.</p><p>(4) Single Depth View Reconstruction. The task of reconstruction from a single depth view is to complete the occluded 3D structures behind the visible parts. 3D ShapeNets <ref type="bibr" target="#b10">[11]</ref> is among the early work using deep neural nets to estimate 3D shapes from a single depth view. Firman et al. <ref type="bibr" target="#b66">[67]</ref> trained a random decision forest to infer unknown voxels. Originally designed for shape denoising, VConv-DAE <ref type="bibr" target="#b0">[1]</ref> can also be used for shape completion. To facilitate robotic grasping, Varley et al. proposed a neural network to infer the full 3D shape from a single depth view in <ref type="bibr" target="#b2">[3]</ref>. However, all these approaches are only able to generate low resolution voxel grids which are less than 40 3 and unlikely to capture fine geometric details. Recent works <ref type="bibr" target="#b11">[12]</ref> [68] <ref type="bibr" target="#b26">[27]</ref> [69] can infer higher resolution 3D shapes. However, the pipeline in <ref type="bibr" target="#b11">[12]</ref> relies on a shape database to synthesize a higher resolution shape after learning a small 32 3 voxel grid from a depth view, while SSCNet <ref type="bibr" target="#b67">[68]</ref> requires voxel-level annotations for supervised scene completion and semantic label prediction. Both <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b68">[69]</ref> were originally designed for shape inpainting instead of directly reconstructing the complete 3D structure from a partial depth view. The recent 3D-PRNN <ref type="bibr" target="#b69">[70]</ref> predicts simple shape primitives using RNNs, but the estimated shapes do not have finer geometric details.</p><p>(5) Deep Generative Frameworks. Deep generative frameworks, such as VAEs <ref type="bibr" target="#b16">[17]</ref> and GANs <ref type="bibr" target="#b15">[16]</ref>, have achieved impressive success in image super-resolution <ref type="bibr" target="#b70">[71]</ref>, image generation <ref type="bibr" target="#b18">[19]</ref>, text to image synthesis <ref type="bibr" target="#b71">[72]</ref>, etc. VAE and GAN are further combined in <ref type="bibr" target="#b72">[73]</ref> and achieve compelling results in learning visual features. Recently, generative networks are applied in <ref type="bibr" target="#b73">[74]</ref> [75] <ref type="bibr">[76] [25]</ref> to generate low resolution 3D structures. However, incorporating generative adversarial learning to estimate high resolution 3D shapes is not straightforward, as it is difficult to generate samples for high dimensional and complex data distributions <ref type="bibr" target="#b76">[77]</ref> and this may lead to the instability of adversarial generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D-RECGAN++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our method aims to estimate a complete and dense 3D structure of an object, which only takes an arbitrary single 2.5D depth view as input. The output 3D shape is automatically aligned with the corresponding 2.5D partial view. To achieve this task, each object model is represented by a high resolution 3D voxel grid. We use the simple occupancy grid for shape encoding, where 1 represents an occupied cell and 0 an empty cell. Specifically, the input 2.5D partial view, denoted as x, is a 64 3 occupancy grid, while the output 3D shape, denoted as y, is a high resolution 256 3 probabilistic voxel grid. The input partial shape is directly calculated from a single depth image given camera parameters. We use the ground truth dense 3D shape with aligned orientation as same as the input partial 2.5D depth view to supervise our network.</p><p>To generate ground truth training and evaluation pairs, we virtually scan 3D objects from ShapeNet <ref type="bibr" target="#b77">[78]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> is the t-SNE visualization <ref type="bibr" target="#b78">[79]</ref> of partial 2.5D views and the corresponding full 3D shapes for multiple general chair and bed models. Each green dot represents the t-SNE embedding of a 2.5D view, whilst a red dot is the embedding of the corresponding 3D shape. It can be seen that multiple categories inherently have similar 2.5D to 3D mapping relationships. Essentially, our neural network is to learn a smooth function, denoted as f , which maps green dots to red dots as close as possible in high dimensional space as shown in Equation <ref type="formula" target="#formula_1">1</ref>.</p><p>The function f is parametrized by neural layers in general.</p><formula xml:id="formula_0">y = f (x) x ∈ Z 64 3</formula><p>, where Z = {0, 1}</p><p>After generating training pairs, we feed them into our network. The first part of our network loosely follows the idea of a 3D encoder-decoder with the U-net connections <ref type="bibr" target="#b79">[80]</ref>. The skip-connected encoder-decoder serves as an initial coarse generator which is followed by an up-sampling module to further generate a higher resolution 3D shape within a 256 3 voxel grid. This whole generator learns a correlation between partial and complete 3D structures. With the supervision of complete 3D labels, the generator is able to learn a function f and infer a reasonable 3D shape given a brand new partial 2.5D view. During testing, however, the results tend to be grainy and without fine details.</p><p>To address this issue, in the training phase, the reconstructed 3D shape from the generator is further fed into a conditional discriminator to verify its plausibility. In particular, a partial 2.5D input view is paired with its corresponding complete 3D shape, which is called the 'real reconstruction', while the partial 2.5D view is paired with its corresponding output 3D shape from generator, which is called the 'fake reconstruction'. The discriminator aims to discriminate all 'fake reconstruction' from 'real reconstruction'. In the original GAN framework <ref type="bibr" target="#b15">[16]</ref>, the task of the discriminator is to simply classify real and fake inputs, but its Jensen-Shannon divergence-based loss function is difficult to converge. The recent WGAN <ref type="bibr" target="#b76">[77]</ref> leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP <ref type="bibr" target="#b80">[81]</ref> further improves the training process using a gradient penalty with respect to its input. In our 3D-RecGAN++, we apply WGAN-GP as the loss function on top of the mean feature of our conditional discriminator, which guarantees fast and stable convergence. The overall network architecture for training is shown in Figure <ref type="figure">2</ref>, while the testing phase only needs the well trained generator as shown in Figure <ref type="figure">3</ref>.</p><p>Overall, the main challenge of 3D reconstruction from an arbitrary single view is to generate new information including filling the missing and occluded regions from unseen views, while keeping the estimated 3D shape corresponding to the specific input 2.5D view. In the training phase, our 3D-RecGAN++ firstly leverages a skip-connected encoder-decoder together with an up-sampling module to generate a reasonable 'fake reconstruction' within a high resolution occupancy grid, then applies adversarial learning to refine the 'fake reconstruction' to make it as similar to 'real reconstruction' by jointly updating parameters of the generator. In the testing phase, given a novel 2.5D view as input, the jointly trained generator is able to recover a full 3D shape with satisfactory accuracy, while the discriminator is no longer used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the detailed architecture of our proposed 3D-RecGAN++. It consists of two main networks: the generator as in Figure <ref type="figure" target="#fig_4">4a</ref> and the discriminator as in Figure <ref type="figure" target="#fig_2">4b</ref>.</p><p>The generator consists of a skip-connected encoderdecoder and an up-sampling module. Unlike the vanilla GAN generator which generates data from arbitrary latent distributions, our 3D-RecGAN++ generator synthesizes data from 2.5D views. Particularly, the encoder has five 3D convolutional layers, each of which has a bank of 4 × 4 × 4 filters with strides of 1 × 1 × 1, followed by a leaky ReLU activation function and a max pooling layer with 2 × 2 × 2 filters and strides of 2×2×2. The number of output channels of max pooling layer starts with 64, doubling at each subsequent layer and ends up with 512. The encoder is lastly followed by two fully-connected layers to embed semantic information into a latent space. The decoder is composed of five symmetric up-convolutional layers which are followed by ReLU activations. Skip-connections between encoder and decoder guarantee propagation of local structures of the input 2.5D view. The skip-connected encoder-decoder is followed by the up-sampling module which simply consists of two layers of up-convolutional layers as detailed in Figure <ref type="figure" target="#fig_4">4a</ref>. This simple up-sampling module directly upgrades the output 3D shape to a higher resolution of 256 3 without requiring complex network design and operations. It should be noted that without the two fully connected layers and skip-connections, the vanilla encoder-decoder would be unable to learn reasonable complete 3D structures as the latent space is limited and the local structure is not preserved. The loss function and optimization methods are described in Section 3.4.</p><p>The discriminator aims to distinguish whether the estimated 3D shapes are plausible or not. Based on the conditional GAN, the discriminator takes both real reconstruction pairs and fake reconstruction pairs as input. In particular, it consists of six 3D convolutional layers, the first of which concatenates the generated 3D shape (i.e., a 256 3 voxel grid) and the input 2.5D partial view (i.e., a 64 3 voxel grid), reshaped as a 256 × 256 × 4 tensor. The reshaping process is done straightforwardly using Tensorflow 'tf.reshape()'. Basically, this is to inject the condition information with a matched tensor dimension, and then leave the network itself to learn useful features from this condition input. Each convolutional layer has a bank of 4 × 4 × 4 filters with strides of 2 × 2 × 2, followed by a ReLU activation function except for the last layer which is followed by a sigmoid activation function. The number of output channels of the convolutional layers starts with 8, doubling at each subsequent layer and ends up with 256. The output of the last neural layer is reshaped as a latent vector which is the latent feature of discriminator, denoted as m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mean Feature for Discriminator</head><p>At the early training stage of GAN, as the high dimensional real and fake distributions may not overlap, the discriminator can separate them perfectly using a single scalar output, which is analyzed in <ref type="bibr" target="#b81">[82]</ref>. In our experiments, due to the extremely high dimensionality (i.e., 256 3 + 64 3 dimensions) of the input data pair, the WGAN-GP always crashes in the early 3 epochs if we use a standard fully-connected layer followed by a single scalar as the final output for the discriminator.</p><p>To stabilize training, we propose to use the mean feature m (i.e., mean of a vector feature m) for discrimination. As the mean vector feature tends to capture more information from the input overall, it is more difficult for the discriminator to easily distinguish between fake or real inputs. This enables useful information to back-propagate to the generator. The final output of the discriminator D() is defined as:</p><formula xml:id="formula_2">m = E(m)<label>(2)</label></formula><p>Mean feature matching is also studied and applied in <ref type="bibr" target="#b82">[83]</ref> [84] to stabilize GAN. However, Bao et al. <ref type="bibr" target="#b82">[83]</ref> minimize the L 2 loss of the mean feature, as well as the original Jensen-Shannon divergence-based loss <ref type="bibr" target="#b15">[16]</ref>, requiring hyper-parameter tuning to balance the two losses. By comparison, in our 3D-RecGAN++ setting, the mean feature of discriminator is directly followed by the existing WGAN-GP loss, which is simple yet effective to stabilize the adversarial training.</p><p>Overall, our discriminator learns to distinguish the distributions of mean feature of fake and real reconstructions, while the generator is trained to make the two mean feature distributions as similar as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objectives</head><p>The objective function of 3D-RecGAN++ includes two main parts: an object reconstruction loss en for the generator; the objective function gan for the conditional GAN.</p><p>(1) en For the generator, inspired by <ref type="bibr" target="#b84">[85]</ref>, we use modified binary cross-entropy loss function instead of the standard version. The standard binary cross-entropy weights both false positive and false negative results equally. However, most of the voxel grid tends to be empty, so the network easily gets a false positive estimation. In this regard, we impose a higher penalty on false positive results than on false negatives. Particularly, a weight hyper-parameter α is assigned to false positives, with (1-α) for false negative results, as shown in Equation <ref type="formula">3</ref>.</p><formula xml:id="formula_3">en = 1 N N i=1 -α ȳi log(y i )-(1-α)(1-ȳi ) log(1-y i ) (3)</formula><p>where ȳi is the target value {0,1} of a specific i th voxel in the ground truth voxel grid ȳ, and y i is the corresponding estimated value (0,1) in the same voxel from the generator output y. We calculate the mean loss over the total N voxels in the whole voxel grid. (2) gan For the discriminator, we leverage the state of the art WGAN-GP loss functions. Unlike the original GAN loss function which presents an overall loss for both real and fake inputs, we separately represent the loss function g gan in Equation <ref type="formula" target="#formula_4">4</ref>for generating fake reconstruction pairs and d gan in Equation <ref type="formula" target="#formula_5">5</ref>for discriminating fake and real reconstruction pairs. Detailed definitions and derivation of the loss functions can be found in <ref type="bibr" target="#b76">[77]</ref> [81], but we modify them for our conditional GAN settings.</p><formula xml:id="formula_4">g gan = -E D(y|x)<label>(4)</label></formula><formula xml:id="formula_5">d gan = E D(y|x) -E D(ȳ|x) +λE ∇ ŷ D(ŷ|x) 2 -1 2<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">ŷ = ȳ + (1 -)y, ∼ U [0, 1],</formula><p>x is the input partial depth view, y is the corresponding output of the generator, ȳ is the corresponding ground truth. λ controls the trade-off between optimizing the gradient penalty and the original objective in WGAN.</p><p>For the generator in our 3D-RecGAN++ network, there are two loss functions, en and g gan , to optimize. As we discussed in Section 3.1, minimizing en tends to learn the overall 3D shapes, whilst minimizing g gan estimates more plausible 3D structures conditioned on input 2.5D views. To minimize d gan is to improve the performance of discriminator to distinguish fake and real reconstruction pairs. To jointly optimize the generator, we assign weights β to en and (1 -β) to g gan . Overall, the loss functions for generator and discriminator are as follows:</p><formula xml:id="formula_7">g = β en + (1 -β) g gan (6) d = d gan (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>We adopt an end-to-end training procedure for the whole network. To simultaneously optimize both generator and discriminator, we alternate between one gradient decent step on the discriminator and then one step on the generator. For the WGAN-GP, λ is set as 10 for gradient penalty as in <ref type="bibr" target="#b80">[81]</ref>. α ends up as 0.85 for our modified cross entropy loss function, while β is 0.2 for the joint loss function g .</p><p>The Adam solver <ref type="bibr" target="#b85">[86]</ref> is used for both discriminator and generator with a batch size of 4. The other three Adam parameters are set to default values. Learning rate is set to 5e -5 for the discriminator and 1e -4 for the generator in all epochs. As we do not use dropout or batch normalization, the testing phase is exactly the same as the training stage. The whole network is trained on a single Titan X GPU from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Data Synthesis</head><p>For the task of 3D dense reconstruction from a single depth view, obtaining a large amount of training data is an obstacle. Existing real RGB-D datasets for surface reconstruction suffer from occlusions and missing data and there is no ground truth of complete and high resolution 256 3 3D shapes for each view. The recent work <ref type="bibr" target="#b11">[12]</ref> synthesizes data for 3D object completion, but the object resolution is only up to 128 3 .</p><p>To tackle this issue, we use the ShapeNet <ref type="bibr" target="#b77">[78]</ref> database to generate a large amount of training and testing data with synthetically rendered depth images and the corresponding complete 3D shape ground truth. Interior parts of individual objects are set to be filled, i.e., '1', while the exterior to be empty, i.e., '0'. A subset of object categories and CAD models are selected for our experiments. As some CAD models in ShapeNet may not be watertight, in our ray tracing based voxelization algorithm, if a specific point is inside of more than 5 faces along X, Y and Z axes, that point is deemed to be interior of the object and set as '1', otherwise '0'.</p><p>For each category, to generate training data, around 220 CAD models are randomly selected. For each CAD model, we create a virtual depth camera to scan it from 125 different viewing angles, 5 uniformly sampled views for each of roll, pitch and yaw space ranging from 0 ∼ 2π individually. Note that, the viewing angles for all 3D models are the same for simplicity. For each virtual scan, both a depth image and the corresponding complete 3D voxelized structure are generated with regard to the same camera angle. That depth image is simultaneously transformed to a point cloud using virtual camera parameters <ref type="bibr" target="#b86">[87]</ref> followed by voxelization which generates a partial 2.5D voxel grid. Then a pair of partial 2.5D view and the complete 3D shape is synthesized. Overall, around 26K training pairs are generated for each 3D object category.</p><p>For each category, to synthesize testing data, around 40 CAD models are randomly selected. For each CAD model, two groups of testing data are generated. Group 1, each model is virtually scanned from 125 viewing angles which are the same as used in training dataset. Around 4.5k testing pairs are generated in total. This group of testing dataset is denoted as same viewing (SV) angles testing dataset. Group 2, each model is virtually scanned from 216 different viewing angles, 6 uniformly sampled views from each of roll, pitch and yaw space ranging from 0 ∼ 2π individually. Note that, these viewing angles for all testing 3D models are completely different from training pairs. Around 8k testing pairs are generated in total. This group of testing dataset is denoted as cross viewing (CV) angles testing dataset. Similarly, we also generate around 1.5k SV and 2.5k CV validation data split from another 12 CAD models, which are used for hyperparameter searching.</p><p>As our network is initially designed to predict an aligned full 3D model given a depth image from an arbitrary viewing angle, these two SV and CV testing datasets are generated separately to evaluate the viewing angle robustness and generality of our model.</p><p>Besides the large quantity of synthesized data, we also collect a real-world dataset in order to test the proposed network in a realistic scenario. We use a Microsoft Kinect camera to manually scan a total of 20 object instances belonging to 4 classes {bench, chair, couch, table}, with 5 instances per class from different environments, including offices, homes, and outdoor university parks. For each object we acquire RGB-D images of the object from multiple angles by moving the camera around the object. Then, we use the dense visual SLAM algorithm ElasticFusion <ref type="bibr" target="#b53">[54]</ref> in order to reconstruct the full 3D shape of each object, as well as the camera pose in each scan.</p><p>We sample 50 random views from the camera trajectory, and for each one we obtain the depth image and the relative camera pose. In each depth image the 3D object is segmented from the background, using a combination of floor removal and manual segmentation. We finally generate ground truth information by aligning the full 3D objects with the partial 2.5D views. It should be noted that, due to noise and quantization artifacts of low-cost RGB-D sensors, and the inaccuracy of the SLAM algorithm, the full 3D ground truth is not 100% accurate, but can still be used as a reasonable approximation. The real-world dataset highlights the challenges related to shape reconstruction from realistic data: noisy depth estimates, missing depth information, depth quantization. In addition, some of the objects are acquired outdoors (e.g., bench), which is challenging for the near-infrared depth sensor of the Micorsoft Kinect. However, we argue that a real-world benchmark for shape reconstruction is necessary for a thorough validation of future approaches. Figure <ref type="figure" target="#fig_3">5</ref> shows an example of the reconstructed object and camera poses in ElasticFusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we evaluate our 3D-RecGAN++ with comparison to the state of the art approaches and an ablation study to fully investigate the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metrics</head><p>To evaluate the performance of 3D reconstruction, we consider two metrics. The first metric is the mean Intersectionover-Union (IoU) between predicted 3D voxel grids and their ground truth. The IoU for an individual voxel grid is formally defined as follows:</p><formula xml:id="formula_8">IoU = N i=1 I(y i &gt; p) * I( ȳi ) N i=1 I I(y i &gt; p) + I( ȳi )</formula><p>where I(•) is an indicator function, y i is the predicted value for the i th voxel, ȳi is the corresponding ground truth, p is the threshold for voxelization, N is the total number of voxels in a whole voxel grid. In all our experiments, p is searched using the validation data split per category for each approach. Particularly, p is searched in the range [0.1, 0.9] with a step size 0.05 using the validation datasets. The higher the IoU value, the better the reconstruction of a 3D model.</p><p>The second metric is the mean value of standard Cross-Entropy loss (CE) between a reconstructed shape and the ground truth 3D model. It is formally defined as:</p><formula xml:id="formula_9">CE = - 1 N N i=1 ȳi log(y i ) + (1 -ȳi ) log(1 -y i )</formula><p>where y i , ȳi and N are the same as defined in above IoU.</p><p>The lower CE value is, the closer the prediction to be either '1' or '0', the more robust and confident the 3D predictions are.</p><p>We also considered the Chamfer Distance (CD) or Earth Mover's Distance (EMD) as an additional metric. However, it is computationally heavy to calculate the distance between two high resolution voxel grids due to the large number of points. In our experiments, it takes nearly 2 minutes to calculate either CD or EMD between two 256 3 shapes on a single Titan X GPU. Although the 256 3 dense shapes can be downsampled to sparse point clouds on object surfaces to quickly compute CD or EMD, the geometric details are inevitably lost due to the extreme downsampling process. Therefore, we did not use CD or EMD for evaluation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Competing Approaches</head><p>We compare against three state of the art deep learning based approaches for single depth view reconstruction. We also compare against the generator alone in our network, i.e., without the GAN, named 3D-RecAE for short.</p><p>(1) 3D-EPN. In <ref type="bibr" target="#b11">[12]</ref>, Dai et al. proposed a neural network, called "3D-EPN", to reconstruct the 3D shape up to a 32 3 voxel grid, after which a high resolution shape is retrieved from an existing 3D shape database, called "Shape Synthesis". In our experiment, we only compared with their neural network (i.e., 3D-EPN) performance because we do not have an existing shape database for similar shape retrieval during testing. Besides, occupancy grid representation is used for the network training and testing.</p><p>(2) Varley et al. In <ref type="bibr" target="#b2">[3]</ref>, a network was designed to complete the 3D shape from a single 2.5D depth view for robot grasping. The output of their network is a 40 3 voxel grid.</p><p>Note that, the low resolution voxel grids generated by 3D-EPN and Varley et al. are all upsampled to 256 3 voxel grids using trilinear interpolation before calculating the IoU and CE metrics. The linear upsampling is a widely used post-processing technique for fair comparison in cases where the output resolution is not identical <ref type="bibr" target="#b12">[13]</ref>. However, as both 3D-EPN and Varley et al. are trained using lower resolution voxel grids for supervision, while the below Han et al. and our 3D-RecGAN++ are trained using 256 3 shapes for supervision, it is not strictly fair comparison in this regard. Considering both 3D-EPN and Varley et al. are among the early works and also solid competing approaches regarding the single depth view reconstruction task, we therefore include them as baselines.</p><p>(3) Han et al. In <ref type="bibr" target="#b26">[27]</ref>, a global structure inference network and a local geometry refinement network are proposed to complete a high resolution shape from a noisy shape. The network is not originally designed for single depth view reconstruction, but its output shape is up to a 256 3 voxel grid and is comparable to our network. For fair comparison, the same occupancy grid representation is used for their network. It should be noted that <ref type="bibr" target="#b26">[27]</ref> involves convoluted designs, thus the training procedure is slower and less efficient due to many LSTMs integrated.</p><p>(4) 3D-RecAE. As for our 3D-RecGAN++, we remove the discriminator and only keep the generator to infer the complete 3D shape from a single depth view. This comparison illustrates the benefits of adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single-category Results</head><p>(1) Results. All networks are separately trained and tested on four different categories, {bench, chair, couch, table}, with the same network configuration. Table <ref type="table" target="#tab_0">1</ref> shows the IoU and CE loss of all methods on the testing dataset with same viewing angles on 256 3 voxel grids, while Table <ref type="table" target="#tab_2">2</ref> shows the IoU and CE loss comparison on testing dataset with cross viewing angles. Figure <ref type="figure" target="#fig_5">6</ref> shows the qualitative results of single category reconstruction on testing datasets with same and cross viewing angles. The meshgrid function in Matlab is used to plot all 3D shapes for better visualization.</p><p>(2) Analysis. Both 3D-RecGAN++ and 3D-RecAE significantly outperform the competing approaches in terms of IoU and CE loss on both the SV and CV testing datasets for dense 3D shape reconstruction (256 3 voxel grids). Although our approach is trained on depth input with a limited set of viewing angles, it still performs well to predict aligned 3D shapes from novel viewing angles. The 3D shapes generated by 3D-RecGAN++ and 3D-RecAE are much more visually compelling than others.</p><p>Compared with 3D-RecAE, 3D-RecGAN++ achieves better IoU scores and smaller CE loss. Basically, adversarial learning of the discriminator serves as a regularizer for finegrained 3D shape estimation, which enables the output of 3D-RecGAN++ to be more robust and confident. We also notice that the increase of 3D-RecGAN++ in IoU and CE scores is not dramatic compared with 3D-RecAE. This is primarily because the main object shape can be reasonably predicted by 3D-RecAE, while the finer geometric details estimated by 3D-RecGAN++ are usually smaller parts of the whole object shape. Therefore, 3D-RecGAN++ only obtains a reasonable better IoU and CE scores than 3D-RecAE. The 4 th row of Figure <ref type="figure" target="#fig_5">6a</ref> shows a good example in terms of finer geometric details prediction of 3D-RecGAN++. In fact, in all the remaining experiments, 3D-RecGAN++ is constantly, but not significantly, better than 3D-RecAE.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-category Results</head><p>(1) Results. All networks are also trained and tested on multiple categories without being given any class labels. The networks are trained on four categories: {bench, chair, couch, table}; and then tested separately on individual categories. Table <ref type="table" target="#tab_4">3</ref> shows the IoU and CE loss comparison of all methods on testing dataset with same viewing angles for dense shape reconstruction, while Table <ref type="table" target="#tab_6">4</ref> shows the IoU and CE loss comparison on testing dataset with cross viewing angles. Figure <ref type="figure" target="#fig_9">7</ref> shows the qualitative results of all approaches on testing datasets of multiple categories with same and cross viewing angles.</p><p>(2) Analysis. Both 3D-RecGAN++ and 3D-RecAE significantly outperforms the state of the art by a large margin in all categories which are trained together on a single model. Besides, the performance of our network trained on multiple categories, does not notably degrade compared with training the network on individual categories as shown in previous Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_2">2</ref>. This confirms that our network has enough capacity and capability to learn diverse features from multiple categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cross-category Results</head><p>(1) Results. To further investigate the generality of networks, we train all networks on {bench, chair, couch, table},  0.530 0.587 0.640 0.610 0.043 0.068 0.096 0.055 3D-RecGAN++ (ours) 0.540 0.594 0.643 0.621 0.038 0.061 0.091 0.048 and then test them on another 6 totally different categories: {car, faucet, firearm, guitar, monitor, plane}. For each of the 6 categories, we generate the same amount of testing datasets with same and cross viewing angles, which is similar to the previous {bench, chair, couch, table}. Table <ref type="table" target="#tab_7">5</ref> and<ref type="table" target="#tab_8">6</ref>        <ref type="figure" target="#fig_11">8</ref> shows the qualitative results of all methods on 6 unseen categories with same and cross viewing angles. We further evaluate the generality of our 3D-RecGAN++ on a specific category. Particularly, we conduct four groups of experiments. In the first group, we train our 3D-RecGAN++ on bench, then separately test it on the re-    (2) Analysis. The proposed 3D-RecGAN++ achieves much higher IoU and smaller CE loss across the unseen categories than competing approaches. Our network not only learns rich features from different object categories, but also is able to generalize well to completely new types of categories. Our intuition is that the network may learn geometric features such as lines, planes, curves which are common across various object categories. As our network involves skip-connections between intermediate neural layers, it is not straightforward to visualize and analyze the learnt latent features.</p><p>It can be also observed that our model trained on bench tends to be more general than others. Intuitively, the bench category tends to have general features such as four legs, seats, and/or a back, which are also common among other categories {chair, couch, table}. However, not all chairs or couches consist of such general features that are shared across different categories.</p><p>Overall, we may safely conclude that the more similar features two categories share, including both the low-level lines/planes/curves and the high-level shape components, better generalization of our model achieves cross those  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Real-world Experiment Results</head><p>(1) Results. Lastly, in order to evaluate the domain adaptation capability of the networks, we train all networks on synthesized data of categories {bench, chair, couch, table}, and then test them on real-world data collected by a Microsoft Kinect camera. Table <ref type="table" target="#tab_13">10</ref> compares the IoU and CE loss of all approaches on the real-world dataset. Figure <ref type="figure" target="#fig_12">9</ref> shows some qualitative results for all methods.</p><p>(2) Analysis. There are two reasons why the IoU is significantly lower compared with testing on the synthetic dataset. First, the ground truth objects obtained from Elastic-Fusion are not as solid as the synthesized datasets. However, all networks predict dense and solid voxel grids, so the interior parts may not match though the overall object shapes are satisfactorily recovered as shown in Figure <ref type="figure" target="#fig_12">9</ref>. Secondly, the input 2.5D depth view from real-world dataset is noisy and incomplete, due to the limitation of the RGB-D sensor (e.g., reflective surfaces, outdoor lighting). In some cases, the input 2.5D view does not capture the whole object and only contains a part of the object, which also leads to inferior reconstruction results (e.g., the 5 th row in Figure <ref type="figure" target="#fig_12">9</ref>) and a lower IoU scores overall. However, our proposed network is still able to reconstruct reasonable 3D dense shapes given the noisy and incomplete 2.5D input depth views, while the competing algorithms (e.g., <ref type="bibr">Varley et al.)</ref> are not robust to real-world noise and unable to generate compelling results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of Adversarial Learning</head><p>(1) Results. In all above experiments, the proposed 3D-RecGAN++ tends to outperform the ablated network 3D-RecAE which does not include the adversarial learning of GAN part. In all visualization of experiment results, the 3D shapes from 3D-RecGAN++ are also more compelling than 3D-RecAE. To further quantitatively investigate how the adversarial learning improves the final 3D results comparing with 3D-RecAE, we calculate the mean precision and recall from the previous multi-category experiment results in Section 4.4. Table <ref type="table" target="#tab_15">11</ref> compares the mean precision and recall of 3D-RecGAN++ and 3D-RecAE on individual categories using the network trained on multiple categories.</p><p>(2) Analysis. It can be seen that the results of 3D-RecGAN++ tend to constantly have higher precision scores than 3D-RecAE, which means 3D-RecGAN++ has less false positive estimations. Therefore, the estimated 3D shapes from 3D-RecAE are likely to be 'fatter' and 'bigger', while 3D-RecGAN++ tends to predict 'thinner' shapes with much more shape details being exposed. Both 3D-RecGAN++ and 3D-RecAE can achieve high recall scores (i.e., above 0.8), which means both 3D-RecGAN++ and 3D-RecAE are capable of estimating the major object shapes without too many false negatives. In other words, the ground truth 3D shape tends to be a subset of the estimated shape result. Overall, with regard to experiments on per-category, multi-category, and cross-category experiments, our 3D-RecGAN++ outperforms others by a large margin, although all other approaches can reconstruct reasonable shapes. In terms of the generality, Varley et al. <ref type="bibr" target="#b2">[3]</ref> and Han et al. <ref type="bibr" target="#b26">[27]</ref> are inferior because Varley et al. <ref type="bibr" target="#b2">[3]</ref> use a single fully connected layers, instead of 3D ConvNets, for shape generation which is unlikely to be general for various shapes, and Han et al. <ref type="bibr" target="#b26">[27]</ref> apply LSTMs for shape blocks generation which is inefficient and unable to learn general 3D structures. However, our 3D-RecGAN++ is superior thanks to the generality of the 3D encoder-decoder and the adversarial discriminator. Besides, the 3D-RecAE tends to over estimate the 3D shape, while the adversarial learning of 3D-RecGAN++ is likely to remove the over-estimated parts, so as to leave the estimated shape to be clearer with more shape details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Computation Analysis</head><p>Table <ref type="table" target="#tab_17">12</ref> compares the computation efficiency of all approaches regarding the total number of model parameters and the average time consumption to recover a single object.</p><p>The model proposed by Han et al. <ref type="bibr" target="#b26">[27]</ref> has the least number of parameters because most of the parameters are shared to predict different blocks of an object. Our 3D-RecGAN++ has reasonable 167.1 millions parameters, which is on a similar scale to VGG-19 (i.e., 144 millions) <ref type="bibr" target="#b87">[88]</ref>.</p><p>To evaluate the average time consumption for a single object reconstruction, we implement all networks in Tensorflow 1.2 and Python 2.7 with CUDA 8.0 and cuDNN 7.1 as the back-end driver and library. All models are tested on a single Titan X GPU in the same hardware and software environments. 3D-EPN <ref type="bibr" target="#b11">[12]</ref> takes the shortest time to predict a 32 3 object on GPU, while our 3D-RecGAN++ only needs around 40 milliseconds to recover a dense 256 3 object. Comparatively, Han et al. takes the longest GPU time to generate a dense object because of the time-consuming sequential processing of LSTMs. The low resolution objects predicted by 3D-EPN and Varley et al. are further upsampled to 256 3 using existing SciPy library on a CPU server (Intel E5-2620 v4, 32 cores). It takes around 7 seconds to finish the upsampling for a single object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Although our 3D-RecGAN++ achieves the state of the art performance in 3D object reconstruction from a single depth view, it has limitations. Firstly, our network takes the volumetric representation of a single depth view as input, instead of taking a raw depth image. Therefore, a preprocessing of raw depth images is required for our network. However, in many application scenarios such as robot grasping, such preprocessing would be trivial and straightforward given the depth camera parameters. Secondly, the input depth view of our network only contains a clean object information without cluttered background. One possible solution is to leverage an existing segmentation algorithm such as Mask-RCNN <ref type="bibr" target="#b88">[89]</ref> to clearly segment the target object instance from the raw depth view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we proposed a framework 3D-RecGAN++ that reconstructs the full 3D structure of an object from an arbitrary depth view. By leveraging the generalization capabilities of 3D encoder-decoder and generative adversarial networks, our 3D-RecGAN++ predicts dense and accurate 3D structures with fine details, outperforming the state of the art in single-view shape completion for individual object category. We further tested our network's ability to reconstruct multiple categories without providing any object class labels during training or testing, and it showed that our network is still able to predict precise 3D shapes. Besides, we investigated the network's reconstruction performance on unseen categories, our proposed approach can also predict satisfactory 3D structures. Finally, our model is robust to real-world noisy data and can infer accurate 3D shapes although the model is purely trained on synthesized data. This confirms that our network has the capability of learning general 3D latent features of the objects, rather than simply fitting a function for the training datasets, and the adversarial learning of 3D-RecGAN++ learns to add geometric details for estimated 3D shapes. In summary, our network only requires a single depth view to recover a dense and complete 3D shape with fine details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: t-SNE embeddings of 2.5D partial views and 3D complete shapes of multiple object categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Overview of the network architecture for training.</figDesc><graphic coords="3,-14.18,13.75,364.76,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Detailed architecture of 3D-RecGAN++, showing the two main building blocks. Note that, although these are shown as two separate modules, they are trained end-to-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: An example of ElasticFusion for generating real world data. Left: reconstructed object; sampled camera poses are shown in black. Right: Input RGB, depth image and segmented depth image.</figDesc><graphic coords="6,314.52,43.70,246.96,115.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Qualitative results of single category reconstruction on testing datasets with same viewing angles. Qualitative results of single category reconstruction on testing datasets with cross viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Qualitative results of single category reconstruction on testing datasets with same and cross viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>shows the IoU and CE loss comparison of all approaches on the testing dataset with same viewing angles,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Qualitative results of multiple category reconstruction on testing datasets with same viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Qualitative results of multiple category reconstruction on testing datasets with cross viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Qualitative results of multiple category reconstruction on testing datasets with same and cross viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Qualitative results of cross category reconstruction on testing datasets with cross viewing angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Qualitative results of cross category reconstruction on testing datasets with same and cross viewing angles. and then tested on the testing dataset with same viewing angles over 256 3 voxel grids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Qualitative results of real-world objects reconstruction from different approaches. The object instance is segmented from the raw depth image in preprocessing step. categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Per-category IoU and CE loss on testing dataset with same viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell>IoU</cell><cell>CE Loss</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>3D-EPN [12]</cell><cell cols="2">0.423 0.488 0.631 0.508 0.087 0.105 0.144 0.101</cell></row><row><cell>Varley et al. [3]</cell><cell cols="2">0.227 0.317 0.544 0.233 0.111 0.157 0.195 0.191</cell></row><row><cell>Han et al. [27]</cell><cell cols="2">0.441 0.426 0.446 0.499 0.045 0.081 0.165 0.058</cell></row><row><cell>3D-RecAE (ours)</cell><cell cols="2">0.577 0.641 0.749 0.675 0.036 0.063 0.067 0.043</cell></row><row><cell>3D-RecGAN++</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>(ours) 0.580 0.647 0.753 0.679 0.034 0.060 0.066 0.040</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Per-category IoU and CE loss on testing dataset with cross viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell>IoU</cell><cell>CE Loss</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>3D-EPN [12]</cell><cell cols="2">0.408 0.446 0.572 0.482 0.086 0.112 0.163 0.103</cell></row><row><cell>Varley et al. [3]</cell><cell cols="2">0.185 0.278 0.475 0.187 0.108 0.171 0.210 0.186</cell></row><row><cell>Han et al. [27]</cell><cell cols="2">0.439 0.426 0.455 0.482 0.047 0.090 0.163 0.060</cell></row><row><cell>3D-RecAE (ours)</cell><cell cols="2">0.524 0.588 0.639 0.610 0.045 0.079 0.117 0.058</cell></row><row><cell>3D-</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>RecGAN++ (ours) 0.531 0.594 0.646 0.618 0.041 0.074 0.111 0.053 2.5D input 3D-EPN Varley et al. Han et al. 3D-RecAE 3D-RecGAN++ Ground Truth 2.5D input 3D-EPN Varley et al. Han et al. 3D-RecAE 3D-RecGAN++ Ground Truth</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Multi-category IoU and CE loss on testing dataset with same viewing angles (256 3 voxel grids).</figDesc><table><row><cell>3D-RecAE (ours)</cell><cell>0.576 0.632 0.740 0.661 0.037 0.060 0.069 0.044</cell></row><row><cell>3D-RecGAN++</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p>IoU</p>CE Loss bench chair couch table bench chair couch table 3D-EPN</p><ref type="bibr" target="#b11">[12]</ref> </p>0.428 0.484 0.634 0.506 0.087 0.107 0.138 0.102 Varley et al.</p><ref type="bibr" target="#b2">[3]</ref> </p>0.234 0.317 0.543 0.236 0.103 0.132 0.197 0.170 Han et al.</p><ref type="bibr" target="#b26">[27]</ref> </p>0.425 0.454 0.440 0.470 0.045 0.087 0.172 0.065</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>(ours) 0.581 0.640 0.745 0.667 0.030 0.051 0.063 0.039</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Multi-category IoU and CE loss on testing dataset with cross viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell>IoU</cell><cell>CE Loss</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>3D-EPN [12]</cell><cell cols="2">0.415 0.452 0.531 0.477 0.091 0.115 0.147 0.111</cell></row><row><cell>Varley et al. [3]</cell><cell cols="2">0.201 0.283 0.480 0.199 0.105 0.143 0.207 0.174</cell></row><row><cell>Han et al. [27]</cell><cell cols="2">0.429 0.444 0.447 0.474 0.045 0.089 0.172 0.063</cell></row><row><cell>3D-RecAE (ours)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Cross-category IoU on testing dataset with same viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell cols="3">car faucet firearm guitar monitor plane</cell></row><row><cell>3D-EPN [12]</cell><cell>0.450 0.442 0.339 0.351</cell><cell>0.444</cell><cell>0.314</cell></row><row><cell>Varley et [3]</cell><cell>0.484 0.260 0.280 0.255</cell><cell>0.341</cell><cell>0.295</cell></row><row><cell>Han et al. [27]</cell><cell>0.360 0.402 0.333 0.353</cell><cell>0.450</cell><cell>0.306</cell></row><row><cell>3D-RecAE (ours)</cell><cell>0.557 0.530 0.422 0.440</cell><cell>0.556</cell><cell>0.390</cell></row><row><cell cols="2">3D-RecGAN++ (ours) 0.555 0.536 0.426 0.442</cell><cell>0.562</cell><cell>0.394</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Cross-category CE loss on testing dataset with same viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell cols="3">car faucet firearm guitar monitor plane</cell></row><row><cell>3D-EPN [12]</cell><cell>0.170 0.088 0.036 0.036</cell><cell>0.123</cell><cell>0.066</cell></row><row><cell>Varley et al. [3]</cell><cell>0.173 0.122 0.029 0.030</cell><cell>0.130</cell><cell>0.042</cell></row><row><cell>Han et al. [27]</cell><cell>0.167 0.077 0.018 0.015</cell><cell>0.088</cell><cell>0.031</cell></row><row><cell>3D-RecAE (ours)</cell><cell>0.110 0.057 0.018 0.016</cell><cell>0.072</cell><cell>0.036</cell></row><row><cell cols="2">3D-RecGAN++ (ours) 0.102 0.053 0.016 0.014</cell><cell>0.067</cell><cell>0.031</cell></row><row><cell cols="4">while Table 7 and 8 shows the IoU and CE loss comparison</cell></row><row><cell cols="4">on the testing dataset with cross viewing angles. Figure</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>Cross-category IoU on testing dataset with cross viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell cols="3">car faucet firearm guitar monitor plane</cell></row><row><cell>3D-EPN [12]</cell><cell>0.446 0.439 0.324 0.359</cell><cell>0.448</cell><cell>0.309</cell></row><row><cell>Varley et al. [3]</cell><cell>0.489 0.260 0.274 0.255</cell><cell>0.334</cell><cell>0.283</cell></row><row><cell>Han et al. [27]</cell><cell>0.349 0.402 0.321 0.363</cell><cell>0.455</cell><cell>0.299</cell></row><row><cell>3D-RecAE (ours)</cell><cell>0.550 0.521 0.411 0.441</cell><cell>0.550</cell><cell>0.382</cell></row><row><cell cols="2">3D-RecGAN++ (ours) 0.553 0.529 0.416 0.449</cell><cell>0.555</cell><cell>0.390</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Cross-category CE loss on testing dataset with cross viewing angles (256 3 voxel grids). {chair, couch, table}. In the second group, the network is trained on chair and separately tested on {bench, couch, table}. Similarly, another two groups of experiments are conducted. Basically, this experiment is to investigate how well our approach learns features from one category and then generalizes to a different category, and vice versa. Table9shows the cross-category IoU and CE loss of our 3D-RecGAN++ trained on individual category</figDesc><table><row><cell></cell><cell cols="3">car faucet firearm guitar monitor plane</cell></row><row><cell>3D-EPN [12]</cell><cell>0.160 0.087 0.033 0.036</cell><cell>0.127</cell><cell>0.065</cell></row><row><cell>Varley et al. [3]</cell><cell>0.171 0.123 0.028 0.030</cell><cell>0.136</cell><cell>0.043</cell></row><row><cell>Han et al. [27]</cell><cell>0.171 0.076 0.018 0.016</cell><cell>0.088</cell><cell>0.031</cell></row><row><cell>3D-RecAE (ours)</cell><cell>0.101 0.059 0.017 0.017</cell><cell>0.079</cell><cell>0.036</cell></row><row><cell cols="2">3D-RecGAN++ (ours) 0.100 0.055 0.014 0.015</cell><cell>0.074</cell><cell>0.031</cell></row><row><cell cols="2">maining 3 categories:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>2.5D input 3D-EPN Varley et al. Han et al. 3D-RecAE 3D-RecGAN++ Ground Truth</head><label></label><figDesc></figDesc><table /><note><p>(a) Qualitative results of cross category reconstruction on testing datasets with same viewing angles.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell>IoU</cell><cell>CE Loss</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>Group 1 (trained on bench)</cell><cell cols="2">0.580 0.510 0.507 0.599 0.034 0.110 0.164 0.062</cell></row><row><cell>Group 2 (trained on chair)</cell><cell cols="2">0.508 0.647 0.469 0.564 0.048 0.060 0.184 0.069</cell></row><row><cell>Group 3 (trained on couch)</cell><cell cols="2">0.429 0.504 0.753 0.437 0.070 0.105 0.066 0.126</cell></row><row><cell>Group 4 (trained on table)</cell><cell cols="2">0.510 0.509 0.402 0.679 0.049 0.111 0.260 0.040</cell></row></table><note><p>Cross-category IoU and CE loss of 3D-RecGAN++ trained on individual category and then tested on the testing dataset with same viewing angles (256 3 voxel grids).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Multi-category IoU and CE loss on real-world dataset (256 3 voxel grids).</figDesc><table><row><cell></cell><cell>IoU</cell><cell>CE Loss</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>3D-EPN [12]</cell><cell cols="2">0.162 0.190 0.508 0.140 0.090 0.158 0.413 0.187</cell></row><row><cell>Varley et al. [3]</cell><cell cols="2">0.118 0.152 0.433 0.075 0.073 0.155 0.436 0.191</cell></row><row><cell>Han et al. [27]</cell><cell cols="2">0.166 0.164 0.235 0.146 0.083 0.167 0.352 0.194</cell></row><row><cell>3D-RecAE (ours)</cell><cell cols="2">0.173 0.203 0.538 0.151 0.065 0.156 0.318 0.180</cell></row><row><cell>3D-</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>RecGAN++ (ours) 0.177 0.208 0.540 0.156 0.061 0.153 0.314 0.177</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11 :</head><label>11</label><figDesc>Multi-category mean precision and recall on testing dataset with same viewing angles (256 3 voxel grids).</figDesc><table><row><cell></cell><cell>mean precision</cell><cell>mean recall</cell></row><row><cell></cell><cell cols="2">bench chair couch table bench chair couch table</cell></row><row><cell>3D-RecAE</cell><cell cols="2">0.668 0.740 0.800 0.750 0.808 0.818 0.907 0.845</cell></row><row><cell cols="2">3D-RecGAN++ 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>.680 0.747 0.804 0.754 0.804 0.820 0.910 0.853</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 12 :</head><label>12</label><figDesc>Comparison of model parameters and average time consumption to reconstruction a single object.</figDesc><table><row><cell></cell><cell>parameters</cell><cell>GPU time</cell><cell>predicted 3D shapes</cell></row><row><cell></cell><cell>(millions)</cell><cell>(milliseconds)</cell><cell>(resolution)</cell></row><row><cell>3D-EPN [12]</cell><cell>52.4</cell><cell>15.8</cell><cell>32 3</cell></row><row><cell>Varley et al. [3]</cell><cell>430.3</cell><cell>16.1</cell><cell>40 3</cell></row><row><cell>Han et al. [27]</cell><cell>7.5</cell><cell>276.4</cell><cell>256 3</cell></row><row><cell>3D-RecGAN++</cell><cell>167.1</cell><cell>38.9</cell><cell>256 3</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VConv-DAE : Deep Volumetric Shape Learning Without Object Labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defo-Net: Learning Body Deformation Using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape Completion Enabled Robotic Grasping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>ISMAR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction at scale using voxel hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-Scale Multi-Resolution Surface Reconstruction from RGB-D Sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Steinbrucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Laplacian Mesh Optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A robust hole-filling algorithm for triangular mesh</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="987" to="997" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poisson Surface Reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for Highresolution 3D Outputs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OctNetFusion: Learning Depth Fusion from Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Surface Prediction for 3D Object Reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Controllable Text Generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Convolutional Inverse Graphics Network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Disentangled Representations for Volumetric Reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a Predictable and Generative Vector Representation for Objects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="25" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D Object Reconstruction from a Single Depth View with Adversarial Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RAPter: Rebuilding Man-made Scenes with Regular Arrangements of Planes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<title level="m">Partial and Approximate Symmetry Detection for 3D Geometry</title>
		<imprint>
			<publisher>SIGGRAPH</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3D geometry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximate Symmetry Detection in Partial 3D Meshes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sipiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="131" to="140" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Symmetry Prior for Convex Variational 3D Reconstruction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Acquiring 3D Indoor Environments with Variability and Repetition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Database-Assisted Object Retrieval for Real-Time 3D Reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="435" to="446" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Search-Classify Approach for Cluttered Indoor Scene Understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An interactive approach to semantic modeling of indoor scenes with an RGBD camera</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data-driven contextual modeling for 3d scene understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Completing 3D Object Shape from One Depth Image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DTAM: Dense Tracking and Mapping in Real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davision</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lucas-Kanade 20 Years On : A Unifying Framework : Part 1</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dense Object Reconstruction with Semantic Priors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dense Reconstruction Using 3D Object Shape Priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Joint Object Pose Estimation and Shape Reconstruction in Urban Street Scenes Using 3D Shape Priors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep Shape from a Low Number of Silhouettes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>3DV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of 3D Structure from Images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a Multi-View Stereo Machine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SurfaceNet: An Endto-end 3D Neural Network for Multiview Stereopsis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Kintinuous: Spatially Extended Kinectfusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>RSS Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ElasticFusion : Dense SLAM Without A Pose Graph</title>
		<author>
			<persName><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face Recognition based on Fitting a 3D Morphable Model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end 3D face reconstruction with deep neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Weakly supervised 3D Reconstruction with Adversarial Constraint</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>3DV</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Using Locally Corresponding CAD Models for Dense 3D Reconstructions from a Single Image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reconstructing Vechicles from a Single Image: Shape Priors for Road Scene Understanding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICCV Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Structured Prediction of Unobserved Voxels From a Single Depth Image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generative Adversarial Text to Image Synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">D Shape Induction from 2D Views of Multiple Objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improved Adversarial Systems for 3D Object Generation and Reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">U-Net : Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">McGAN: Mean and Covariance Feature Matching GAN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Generative and Discriminative Voxel Modeling with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Accuracy and Resolution of Kinect Depth Data for Indoor Mapping Applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Elberink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1437" to="1454" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
