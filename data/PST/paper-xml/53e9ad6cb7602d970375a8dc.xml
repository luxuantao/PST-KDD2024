<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Realistic Modeling for Facial Animation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuencheng</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Digital Equipment Corporation</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>10 King&apos;s College Road</addrLine>
									<postCode>M5S 1A4. fvlee</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Digital Equipment Corporation</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>10 King&apos;s College Road</addrLine>
									<postCode>M5S 1A4. fvlee</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Keith</forename><surname>Waters</surname></persName>
							<email>waters@crl.dec.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Cambridge Research Lab</orgName>
								<orgName type="institution">One Kendall Square</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Realistic Modeling for Facial Animation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C6A81E6DFC4441D40A46CC4D73959475</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.3.5 [Computer Graphics]: Physically based modeling; I.3.7 [Computer Graphics]: Animation Physics-based Facial Modeling</term>
					<term>Facial Animation</term>
					<term>RGB/Range Scanners</term>
					<term>Feature-Based Facial Adaptation</term>
					<term>Texture Mapping</term>
					<term>Discrete Deformable Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major unsolved problem in computer graphics is the construction and animation of realistic human facial models. Traditionally, facial models have been built painstakingly by manual digitization and animated by ad hoc parametrically controlled facial mesh deformations or kinematic approximation of muscle actions. Fortunately, animators are now able to digitize facial geometries through the use of scanning range sensors and animate them through the dynamic simulation of facial tissues and muscles. However, these techniques require considerable user input to construct facial models of individuals suitable for animation. In this paper, we present a methodology for automating this challenging task. Starting with a structured facial mesh, we develop algorithms that automatically construct functional models of the heads of human subjects from laser-scanned range and reflectance data. These algorithms automatically insert contractile muscles at anatomically correct positions within a dynamic skin model and root them in an estimated skull structure with a hinged jaw. They also synthesize functional eyes, eyelids, teeth, and a neck and fit them to the final model. The constructed face may be animated via muscle actuations. In this way, we create the most authentic and functional facial models of individuals available to date and demonstrate their use in facial animation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Two decades have passed since Parke's pioneering work in animating faces <ref type="bibr" target="#b12">[13]</ref>. In the span of time, significant effort has been devoted to the development of computational models of the human face for applications in such diverse areas as entertainment, low bandwidth teleconferencing, surgical facial planning, and virtual reality. However, the task of accurately modeling the expressive human face by computer remains a major challenge.</p><p>Traditionally, computer facial animation follows three basic procedures: (1) design a 3D facial mesh, (2) digitize the 3D mesh, and (3) animate the 3D mesh in a controlled fashion to simulate facial actions.</p><p>In procedure <ref type="bibr" target="#b0">(1)</ref>, it is desirable to have a refined topological mesh that captures the facial geometry. Often this entails digitizing as many nodes as possible. Care must be taken not to oversample the surface because there is a trade-off between the number of nodes and the computational cost of the model. Consequently, meshes developed to date capture the salient features of the face with as few nodes as possible (see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> for several different mesh designs).</p><p>In procedure <ref type="bibr" target="#b1">(2)</ref>, a general 3D digitization technique uses photogrammetry of several images of the face taken from different angles. A common technique is to place markers on the face that can be seen from two or more cameras. An alternative technique is to manually digitize a plaster cast of the face using manual 3D digitization devices such as orthogonal magnetic fields sound captors <ref type="bibr" target="#b8">[9]</ref>, or one to two photographs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>. More recently, automated laser range finders can digitize on the order of 10 5 3D points from a solid object such as a person's head and shoulders in just a few seconds <ref type="bibr" target="#b22">[23]</ref>.</p><p>In procedure (3), an animator must decide which mesh nodes to articulate and how much they should be displaced in order to produce a specific facial expression. Various approaches have been proposed for deforming a facial mesh to produce facial expressions; for example, parameterized models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, control-point models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>, kinematic muscle models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>, a texture-mapassembly model <ref type="bibr" target="#b24">[25]</ref>, a spline model <ref type="bibr" target="#b10">[11]</ref>, feature-tracking models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>, a finite element model <ref type="bibr" target="#b5">[6]</ref>, and dynamic muscle models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Approach</head><p>The goal of our work is to automate the challenging task of creating realistic facial models of individuals suitable for animation. We develop an algorithm that begins with cylindrical range and reflectance data acquired by a Cyberware scanner and automatically constructs an efficient and fully functional model of the subject's head, as shown in Plate 1. The algorithm is applicable to various individuals (Plate 2 shows the raw scans of several individuals). It proceeds in two steps:</p><p>In step 1, the algorithm adapts a well-structured face mesh from <ref type="bibr" target="#b20">[21]</ref> to the range and reflectance data acquired by scanning the subject, thereby capturing the shape of the subject's face. This approach has significant advantages because it avoids repeated manual modification of control parameters to compensate for geometric variations in the facial features from person to person. More specifically, it allows the automatic placement of facial muscles and enables the use of a single control process across different facial models.</p><p>The generic face mesh is adapted automatically through an image analysis technique that searches for salient local minima and maxima in the range image of the subject. The search is directed according to the known relative positions of the nose, eyes, chin, ears, and other facial features with respect to the generic mesh. Facial muscle emergence and attachment points are also known relative to the generic mesh and are adapted automatically as the mesh is conformed to the scanned data.</p><p>In step 2, the algorithm elaborates the geometric model constructed in step 1 into a functional, physics-based model of the subject's face which is capable of facial expression, as shown in the lower portion of Plate 1.</p><p>We follow the physics-based facial modeling approach proposed by Terzopoulos and Waters <ref type="bibr" target="#b19">[20]</ref>. Its basic features are that it animates facial expressions by contracting synthetic muscles embedded in an anatomically motivated model of skin composed of three spring-mass layers. The physical simulation propagates the muscle forces through the physics-based synthetic skin thereby deforming the skin to produce facial expressions. Among the advantages of the physics-based approach are that it greatly enhances the degree of realism over purely geometric facial modeling approaches, while reducing the amount of work that must be done by the animator. It can be computationally efficient. It is also amenable to improvement, with an increase in computational expense, through the use of more sophisticated biomechanical models and more accurate numerical simulation methods.</p><p>We propose a more accurate biomechanical model for facial animation compared to previous models. We develop a new biomechanical facial skin model which is simpler and better than the one proposed in <ref type="bibr" target="#b19">[20]</ref>. Furthermore, we argue that the skull is an important biomechanical structure with regard to facial expression <ref type="bibr" target="#b21">[22]</ref>. To date, the skin-skull interface has been underemphasized in facial animation despite its importance in the vicinity of the articulate jaw; therefore we improve upon previous facial models by developing an algorithm to estimate the skull structure from the acquired range data, and prevent the synthesized facial skin from penetrating the skull.</p><p>Finally, our algorithm includes an articulated neck and synthesizes subsidiary organs, including eyes, eyelids, and teeth, which cannot be adequately imaged or resolved in the scanned data, but which are nonetheless crucial for realistic facial animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generic Face Mesh and Mesh Adaptation</head><p>The first step of our approach to constructing functional facial models of individuals is to scan a subject using a Cyberware Color Digitizer TM . The scanner rotates 360 degrees around the subject, who sits motionless on a stool as a laser stripe is projected onto the head and shoulders. Once the scan is complete, the device has acquired two registered images of the subject: a range image (Figure <ref type="figure">1</ref>) -a topographic map that records the distance from the sensor to points on the facial surface, and a reflectance (RGB) image (Figure <ref type="figure" target="#fig_2">2</ref>) -which registers the color of the surface at those points. The images are in cylindrical coordinates, with longitude (0-360) degrees along the x axis and vertical height along the y axis. The resolution of the images is typically 512 256 pixels (cf. Plate 1)</p><p>The remainder of this section describes an algorithm which reduces the acquired geometric and photometric data to an efficient geometric model of the subject's head. The algorithm is a two-part process which repairs defects in the acquired images and conforms a generic facial mesh to the processed images using a feature-based matching scheme. The resulting mesh captures the facial geometry as a polygonal surface that can be texture mapped with the full resolution reflectance image, thereby maintaining a realistic facsimile of the subject's face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Processing</head><p>One of the problems of range data digitization is illustrated in Figure <ref type="figure">1(a</ref>). In the hair area, in the chin area, nostril area, and even in the pupils, laser beams tend to disperse and the sensor observes no range value for these corresponding 3D surface points. We must correct for missing range and texture information.</p><p>We use a relaxation method to interpolate the range data. In particular, we apply a membrane interpolation method described in <ref type="bibr" target="#b17">[18]</ref>. The relaxation interpolates values for the missing points so as to bring them into successively closer agreement with surrounding points by repeatedly indexing nearest neighbor values. Intuitively, it stretches an elastic membrane over the gaps in the surface. The images interpolated through relaxation are shown in Figure <ref type="figure">1</ref> illustrate improvements in the hair area and chin area. Relaxation works effectively when the range surface is smooth, and particularly in the case of human head range data, the smoothness requirement of the solutions is satisfied quite effectively.</p><p>Figure <ref type="figure" target="#fig_2">2</ref>(a) shows two 512 256 reflectance (RGB) texture maps as monochrome images. Each reflectance value represents the surface color of the object in cylindrical coordinates with corresponding longitude (0-360 degrees) and latitude. Like range images, the acquired reflectance images are lacking color information at certain points. This situation is especially obvious in the hair area and the shoulder area (see Figure <ref type="figure" target="#fig_6">2(a)</ref>). We employ the membrane relaxation approach to interpolate the texture image by repeated averaging of neighboring known colors. The original texture image in Figure <ref type="figure" target="#fig_2">2</ref>(a) can be compared with the interpolated texture image in Figure <ref type="figure" target="#fig_2">2</ref>  The method is somewhat problematic in the hair area where range variations may be large and there is a relatively high percentage of missing surface points. A thin-plate relaxation algorithm <ref type="bibr" target="#b17">[18]</ref> may be more effective in these regions because it would fill in the larger gaps with less "flattening" than a membrane <ref type="bibr" target="#b9">[10]</ref>.</p><p>Although the head structure in the cylindrical laser range data is distorted along the longitudinal direction, important features such as the slope changes of the nose, forehead, chin, and the contours of the mouth, eyes, and nose are still discernible. In order to locate the contours of those facial features for use in adaptation (see below), we use a modified Laplacian operator (applied to the discrete image through local pixel differencing) to detect edges from the range map shown in Figure <ref type="figure" target="#fig_3">3</ref>(a) and produce the field function in Fig. <ref type="figure" target="#fig_12">3(b)</ref>. For details about the operator, see <ref type="bibr" target="#b7">[8]</ref>. The field function highlights important features of interest. For example, the local maxima of the modified Laplacian reveals the boundaries of the lips, eyes, and chin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generic Face Mesh and Mesh Adaptation</head><p>The next step is to reduce the large arrays of data acquired by the scanner into a parsimonious geometric model of the face that can eventually be animated efficiently. Motivated by the adaptive meshing techniques <ref type="bibr" target="#b18">[19]</ref> that were employed in <ref type="bibr" target="#b22">[23]</ref>, we significantly improved the technique by adapting a generic face mesh to the data. Figure <ref type="figure" target="#fig_4">4</ref> shows the planar generic mesh which we obtain through a cylindrical projection of the 3D face mesh from <ref type="bibr" target="#b20">[21]</ref>. One of the advantages of the generic mesh is that it has well-defined features which form the basis for accurate feature based adaptation to the scanned data and automatic scaling and positioning of facial muscles as the mesh is deformed to fit the images. Another advantage is that it automatically produces an efficient triangulation, with finer triangles over the highly curved and/or highly articulate regions of the face, such as the eyes and mouth, and larger triangles elsewhere. We label all facial feature nodes in the generic face prior to the adaptation step. The feature nodes include eye contours, nose contours, mouth contours, and chin contours.</p><p>For any specific range image and its positive Laplacian field function (Figure <ref type="figure" target="#fig_3">3</ref>), the generic mesh adaptation procedure performs the following steps to locate feature points in the range data (see <ref type="bibr" target="#b7">[8]</ref> for details): Mesh Adaptation Procedures Once the mesh has been fitted by the above feature based matching technique (see Plate 3), the algorithm samples the range image at the location of the nodes of the face mesh to capture the facial geometry, as is illustrated in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>The node positions also provide texture map coordinates that are used to map the full resolution color image onto the triangles (see Plate 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Estimation of Relaxed Face Model</head><p>Ideally, the subject's face should be in a neutral, relaxed expression when he or she is being scanned. However, the scanned woman in the "Heidi" dataset is smiling and her mouth is open (see Plate 2). We have made our algorithm tolerant of these situations. To construct a functional model, it is important to first estimate the relaxed geometry. That is, we must infer what the "Heidi" subject would look like had her face been in a relaxed pose while she was being scanned. We therefore estimate the range values of the closed mouth contour from the range values of the open mouth contour by the following steps:</p><p>1. Perform adaptation procedures in Sec. 2.2 without step 3. As a result, the final reconstructed face model in Figure <ref type="figure" target="#fig_5">5</ref>(b) will have a relaxed mouth because the longitude and latitude recorded is the default shape of our closed mouth model (see Figure <ref type="figure" target="#fig_4">4</ref>). Moreover, the shape of the final reconstructed face is still faithful to the head data because the range value at each facial nodal point is obtained correctly after the lip adaptation procedure has been performed. Relaxing the face shown in Figure <ref type="figure" target="#fig_5">5</ref>(a) results in the image in Figure <ref type="figure" target="#fig_5">5</ref>(b) (with eyelids inserted -see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Dynamic Skin and Muscle Model</head><p>This section describes how our system proceeds with the construction of a fully functional model of the subject's face from the facial mesh produced by the adaptation algorithm described in the previous section. To this end, we automatically create a dynamic model of facial tissue, estimate a skull surface, and insert the major muscles of facial expression into the model. The following sections describe each of these components. We also describe our high-performance parallel, numerical simulation of the dynamic facial tissue model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Layered Synthetic Tissue Model</head><p>The skull is covered by deformable tissue which has five distinct layers <ref type="bibr" target="#b3">[4]</ref>. Four layers-epidermis, dermis, sub-cutaneous connective tissue, and fascia-comprise the skin, and the fifth consists of the muscles of facial expression. Following <ref type="bibr" target="#b19">[20]</ref>, and in accordance with the structure of real skin <ref type="bibr" target="#b4">[5]</ref>, we have designed a new, synthetic tissue model (Figure <ref type="figure" target="#fig_9">6(a)</ref>).</p><p>The tissue model is composed of triangular prism elements (see Figure <ref type="figure" target="#fig_9">6(a)</ref>) which match the triangles in the adapted facial mesh. The epidermal surface is defined by nodes 1, 2, and 3, which are connected by epidermal springs. The epidermis nodes are also connected by dermal-fatty layer springs to nodes 4, 5, and 6, which define the fascia surface. Fascia nodes are interconnected by fascia   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discrete Deformable Models (DDMs)</head><p>A discrete deformable model has a node-spring-node structure, which is a uniaxial finite element. The data structure for the node consists of the nodal mass mi, position xi(t) = xi(t) y i(t) z i(t)] 0 , velocity vi = dxi=dt, acceleration ai = d 2 xi=dt 2 , and net nodal forces f n i (t). The data structure for the spring in this DDM consists of pointers to the head node i and the tail node j which the spring interconnects, the natural or rest length lk of the spring, and the spring stiffness ck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tissue Model Spring Forces</head><p>By assembling the discrete deformable model according to histological knowledge of skin (see Figure <ref type="figure" target="#fig_7">6</ref>(a)), we are able to construct an anatomically consistent, albeit simplified, tissue model. Figure <ref type="figure" target="#fig_9">6(b)</ref> shows a close-up view of the tissue model around its eye and nose parts of a face which is automatically assembled by following the above approach.</p><p>The force spring j exerts on node i is gj = cj(lj ; l r j )sj -each layer has its own stress-strain relationship cj and the dermal-fatty layer uses biphasic springs (non-constant cj) <ref type="bibr" target="#b19">[20]</ref> -l r j and lj = jjxj ; xijj are the rest and current lengths for spring j sj = ( xj ; xi)=lj is the spring direction vector for spring j</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linear Muscle Forces</head><p>The muscles of facial expression, or the muscular plate, spreads out below the facial tissue. The facial musculature is attached to the skin tissue by short elastic tendons at many places in the fascia, but is fixed to the facial skeleton only at a few points. Contractions of the facial muscles cause movement of the facial tissue. We model 28 of the primary facial muscles, including the zygomatic major and minor, frontalis, nasii, corrugator, mentalis, buccinator, and angulii depressor groups. Plate 4 illustrates the effects of automatic scaling and positioning of facial muscle vectors as the generic mesh adapts to different faces.</p><p>To better emulate the facial muscle attachments to the fascia layer in our model, a group of fascia nodes situated along the muscle path-i.e., within a predetermined distance from a central muscle vector, in accordance with the muscle width-experience forces from the contraction of the muscle. The face construction algorithm determines the nodes affected by each muscle in a precomputation step.</p><p>To apply muscle forces to the fascia nodes, we calculate a force for each node by multiplying the muscle vector with a force length scaling factor and a force width scaling factor (see Figure <ref type="figure">7(a)</ref>). Function Θ1 (Figure <ref type="figure">8(a)</ref>) scales the muscle force according to the length ratio "j i, while Θ2 (Figure <ref type="figure">8(b)</ref>) scales it according to the width !j i at node i of muscle j:</p><p>"j i = ((m F j ; xi) mj)=(km A j ; m F j k) !j i = kpi ; (pi nj)njk</p><p>The force muscle j exerts on node i is f j i = Θ1("j i)Θ2(!j i)mj -Θ1 scales the force according to the distance ratio "j i,</p><p>where "j i = j i=dj, with dj the muscle j length. -Θ2 scales the force according to the width ratio !j i=wj, with wj the muscle j width.</p><p>-mj is the normalized muscle vector for muscle j</p><p>Note that the muscle force is scaled to zero at the root of the muscle fiber in the bone and reaches its full strength near the end of the muscle fiber. Figure <ref type="figure" target="#fig_8">9</ref>(b) shows an example of the effect of muscle forces applied to a synthetic skin patch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Piecewise Linear Muscle Forces</head><p>In addition to using linear muscle fibers in section 3.4 to simulate sheet facial muscles like the frontalis and the zygomatics, we also model sphincter muscles, such as the orbicularis oris circling the mouth, by generalizing the linear muscle fibers to be piecewise linear and allowing them to attach to fascia at each end of the segments. Figure <ref type="figure">7</ref>(b) illustrates two segments of an N-segment piecewise linear muscle j showing three nodes m l j , m l+1 j , and m l+2 j . The unit vectors mj l, mj l+1 and nj l, nj l+1 are parallel and normal to the segments, respectively. The figure indicates fascia node i at xi, as well as the distance j i = a + b, the width !j i, and the perpendicular vector pi from fascia node i to the nearest segment of the muscle. The length ratio "j i for fascia node i in muscle fiber j is "j i = (m l+1 j ; xi) mj l + P N k=l+1 k m k+1 j ; m k j k P N k=1 k m k+1 j ; m k j k</p><p>The width !j i calculation is the same as for linear muscles.</p><p>The remaining muscle force computations are the same as in section 3.4. Plate 4 shows all the linear muscles and the piecewise linear sphincter muscles around the mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Volume Preservation Forces</head><p>In order to faithfully exhibit the incompressibility <ref type="bibr" target="#b1">[2]</ref> of real human skin in our model, a volume constraint force based on the change of volume (see Figure <ref type="figure" target="#fig_8">9</ref>(a)) and displacements of nodes is calculated and applied to nodes. In Figure <ref type="figure" target="#fig_8">9</ref>(b) the expected effect of volume preservation is demonstrated. For example, near the origin of the muscle fiber, the epidermal skin is bulging out, and near the end of the muscle fiber, the epidermal skin is depressed.</p><p>The volume preservation force element e exerts on nodes i in element e is q e i = k1(V e ; Ṽ e )n e i + k2(p e i ; pe i )</p><p>-Ṽ e and V e are the rest and current volumes for e</p><p>n e i is the epidermal normal for epidermal node i -pe i and p e i are the rest and current nodal coordinates for node i with respect to the center of mass of e -k1 k 2 are force scaling constants </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Skull Penetration Constraint Forces</head><p>Because of the underlying impenetrable skull of a human head, the facial tissue during a facial expression will slide over the underlying bony structure. With this in mind, for each individual's face model reconstructed from the laser range data, we estimate the skull surface normals to be the surface normals in the range data image. The skull is then computed as an offset surface. To prevent nodes from penetrating the estimated skull (see Figure <ref type="figure" target="#fig_8">9</ref>(a)), we apply a skull non-penetration constraint to cancel out the force component on the fascia node which points into the skull; therefore, the resulting force will make the nodes slide over the skull.</p><p>The force to penalize fascia node i during motion is:</p><formula xml:id="formula_0">si = ;(f n i ni)ni when f n i ni &lt; 0 0 otherwise -f n</formula><p>i is the net force on fascia node i ni is the nodal normal of node i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Equations of Motion for Tissue Model</head><p>Newton's law of motion governs the response of the tissue model to forces. This leads to a system of coupled second order ODEs that relate the node positions, velocities, and accelerations to the nodal forces. The equation for node i is mi d 2 xi dt 2 + i dxi dt + gi + qi + si + hi = fi mi is the nodal mass, -i is the damping coefficient, -gi is the total spring force at node i, -qi is the total volume preservation force at node i, -si is the total skull penetration force at node i, -hi is the total nodal restoration force at node i, -fi is the total applied muscle force at node i,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Numerical Simulation</head><p>The solution to the above system of ODEs is approximated by using the well-known, explicit Euler method. At each iteration, the nodal acceleration at time t is computed by dividing the net force by nodal mass. The nodal velocity is then calculated by integrating once, and another integration is done to compute the nodal positions at the next time step t + ∆t, as follows:</p><formula xml:id="formula_1">a t i = 1</formula><p>mi ( f t i ; iv t i ; gt i ; qt i ; st i ; ht i ) v t+∆t i = v t i + ∆ta t i</p><p>x t+∆t </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11">Parallel Processing for Facial Animation</head><p>The explicit Euler method allows us to easily carry out the numerical simulation of the dynamic skin/muscle model in parallel. This is because at each time step all the calculations are based on the results from the previous time step. Therefore, parallelization is achieved by evenly distributing calculations at each time step to all available processors. This parallel approach increases the animation speed to allow us to simulate facial expressions at interactive rates on our Silicon Graphics multiprocessor workstation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Geometry Models for Other Head Components</head><p>To complete our physics-based face model, additional geometric models are combined along with the skin/muscle/skull models developed in the previous section. These include the eyes, eyelids, teeth, neck, hair, and bust (Figure <ref type="figure" target="#fig_13">10</ref>). See Plate 5 for an example of a complete model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Eyes</head><p>Eyes are constructed from spheres with adjustable irises and adjustable pupils (Figure <ref type="figure" target="#fig_13">10(a)</ref>). The eyes are automatically scaled to fit the facial model and are positioned into it. The eyes rotate kinematically in a coordinated fashion so that they will always converge on a specified fixation point in three-dimensional space that defines the field of view. Through a simple illumination computation, the eyes can automatically dilate and contract the pupil size in accordance with the amount of light entering the eye.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Eyelids</head><p>The eyelids are polygonal models which can blink kinematically during animation (see Figure <ref type="figure" target="#fig_13">10(a)</ref>). Note that the eyelids are open in Figure <ref type="figure" target="#fig_13">10(a)</ref>.</p><p>If the subject is scanned with open eyes, the sensor will not observe the eyelid texture. An eyelid texture is synthesized by a relaxation based interpolation algorithm similar to the one described in section 2.1. The relaxation algorithm interpolates a suitable eyelid texture from the immediately surrounding texture map. Figure <ref type="figure" target="#fig_14">11</ref> shows the results of the eyelid texture interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Teeth</head><p>We have constructed a full set of generic teeth based on dental images. Each tooth is a NURBS surfaces of degree 2. Three different teeth shapes, the incisor, canine, and molar, are modeled (Figure <ref type="figure" target="#fig_13">10(b)</ref>). We use different orientations and scalings of these basic shapes to model the full set of upper and lower teeth shown in Figure <ref type="figure" target="#fig_13">10(a)</ref>. The dentures are automatically scaled to fit in length, curvature, etc., and are positioned behind the mouth of the facial model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hair, Neck, and Bust Geometry</head><p>The hair and bust are both rigid polygonal models (see Figure <ref type="figure" target="#fig_13">10(c)</ref>). They are modeled from the range data directly, by extending the facial mesh in a predetermined fashion to the boundaries of the range and reflectance data, and sampling the images as before.</p><p>The neck can be twisted, bent and rotated with three degrees of freedom. See Figure <ref type="figure" target="#fig_15">12</ref> for illustrations of the possible neck articulations. The surprise expression results from contraction of the outer frontalis, major frontalis, inner frontalis, zygomatics major, zygomatics minor, depressor labii, and mentalis, and rotation of the jaw. The anger expression results from contraction of the corrugator, lateral corrugator, levator labii, levator labii nasi, anguli depressor, depressor labii, and mentalis. The quizzical look results from an asymmetric contraction of the major frontalis, outer frontalis, corrugator, lateral corrugator, levator labii, and buccinator. The sadness expression results from a contraction of the inner frontalis, corrugator, lateral corrugator, anguli depressor, and depressor labii.</p><p>Plate 6 demonstrates the performance of our face model construction algorithm on two male individuals ("Giovanni" and "Mick"). Note that the algorithm is tolerant of some amount of facial hair.</p><p>Plate 7 shows a third individual "George." Note the image at the lower left, which shows two additional expression effects-cheek puffing, and lip puckering-that combine to simulate the vigorous blowing of air through the lips. The cheek puffing was created by applying outwardly directed radial forces to "inflate" the deformable cheeks. The puckered lips were created by applying radial pursing forces and forward protruding forces to simulate the action of the orbicularis oris sphincter muscle which circles the mouth.</p><p>Finally, Plate 8 shows several frames from a two-minute animation "Bureaucrat Too" (a second-generation version of the 1990 "Bureaucrat" which was animated using the generic facial model in <ref type="bibr" target="#b19">[20]</ref>). Here "George" tries to read landmark papers on facial modeling and deformable models in the SIGGRAPH '87 proceedings, only to realize that he doesn't yet have a brain!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>The human face consists of a biological tissue layer with nonlinear deformation properties, a muscle layer knit together under the skin, and an impenetrable skull structure beneath the muscle layer. We have presented a physics-based model of the face which takes all of these structures into account. Furthermore, we have demonstrated a new technique for automatically constructing face models of this sort and conforming them to individuals by exploiting highresolution laser scanner data. The conformation process is carried out by a feature matching algorithm based on a reusable generic mesh. The conformation process, efficiently captures facial geometry and photometry, positions and scales facial muscles, and also estimates the skull structure over which the new synthetic facial tissue model can slide. Our facial modeling approach achieves an unprecedented level of realism and fidelity to any specific individual. It also achieves a good compromise between the complete emulation of the complex biomechanical structures and functionality of the human face and real-time simulation performance on state-of-the-art computer graphics and animation hardware.</p><p>Although we formulate the synthetic facial skin as a layered tissue model, our work does not yet exploit knowledge of the variable thickness of the layers in different areas of the face. This issue will in all likelihood be addressed in the future by incorporating additional input data about the subject acquired using noninvasive medical scanners such as CT or MR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) and (a) (b) Figure 1: (a) Range data of "Grace" from a Cyberware scanner. (b) Recovered plain data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) Texture data of "George" with void points displayed in white and (b) texture image interpolated using relaxation method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Original range map. (b) Modified Laplacian field function of (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Facial portion of generic mesh in 2D cylindrical coordinates. Dark lines are features for adaptation.</figDesc><graphic coords="3,86.16,303.42,180.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) Generic geometric model conformed to Cyberware scan of "Heidi". (b) Same as (a). Note that "Heidi's" mouth is now closed, subsequent to estimation of the relaxed face geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 .</head><label>2</label><figDesc>Store nodal longitude/latitude into adapted face model. 3. Perform lip adaptation in step 3 in sec. 2.2 4. Store nodal range values into adapted face model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>(a) Triangular skin tissue prism element. (b) Close-up view of right side of an individual with conformed elements. springs. They are also connected by muscle layer springs to skull surface nodes 7, 8, 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 (</head><label>9</label><figDesc>b) shows 684 such skin elements assembled into an extended skin patch. Several synthetic muscles are embedded into the muscle layer of the skin patch and the figure shows the skin deformation due to muscle contraction. Muscles are fixed in an estimated bony subsurface at their point of emergence and are attached to fascia nodes as they run through several tissue elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 (</head><label>6</label><figDesc>b) shows a close-up view of the right half of the facial tissue model adapted to an individual's face which consists of 432 elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 8: (a) Muscle force scaling function Θ1 wrt "j i, (b) Muscle force scaling function Θ2 wrt !j i=wj</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>(a) Volume preservation and skull nonpenetration element. (b) Assembled layered tissue elements under multiple muscle forces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>i = x t i + ∆tv t+∆t i 3 .</head><label>3</label><figDesc>10 Default Parameters The default parameters for the physical/numerical simulation and the spring stiffness values of different layers are as follows: Mass (m) Time step (∆t) Damping ( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (a) Geometric models of eyes, eyelids, and teeth (b) Incisor, canine, and molar teeth. (c) hair and neck.</figDesc><graphic coords="6,169.44,65.34,122.40,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>(a) Face texture image with adapted mesh before eyelid texture synthesis (b) after eyelid texture synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: articulation of neck.</figDesc><graphic coords="6,314.16,115.01,252.01,86.41" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Lisa White and Jim Randall for developing the piecewise linear muscle model used to model the mouth. Range/RGB facial data were provided courtesy of Cyberware, Inc., Monterey, CA. The first two authors thank the Natural Science and Engineering Research Council of Canada for financial support. DT is a fellow of the Canadian Institute for Advanced Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic creation of 3D facial models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="22" />
			<date type="published" when="1993-09">September 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manual on Experimental Stress Analysis</title>
		<author>
			<persName><forename type="first">James</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Experimental Mechanics</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>fifth edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual Interpretation of Facial Expressions using Dynamic Modeling</title>
		<author>
			<persName><forename type="first">Irfan</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Human</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><surname>Anatomy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thieme Medical Publishers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1991">1991</date>
			<pubPlace>Stuttgart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anatomy of the Human Body</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lea &amp; Febiber</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
	<note>29th edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A system for simulating human facial expression</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the Art in Computer Animation</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A transformation method for modeling and animation of the human face from photographs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the Art in Computer Animation</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constructing physics-based facial models of individuals</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface &apos;93</title>
		<meeting>Graphics Interface &apos;93<address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design, transformation and animation of human faces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Magneneat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reconstruction of a color image from nonuniformly distributed sparse and noisy data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1992-03">March 1992</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial image synthesis using skin texture recording</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nahas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hutric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rioux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time manipulation of texture-mapped surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsutsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ohba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kurauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 21</title>
		<imprint>
			<publisher>ACM Computer Graphics</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computer generated animation of faces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM National Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="451" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameterized models for facial animation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="1982-11">November 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameterized models for facial animation revisited</title>
		<author>
			<persName><forename type="first">F</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH Facial Animation Tutorial Notes</title>
		<imprint>
			<publisher>ACM SIG-GRAPH</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facial animation by spatial mapping</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">C</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Litwinowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the Art in Computer Animation</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Animating facial expression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="252" />
			<date type="published" when="1981-08">August 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The computation of visible-surface representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="438" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sampling and reconstruction with adaptive meshes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vasilescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition Conference</title>
		<meeting>Computer Vision and Pattern Recognition Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Physically-based facial modeling, analysis, and animation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A muscle model for animating three-dimensional facial expression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A physcial model of facial tissue and muscle articulation derived from computer tomography data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1808</biblScope>
			<biblScope unit="page" from="574" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modeling and animating faces using scanned data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
			<affiliation>
				<orgName type="collaboration">Visualization and Computer Animation</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
			<affiliation>
				<orgName type="collaboration">Visualization and Computer Animation</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="123" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance-driven facial animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 24</title>
		<imprint>
			<publisher>ACM Computer Graphics</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Plate 3: Adapted face mesh overlaying texture map and Laplacian filtered range map of Heidi. Plate 4: Muscle fiber vector embedded in generic face model and two adapted faces of Heidi and George. Plate 5: Complete, functional head model of Heidi with physicsbased face and geometric eyes, teeth, hair, neck, and shoulders (in Monument Valley). Plate 6: Animation examples of Giovanni and Mick</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Plate 1: Objective. Input: Range map in 3D and texture map (top)</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
	<note>Output: Functional face model for animation. Plate 2: Raw 512 256 digitized data for Heidi. Plate 7: Animation example of George. Plate 8: George in four scenes from &quot;Bureaucrat Too</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
