<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mastering the Game of Go without Human Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">George van den Driessche</orgName>
								<orgName type="department" key="dep2">Demis Hassabis. DeepMind</orgName>
								<orgName type="institution">Thore Graepel</orgName>
								<address>
									<addrLine>5 New Street Square</addrLine>
									<postCode>EC4A 3TW</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>42 43 44 45 46 47 48 49</addrLine>
									<postBox>50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49</addrLine>
									<postBox>50 51 52 53 54 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100</postBox>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mastering the Game of Go without Human Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from selfplay. Here, we introduce an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.</p><p>Much progress towards artificial intelligence has been made using supervised learning systems that are trained to replicate the decisions of human experts 1-4 . However, expert data is often expensive, unreliable, or simply unavailable. Even when reliable data is available it may impose a ceiling on the performance of systems trained in this manner 5 . In contrast, reinforcement learning systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning.</p><p>These systems have outperformed humans in computer games such as Atari 6, 7 and 3D virtual environments <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> . However, the most challenging domains in terms of human intellect -such as the 1 game of Go, widely viewed as a grand challenge for artificial intelligence 11 -require precise and sophisticated lookahead in vast search spaces. Fully general methods have not previously achieved human-level performance in these domains.</p><p>AlphaGo was the first program to achieve superhuman performance in Go. The published version 12 , which we refer to as AlphaGo Fan, defeated the European champion Fan Hui in October 2015. AlphaGo Fan utilised two deep neural networks: a policy network that outputs move probabilities, and a value network that outputs a position evaluation. The policy network was trained initially by supervised learning to accurately predict human expert moves, and was subsequently refined by policy-gradient reinforcement learning. The value network was trained to predict the winner of games played by the policy network against itself. Once trained, these networks were combined with a Monte-Carlo Tree Search (MCTS) <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> to provide a lookahead search, using the policy network to narrow down the search to high-probability moves, and using the value network (in conjunction with Monte-Carlo rollouts using a fast rollout policy) to evaluate positions in the tree. A subsequent version, which we refer to as AlphaGo Lee, used a similar approach (see Methods), and defeated Lee Sedol, the winner of 18 international titles, in March 2016.</p><p>Our program, AlphaGo Zero, differs from AlphaGo Fan and AlphaGo Lee 12 in several important aspects. First and foremost, it is trained solely by self-play reinforcement learning, starting from random play, without any supervision or use of human data. Second, it only uses the black and white stones from the board as input features. Third, it uses a single neural network, rather than separate policy and value networks. Finally, it uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves, without performing any Monte-Carlo rollouts. To achieve these results, we introduce a new reinforcement learning algorithm that incorporates lookahead search inside the training loop, resulting in rapid improvement and precise and stable learning. Further technical differences in the search algorithm, training procedure and network architecture are described in Methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Reinforcement Learning in AlphaGo Zero</head><p>Our new method uses a deep neural network f θ with parameters θ. This neural network takes as an input the raw board representation s of the position and its history, and outputs both move probabilities and a value, (p, v) = f θ (s). The vector of move probabilities p represents the probability of selecting each move (including pass), p a = P r(a|s). The value v is a scalar evaluation, estimating the probability of the current player winning from position s. This neural network combines the roles of both policy network and value network <ref type="bibr" target="#b11">12</ref> into a single architecture. The neural network consists of many residual blocks <ref type="bibr" target="#b3">4</ref> of convolutional layers <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> with batch normalisation <ref type="bibr" target="#b17">18</ref> and rectifier non-linearities <ref type="bibr" target="#b18">19</ref> (see Methods).</p><p>The neural network in AlphaGo Zero is trained from games of self-play by a novel reinforcement learning algorithm. In each position s, an MCTS search is executed, guided by the neural network f θ . The MCTS search outputs probabilities π π π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network f θ (s); MCTS may therefore be viewed as a powerful policy improvement operator <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> . Self-play with search -using the improved MCTS-based policy to select each move, then using the game winner z as a sample of the value -may be viewed as a powerful policy evaluation operator. The main idea of our reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> : the neural network's parameters are updated to make the move probabilities and value (p, v) = f θ (s) more closely match the improved search probabilities and self-play winner (π π π, z); these new parameters are used in the next iteration of self-play to make the search even stronger. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the self-play training pipeline.</p><p>The Monte-Carlo tree search uses the neural network f θ to guide its simulations (see Figure <ref type="figure" target="#fig_1">2</ref>). Each edge (s, a) in the search tree stores a prior probability P (s, a), a visit count N (s, a), and an action-value Q(s, a). Each simulation starts from the root state and iteratively selects moves that maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) ∝ P (s, a)/(1 + N (s, a)) <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24</ref> , until a leaf node s is encountered. This leaf position is expanded and evaluated just In each position s t , a Monte-Carlo tree search (MCTS) α θ is executed (see Figure <ref type="figure" target="#fig_1">2</ref>) using the latest neural network f θ . Moves are selected according to the search probabilities computed by the MCTS, a t ∼ π π π t . The terminal position s T is scored according to the rules of the game to compute the game winner z. b Neural network training in AlphaGo Zero. The neural network takes the raw board position s t as its input, passes it through many convolutional layers with parameters θ, and outputs both a vector p t , representing a probability distribution over moves, and a scalar value v t , representing the probability of the current player winning in position s t . The neural network parameters θ are updated so as to maximise the similarity of the policy vector p t to the search probabilities π t π t π t , and to minimise the error between the predicted winner v t and the game winner z (see Equation <ref type="formula" target="#formula_1">1</ref>). The new parameters are used in the next iteration of self-play a. Once the search is complete, search probabilities π π π are returned, proportional to N 1/τ , where N is the visit count of each move from the root state and τ is a parameter controlling temperature.</p><p>once by the network to generate both prior probabilities and evaluation, (P (s , •), V (s )) = f θ (s ).</p><p>Each edge (s, a) traversed in the simulation is updated to increment its visit count N (s, a), and to update its action-value to the mean evaluation over these simulations, Q(s, a) = 1/N (s, a) s |s,a→s V (s ),</p><p>where s, a → s indicates that a simulation eventually reached s after taking move a from position s.</p><p>MCTS may be viewed as a self-play algorithm that, given neural network parameters θ and a root position s, computes a vector of search probabilities recommending moves to play, π π π = α θ (s), proportional to the exponentiated visit count for each move, π a ∝ N (s, a) 1/τ , where τ is a temperature parameter.</p><p>The neural network is trained by a self-play reinforcement learning algorithm that uses MCTS to play each move. First, the neural network is initialised to random weights θ 0 . At each subsequent iteration i ≥ 1, games of self-play are generated (Figure <ref type="figure" target="#fig_10">1a</ref>). At each time-step t,</p><p>an MCTS search π π π t = α θ i−1 (s t ) is executed using the previous iteration of neural network</p><formula xml:id="formula_0">f θ i−1 ,</formula><p>and a move is played by sampling the search probabilities π π π t . A game terminates at step T when both players pass, when the search value drops below a resignation threshold, or when the game exceeds a maximum length; the game is then scored to give a final reward of r T ∈ {−1, +1} (see Methods for details). The data for each time-step t is stored as (s t , π π π t , z t ) where z t = ±r T is the game winner from the perspective of the current player at step t. In parallel (Figure <ref type="figure" target="#fig_10">1b</ref>), new network parameters θ i are trained from data (s, π π π, z) sampled uniformly among all time-steps of the last iteration(s) of self-play. The neural network (p, v) = f θ i (s) is adjusted to minimise the error between the predicted value v and the self-play winner z, and to maximise the similarity of the neural network move probabilities p to the search probabilities π π π. Specifically, the parameters θ are adjusted by gradient descent on a loss function l that sums over mean-squared error and cross-entropy losses respectively,</p><formula xml:id="formula_1">(p, v) = f θ (s), l = (z − v) 2 − π π π log p + c||θ|| 2<label>(1)</label></formula><p>where c is a parameter controlling the level of L2 weight regularisation (to prevent overfitting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Empirical Analysis of AlphaGo Zero Training</head><p>We applied our reinforcement learning pipeline to train our program AlphaGo Zero. Training started from completely random behaviour and continued without human intervention for approximately 3 days.</p><p>Over the course of training, 4.9 million games of self-play were generated, using 1,600 simulations for each MCTS, which corresponds to approximately 0.4s thinking time per move. Parameters were updated from 700,000 mini-batches of 2,048 positions. The neural network contained 20 residual blocks (see Methods for further details).</p><p>Figure <ref type="figure" target="#fig_7">3a</ref> shows the performance of AlphaGo Zero during self-play reinforcement learning, as a function of training time, on an Elo scale <ref type="bibr" target="#b24">25</ref> . Learning progressed smoothly throughout training, and did not suffer from the oscillations or catastrophic forgetting suggested in prior literature The MSE of a neural network trained by supervised learning is also shown. <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> . Surprisingly, AlphaGo Zero outperformed AlphaGo Lee after just 36 hours; for comparison, AlphaGo Lee was trained over several months. After 72 hours, we evaluated AlphaGo Zero against the exact version of AlphaGo Lee that defeated Lee Sedol, under the 2 hour time controls and match conditions as were used in the man-machine match in Seoul (see Methods). AlphaGo Zero used a single machine with 4 Tensor Processing Units (TPUs) <ref type="bibr" target="#b28">29</ref> , while AlphaGo Lee was distributed over many machines and used 48 TPUs. AlphaGo Zero defeated AlphaGo Lee by 100 games to 0 (see Extended Data Figure <ref type="figure" target="#fig_5">5</ref> and Supplementary Information).</p><p>To assess the merits of self-play reinforcement learning, compared to learning from human data, we trained a second neural network (using the same architecture) to predict expert moves in the KGS data-set; this achieved state-of-the-art prediction accuracy compared to prior work  <ref type="table">1</ref> and 2 respectively). Supervised learning achieved better initial performance, and was better at predicting the outcome of human professional games (Figure <ref type="figure" target="#fig_2">3</ref>). Notably, although supervised learning achieved higher move prediction accuracy, the self-learned player performed much better overall, defeating the human-trained player within the first 24 hours of training. This suggests that AlphaGo Zero may be learning a strategy that is qualitatively different to human play.</p><p>To separate the contributions of architecture and algorithm, we compared the performance of the neural network architecture in AlphaGo Zero with the previous neural network architecture used in AlphaGo Lee (see Figure <ref type="figure" target="#fig_8">4</ref>). Four neural networks were created, using either separate policy and value networks, as in AlphaGo Lee, or combined policy and value networks, as in AlphaGo Zero; and using either the convolutional network architecture from AlphaGo Lee or the residual network architecture from AlphaGo Zero. Each network was trained to minimise the same loss function (Equation <ref type="formula" target="#formula_1">1</ref>) using a fixed data-set of self-play games generated by AlphaGo Zero after 72 hours of self-play training. Using a residual network was more accurate, achieved lower error, and improved performance in AlphaGo by over 600 Elo. Combining policy and value together into a single network slightly reduced the move prediction accuracy, but reduced the value error and boosted playing performance in AlphaGo by around another 600 Elo. This is partly due to Figure <ref type="figure" target="#fig_8">4</ref>: Comparison of neural network architectures in AlphaGo Zero and AlphaGo Lee. Comparison of neural network architectures using either separate ("sep") or combined policy and value networks ("dual"), and using either convolutional ("conv") or residual networks ("res"). The combinations "dual-res" and "sep-conv" correspond to the neural network architectures used in AlphaGo Zero and AlphaGo Lee respectively. Each network was trained on a fixed data-set generated by a previous run of AlphaGo Zero. a Each trained network was combined with AlphaGo Zero's search to obtain a different player. Elo ratings were computed from evaluation games between these different players, using 5 seconds of thinking time per move. b Prediction accuracy on human professional moves (from the GoKifu data-set) for each network architecture. c Mean-squared error on human professional game outcomes (from the GoKifu data-set) for each network architecture. improved computational efficiency, but more importantly the dual objective regularises the network to a common representation that supports multiple use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge Learned by AlphaGo Zero</head><p>AlphaGo Zero discovered a remarkable level of Go knowledge during its self-play training process. This included fundamental elements of human Go knowledge, and also non-standard strategies beyond the scope of traditional Go knowledge.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows a timeline indicating when professional joseki (corner sequences) were discovered (Figure <ref type="figure" target="#fig_5">5a</ref>, Extended Data Figure <ref type="figure" target="#fig_0">1</ref>); ultimately AlphaGo Zero preferred new joseki variants that were previously unknown (Figure <ref type="figure" target="#fig_5">5b</ref>, Extended Data Figure <ref type="figure" target="#fig_1">2</ref>). Figure <ref type="figure" target="#fig_5">5c</ref>  Data Figure <ref type="figure" target="#fig_2">3</ref> and Supplementary Information. AlphaGo Zero rapidly progressed from entirely random moves towards a sophisticated understanding of Go concepts including fuseki (opening), tesuji (tactics), life-and-death, ko (repeated board situations), yose (endgame), capturing races, sente (initiative), shape, influence and territory, all discovered from first principles. Surprisingly, shicho ("ladder" capture sequences that may span the whole board) -one of the first elements of Go knowledge learned by humans -were only understood by AlphaGo Zero much later in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Final Performance of AlphaGo Zero</head><p>We subsequently applied our reinforcement learning pipeline to a second instance of AlphaGo Zero  The timestamp of that iteration is indicated on the timeline. At 10 hours a weak corner move was preferred. At 47 hours the 3-3 invasion was most frequently played. This joseki is also common in human professional play; however AlphaGo Zero later discovered and preferred a new variation. Extended Data Figure <ref type="figure" target="#fig_1">2</ref> provides the frequency of occurence over time for all five sequences and the new variation. c The first 80 moves of three self-play games that were played at different stages of training, using 1,600 simulations (around 0.4s) per search. At 3 hours, the game focuses greedily on capturing stones, much like a human beginner. At 19 hours, the game exhibits the fundamentals of life-and-death, influence and territory. At 70 hours, the game is beautifully balanced, involving multiple battles and a complicated ko fight, eventually resolving into a half-point win for white. See Supplementary Information for the full games.</p><p>throughout training are shown in Extended Data Figure <ref type="figure" target="#fig_8">4</ref> and Supplementary Information.</p><p>We evaluated the fully trained AlphaGo Zero using an internal tournament against AlphaGo Fan, AlphaGo Lee, and several previous Go programs. We also played games against the strongest existing program, AlphaGo Master -a program based on the algorithm and architecture presented in this paper but utilising human data and features (see Methods) -which defeated the strongest human professional players 60-0 in online games <ref type="bibr" target="#b33">34</ref> in January 2017. In our evaluation, all programs were allowed 5 seconds of thinking time per move; AlphaGo Zero and AlphaGo Master each played on a single machine with 4 TPUs; AlphaGo Fan and AlphaGo Lee were distributed over 176 GPUs and 48 TPUs respectively. We also included a player based solely on the raw neural network of AlphaGo Zero; this player simply selected the move with maximum probability.</p><p>Figure <ref type="figure" target="#fig_16">6b</ref> shows the performance of each program on an Elo scale. The raw neural network, without using any lookahead, achieved an Elo rating of 3,055. AlphaGo Zero achieved a rating of 5,185, compared to 4,858 for AlphaGo Master, 3,739 for AlphaGo Lee and 3,144 for AlphaGo Fan.</p><p>Finally, we evaluated AlphaGo Zero head to head against AlphaGo Master in a 100 game match with 2 hour time controls. AlphaGo Zero won by 89 games to 11 (see Extended Data Figure <ref type="figure" target="#fig_6">6</ref>) and Supplementary Information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our results comprehensively demonstrate that a pure reinforcement learning approach is fully feasible, even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance, given no knowledge of the domain beyond basic rules. Furthermore, a pure reinforcement learning approach requires just a few more hours to train, and achieves much better asymptotic performance, compared to training on human expert data. Using this ap- AlphaGo Lee were distributed over many machines. The raw neural network from AlphaGo Zero is also included, which directly selects the move a with maximum probability p a , without using MCTS. Programs were evaluated on an Elo scale <ref type="bibr" target="#b24">25</ref> : a 200 point gap corresponds to a 75% probability of winning. from human data using handcrafted features, by a large margin.</p><p>Humankind has accumulated Go knowledge from millions of games played over thousands of years, collectively distilled into patterns, proverbs and books. In the space of a few days, starting tabula rasa, AlphaGo Zero was able to rediscover much of this Go knowledge, as well as novel strategies that provide new insights into the oldest of games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Reinforcement learning Policy iteration <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> is a classic algorithm that generates a sequence of improving policies, by alternating between policy evaluation -estimating the value function of the current policy -and policy improvement -using the current value function to generate a better policy. A simple approach to policy evaluation is to estimate the value function from the outcomes of sampled trajectories <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> . A simple approach to policy improvement is to select actions greedily with respect to the value function <ref type="bibr" target="#b19">20</ref> . In large state spaces, approximations are necessary to evaluate each policy and to represent its improvement <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> .</p><p>Classification-based reinforcement learning <ref type="bibr" target="#b36">37</ref> improves the policy using a simple Monte-Carlo search. Many rollouts are executed for each action; the action with the maximum mean value provides a positive training example, while all other actions provide negative training examples; a policy is then trained to classify actions as positive or negative, and used in subsequent rollouts. This may be viewed as a precursor to the policy component of AlphaGo Zero's training algorithm when τ → 0.</p><p>A more recent instantiation, classification-based modified policy iteration (CBMPI), also performs policy evaluation by regressing a value function towards truncated rollout values, similar to the value component of AlphaGo Zero; this achieved state-of-the-art results in the game of Tetris <ref type="bibr" target="#b37">38</ref> . However, this prior work was limited to simple rollouts and linear function approximation using handcrafted features.</p><p>The AlphaGo Zero self-play algorithm can similarly be understood as an approximate policy iteration scheme in which MCTS is used for both policy improvement and policy evaluation.</p><p>Policy improvement starts with a neural network policy, executes an MCTS based on that policy's recommendations, and then projects the (much stronger) search policy back into the function space of the neural network. Policy evaluation is applied to the (much stronger) search policy: the outcomes of self-play games are also projected back into the function space of the neural network.</p><p>These projection steps are achieved by training the neural network parameters to match the search probabilities and self-play game outcome respectively. Guo et al. <ref type="bibr" target="#b6">7</ref> also project the output of MCTS into a neural network, either by regressing a value network towards the search value, or by classifying the action selected by MCTS. This approach was used to train a neural network for playing Atari games; however, the MCTS was fixed -there was no policy iteration -and did not make any use of the trained networks.</p><p>Self-play reinforcement learning in games Our approach is most directly applicable to zero-sum games of perfect information. We follow the formalism of alternating Markov games described in previous work <ref type="bibr" target="#b11">12</ref> , noting that algorithms based on value or policy iteration extend naturally to this setting <ref type="bibr" target="#b38">39</ref> .</p><p>Self-play reinforcement learning has previously been applied to the game of Go. NeuroGo 40, 41 used a neural network to represent a value function, using a sophisticated architecture based on Go knowledge regarding connectivity, territory and eyes. This neural network was trained by temporal-difference learning <ref type="bibr" target="#b41">42</ref> to predict territory in games of self-play, building on prior work <ref type="bibr" target="#b42">43</ref> . A related approach, RLGO 44 , represented the value function instead by a linear combination of features, exhaustively enumerating all 3 × 3 patterns of stones; it was trained by temporaldifference learning to predict the winner in games of self-play. Both NeuroGo and RLGO achieved a weak amateur level of play.</p><p>Monte-Carlo tree search (MCTS) may also be viewed as a form of self-play reinforcement learning <ref type="bibr" target="#b44">45</ref> . The nodes of the search tree contain the value function for the positions encountered during search; these values are updated to predict the winner of simulated games of self-play.</p><p>MCTS programs have previously achieved strong amateur level in Go <ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47</ref> , but used substantial domain expertise: a fast rollout policy, based on handcrafted features <ref type="bibr">48 13</ref> , that evaluates positions by running simulations until the end of the game; and a tree policy, also based on handcrafted features, that selects moves within the search tree <ref type="bibr" target="#b46">47</ref> .</p><p>Self-play reinforcement learning approaches have achieved high levels of performance in other games: chess <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref> , checkers <ref type="bibr" target="#b51">52</ref> , backgammon <ref type="bibr" target="#b52">53</ref> , othello <ref type="bibr" target="#b53">54</ref> , Scrabble <ref type="bibr" target="#b54">55</ref> and most recently poker <ref type="bibr" target="#b55">56</ref> . In all of these examples, a value function was trained by regression <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> or temporaldifference learning <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> from training data generated by self-play. The trained value function was used as an evaluation function in an alpha-beta search <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> , a simple Monte-Carlo search <ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57</ref> , or counterfactual regret minimisation <ref type="bibr" target="#b55">56</ref> . However, these methods utilised handcrafted input features 49-53, 56 or handcrafted feature templates <ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55</ref> . In addition, the learning process used supervised learning to initialise weights <ref type="bibr" target="#b57">58</ref> , hand-selected weights for piece values <ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52</ref> , handcrafted restrictions on the action space <ref type="bibr" target="#b55">56</ref> , or used pre-existing computer programs as training opponents <ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50</ref> or to generate game records <ref type="bibr" target="#b50">51</ref> .</p><p>Many of the most successful and widely used reinforcement learning methods were first introduced in the context of zero-sum games: temporal-difference learning was first introduced for a checkers-playing program <ref type="bibr" target="#b58">59</ref> , while MCTS was introduced for the game of Go 13 . However, very similar algorithms have subsequently proven highly effective in video games <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">10</ref> , robotics <ref type="bibr" target="#b59">60</ref> , industrial control <ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref> , and online recommendation systems <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AlphaGo versions</head><p>We compare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type="bibr" target="#b11">12</ref> that played against Fan Hui in October 2015. This program was distributed over many machines using 176 GPUs.</p><p>2. AlphaGo Lee is the program that defeated Lee Sedol 4-1 in March, 2016. It was previously unpublished but is similar in most regards to AlphaGo Fan 12 . However, we highlight several key differences to facilitate a fair comparison. First, the value network was trained from the outcomes of fast games of self-play by AlphaGo, rather than games of self-play by the policy network; this procedure was iterated several times -an initial step towards the tabula rasa algorithm presented in this paper. Second, the policy and value networks were larger than those described in the original paper -using 12 convolutional layers of 256 planes respectively -and were trained for more iterations. This player was also distributed over many machines using 48 TPUs, rather than GPUs, enabling it to evaluate neural networks faster during search.</p><p>3. AlphaGo Master is the program that defeated top human players by 60-0 in January, 2017 <ref type="bibr" target="#b33">34</ref> .</p><p>It was previously unpublished but uses the same neural network architecture, reinforcement learning algorithm, and MCTS algorithm as described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type="bibr" target="#b11">12</ref> and training was initialised by supervised learning from human data.</p><p>4. AlphaGo Zero is the program described in this paper. It learns from self-play reinforcement learning, starting from random initial weights, without using rollouts, with no human supervision, and using only the raw board history as input features. It uses just a single machine in the Google Cloud with 4 TPUs (AlphaGo Zero could also be distributed but we chose to use the simplest possible search algorithm). Domain Knowledge Our primary contribution is to demonstrate that superhuman performance can be achieved without human domain knowledge. To clarify this contribution, we enumerate the domain knowledge that AlphaGo Zero uses, explicitly or implicitly, either in its training procedure or its Monte-Carlo tree search; these are the items of knowledge that would need to be replaced for AlphaGo Zero to learn a different (alternating Markov) game.</p><p>1. AlphaGo Zero is provided with perfect knowledge of the game rules. These are used during MCTS, to simulate the positions resulting from a sequence of moves, and to score any simulations that reach a terminal state. Games terminate when both players pass, or after 19 • 19 • 2 = 722 moves. In addition, the player is provided with the set of legal moves in each position.</p><p>2. AlphaGo Zero uses Tromp-Taylor scoring <ref type="bibr" target="#b65">66</ref> during MCTS simulations and self-play training. This is because human scores (Chinese, Japanese or Korean rules) are not well-defined if the game terminates before territorial boundaries are resolved. However, all tournament and evaluation games were scored using Chinese rules.  It only uses its deep neural network to evaluate leaf nodes and to select moves (see section below).</p><p>It does not use any rollout policy or tree policy, and the MCTS is not augmented by any other heuristics or domain-specific rules. No legal moves are excluded -even those filling in the player's own eyes (a standard heuristic used in all previous programs <ref type="bibr" target="#b66">67</ref> ).</p><p>The algorithm was started with random initial parameters for the neural network. The neural network architecture (see Neural Network Architecture) is based on the current state of the art in image recognition <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18</ref> , and hyperparameters for training were chosen accordingly (see Self-Play Training Pipeline). MCTS search parameters were selected by Gaussian process optimisation <ref type="bibr">68</ref> , so as to optimise self-play performance of AlphaGo Zero using a neural network trained in a preliminary run. For the larger run (40 block, 40 days), MCTS search parameters were reoptimised using the neural network trained in the smaller run (20 block, 3 days). The training algorithm was executed autonomously without human intervention.</p><p>Self-Play Training Pipeline AlphaGo Zero's self-play training pipeline consists of three main components, all executed asynchronously in parallel. Neural network parameters θ i are continually optimised from recent self-play data; AlphaGo Zero players α θ i are continually evaluated; and the best performing player so far, α θ * , is used to generate new self-play data.</p><p>Optimisation Each neural network f θ i is optimised on the Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU parameter servers. The batch-size is 32 per worker, for a total mini-batch size of 2,048. Each mini-batch of data is sampled uniformly at random from all positions from the most recent 500,000 games of self-play. Neural network parameters are optimised by stochastic gradient descent with momentum and learning rate annealing, using the loss in Equation <ref type="formula" target="#formula_1">1</ref>. The learning rate is annealed according to the standard schedule in Extended Data Table <ref type="table">3</ref>. The momentum parameter is set to 0.9. The cross-entropy and mean-squared error losses are weighted equally (this is reasonable because rewards are unit scaled, r ∈ {−1, +1}) and the L2 regularisation parameter is set to c = 10 −4 . The optimisation process produces a new checkpoint every 1,000 training steps. This checkpoint is evaluated by the evaluator and it may be used for generating the next batch of self-play games, as we explain next.</p><p>Evaluator To ensure we always generate the best quality data, we evaluate each new neural network checkpoint against the current best network f θ * before using it for data generation. The neural network f θ i is evaluated by the performance of an MCTS search α θ i that uses f θ i to evaluate leaf positions and prior probabilities (see Search Algorithm). Each evaluation consists of 400 games, using an MCTS with 1,600 simulations to select each move, using an infinitesimal temperature τ → 0 (i.e. we deterministically select the move with maximum visit count, to give the strongest possible play). If the new player wins by a margin of &gt; 55% (to avoid selecting on noise alone) then it becomes the best player α θ * , and is subsequently used for self-play generation, and also becomes the baseline for subsequent comparisons.</p><p>Self-Play The best current player α θ * , as selected by the evaluator, is used to generate data.</p><p>In each iteration, α θ * plays 25,000 games of self-play, using 1,600 simulations of MCTS to select each move (this requires approximately 0.4s per search). For the first 30 moves of each game, the temperature is set to τ = 1; this selects moves proportionally to their visit count in MCTS, and ensures a diverse set of positions are encountered. For the remainder of the game, an infinitesimal temperature is used, τ → 0. Additional exploration is achieved by adding Dirichlet noise to the prior probabilities in the root node s 0 , specifically P (s, a) = (1 − )p a + η a , where η η η ∼ Dir(0.03) and = 0.25; this noise ensures that all moves may be tried, but the search may still overrule bad moves. In order to save computation, clearly lost games are resigned. The resignation threshold v resign is selected automatically to keep the fraction of false positives (games that could have been won if AlphaGo had not resigned) below 5%. To measure false positives, we disable resignation in 10% of self-play games and play until termination.</p><p>Supervised Learning For comparison, we also trained neural network parameters θ SL by supervised learning. The neural network architecture was identical to AlphaGo Zero. Mini-batches of data (s, π π π, z) were sampled at random from the KGS data-set, setting π a = 1 for the human expert move a. Parameters were optimised by stochastic gradient descent with momentum and learning rate annealing, using the same loss as in Equation <ref type="formula" target="#formula_1">1</ref>, but weighting the mean-squared error component by a factor of 0.01. The learning rate was annealed according to the standard schedule in Extended Data Table <ref type="table">3</ref>. The momentum parameter was set to 0.9, and the L2 regularisation parameter was set to c = 10 −4 . By using a combined policy and value network architecture, and by using a low weight on the value component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type="bibr" target="#b11">12</ref> ). After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> , reaching 60.4% on the KGS test set; the value prediction error was also substantially better than previously reported <ref type="bibr" target="#b11">12</ref> . The validation set was composed of professional games from GoKifu. Accuracies and mean squared errors are reported in Extended Data Table <ref type="table">1</ref> and Extended Data Table <ref type="table">2</ref> respectively.</p><p>Search Algorithm AlphaGo Zero uses a much simpler variant of the asynchronous policy and value MCTS algorithm (APV-MCTS) used in AlphaGo Fan and AlphaGo Lee.</p><p>Each node s in the search tree contains edges (s, a) for all legal actions a ∈ A(s). Each edge stores a set of statistics, {N (s, a), W (s, a), Q(s, a), P (s, a)}, where N (s, a) is the visit count, W (s, a) is the total action-value, Q(s, a) is the mean action-value, and P (s, a) is the prior probability of selecting that edge. Multiple simulations are executed in parallel on separate search threads. The algorithm proceeds by iterating over three phases (a-c in Figure <ref type="figure" target="#fig_1">2</ref>), and then selects a move to play (d in Figure <ref type="figure" target="#fig_1">2</ref>). Select (Figure <ref type="figure" target="#fig_1">2a</ref>). The selection phase is almost identical to AlphaGo Fan 12 ; we recapitulate here for completeness. The first in-tree phase of each simulation begins at the root node of the search tree, s 0 , and finishes when the simulation reaches a leaf node s L at time-step L. At each of these time-steps, t &lt; L, an action is selected according to the statistics in the search tree, a t = argmax a Q(s t , a) + U (s t , a) , using a variant of the PUCT algorithm <ref type="bibr" target="#b23">24</ref> ,</p><formula xml:id="formula_2">U (s, a) = c puct P (s, a) b N (s, b) 1 + N (s, a)</formula><p>where c puct is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asympotically prefers actions with high action-value.</p><p>Expand and evaluate (Figure <ref type="figure" target="#fig_1">2b</ref>). The leaf node s L is added to a queue for neural network evaluation,</p><formula xml:id="formula_3">(d i (p), v) = f θ (d i (s L ))</formula><p>, where d i is a dihedral reflection or rotation selected uniformly</p><formula xml:id="formula_4">at random from i ∈ [1..8].</formula><p>Positions in the queue are evaluated by the neural network using a mini-batch size of 8; the search thread is locked until evaluation completes. The leaf node is expanded and each edge (s L , a) is initialised to {N (s L , a) = 0, W (s L , a) = 0, Q(s L , a) = 0, P (s L , a) = p a }; the value v is then backed up.</p><p>Backup (Figure <ref type="figure" target="#fig_1">2c</ref>). The edge statistics are updated in a backward pass through each step t ≤ L. The visit counts are incremented, N (s t , a t ) = N (s t , a t ) + 1, and the action-value is updated to the mean value, W (s t , a t ) = W (s t , a t ) + v, Q(s t , a t ) = W (st,at) N (st,at) . We use virtual loss to ensure each thread evaluates different nodes <ref type="bibr">69</ref> .</p><p>Play (Figure <ref type="figure" target="#fig_1">2d</ref>). At the end of the search AlphaGo Zero selects a move a to play in the root position s 0 , proportional to its exponentiated visit count, π(a|s</p><formula xml:id="formula_5">0 ) = N (s 0 , a) 1/τ / b N (s 0 , b) 1/τ ,</formula><p>where τ is a temperature parameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. AlphaGo Zero resigns if its root value and best child value are lower than a threshold value v resign .</p><p>Compared to the MCTS in AlphaGo Fan and AlphaGo Lee, the principal differences are that AlphaGo Zero does not use any rollouts; it uses a single neural network instead of separate policy and value networks; leaf nodes are always expanded, rather than using dynamic expansion; each search thread simply waits for the neural network evaluation, rather than performing evaluation and backup asynchronously; and there is no tree policy. A transposition table was also used in the large (40 block, 40 day) instance of AlphaGo Zero.</p><p>Neural Network Architecture The input to the neural network is a 19 × 19 × 17 image stack comprising 17 binary feature planes. 8 feature planes X t consist of binary values indicating the presence of the current player's stones (X i t = 1 if intersection i contains a stone of the player's colour at time-step t; 0 if the intersection is empty, contains an opponent stone, or if t &lt; 0). A further 8 feature planes, Y t , represent the corresponding features for the opponent's stones. The final feature plane, C, represents the colour to play, and has a constant value of either 1 if black is to play or 0 if white is to play. These planes are concatenated together to give input features</p><formula xml:id="formula_6">s t = [X t , Y t , X t−1 , Y t−1 , ..., X t−7 , Y t−7 , C].</formula><p>History features X t , Y t are necessary because Go is not fully observable solely from the current stones, as repetitions are forbidden; similarly, the colour feature C is necessary because the komi is not observable. The input features s t are processed by a residual tower that consists of a single convolutional block followed by either 19 or 39 residual blocks <ref type="bibr" target="#b3">4</ref> .</p><p>The convolutional block applies the following modules:</p><p>1. A convolution of 256 filters of kernel size 3 × 3 with stride 1 2. Batch normalisation <ref type="bibr" target="#b17">18</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A rectifier non-linearity</head><p>Each residual block applies the following modules sequentially to its input: The output of the residual tower is passed into two separate "heads" for computing the policy and value respectively. The policy head applies the following modules: The overall network depth, in the 20 or 40 block network, is 39 or 79 parameterised layers respectively for the residual tower, plus an additional 2 layers for the policy head and 3 layers for the value head.</p><p>We note that a different variant of residual networks was simultaneously applied to computer Go <ref type="bibr" target="#b32">33</ref> and achieved amateur dan-level performance; however this was restricted to a single-headed policy network trained solely by supervised learning.</p><p>Neural Network Architecture Comparison Figure <ref type="figure" target="#fig_8">4</ref> shows the results of a comparison between network architectures. Specifically, we compared four different neural networks:</p><p>1. dual-res: The network contains a 20-block residual tower, as described above, followed by both a policy head and a value head. This is the architecture used in AlphaGo Zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">sep-res:</head><p>The network contains two 20-block residual towers. The first tower is followed by a policy head and the second tower is followed by a value head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">dual-conv:</head><p>The network contains a non-residual tower of 12 convolutional blocks, followed by both a policy head and a value head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">sep-conv:</head><p>The network contains two non-residual towers of 12 convolutional blocks. The first tower is followed by a policy head and the second tower is followed by a value head. This is the architecture used in AlphaGo Lee.</p><p>Each network was trained on a fixed data-set containing the final 2 million games of selfplay data generated by a previous run of AlphaGo Zero, using stochastic gradient descent with the annealing rate, momentum, and regularisation hyperparameters described for the supervised learning experiment; however, cross-entropy and mean-squared error components were weighted equally, since more data was available.</p><p>Evaluation We evaluated the relative strength of AlphaGo Zero (Figure <ref type="figure" target="#fig_16">3a and 6</ref>) by measuring the Elo rating of each player. We estimate the probability that player a will defeat player b by Elo ratings were computed from the results of a 5 second per move tournament between AlphaGo Zero, AlphaGo Master, AlphaGo Lee, and AlphaGo Fan. The raw neural network from AlphaGo Zero was also included in the tournament. The Elo ratings of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type="bibr" target="#b11">12</ref> , and correspond to the players reported in that work. The results of the matches of AlphaGo Fan against Fan Hui and AlphaGo Lee against Lee Sedol were also included to ground the scale to human references, as otherwise the Elo ratings of AlphaGo are unrealistically high due to self-play bias.</p><p>The Elo ratings in Figure <ref type="figure" target="#fig_7">3a</ref>, 4a and 6a were computed from the results of evaluation games between each iteration of player α θ i during self-play training. Further evaluations were also performed against baseline players with Elo ratings anchored to the previously published values <ref type="bibr" target="#b11">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo Zero against AlphaGo Lee, and the 40 block instance of AlphaGo Zero against AlphaGo Master, using the same player and match conditions as were used against Lee Sedol in Seoul, 2016. Each player received 2 hours of thinking time plus 3 byoyomi periods of 60 seconds per move. All games were scored using Chinese rules with a komi of 7.5 points.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Self-play reinforcement learning in AlphaGo Zero. a The program plays a game s 1 , ..., s T against itself.</figDesc><graphic url="image-1.png" coords="4,142.20,142.76,327.61,284.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Monte-Carlo tree search in AlphaGo Zero. a Each simulation traverses the tree by selecting the edge with maximum action-value Q, plus an upper confidence bound U that depends on a stored prior probability P and visit count N for that edge (which is incremented once traversed). b The leaf node is expanded and the associated position s is evaluated by the neural network (P (s, •), V (s)) = f θ (s); the vector of P values are stored in the outgoing edges from s. c Action-values Q are updated to track the mean of all evaluations V in the subtree below that action. d</figDesc><graphic url="image-2.png" coords="5,72.00,72.00,468.00,127.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Empirical evaluation of AlphaGo Zero. a Performance of self-play reinforcement learning. The plot shows the performance of each MCTS player α θi from each iteration i of reinforcement learning in AlphaGo Zero.Elo ratings were computed from evaluation games between different players, using 0.4 seconds of thinking time per move (see Methods). For comparison, a similar player trained by supervised learning from human data, using the KGS data-set, is also shown. b Prediction accuracy on human professional moves. The plot shows the accuracy of the neural network f θi , at each iteration of self-play i, in predicting human professional moves from the GoKifu data-set.The accuracy measures the percentage of positions in which the neural network assigns the highest probability to the human move. The accuracy of a neural network trained by supervised learning is also shown. c Mean-squared error (MSE) on human professional game outcomes. The plot shows the MSE of the neural network f θi , at each iteration of self-play i, in predicting the outcome of human professional games from the GoKifu data-set. The MSE is between the actual outcome z ∈ {−1, +1} and the neural network value v, scaled by a factor of1  4  to the range [0, 1]. The MSE of a neural network trained by supervised learning is also shown.</figDesc><graphic url="image-3.png" coords="7,72.00,191.43,468.00,151.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and the Supplementary Information show several fast self-play games played at different stages of training. Tournament length games played at regular intervals throughout training are shown in Extended</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>using a larger neural network and over a longer duration. Training again started from completely random behaviour and continued for approximately 40 days. Over the course of training, 29 million games of self-play were generated. Parameters were updated from 3.1 million mini-batches of 2,048 positions each. The neural network contained</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Go knowledge learned by AlphaGo Zero. a Five human joseki (common corner sequences) discovered during AlphaGo Zero training. The associated timestamps indicate the first time each sequence occured (taking account of rotation and reflection) during self-play training. Extended Data Figure 1 provides the frequency of occurence over training for each sequence. b Five joseki favoured at different stages of self-play training. Each displayed corner sequence was played with the greatest frequency, among all corner sequences, during an iteration of self-play training.</figDesc><graphic url="image-5.png" coords="11,72.00,72.00,468.00,395.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of AlphaGo Zero. a Learning curve for AlphaGo Zero using larger 40 block residual network over 40 days. The plot shows the performance of each player α θi from each iteration i of our reinforcement learning algorithm. Elo ratings were computed from evaluation games between different players, using 0.4 seconds per search (see Methods). b Final performance of AlphaGo Zero. AlphaGo Zero was trained for 40 days using a 40 residual block neural network. The plot shows the results of a tournament between: AlphaGo Zero, AlphaGo Master (defeated top human professionals 60-0 in online games), AlphaGo Lee (defeated Lee Sedol), AlphaGo Fan (defeated Fan Hui), as well as previous Go programs Crazy Stone, Pachi and GnuGo. Each program was given 5 seconds of thinking time per move. AlphaGo Zero and AlphaGo Master played on a single machine on the Google Cloud; AlphaGo Fan and</figDesc><graphic url="image-6.png" coords="13,72.00,188.74,468.00,174.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 .</head><label>3</label><figDesc>The input features describing the position are structured as a 19 × 19 image; i.e. the neural network architecture is matched to the grid-structure of the board.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 .</head><label>4</label><figDesc>The rules of Go are invariant under rotation and reflection; this knowledge has been utilised in AlphaGo Zero both by augmenting the data set during training to include rotations and reflections of each position, and to sample random rotations or reflections of the position during MCTS (see Search Algorithm). Aside from komi, the rules of Go are also invariant to colour transposition; this knowledge is exploited by representing the board from the perspective of the current player (see Neural network architecture) AlphaGo Zero does not use any form of domain knowledge beyond the points listed above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 .</head><label>1</label><figDesc>A convolution of 256 filters of kernel size 3 × 3 with stride 1 2. Batch normalisation 3. A rectifier non-linearity 4. A convolution of 256 filters of kernel size 3 × 3 with stride 1 5. Batch normalisation 6. A skip connection that adds the input to the block 7. A rectifier non-linearity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 .</head><label>1</label><figDesc>A convolution of 2 filters of kernel size 1 × 1 with stride 1 2. Batch normalisation 3. A rectifier non-linearity 4. A fully connected linear layer that outputs a vector of size 19 2 + 1 = 362 corresponding to logit probabilities for all intersections and the pass move The value head applies the following modules: 1. A convolution of 1 filter of kernel size 1 × 1 with stride 1 2. Batch normalisation 3. A rectifier non-linearity 4. A fully connected linear layer to a hidden layer of size 256 5. A rectifier non-linearity 6. A fully connected linear layer to a scalar 7. A tanh non-linearity outputting a scalar in the range [−1, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>a</head><label></label><figDesc>logistic function p(a defeats b) = 1 1+exp (c elo (e(b)−e(a)) , and estimate the ratings e(•) by Bayesian logistic regression, computed by the BayesElo program 25 using the standard constant c elo = 1/400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Extended Data Figure 1 :Extended Data Figure 2 :</head><label>12</label><figDesc>68. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. &amp; de Freitas, N. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE 104, 148-175 (2016). 69. Segal, R. B. On the scalability of parallel UCT. Computers and Games 6515, 36-47 (2011). Frequency of occurence over time during training, for each joseki from Figure 5a (corner sequences common in professional play that were discovered by AlphaGo Zero). The corresponding joseki are reproduced in this figure as insets. Frequency of occurence over time during training, for each joseki from Figure 5b, (corner sequences that AlphaGo Zero favoured for at least one iteration), and one additional variation. The corresponding joseki are reproduced in this figure as insets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>48 Extended Data Figure 3 :</head><label>483</label><figDesc>68 83 at 63 86 at 68 89 at 63 93 at 68 Game 15, B: AG Zero, W: AG Zero, Result: W+R 40 67 at 54 77 at 69 Game 19, B: AG Zero, W: AG Zero, Result: B+R AlphaGo Zero (20 block) self-play games. The 3 day training run was subdivided into</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>55 Extended Data Figure 4 :</head><label>554</label><figDesc>Game 16, B: AG Zero, W: AG Zero, Result: W+R AlphaGo Zero (40 block) self-play games. The 40 day training run was subdivided into 20 periods. The best player from each period (as selected by the evaluator) played a single game against itself, with 2 time controls. 100 moves are shown for each game; full games are provided in Supplementary Information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Extended Data Figure 6 :</head><label>6</label><figDesc>73 87 at 83 90 at 84 93 at 83 97 at 80 AlphaGo Zero (40 block, 40 day) versus AlphaGo Master tournament games using 2 hour time controls. 100 moves of the first 20 games are shown; full games are provided in Supplementary Information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="9,95.40,208.29,421.20,171.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Extended Data Figure 5: Tournament games between AlphaGo Zero (20 block, 3 day) versus AlphaGo Lee using 2 hour time controls. 100 moves of the first 20 games are shown; full games are provided in Supplementary Information.</figDesc><table><row><cell cols="19">Game 1, B: AG Lee, W: AG Zero, Result: W+R Game 1, B: AG Master, W: AG Zero, Result: W+R</cell><cell cols="20">Game 2, B: AG Lee, W: AG Zero, Result: W+R Game 2, B: AG Zero, W: AG Master, Result: B+R</cell><cell cols="14">Game 3, B: AG Lee, W: AG Zero, Result: W+R Game 3, B: AG Master, W: AG Zero, Result: W+R</cell><cell>Game 4, B: AG Lee, W: AG Zero, Result: W+0.50 Game 4, B: AG Zero, W: AG Master, Result: B+R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51 52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>32 12</cell><cell>34 31</cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61</cell><cell>60</cell><cell>32</cell><cell>34 66</cell><cell>36 63</cell><cell>56</cell><cell>69</cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell cols="2">28 29</cell><cell></cell><cell>79</cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>49</cell><cell>45</cell><cell>48</cell><cell>18</cell><cell>20</cell><cell>30</cell><cell>67</cell><cell>69</cell><cell>59</cell></row><row><cell></cell><cell></cell><cell>26 6</cell><cell cols="2">28 29 8 10</cell><cell>31 9</cell><cell>33 32</cell><cell>35</cell><cell></cell><cell></cell><cell>97</cell><cell></cell><cell>30</cell><cell>47 19</cell><cell cols="2">51 52 26</cell><cell>24</cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell cols="3">28 7 8 30 31 64</cell><cell>33 55</cell><cell>35 72</cell><cell cols="2">37 70 71 84</cell><cell>83</cell><cell>49</cell><cell>53</cell><cell></cell><cell>41 6</cell><cell cols="2">45 46</cell><cell cols="2">68</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell cols="2">20 21 3</cell><cell cols="3">31 77 78 98</cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell>55</cell><cell>58 7</cell><cell>6</cell><cell>32 44</cell><cell>14 41</cell><cell>16 17 46 55</cell><cell>19 17</cell><cell>21</cell><cell>31</cell><cell>54</cell><cell>61</cell><cell>66</cell><cell>93 5</cell><cell>71</cell><cell>60</cell></row><row><cell></cell><cell>73</cell><cell>27 7</cell><cell>1 1</cell><cell>11</cell><cell>33</cell><cell cols="3">35 100</cell><cell></cell><cell>67</cell><cell>66</cell><cell>49 28</cell><cell>48 68</cell><cell></cell><cell>2 4</cell><cell>23</cell><cell>56</cell><cell>62</cell><cell></cell><cell></cell><cell>29 9</cell><cell>1 4</cell><cell>51 59</cell><cell>58</cell><cell>67</cell><cell>47</cell><cell></cell><cell>82</cell><cell></cell><cell>59 52</cell><cell>43 54</cell><cell>42</cell><cell cols="4">2 66 67 2</cell><cell>60</cell><cell>69</cell><cell>40</cell><cell></cell><cell>22</cell><cell>1</cell><cell>76</cell><cell>80</cell><cell>66</cell><cell>69</cell><cell>64</cell><cell>73 13</cell><cell></cell><cell>59</cell><cell>53</cell><cell>52</cell><cell>56</cell><cell>2 1</cell><cell>8</cell><cell>29 53</cell><cell>15 40</cell><cell>1 3</cell><cell>59</cell><cell>65</cell><cell>63 64</cell><cell>70</cell><cell>68</cell><cell>72</cell><cell>2 4</cell></row><row><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65</cell><cell>60</cell><cell>63 59</cell><cell>50 63</cell><cell></cell><cell>58</cell><cell>60 25</cell><cell>53</cell><cell></cell><cell></cell><cell>11</cell><cell>10</cell><cell>62</cell><cell>50 57</cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell>44</cell><cell></cell><cell>65</cell><cell cols="2">50</cell><cell>49</cell><cell></cell><cell></cell><cell>32</cell><cell>27</cell><cell>26</cell><cell>82</cell><cell>43</cell><cell>81</cell><cell cols="2">70 71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54</cell><cell>11</cell><cell>50 10</cell><cell>81 42</cell><cell>80 47</cell><cell>48</cell><cell>50 51</cell><cell>73</cell><cell>58</cell><cell>37</cell></row><row><cell></cell><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>98</cell><cell></cell><cell></cell><cell>91</cell><cell cols="3">80 81 61 62 64</cell><cell>22</cell><cell>20</cell><cell>17 17</cell><cell>61</cell><cell></cell><cell></cell><cell>13</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell>72</cell><cell>91</cell><cell>93 51</cell><cell cols="2">27 5</cell><cell></cell><cell></cell><cell>39</cell><cell>24</cell><cell>23</cell><cell>88 5</cell><cell>62 32</cell><cell>42</cell><cell>86</cell><cell>68</cell><cell>65</cell><cell></cell><cell>75</cell><cell></cell><cell>61</cell><cell>51</cell><cell>49 9</cell><cell>97 12</cell><cell>79</cell><cell>58 38 39 75</cell><cell>82</cell><cell>53</cell><cell>6</cell></row><row><cell></cell><cell>88</cell><cell></cell><cell cols="3">73 86 87 84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell>90</cell><cell>94</cell><cell>59</cell><cell>75 29</cell><cell>55 21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell></cell><cell cols="2">63 64</cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell>25 56</cell><cell>89 54</cell><cell></cell><cell>33 26</cell><cell></cell><cell>92</cell><cell>85</cell><cell>72</cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell>57</cell><cell>78</cell><cell>77 43</cell><cell>73 35</cell><cell>76</cell><cell>52</cell><cell>49</cell><cell>62</cell></row><row><cell></cell><cell></cell><cell cols="3">64 70 71 72 85</cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97</cell><cell>96</cell><cell cols="2">94 95 89 92</cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell>74</cell><cell>73</cell><cell>80</cell><cell>71</cell><cell>62</cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell>53</cell><cell>19 16</cell><cell></cell><cell>63 58</cell><cell>90</cell><cell>84 96</cell><cell>87 94</cell><cell>91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>74</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>36</cell><cell></cell><cell cols="2">68 69</cell><cell>71</cell><cell></cell><cell></cell><cell>90</cell><cell>92</cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell>27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell>75</cell><cell>78</cell><cell>89</cell><cell>77</cell><cell>88</cell><cell>77</cell><cell cols="2">74</cell><cell></cell><cell></cell><cell cols="3">100 49 75 78</cell><cell>55</cell><cell cols="2">18 51 52</cell><cell>89</cell><cell cols="2">90 91 92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>54 55</cell><cell>83</cell><cell>76</cell><cell>89</cell></row><row><cell></cell><cell>56</cell><cell></cell><cell>13</cell><cell></cell><cell>70</cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell>89</cell><cell>93 93</cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54</cell><cell>53</cell><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79</cell><cell></cell><cell></cell><cell cols="3">98 100</cell><cell cols="3">100</cell><cell></cell><cell>67</cell><cell>99</cell><cell>98 59</cell><cell>30</cell><cell>72</cell><cell cols="2">87 88</cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell>46</cell><cell>19</cell><cell>56</cell><cell>84 85 96</cell><cell>92</cell><cell>84</cell><cell>79</cell><cell>75</cell><cell>99</cell><cell>80</cell><cell>97 98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>53</cell><cell></cell><cell></cell><cell></cell><cell cols="3">99 100</cell><cell>91</cell><cell></cell><cell cols="3">77 78 79</cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56</cell><cell>11</cell><cell>52</cell><cell>57 45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90 99</cell><cell cols="2">87 92</cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>11 68</cell><cell cols="2">13 14 70 71</cell><cell cols="2">83 84</cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">35 36</cell><cell>86</cell><cell>41</cell><cell>95</cell><cell>91</cell><cell>81</cell><cell>65 85 86</cell><cell>36</cell><cell>92</cell></row><row><cell></cell><cell>57</cell><cell></cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell>98</cell><cell>84</cell><cell>83</cell><cell>76</cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58</cell><cell></cell><cell>55</cell><cell cols="2">43 44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86 98</cell><cell cols="2">23 97</cell><cell>85</cell><cell></cell><cell></cell><cell>65</cell><cell cols="3">12 15 16 69 73</cell><cell>46</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47</cell><cell>45</cell><cell>13</cell><cell>57</cell><cell>35</cell><cell>43</cell><cell>46</cell><cell>62 74</cell><cell>82</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell>29</cell><cell>27</cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">95 96 97 76 84</cell><cell>83</cell><cell></cell><cell></cell><cell>35 64</cell><cell>34 57</cell><cell>37 15</cell><cell>47</cell><cell cols="2">43 44</cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23</cell><cell>45</cell><cell>44</cell><cell>42</cell><cell>33</cell><cell>40</cell><cell>66</cell><cell>63 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66</cell><cell></cell><cell></cell><cell>45</cell><cell>47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell>8</cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell cols="2">22 23</cell><cell cols="2">41 42</cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell>99</cell><cell>8 80</cell><cell cols="2">17</cell><cell></cell><cell></cell><cell></cell><cell>36 66</cell><cell>38 14</cell><cell>33</cell><cell>79</cell><cell>44 42</cell><cell>31</cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29</cell><cell>8 22</cell><cell>93</cell><cell>18</cell><cell>36</cell><cell>24</cell><cell>34</cell><cell>88</cell><cell>39</cell><cell>78</cell><cell>67</cell><cell>8</cell><cell>94</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>14</cell><cell></cell><cell>12</cell><cell>65</cell><cell>39</cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>40</cell><cell></cell><cell></cell><cell>30</cell><cell>10</cell><cell cols="3">24 25 26</cell><cell cols="2">14 15</cell><cell>12</cell><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48 78</cell><cell cols="2">47 39</cell><cell>25 87</cell><cell></cell><cell></cell><cell></cell><cell>10 34</cell><cell></cell><cell></cell><cell>80</cell><cell>48</cell><cell></cell><cell>25</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell>20 21</cell><cell>37</cell><cell>25</cell><cell>89 90 21 22</cell><cell>38</cell><cell>72</cell><cell>68</cell><cell>77</cell></row><row><cell></cell><cell>24</cell><cell>13 3</cell><cell>20 54</cell><cell></cell><cell>19 5</cell><cell>21</cell><cell cols="2">37 38</cell><cell>9</cell><cell></cell><cell>51</cell><cell>15</cell><cell>7 43</cell><cell>44 48</cell><cell>4 2</cell><cell>78</cell><cell>76</cell><cell></cell><cell></cell><cell>24 35</cell><cell>33</cell><cell>18 3</cell><cell>16</cell><cell></cell><cell>20</cell><cell></cell><cell>13 19</cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>79</cell><cell>4 1</cell><cell cols="2">38</cell><cell>81</cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell>2</cell><cell>38</cell><cell>17 40</cell><cell>28</cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell>27</cell><cell></cell><cell>7</cell><cell>4 4</cell><cell>19</cell><cell>94</cell><cell>23 30</cell><cell>22 1</cell><cell>26 23</cell><cell>87 20</cell><cell>28 25</cell><cell>70</cell><cell>9</cell><cell>87</cell><cell>7</cell><cell>93</cell><cell>4 2</cell><cell>96 8</cell><cell>99 56</cell></row><row><cell></cell><cell></cell><cell>22 58</cell><cell>3 55</cell><cell></cell><cell>15 41</cell><cell>18 40</cell><cell>16</cell><cell>82</cell><cell>46</cell><cell>52</cell><cell>49</cell><cell>44</cell><cell cols="2">5 6 14</cell><cell>43</cell><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19 34</cell><cell>3 32</cell><cell>17</cell><cell>22 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell cols="2">5 6 21 37</cell><cell>36</cell><cell cols="2">26 82</cell><cell>86</cell><cell>96</cell><cell cols="2">99 100</cell><cell>63</cell><cell>3 41</cell><cell>37</cell><cell>39</cell><cell>62</cell><cell></cell><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5 6</cell><cell>18</cell><cell>95 17</cell><cell>27 32</cell><cell>3 31</cell><cell>24</cell><cell>61 26 27 91</cell><cell>47</cell><cell>71</cell><cell>11 88</cell><cell>69 16</cell><cell>14</cell><cell>5 6 10 12</cell><cell>9</cell><cell>95 7</cell><cell>57</cell></row><row><cell></cell><cell></cell><cell>23 69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell></cell><cell></cell><cell>50</cell><cell cols="2">38 79 80 39</cell><cell cols="2">41 42</cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell>34</cell><cell>33</cell><cell>28 29</cell><cell>10</cell><cell>94 15</cell><cell>12 13</cell><cell>98 11</cell><cell>97</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell cols="3">93 94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">81 at 53 85 at 78 97 at 88</cell><cell></cell><cell></cell><cell>83 at 58</cell></row><row><cell cols="19">Game 5, B: AG Lee, W: AG Zero, Result: W+R Game 5, B: AG Master, W: AG Zero, Result: W+R</cell><cell cols="20">Game 6, B: AG Lee, W: AG Zero, Result: W+0.50 Game 6, B: AG Zero, W: AG Master, Result: B+R</cell><cell cols="14">Game 7, B: AG Lee, W: AG Zero, Result: W+R Game 7, B: AG Master, W: AG Zero, Result: W+R</cell><cell>Game 8, B: AG Lee, W: AG Zero, Result: W+R Game 8, B: AG Zero, W: AG Master, Result: B+R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66</cell></row><row><cell></cell><cell>67</cell><cell>43 34</cell><cell></cell><cell>36</cell><cell>65</cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell cols="2">32 33</cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell>72</cell><cell>74</cell><cell>76</cell><cell>11</cell><cell>85 13</cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>18</cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61</cell><cell>58</cell><cell>34</cell><cell>11</cell><cell>32 13</cell><cell>35 15</cell><cell>29</cell><cell>55 28</cell><cell>50 51 33 34</cell></row><row><cell></cell><cell>64</cell><cell cols="2">36 37 31 32</cell><cell>39</cell><cell>44 68</cell><cell>43</cell><cell cols="3">24 25 16 46</cell><cell cols="2">30 31</cell><cell>29</cell><cell>14</cell><cell></cell><cell>15 39</cell><cell>14 42</cell><cell></cell><cell></cell><cell>88</cell><cell>71 43</cell><cell>70 7</cell><cell cols="2">10 9 10 73</cell><cell>96 12</cell><cell>14</cell><cell>15 18</cell><cell>11</cell><cell></cell><cell></cell><cell>19</cell><cell></cell><cell>23</cell><cell>27</cell><cell>22</cell><cell cols="2">20</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>14</cell><cell>17</cell><cell>12</cell><cell></cell><cell>59</cell><cell>63</cell><cell></cell><cell>18</cell><cell></cell><cell></cell><cell>57 17</cell><cell>62</cell><cell>40</cell><cell>33 55</cell><cell>26 7</cell><cell>28 29 30 9 10 12</cell><cell>37 14</cell><cell>16</cell><cell>71</cell><cell>57</cell><cell>27</cell><cell>26</cell><cell>12 31 32 56</cell><cell>49</cell></row><row><cell></cell><cell></cell><cell>3 3</cell><cell>34 30</cell><cell>38</cell><cell>13 5</cell><cell>66 74</cell><cell>45</cell><cell>28 47</cell><cell cols="2">26 27 63</cell><cell></cell><cell>15</cell><cell>26</cell><cell></cell><cell>1 2</cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell>86 42</cell><cell>3 8</cell><cell>75 4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">12 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell>1 3</cell><cell cols="2">21</cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>1 2</cell><cell>19</cell><cell>87</cell><cell>19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 4</cell><cell>38 54</cell><cell>27 8</cell><cell>1 4</cell><cell>31</cell><cell>36</cell><cell>25</cell><cell>20</cell><cell>10 23</cell><cell>46 3</cell><cell>3 30</cell><cell>59</cell></row><row><cell></cell><cell></cell><cell>41</cell><cell>35 33</cell><cell>40</cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29</cell><cell></cell><cell>27</cell><cell>40</cell><cell></cell><cell></cell><cell cols="2">17 18</cell><cell></cell><cell cols="2">87 100</cell><cell></cell><cell></cell><cell>31</cell><cell>94 80</cell><cell>83</cell><cell>16</cell><cell>14</cell><cell>95</cell><cell>18</cell><cell></cell><cell>69</cell><cell>53</cell><cell>51</cell><cell>33</cell><cell cols="2">32 27</cell><cell>29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63 45</cell><cell>50</cell><cell>49</cell><cell>74</cell><cell>39</cell><cell>84</cell><cell>41</cell><cell>70</cell><cell>58</cell><cell>21 22</cell><cell>47</cell><cell>52</cell></row><row><cell></cell><cell></cell><cell>35</cell><cell>88</cell><cell></cell><cell></cell><cell cols="3">72 73</cell><cell>62</cell><cell>60</cell><cell>58</cell><cell>48</cell><cell>41</cell><cell cols="2">37 38</cell><cell cols="2">19 20</cell><cell></cell><cell></cell><cell>98</cell><cell>84</cell><cell>17 79</cell><cell>60</cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell>93 85</cell><cell></cell><cell></cell><cell></cell><cell>54</cell><cell>52</cell><cell>50 25</cell><cell cols="2">34 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13 14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51</cell><cell>47 48</cell><cell>52</cell><cell>72 73</cell><cell>24</cell><cell>60</cell><cell>53 54 18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67</cell><cell>64</cell><cell>61</cell><cell>59</cell><cell>57</cell><cell></cell><cell></cell><cell>28</cell><cell cols="2">21 22</cell><cell></cell><cell></cell><cell>99</cell><cell>50</cell><cell>32</cell><cell>59</cell><cell>92</cell><cell>69</cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">41</cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell>55 44</cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97</cell><cell></cell><cell></cell><cell>53</cell><cell>46</cell><cell>56</cell><cell>96</cell><cell>44</cell><cell>61</cell><cell>48</cell></row><row><cell></cell><cell></cell><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell>66</cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell>23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">97 47 48</cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell cols="3">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79</cell><cell></cell><cell></cell><cell>47</cell><cell>46 70</cell><cell cols="2">53 54 69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55</cell><cell>54</cell><cell>85</cell><cell>63</cell><cell>62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">99100 71</cell><cell>69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49</cell><cell>53</cell><cell></cell><cell>58</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell>99</cell><cell></cell><cell></cell><cell>40</cell><cell cols="2">19</cell><cell></cell><cell></cell><cell></cell><cell>45</cell><cell>44</cell><cell>37</cell><cell>52</cell><cell></cell><cell>65</cell><cell>67</cell><cell>91</cell><cell cols="3">88 89 96 82 96</cell><cell>95</cell><cell>61</cell><cell>56</cell><cell>52</cell><cell>67</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>9 13</cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell>9</cell><cell></cell><cell></cell><cell>54</cell><cell>39</cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell></cell><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">35 36</cell><cell></cell><cell></cell><cell>66</cell><cell>90</cell><cell cols="3">92 93 94 81</cell><cell>80</cell><cell cols="2">93 94</cell><cell>62</cell><cell>9</cell><cell>65 19</cell><cell>68</cell></row><row><cell></cell><cell></cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell cols="2">64 65</cell><cell>35</cell><cell></cell><cell>46</cell><cell>51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">33 34</cell><cell>26</cell><cell>64 43</cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell>79</cell><cell>78</cell><cell>68</cell><cell>79</cell><cell>81</cell><cell>93</cell><cell>95</cell><cell>69</cell></row><row><cell></cell><cell>10</cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell cols="2">94 95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell>95</cell><cell></cell><cell></cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell>73</cell><cell>26</cell><cell cols="2">23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22 16</cell><cell cols="2">51 22 23</cell><cell>71 27</cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell>71</cell><cell>83</cell><cell>90</cell><cell>21</cell><cell>83</cell><cell>76</cell><cell>45</cell><cell>80</cell><cell>87</cell><cell>92</cell><cell>94</cell><cell>97</cell></row><row><cell></cell><cell>96</cell><cell cols="2">92 93</cell><cell></cell><cell></cell><cell>97</cell><cell>98</cell><cell></cell><cell>93</cell><cell>88</cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell>34</cell><cell>94</cell><cell>33</cell><cell></cell><cell>38</cell><cell></cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>48</cell><cell></cell><cell></cell><cell>77</cell><cell></cell><cell></cell><cell>65</cell><cell cols="3">68 69 20 21 67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell>57 58</cell><cell>77 78</cell><cell>89</cell><cell>45</cell><cell>17</cell></row><row><cell></cell><cell>12</cell><cell>5 82</cell><cell>7 91</cell><cell></cell><cell></cell><cell cols="4">89 99 100 97</cell><cell>87</cell><cell>54</cell><cell>56</cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell>57</cell><cell></cell><cell>43</cell><cell>36</cell><cell>5 6</cell><cell>7</cell><cell></cell><cell></cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82 75</cell><cell></cell><cell></cell><cell></cell><cell>22</cell><cell cols="2">20</cell><cell>68</cell><cell></cell><cell></cell><cell>64</cell><cell>41</cell><cell>5</cell><cell>70 29</cell><cell></cell><cell cols="3">85 100 89</cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>9</cell><cell>12</cell><cell>82</cell><cell>14 6</cell><cell>98</cell><cell>86</cell><cell>88 56</cell><cell>7</cell><cell>5</cell><cell>16 49</cell><cell>23</cell></row><row><cell></cell><cell>91 83</cell><cell>6 85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell>55</cell><cell>51</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell>56</cell><cell>55</cell><cell></cell><cell></cell><cell>39</cell><cell>6</cell><cell>42</cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell>84 63</cell><cell>76</cell><cell></cell><cell>68</cell><cell>71</cell><cell>47</cell><cell></cell><cell>66</cell><cell cols="2">58 21</cell><cell>63</cell><cell>78</cell><cell>66</cell><cell>84 39</cell><cell>10 38</cell><cell cols="3">72 34 35 86 28</cell><cell>83</cell><cell></cell><cell>88</cell><cell>85</cell><cell>99</cell><cell></cell><cell></cell><cell>11</cell><cell>10</cell><cell>26</cell><cell>57</cell><cell>100</cell><cell>22</cell><cell>6 43</cell><cell>19 42</cell></row><row><cell></cell><cell>94 78</cell><cell>7</cell><cell>4 1</cell><cell>11</cell><cell>8 80</cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52</cell><cell>49</cell><cell>47 22</cell><cell>20</cell><cell>2 4</cell><cell>18</cell><cell>58</cell><cell></cell><cell></cell><cell cols="2">40 41</cell><cell>4 2</cell><cell></cell><cell>8 29</cell><cell></cell><cell>36</cell><cell></cell><cell>90 30</cell><cell>49</cell><cell>83 62</cell><cell>67</cell><cell></cell><cell>65</cell><cell>2 1</cell><cell cols="2">55</cell><cell>59</cell><cell></cell><cell></cell><cell>49 40</cell><cell>41 37</cell><cell cols="2">43 24 25</cell><cell>11 31</cell><cell>74</cell><cell>97</cell><cell>74</cell><cell>9 13</cell><cell>84</cell><cell></cell><cell></cell><cell>7</cell><cell>28</cell><cell>4 1</cell><cell>8</cell><cell>2 2</cell><cell>40</cell><cell>90 91 50 53</cell><cell>43</cell><cell>8</cell><cell>4 1</cell><cell>21</cell><cell>20 44</cell></row><row><cell>84</cell><cell>75</cell><cell>89 6</cell><cell>8</cell><cell>80 10</cell><cell>60 9</cell><cell>72 81</cell><cell>50</cell><cell>63</cell><cell>49</cell><cell></cell><cell></cell><cell>53</cell><cell cols="2">45 46 21 23</cell><cell>19</cell><cell>51 17</cell><cell></cell><cell></cell><cell></cell><cell>44 92</cell><cell>35 90</cell><cell></cell><cell>28</cell><cell>5</cell><cell></cell><cell>31</cell><cell></cell><cell></cell><cell>91 72</cell><cell>46 37</cell><cell></cell><cell>45 15</cell><cell>60</cell><cell>57</cell><cell cols="2">56</cell><cell>62</cell><cell></cell><cell></cell><cell>42</cell><cell>38 32</cell><cell>3 3</cell><cell>30</cell><cell cols="2">75 76</cell><cell>99</cell><cell>98 75</cell><cell cols="2">72 73</cell><cell>87</cell><cell></cell><cell>5 6</cell><cell>27 7</cell><cell>23 6</cell><cell>42 36</cell><cell>99 35</cell><cell>38</cell><cell>13 5</cell><cell>51</cell><cell>11</cell><cell>17</cell><cell>47</cell><cell>46</cell><cell>15</cell><cell>24</cell></row><row><cell></cell><cell>76</cell><cell>90</cell><cell>78</cell><cell>73 79</cell><cell>59 12</cell><cell>70 77</cell><cell>69</cell><cell>74</cell><cell>79</cell><cell></cell><cell></cell><cell>53</cell><cell>52</cell><cell>62</cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell cols="2">38 87 88</cell><cell>45</cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67</cell><cell>64</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell cols="2">39 40 33 36</cell><cell>48</cell><cell cols="4">77 78 100 82 77</cell><cell>76</cell><cell>86</cell><cell></cell><cell>25</cell><cell>24</cell><cell>29 30</cell><cell>32 59 60</cell><cell>75 37</cell><cell>25 39</cell><cell>41</cell><cell>18</cell><cell>48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>71</cell><cell cols="2">75 76</cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell cols="2">79 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">42 at 37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="18">Game 9, B: AG Lee, W: AG Zero, Result: W+R</cell><cell></cell><cell cols="19">Game 10, B: AG Lee, W: AG Zero, Result: W+R</cell><cell></cell><cell cols="14">Game 11, B: AG Zero, W: AG Lee, Result: B+R</cell><cell>Game 12, B: AG Zero, W: AG Lee, Result: B+1.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>30</cell><cell cols="2">28 29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell cols="2">89 90</cell><cell></cell><cell></cell><cell></cell><cell>42</cell><cell>41</cell><cell cols="2">44 45</cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell>22</cell><cell></cell><cell></cell><cell>50</cell><cell>49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell>50 51 52 53</cell><cell>27 29 30</cell><cell>64</cell><cell>34 35</cell></row><row><cell></cell><cell></cell><cell cols="2">20 21</cell><cell></cell><cell>31</cell><cell></cell><cell></cell><cell>60</cell><cell>72</cell><cell></cell><cell>62</cell><cell></cell><cell>45</cell><cell>88</cell><cell cols="2">83 84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">24 25</cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell>31</cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21</cell><cell>4</cell><cell></cell><cell></cell><cell>42</cell><cell>41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57</cell><cell>3</cell><cell>21 22 23</cell><cell>25</cell><cell>6</cell><cell>61</cell><cell>63</cell><cell>48</cell><cell>8</cell><cell>7</cell></row><row><cell>34</cell><cell></cell><cell>22</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>85</cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell></cell><cell>2</cell><cell cols="2">36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell>43</cell><cell></cell><cell></cell><cell>59</cell><cell>51</cell><cell></cell><cell>39</cell><cell>36 37</cell><cell>4</cell><cell>19</cell><cell>33</cell><cell>31</cell><cell>26</cell><cell>28</cell><cell>62</cell><cell>68</cell><cell>66</cell><cell>18</cell><cell>2</cell><cell>9</cell></row><row><cell></cell><cell>32</cell><cell>27</cell><cell>26</cell><cell></cell><cell>57</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell cols="2">28 29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell cols="2">35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19</cell><cell></cell><cell>48</cell><cell>47</cell><cell></cell><cell></cell><cell>58</cell><cell></cell><cell></cell><cell></cell><cell>56</cell><cell>55</cell><cell>32</cell><cell>65</cell><cell>17</cell><cell>47</cell><cell>10 11</cell></row><row><cell>33</cell><cell>24</cell><cell cols="3">23 100 58</cell><cell>56</cell><cell>66</cell><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>27</cell><cell>46</cell><cell>49</cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell>32</cell><cell cols="2">23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell></cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell>46</cell><cell>74</cell><cell>20</cell><cell>44</cell><cell>67</cell><cell>12 13</cell></row><row><cell></cell><cell>25</cell><cell></cell><cell></cell><cell>35</cell><cell>65</cell><cell>68</cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43</cell><cell>47</cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell cols="3">76 77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">31 32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45</cell><cell>42</cell><cell>73</cell><cell>55</cell><cell>57</cell><cell>92</cell><cell>59</cell><cell>14 15</cell></row><row><cell></cell><cell></cell><cell>19</cell><cell></cell><cell>59</cell><cell>64</cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">78 79</cell><cell></cell><cell></cell><cell>29</cell><cell cols="2">25 26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell>43</cell><cell>87</cell><cell>98</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell>75</cell><cell>18</cell><cell>69</cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">50 51</cell><cell></cell><cell></cell><cell cols="3">6 27 28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell>86</cell><cell>85</cell><cell>84</cell><cell>58</cell><cell>69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>71</cell><cell>74</cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>74</cell><cell>82</cell><cell>93</cell><cell></cell><cell>40</cell><cell>56</cell><cell>83</cell><cell>79</cell><cell>78</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell>11</cell><cell cols="2">13 14</cell><cell>99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell>62</cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell>84</cell><cell>88</cell><cell>73</cell><cell>72</cell><cell>81</cell><cell>76</cell><cell>94</cell><cell>99</cell><cell>54</cell><cell>88 89</cell><cell>91</cell><cell>71</cell></row><row><cell></cell><cell></cell><cell cols="3">12 15 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>75</cell><cell>86</cell><cell></cell><cell cols="3">88 100 98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell>78</cell><cell>80</cell><cell>83</cell><cell>86</cell><cell>65</cell><cell>75</cell><cell>71</cell><cell></cell><cell></cell><cell>100</cell><cell>90</cell><cell>93</cell><cell>77</cell><cell>75</cell><cell>72</cell><cell>70</cell></row><row><cell></cell><cell>37</cell><cell>36</cell><cell>39</cell><cell>44</cell><cell>43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59</cell><cell>61</cell><cell>64</cell><cell>84</cell><cell></cell><cell>87</cell><cell>99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18</cell><cell>77</cell><cell>79</cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell>94 95</cell><cell>76</cell><cell>82</cell></row><row><cell></cell><cell>38</cell><cell>40</cell><cell></cell><cell>42</cell><cell>41</cell><cell></cell><cell>55</cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell>60</cell><cell>65</cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell>54</cell><cell>66</cell><cell>80</cell><cell>5</cell><cell>97</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">53 54</cell><cell>51</cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell>77</cell><cell>81</cell><cell></cell><cell></cell><cell>95</cell><cell>58</cell><cell>10</cell><cell>72</cell><cell>66</cell><cell>69</cell><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17</cell><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">61 62</cell><cell>60</cell><cell>39</cell><cell>81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17</cell><cell>52</cell><cell>47</cell><cell>50</cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell>4</cell><cell>80</cell><cell>79</cell><cell></cell><cell></cell><cell>96</cell><cell>55</cell><cell></cell><cell></cell><cell>71</cell><cell>70</cell><cell>68</cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>20</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell>8</cell><cell>2</cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell>67</cell><cell></cell><cell></cell><cell>70</cell><cell>52</cell><cell>5</cell><cell>97</cell><cell>1</cell><cell>36</cell><cell>49</cell><cell>38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell cols="2">48 49</cell><cell></cell><cell></cell><cell></cell><cell cols="2">5 6</cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell>56</cell><cell>12</cell><cell>3</cell><cell>53</cell><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5 6</cell><cell>19</cell><cell cols="2">13</cell><cell></cell><cell></cell><cell></cell><cell>35</cell><cell>7</cell><cell cols="2">9 10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>68</cell><cell>64</cell><cell></cell><cell></cell><cell>69</cell><cell>53</cell><cell>96</cell><cell>98 99</cell><cell>3</cell><cell>37</cell><cell>40</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">94 95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell>91</cell><cell>52</cell><cell>54</cell><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>14</cell><cell cols="2">17 18</cell><cell cols="2">22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11</cell><cell>13</cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="18">Game 13, B: AG Zero, W: AG Lee, Result: B+R</cell><cell></cell><cell cols="19">Game 14, B: AG Zero, W: AG Lee, Result: B+R</cell><cell></cell><cell cols="14">Game 15, B: AG Zero, W: AG Lee, Result: B+R</cell><cell>Game 16, B: AG Zero, W: AG Lee, Result: B+R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell>58</cell><cell>39</cell><cell></cell><cell></cell><cell>57</cell><cell>49</cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61</cell><cell></cell><cell>57</cell><cell>56</cell><cell>49</cell><cell>36</cell><cell>61</cell><cell>57 58</cell><cell>62 63</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell>4</cell><cell cols="2">22 23</cell><cell>26</cell><cell></cell><cell></cell><cell cols="2">7 8</cell><cell></cell><cell></cell><cell>40</cell><cell>37</cell><cell cols="2">50 51</cell><cell></cell><cell>45</cell><cell>47</cell><cell>18</cell><cell>53</cell><cell>55</cell><cell cols="3">20 21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59</cell><cell>58</cell><cell>9</cell><cell>46</cell><cell>42 43</cell><cell>38</cell><cell>35</cell><cell>34</cell><cell>4</cell><cell>32</cell><cell>67</cell><cell>49</cell><cell>45</cell><cell>39</cell><cell>56</cell><cell>59</cell><cell>3</cell><cell>65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19</cell><cell>21</cell><cell>36</cell><cell>35</cell><cell></cell><cell></cell><cell>9</cell><cell>4</cell><cell></cell><cell></cell><cell>42</cell><cell>60</cell><cell>52</cell><cell>28</cell><cell></cell><cell>46</cell><cell>48</cell><cell>54</cell><cell></cell><cell>2</cell><cell cols="2">19</cell><cell>23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>47</cell><cell>1</cell><cell>40</cell><cell>39</cell><cell>33</cell><cell>31</cell><cell>66</cell><cell>53</cell><cell>52</cell><cell>48</cell><cell>44</cell><cell>60</cell><cell>20 21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell>11</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell cols="2">43 44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">22 24 25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69</cell><cell></cell><cell>44</cell><cell>41</cell><cell>38</cell><cell>37</cell><cell>41</cell><cell>85</cell><cell>69</cell><cell>55</cell><cell>54</cell><cell>23 24 25</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35</cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell cols="2">17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell>68</cell><cell>63</cell><cell>55</cell><cell>53</cell><cell>51</cell><cell>45</cell><cell>48</cell><cell>84</cell><cell>68</cell><cell>22</cell><cell>42</cell><cell>72</cell></row><row><cell></cell><cell></cell><cell>53</cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell>29</cell><cell cols="2">27</cell><cell></cell><cell></cell><cell></cell><cell cols="3">99 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell>62</cell><cell>54</cell><cell>52</cell><cell>50</cell><cell>81</cell><cell>26</cell><cell>71</cell></row><row><cell></cell><cell></cell><cell cols="2">51 52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">16 100</cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73 74</cell><cell>43</cell></row><row><cell></cell><cell>65</cell><cell>42</cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell></cell><cell>73</cell><cell>97 98</cell><cell>6</cell><cell>82</cell><cell>75 76</cell></row><row><cell></cell><cell>67</cell><cell cols="3">61 63 64</cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">29 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">89 90</cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84</cell><cell></cell><cell></cell><cell>71</cell><cell>82</cell><cell>78</cell><cell>51</cell><cell>47</cell></row><row><cell></cell><cell></cell><cell cols="2">57 58</cell><cell>62</cell><cell>85</cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell></cell><cell></cell><cell></cell><cell cols="2">31 32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell>83</cell><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell>83</cell><cell>70</cell><cell></cell><cell>67</cell><cell>81</cell><cell>80</cell><cell>88</cell><cell>77</cell><cell>50</cell></row><row><cell></cell><cell>66</cell><cell>60</cell><cell>59</cell><cell>79</cell><cell>77</cell><cell>84</cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">33 34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62</cell><cell></cell><cell>85</cell><cell>84</cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72</cell><cell cols="2">70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell>77</cell><cell>75</cell><cell>65</cell><cell>87</cell><cell>72</cell><cell>28</cell><cell>19</cell><cell>70</cell><cell>40</cell><cell>79</cell><cell>83</cell></row><row><cell></cell><cell></cell><cell>56</cell><cell></cell><cell>78</cell><cell>80</cell><cell>83</cell><cell>91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell></cell><cell>73</cell><cell>69</cell><cell cols="2">68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell>66</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">68 69</cell><cell>90</cell><cell>92</cell><cell></cell><cell>99</cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell>43</cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell>5</cell><cell></cell><cell>71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79</cell><cell>8</cell><cell>18</cell><cell>46</cell></row><row><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell>98</cell><cell>97</cell><cell></cell><cell></cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>26</cell><cell>94</cell><cell>17</cell><cell>27</cell><cell>99</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>70</cell><cell>5</cell><cell></cell><cell>73</cell><cell></cell><cell cols="3">100</cell><cell></cell><cell></cell><cell>45</cell><cell>2</cell><cell>8</cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66</cell><cell>65</cell><cell>63</cell><cell>67</cell><cell>80</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>35</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell>76</cell><cell>20</cell><cell>4</cell><cell>23</cell><cell>25</cell><cell>8</cell><cell>2</cell><cell>30</cell><cell>29</cell><cell>5</cell><cell>95</cell><cell>93</cell><cell>1</cell><cell>98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">74 75</cell><cell>44</cell><cell>71</cell><cell>47</cell><cell></cell><cell>16</cell><cell>14</cell><cell>12</cell><cell>10</cell><cell>9</cell><cell>7</cell><cell>49</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>77</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>64</cell><cell>32</cell><cell>81</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell>34</cell><cell>11</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell></cell><cell></cell><cell>92</cell><cell>7</cell><cell>18</cell><cell>15 16</cell><cell>24</cell><cell>95</cell><cell>86</cell><cell>7</cell><cell>9 10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>97</cell><cell>94</cell><cell>92</cell><cell>96 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">81 82</cell><cell></cell><cell></cell><cell>76</cell><cell>72</cell><cell>50</cell><cell></cell><cell>48</cell><cell>15</cell><cell>13</cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">98 99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell>33</cell><cell>32</cell><cell cols="2">29 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell>21</cell><cell>19</cell><cell>17</cell><cell>27</cell><cell>90</cell><cell>87 88 89</cell><cell>11</cell><cell>13</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell></row><row><cell cols="3">39 at 23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="18">Game 17, B: AG Zero, W: AG Lee, Result: B+R</cell><cell></cell><cell cols="19">Game 18, B: AG Zero, W: AG Lee, Result: B+R</cell><cell></cell><cell cols="14">Game 19, B: AG Zero, W: AG Lee, Result: B+1.50</cell><cell>Game 20, B: AG Zero, W: AG Lee, Result: B+R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47</cell><cell>45</cell><cell></cell><cell></cell><cell>43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell cols="4">98 99 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27</cell></row><row><cell></cell><cell>69</cell><cell>62</cell><cell>65</cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell>37</cell><cell>41</cell><cell>32</cell><cell>40</cell><cell>44</cell><cell cols="2">18 19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell cols="2">29</cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42</cell><cell></cell><cell>24</cell><cell>21</cell><cell>20</cell><cell>25 26</cell></row><row><cell></cell><cell>63</cell><cell cols="2">59 60</cell><cell>66</cell><cell>68</cell><cell></cell><cell></cell><cell>6</cell><cell>38</cell><cell>36</cell><cell>31</cell><cell>35</cell><cell>29</cell><cell></cell><cell>8</cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>12</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell>30</cell><cell cols="3">27 28</cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7 8</cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44</cell><cell>16</cell><cell>18 19</cell><cell>13 14</cell><cell>62</cell><cell>28</cell><cell>60</cell><cell>78</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>61</cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell>30</cell><cell>27</cell><cell></cell><cell></cell><cell>2</cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>11</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">4 32 33</cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell>2</cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell></cell><cell>43</cell><cell>4</cell><cell>17</cell><cell>21</cell><cell>15</cell><cell>4</cell><cell>59</cell><cell>71 72</cell><cell>58</cell><cell>6 7</cell></row><row><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell>24</cell><cell>23</cell><cell>17</cell><cell>39</cell><cell cols="2">10 11</cell><cell></cell><cell></cell><cell>17</cell><cell>14</cell><cell>16</cell><cell></cell><cell></cell><cell cols="2">42 43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11</cell><cell>10</cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 22 23</cell><cell>74</cell><cell>17 18</cell><cell>61</cell><cell>69</cell><cell>9 10 11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell>20</cell><cell>25</cell><cell>42</cell><cell cols="2">12 13</cell><cell></cell><cell></cell><cell>21</cell><cell>9</cell><cell>19</cell><cell></cell><cell></cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26</cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>12</cell><cell></cell><cell>97</cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell>15</cell><cell>27</cell><cell>76</cell><cell>19</cell><cell>16</cell><cell>23</cell><cell>29</cell><cell>68</cell><cell>70</cell><cell>80</cell><cell>79</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell></cell><cell cols="2">14 15</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell>75</cell><cell>22</cell><cell>24</cell><cell>32</cell><cell>63</cell><cell>66</cell><cell>12</cell><cell>81</cell><cell>83</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell></cell><cell>91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>73</cell><cell>65</cell><cell>30</cell><cell>82</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell></cell><cell>75</cell><cell>93</cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell>23</cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29</cell><cell>31</cell><cell>67</cell><cell>64</cell><cell>88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell>82</cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47</cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell>53</cell><cell>50 51 52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell></cell><cell>85</cell><cell>84</cell><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69</cell><cell>22</cell><cell>46</cell><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell>28</cell><cell>39</cell><cell>36 37</cell><cell>45 46</cell><cell>87</cell><cell>89 90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>88</cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35</cell><cell>41</cell><cell>35</cell><cell>34</cell><cell>38</cell><cell>85</cell></row><row><cell></cell><cell></cell><cell>74</cell><cell>58</cell><cell></cell><cell>99</cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>59</cell><cell></cell><cell>71</cell><cell>61</cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">88 100</cell><cell></cell><cell></cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47</cell><cell>42</cell><cell>40</cell><cell>44</cell><cell>91</cell></row><row><cell></cell><cell>81</cell><cell>57</cell><cell></cell><cell></cell><cell cols="3">55 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell>78</cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell cols="3">86 87</cell><cell></cell><cell></cell><cell>74</cell><cell>5</cell><cell>49</cell><cell>53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73</cell><cell>71</cell><cell>30</cell><cell>43</cell><cell>33</cell><cell>48</cell><cell>99</cell><cell>93</cell><cell>92</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>52</cell><cell>56</cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">70 71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51</cell><cell>50</cell><cell>58</cell><cell>66</cell><cell>68</cell><cell></cell><cell cols="2">89 90</cell><cell>92</cell><cell>95</cell><cell></cell><cell></cell><cell>99</cell><cell cols="3">84 85</cell><cell></cell><cell></cell><cell>78</cell><cell>48</cell><cell cols="3">50 51 52</cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell>63</cell><cell>34</cell><cell>65</cell><cell>67</cell><cell>69</cell><cell>49</cell><cell>54</cell><cell>86 100</cell><cell>94 95</cell><cell>97</cell><cell>84</cell></row><row><cell></cell><cell></cell><cell></cell><cell>48</cell><cell>51</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell>91</cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56</cell><cell>52</cell><cell cols="2">48 49</cell><cell>54</cell><cell>65</cell><cell>62</cell><cell></cell><cell>94</cell><cell>91</cell><cell cols="2">96 97</cell><cell>6</cell><cell></cell><cell cols="3">80 81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell cols="2">46 47</cell><cell></cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell>62</cell><cell>61</cell><cell>59</cell><cell>64</cell><cell>66</cell><cell>68</cell><cell>2</cell><cell>96</cell><cell>98</cell><cell>5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell>49</cell><cell>53</cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96</cell><cell>95</cell><cell>72</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53</cell><cell>1</cell><cell>55</cell><cell>57</cell><cell>67</cell><cell cols="2">63 64</cell><cell>8</cell><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell>1</cell><cell></cell><cell>54</cell><cell>45</cell><cell></cell><cell>89</cell><cell>6</cell><cell></cell><cell>90</cell><cell></cell><cell>60</cell><cell>32</cell><cell>57</cell><cell>3</cell><cell>72</cell><cell>55 56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84</cell><cell>81</cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58</cell><cell>55 56</cell><cell>75 76</cell><cell>57</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79 80</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_0">periods. The best player from each period (as selected by the evaluator) played a single game against itself, with 2 hour time controls. 100 moves are shown for each game; full games are provided in Supplementary Information.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank A. Cain for work on the visuals; A. Barreto, G. Ostrovski, T. Ewalds, T. Schaul, J. Oh and N. Heess for reviewing the paper; and the rest of the DeepMind team for their support.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>The datasets used for validation and testing are the GoKifu dataset (available from http://gokifu.com/ ) and the KGS dataset (available from https://u-go.net/gamerecords/ ).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>Supplementary Information is available in the online version of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Information</head><p>Reprints and permissions information is available at www.nature.com/reprints. The authors declare no competing financial interests. Readers are welcome to comment on the online version of the paper. Correspondence and requests for materials should be addressed to D.S. (davidsilver@google.com). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KGS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
				<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Building expert systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hayes-Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3338" to="3346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to act by predicting the future</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computational intelligence in mind games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mandziuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Challenges for Computational Intelligence</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="407" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in Monte-Carlo tree search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of Monte-Carlo tree search methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions of Computational Intelligence and AI in Games</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Arbib, M.</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="276" to="278" />
		</imprint>
	</monogr>
	<note>The Handbook of Brain Theory and Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Markov Processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: an Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximate policy iteration: a survey and some new methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Control Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="310" to="335" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximate policy iteration schemes: A comparison</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1314" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-armed bandits with episode context</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Whole-history rating: A Bayesian rating system for players of time-varying strength</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The world of Independent learners is not Markovian</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Fort-Piat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Knowledge-Based and Intelligent Engineering Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stabilising experience replay for deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning from self-play in imperfect-information games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Reinforcement Learning Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Move evaluation in Go using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training deep convolutional neural networks to play Go</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1766" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Better computer Go player with neural network and long-term prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual networks for computer Go</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">AlphaGo Master online series of games</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://deepmind.com/research/alphago/match-archive/master.References" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monte Carlo matrix inversion and reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="687" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reinforcement learning with replacing eligibility traces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="123" to="158" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reinforcement learning as classification: Leveraging modern classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="424" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximate modified policy iteration and its application to the game of Tetris</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1629" to="1676" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The integration of a priori knowledge into a Go playing neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
		<ptr target="http://www.cgl.ucsf.edu/go/Programs/neurogo-html/neurogo.html" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluation in Go by a neural network using soft segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Games Conference</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to predict by the method of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal difference learning of position evaluation in the game of Go</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="817" to="824" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal-difference search in computer Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="183" to="219" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Simulation-Based Search in Computer Go</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Edmonton, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monte-Carlo tree search and rapid action value estimation in computer Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="1856" to="1875" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Computing Elo ratings of move patterns in the game of Go</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Computer Games Association Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modification of UCT with patterns in Monte-Carlo Go</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<biblScope unit="volume">6062</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to play chess using temporal differences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tridgell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="243" to="263" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bootstrapping from game tree search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Uther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1937" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Giraffe: Using Deep Reinforcement Learning to Play Chess</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Imperial College London</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal difference learning applied to a highperformance game-playing program</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hlynka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jussila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">TD-gammon, a self-teaching backgammon program, achieves master-level play</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="215" to="219" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From simple features to sophisticated evaluation functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="126" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">World-championship-caliber Scrabble</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="241" to="275" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deepstack: Expert-level artificial intelligence in heads-up no-limit poker</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moravčík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On-line policy improvement using Monte-Carlo search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galperin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1068" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neurogammon: a neural-network backgammon program</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Some studies in machine learning using the game of checkers II -recent progress</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="601" to="617" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reinforcement learning in robotics: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1238" to="1274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to job-shop scheduling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1114" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Using a Monte-Carlo approach for bus regulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cazenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Balbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International IEEE Conference on Intelligent Transportation Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deepmind AI reduces Google data centre cooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Empirical comparison of various reinforcement learning strategies for sequential targeted marketing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Concurrent reinforcement learning from customer interactions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Newnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcfall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Tromp</surname></persName>
		</author>
		<ptr target="http://tromp.github.io/go.html" />
		<title level="m">Tromp-Taylor rules</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Thousands of steps Reinforcement learning Supervised learning 0-200 5 &gt;800 10 −4 -Extended Data Table 3: Learning rate schedule. Learning rate used during reinforcement learning and supervised learning experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Computer</surname></persName>
		</author>
		<author>
			<persName><surname>Go</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="145" to="179" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>measured in thousands of steps (mini-batch updates</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
