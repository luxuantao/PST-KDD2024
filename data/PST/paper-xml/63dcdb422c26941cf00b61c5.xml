<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving stochastic weak Minty variational inequalities without increasing batch size</title>
				<funder ref="#_aCUbtkD">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_Uhkj4Tg #_6uGPXgJ #_ZMYeFzM #_AqnKh2m">
					<orgName type="full">Research Foundation Flanders (FWO)</orgName>
				</funder>
				<funder ref="#_Tu8qrya">
					<orgName type="full">Swiss National Science Foundation</orgName>
					<orgName type="abbreviated">SNSF</orgName>
				</funder>
				<funder ref="#_jma8ahw">
					<orgName type="full">Agence National de la Recherche</orgName>
				</funder>
				<funder ref="#_e4XFTvU">
					<orgName type="full">Research Council KU Leuven</orgName>
				</funder>
				<funder ref="#_Um4uSZg">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_xHbqJKc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-17">17 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Thomas</forename><surname>Pethick</surname></persName>
							<email>thomas.pethick@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Information and Inference Systems (LIONS)</orgName>
								<address>
									<region>EPFL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institut Polytechnique de Paris</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering (ESAT-STADIUS)</orgName>
								<orgName type="laboratory">Laboratoire Traitement et Communication d&apos;Information</orgName>
								<address>
									<settlement>T?l?com Paris</settlement>
									<country>KU Leuven</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Puya</forename><surname>Latafat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Panagiotis</forename><surname>Patrinos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Information and Inference Systems (LIONS)</orgName>
								<address>
									<region>EPFL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Solving stochastic weak Minty variational inequalities without increasing batch size</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-17">17 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.09029v1[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a family of stochastic extragradient-type algorithms for a class of nonconvex-nonconcave problems characterized by the weak Minty variational inequality (MVI). Unlike existing results on extragradient methods in the monotone setting, employing diminishing stepsizes is no longer possible in the weak MVI setting. This has led to approaches such as increasing batch sizes per iteration which can however be prohibitively expensive. In contrast, our proposed methods involves two stepsizes and only requires one additional oracle evaluation per iteration. We show that it is possible to keep one fixed stepsize while it is only the second stepsize that is taken to be diminishing, making it interesting even in the monotone setting. Almost sure convergence is established and we provide a unified analysis for this family of schemes which contains a nonlinear generalization of the celebrated primal dual hybrid gradient algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stochastic first-order methods have been at the core of the current success in deep learning applications. These methods are mostly well-understood for minimization problems at this point. This is even the case in the nonconvex setting where there exists matching upper and lower bounds on the complexity for finding an approximately stable point <ref type="bibr" target="#b1">(Arjevani et al., 2019)</ref>.</p><p>The picture becomes less clear when moving beyond minimization into nonconvex-nonconcave minimax problems-or more generally nonmonotone variational inequalities. Even in the deterministic case, finding a stationary point is in general intractable <ref type="bibr" target="#b12">(Daskalakis et al., 2021;</ref><ref type="bibr" target="#b17">Hirsch &amp; Vavasis, 1987)</ref>. This is in stark contrast with minimization where only global optimality is NP-hard.</p><p>An interesting nonmonotone class for which we do have efficient algorithms is characterized by the so called weak Minty variational inequality (MVI) <ref type="bibr" target="#b13">(Diakonikolas et al., 2021)</ref>. This problem class captures nontrivial structures such as attracting limit cycles and is governed by a parameter ? whose negativity increases the degree of nonmonotonicity. It turns out that the stepsize ? for the exploration step in extragradient-type schemes lower bounds the problem class through ? &gt; -? /2 <ref type="bibr" target="#b29">(Pethick et al., 2022)</ref>. In other words, it seems that we need to take ? large to guarantee convergence for a large class. This reliance on a large stepsize is at the core of why the community has struggled to provide a stochastic variants for weak MVIs. The only known results effectively increase the batch size at every iteration <ref type="bibr">(Diakonikolas et al., 2021, Thm. 4</ref>.5)-a strategy that would be prohibitively expensive in most machine learning applications. <ref type="bibr" target="#b29">Pethick et al. (2022)</ref> proposed (SEG+) which attempts to tackle the noise by only diminishing the second stepsize. This suffices in the special case of unconstrained quadratic games but can fail even in the monotone case as illustrated in Figure <ref type="figure" target="#fig_6">1</ref>. This naturally raises the following research question:</p><p>Can stochastic weak Minty variational inequalities be solved without increasing the batch size?</p><p>We resolve this open problem in the affirmative when the stochastic oracles are Lipschitz in mean, with a modification of stochastic extragradient called bias-corrected stochastic extragradient (BC-SEG+). The scheme only requires one additional first order oracle call, while crucially maintaining the fixed stepsize. Specifically, we make the following contributions:</p><p>(i) We show that it is possible to converge for weak MVI without increasing the batch size, by introducing a bias-correction term. The scheme introduces no additional hyperparameters and recovers the maximal range ? ? (-? /2, ?) of explicit deterministic schemes. The rate we establish is interesting already in the star-monotone case where only asymptotic convergence of the norm of the operator was known when refraining from increasing the batch size <ref type="bibr">(Hsieh et al., 2020, Thm. 1)</ref>. Our result additionally carries over to another class of problem treated in Appendix G, which we call negative weak MVIs.</p><p>(ii) We generalize the result to a whole family of schemes that can treat constrained and regularized settings. First and foremost the class includes a generalization of the forward-backwardforward (FBF) algorithm of <ref type="bibr" target="#b32">Tseng (2000)</ref> to stochastic weak MVIs. The class also contains a stochastic nonlinear extension of the celebrated primal dual hybrid gradient (PDHG) algorithm <ref type="bibr" target="#b9">(Chambolle &amp; Pock, 2011)</ref>. Both methods are obtained as instantiations of the same template scheme, thus providing a unified analysis and revealing an interesting requirement on the update under weak MVI when only stochastic feedback is available.</p><p>(iii) We prove almost sure convergence under the classical Robbins-Monro stepsize schedule of the second stepsize. This provides a guarantee on the last iterate, which is especially important in the nonmonotone case, where average guarantees cannot be converted into a single candidate solution. Almost sure convergence is challenging already in the monotone case where even stochastic extragradient may not converge <ref type="bibr">(Hsieh et al., 2020, Fig. 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Weak <ref type="bibr">MVI Diakonikolas et al. (2021)</ref> was the first to observe that an extragradient-like scheme called extragradient+ (EG+) converges globally for weak MVIs with ? ? (-1 /8L F , ?). This results was later tightened to ? ? (-1 /2L F , ?) and extended to constrained and regularized settings in <ref type="bibr" target="#b29">(Pethick et al., 2022)</ref>. A single-call variant has been analysed in <ref type="bibr" target="#b6">B?hm (2022)</ref>. Weak MVI is a star variant of cohypomonotonicity, for which an inexact proximal point method was originally studied in <ref type="bibr" target="#b11">Combettes &amp; Pennanen (2004)</ref>. Later, a tight characterization was carried out by <ref type="bibr" target="#b3">Bauschke et al. (2021)</ref> for the exact case. It was shown that acceleration is achievable for an extragradient-type scheme even for cohypomonotone problems <ref type="bibr" target="#b23">(Lee &amp; Kim, 2021)</ref>. Despite this array of positive results the stochastic case is largely untreated for weak MVIs. The only known result <ref type="bibr">(Diakonikolas et al., 2021, Theorem 4.5)</ref> requires the batch size to be increasing. Similarly, the accelerated method in <ref type="bibr">Lee &amp; Kim (2021, Thm. 6</ref>.1) requires the variance of the stochastic oracle to decrease as O(1/k).</p><p>Stochastic &amp; monotone When more structure is present the story is different since diminishing stepsizes becomes permissible. In the monotone case rates for the gap function was obtained for stochastic Mirror-Prox in <ref type="bibr" target="#b20">Juditsky et al. (2011)</ref> under bounded domain assumption, which was later relaxed for the extragradient method under additional assumptions <ref type="bibr" target="#b27">(Mishchenko et al., 2020)</ref>. The norm of the operator was shown to asymptotically converge for unconstrained MVIs in <ref type="bibr" target="#b19">Hsieh et al. (2020)</ref> with a double stepsize policy. There exists a multitude of extensions for monotone problems: Single-call stochastic methods are covered in detail by <ref type="bibr" target="#b18">Hsieh et al. (2019)</ref>, variance reduction was applied to Halpern-type iterations <ref type="bibr" target="#b8">(Cai et al., 2022)</ref>, cocoercivity was used in <ref type="bibr" target="#b5">Beznosikov et al. (2022)</ref>, and bilinear games studied in <ref type="bibr" target="#b24">Li et al. (2022)</ref>. Beyond monotonicity, a range of structures have been explored such as MVIs <ref type="bibr" target="#b31">(Song et al., 2020)</ref>, pseudomonotonicity <ref type="bibr" target="#b21">(Kannan &amp; Shanbhag, 2019;</ref><ref type="bibr" target="#b7">Bo? et al., 2021)</ref>, two-sided Polyak-?ojasiewicz condition <ref type="bibr" target="#b33">(Yang et al., 2020)</ref>, expected cocoercivity <ref type="bibr" target="#b26">(Loizou et al., 2021)</ref>, sufficiently bilinear <ref type="bibr" target="#b25">(Loizou et al., 2020)</ref>, and strongly star-monotone <ref type="bibr" target="#b16">(Gorbunov et al., 2022)</ref>.</p><p>Variance reduction The assumptions we make about the stochastic oracle in Section 3 are similar to what is found in the variance reduction literature (see for instance <ref type="bibr">Alacaoglu &amp; Malitsky (2021, Assumption 1)</ref> or <ref type="bibr" target="#b1">Arjevani et al. (2019)</ref>). However, our use of the assumption are different in a crucial way. Whereas the variance reduction literature uses the stepsize ? ? 1/L F (see e.g. <ref type="bibr">Alacaoglu &amp; Malitsky (2021, Theorem 2.5</ref>)), we aim at using the much larger ? ? 1/L F . For instance, in the special case of a finite sum problem of size N, the mean square smoothness constant L F from Assumption III can be ? N times larger than L F (see Appendix I for details). This would lead to a prohibitively strict requirement on the degree of allowed nonmonotonicity through the relationship ? &gt; -? /2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias-correction</head><p>The idea of adding a correction term has also been exploited in minimization, specifically in the context of compositional optimization <ref type="bibr" target="#b10">Chen et al. (2021)</ref>. Due to their distinct problem setting it suffices to simply extend stochastic gradient descent (SGD), albeit under additional assumptions such as <ref type="bibr">(Chen et al., 2021, Assumption 3)</ref>. In our setting, however, SGD is not possible even when restricting ourselves to monotone problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem formulation and preliminaries</head><p>We are interested in finding z ? n such that the following inclusion holds, 0 ? T z := Az + Fz.</p><p>(3.1) A wide range of machine learning applications can be cast as an inclusion. Most noticeable, a structured minimax problem can be reduced to (3.1) as shown in Section 8.1. We will rely on common notation and concepts from monotone operators (see Appendix B for precise definitions). Assumption I. In problem (3.1),</p><formula xml:id="formula_0">(i) The operator F : n ? n is L F -Lipschitz with L F ? [0, ?), i.e., Fz -Fz ? L F z -z ?z, z ? n . (3.2) (ii)</formula><p>The operator A : n ? n is a maximally monotone operator.</p><p>(iii) Weak Minty variational inequality (MVI) holds, i.e., there exists a nonempty set S ? zer T such that for all z ? S and some ?</p><formula xml:id="formula_1">? (-1 2L F , ?) v, z -z ? ? v 2 , for all (z, v) ? gph T .</formula><p>(3.3) Remark 1. In the unconstrained and smooth case (A ? 0), Assumption I(iii) reduces to Fz, z-z ? ? Fz 2 for all z ? n . When ? = 0 this condition reduces to the MVI (i.e. star-monotonicity), while negative ? makes the problem increasingly nonmonotone. Interestingly, the inequality is not symmetric and one may instead consider that the assumption holds for -F. Through this observation, Appendix G extends the reach of the extragradient-type algorithms developed for weak MVIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic oracle</head><p>We assume that we cannot compute Fz easily, but instead we have access to the stochastic oracle F(z, ?), which we assume is unbiased with bounded variance. We additionally assume that z ? F(z, ?) is L F Lipschitz continuous in mean and that it can be simultaneously queried under the same randomness. Assumption II. For the operator F(?, ?) : n ? n the following holds.</p><p>(i) Two-point oracle: The stochastic oracle can be queried for any two points z, z ? n , F(z, ?), F(z , ?) where ? ? P.</p><p>(3.4)</p><formula xml:id="formula_2">(ii) Unbiased: E ? F(z, ?) = Fz ?z ? n . (iii) Bounded variance: E ? F(z, ?) -F(z) 2 ? ? 2 F ?z ? n . Assumption III. The operator F(?, ?) : n ? n is Lipschitz continuous in mean with L F ? [0, ?): E ? F(z, ?) -F(z , ?) 2 L 2 F z -z 2 for all z, z ? n .</formula><p>(3.5) Remark 2. Assumptions II(i) and III are also common in the variance reduction literature <ref type="bibr" target="#b14">(Fang et al., 2018;</ref><ref type="bibr" target="#b28">Nguyen et al., 2019;</ref><ref type="bibr" target="#b0">Alacaoglu &amp; Malitsky, 2021)</ref>, but in contrast with variance reduction we will not necessarily need knowledge of L F to specify the algorithm, in which case the problem constant will only affect the complexity. Crucially, this decoupling of the stepsize from L F will allow the proposed scheme to converge for a larger range of ? in Assumption I(iii). Finally, note that Assumption II(i) commonly holds in machine learning applications, where usually the stochasticity is induced by the sampled mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>To arrive at a stochastic scheme for weak MVI we first need to understand the crucial ingredients in the deterministic setting. For simplicity we will initially consider the unconstrained and smooth Algorithm 1 (BC-SEG+) Stochastic algorithm for problem (3.1) when</p><formula xml:id="formula_3">A ? 0 Require z -1 = z-1 = z 0 ? n ? k ? (0, 1), ? ? ( -2? + , 1/L F ) Repeat for k = 0, 1, . . . until convergence 1.1: Sample ? k ? P 1.2: zk = z k -? F(z k , ? k ) + (1 -? k ) zk-1 -z k-1 + ? F(z k-1 , ? k ) 1.3: Sample ?k ? P 1.4: z k+1 = z k -? k ? F(z k , ?k ) Return z k+1</formula><p>setting, i.e. A ? 0 in (3.1). The first component is taking the second stepsize ? smaller as done in extragradient+ (EG+), zk</p><formula xml:id="formula_4">= z k -?Fz k z k+1 = z k -??F zk (EG+)</formula><p>where ? ? (0, 1). Convergence in weak MVI was first shown in <ref type="bibr" target="#b13">Diakonikolas et al. (2021)</ref> and later tightened by <ref type="bibr" target="#b29">Pethick et al. (2022)</ref>, who characterized that smaller ? allows for a larger range of the problem constant ?. Taking ? small is unproblematic for a stochastic scheme where usually the stepsize is taken diminishing regardless.</p><p>However, <ref type="bibr" target="#b29">Pethick et al. (2022)</ref> also showed that the extrapolation stepsize ? plays a critical role for convergence under weak MVI. Specifically, they proved that a larger stepsize ? leads to a looser bound on the problem class through ? &gt; -?/2. While a lower bound has not been established we provide an example in Figure <ref type="figure">3</ref> of Appendix H where small stepsize prevents convergence.</p><p>Unfortunately, picking ? large (e.g. as ? = 1 /L F ) causes significant complications in the stochastic case where both stepsizes are usually taken to be diminishing as in the following scheme, zk</p><formula xml:id="formula_5">= z k -? k ? F(z k , ? k ) with ? k ? P z k+1 = z k -? k ? F(z k , ?k ) with ?k ? P (SEG)</formula><p>where</p><formula xml:id="formula_6">? k = ? k ? 1 /k.</formula><p>Even with a two-timescale variant (when ? k &gt; ? k ) it has only been possible to show convergence for MVI (i.e. when ? = 0) <ref type="bibr" target="#b19">(Hsieh et al., 2020)</ref>. Instead of decreasing both stepsizes, <ref type="bibr" target="#b29">Pethick et al. (2022)</ref> proposes a scheme that keeps the first stepsize constant, zk = z k? F(z k , ? k ) with ? k ? P</p><formula xml:id="formula_7">z k+1 = z k -? k ? F(z k , ?k ) with ?k ? P (SEG+)</formula><p>However, (SEG+) does not necessarily converge even in the monotone case as we illustrate in Figure <ref type="figure" target="#fig_6">1</ref>. The non-convergence stems from the bias term introduced by the randomness of zk in F(z k , ?k ). Intuitively, the role of zk is to approximate the deterministic exploration step zk := z k -?Fz k . While zk is an unbiased estimate of zk this does not imply that F(z k , ?k ) is an unbiased estimate of F( zk ). Unbiasedness only holds in special cases, such as when F is linear and A ? 0 for which we show convergence of (SEG+) in Section 5 under weak MVI. In the monotone case it suffice to take the exploration stepsize ? diminishing <ref type="bibr">(Hsieh et al., 2020, Thm. 1)</ref>, but this runs counter to the fixed stepsize requirement of weak MVI.</p><p>Instead we propose bias-corrected stochastic extragradient+ (BC-SEG+) in Algorithm 1. BC-SEG+ adds a bias correction term of the previous operator evaluation using the current randomness ? k . This crucially allows us to keep the first stepsize fixed. We further generalize this scheme to constrained and regularized setting with Algorithm 2 by introducing the use of the resolvent, (id + ?A) -1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of SEG+</head><p>In the special case where F is affine and A ? 0 we can show convergence of (SEG+) under weak MVI up to arbitrarily precision even with a large stepsize ?. Theorem 5.1. Suppose that Assumptions I and II hold. Assume Fz := Bz + v and choose ? k ? (0, 1) and ? ? (0, 1/L F ) such that ? ? ?(? k -1)/2. Consider the sequence (z k ) k? generated by (SEG+).</p><p>Then for all z ? S , K k=0</p><formula xml:id="formula_8">? k K j=0 ? j E Fz k 2 ? z 0 -z 2 +? 2 (? 2 L 2 F +1)? 2 F K j=0 ? 2 j ? 2 (1-? 2 L 2 F ) K j=0 ? j .</formula><p>(5.1)</p><p>Figure <ref type="figure" target="#fig_6">1</ref>: Monotone constrained case illustrating the issue for projected variants of (SEG+) (see Appendix H.2 for algorithmic details). The objective is bilinear ?(x, y) = (x -0.9) ? (y -0.9) under box constraints (x, y) ? ? 1. The unique stationary point (x , y ) = (0.9, 0.9) lies in the interior, so even Fz can be driven to zero. Despite the simplicity of the problem both projected variants of (SEG+) only converges to a ?-dependent neighborhood. For weak MVI with ? &lt; 0 this neighborhood cannot be made arbitrarily small since ? cannot be taken arbitrarily small (see Figure <ref type="figure">3</ref> of Appendix H).</p><p>The underlying reason for this positive results is that F(z k , ?k ) is unbiased when F is linear. This no longer holds when either linearity of F is dropped or when the resolvent is introduced for A 0, in which case the scheme only converges to a ?-dependent neighborhood as illustrated in Figure <ref type="figure" target="#fig_6">1</ref>. This is problematic in weak MVI where ? cannot be taken arbitrarily small (see Figure <ref type="figure">3</ref> of Appendix H).</p><p>6 Analysis for unconstrained and smooth case</p><p>For simplicity we first consider the case where A ? 0. To mitigate the bias introduced in F(z k , ?k ) for (SEG+), we propose Algorithm 1 which modifies the exploration step. The algorithm can be seen as a particular instance of the more general scheme treated in Section 7. Theorem 6.1. Suppose that Assumptions I to III hold. Suppose in addition that ? ? ( -2? + , 1 /L F ) and (? k ) k? ? (0, 1) is a diminishing sequence such that</p><formula xml:id="formula_9">2?L F ? ? 0 + 1 + 1+? 2 L 2 F 1-? 2 L 2 F ? 2 L 2 F ? 2 L 2 F ? 0 ? 1 + 2? ? . (6.1)</formula><p>Then, the following estimate holds for all z ? S</p><formula xml:id="formula_10">E[ F(z k ) 2 ] ? (1 + ?? 2 L 2 F ) z 0 -z 2 + C? 2 F ? 2 K j=0 ? 2 j ? K j=0 ? j (6.2) where C = 1 + 2? (? 2 L 2 F + 1) + 2? 0 , ? = 1 2 1+? 2 L 2 F 1-? 2 L 2 F ? 2 L 2 F + 1 ?L F ? ? 0 , ? = ? 2 (1 -? 2 L 2 F )/2 and k is chosen from {0, 1, . . . , K} according to probability P[k = k] = ? k K j=0 ? j .</formula><p>Remark 6.2. As ? 0 ? 0, the requirement (6.1) reduces to ? &gt; -? /2 as in the deterministic setting of <ref type="bibr" target="#b29">Pethick et al. (2022)</ref>. Letting ? k = ? 0/ ? k+r the rate becomes O( 1 / ? k), thus matching the rate for the gap function of stochastic extragradient in the monotone case (see e.g. <ref type="bibr" target="#b20">Juditsky et al. (2011))</ref>.</p><p>The above result provides a rate for a random iterate as pioneered by <ref type="bibr" target="#b15">Ghadimi &amp; Lan (2013)</ref>. Showing last iterate results even asymptotically is more challenging. Already in the monotone case, vanilla (SEG) (where ? k = ? k ) only has convergence guarantees for the average iterate <ref type="bibr" target="#b20">(Juditsky et al., 2011)</ref>. In fact, the scheme can cycle even in simple examples <ref type="bibr">(Hsieh et al., 2020, Fig. 1)</ref>.</p><p>Under the classical (but more restrictive) Robbins-Monro stepsize policy, it is possible to show almost sure convergence for the iterates generates by Algorithm 1. The following theorem demonstrates the result in the particular case of ? k = 1 /k+r. The more general statement is deferred to Appendix D. Theorem 6.3 (almost sure convergence). Suppose that Assumptions I to III hold. Suppose ? ? ( -2? + , 1 /L F ), ? k = 1 k+r for any positive natural number r and</p><formula xml:id="formula_11">(?L F + 1)? k + 2 1+? 2 L 2 F 1-? 2 L 2 F ? 4 L 2 F L 2 F ? k+1 + ?L F (? k+1 + 1)? k+1 ? 1 + 2? ? . (6.3) Algorithm 2 (BC-PSEG+) Stochastic algorithm for problem (3.1) Require z -1 = z 0 ? n , h -1 ? n , ? k ? (0, 1), ? ? ( -2? + , 1/L F ) Repeat for k = 0, 1, . . . until convergence 2.1: Sample ? k ? P 2.2: h k = z k -? F(z k , ? k ) + (1 -? k ) h k-1 -z k-1 -? F(z k-1 , ? k ) 2.3: zk = (id + ?A) -1 h k 2.4: Sample ?k ? P 2.5: z k+1 = z k -? k h k -zk + ? F(z k , ?k ) Return z k+1</formula><p>Then, the sequence (z k ) k? generated by Algorithm 1 converges almost surely to some z ? zer T . Remark 6.4. As ? k ? 0 the condition on ? reduces to ? &gt; -? /2 like in the deterministic case.</p><p>To make the results more accessible, both theorems have made particular choices of the free parameters from the proof, that ensures convergence for a given ? and ?. However, since the parameters capture inherent tradeoffs, the choice above might not always provide the tightest rate. Thus, the more general statements of the theorems have been preserved in the appendix.</p><p>7 Analysis for constrained case</p><p>The result for the unconstrained smooth case can be extended when the resolvent is available. Algorithm 2 provides a direct generalization of the unconstrained Algorithm 1. The construction relies on approximating the deterministic algorithm proposed in <ref type="bibr" target="#b29">Pethick et al. (2022)</ref>, which iteratively projects onto a half-space which is guaranteed to contain the solutions. By defining Hz = z -?Fz, the scheme can concisely be written as, zk = (I + ?A) -1 (Hz k )</p><formula xml:id="formula_12">z k+1 = z k -? k (Hz k -Hz k ), (CEG+)</formula><p>for a particular adaptive choice of ? k ? (0, 1). With a fair amount of hindsight we choose to replace Hz k with the bias-corrected estimate h k (as defined in Step 2.2 in Algorithm 2), such that the estimate is also reused in the second update.</p><p>Theorem 7.1. Suppose that Assumptions I to III hold. Moreover, suppose that ? k ? (0, 1), ? ? ( -2? + , 1 /L F ) and the following holds,</p><formula xml:id="formula_13">? 1- ? ? 0 1+ ? ? 0 -? 0 (1 + 2? 2 L 2 F ?) + 2? ? &gt; 0 (7.1)</formula><p>where ? ?</p><formula xml:id="formula_14">1 ? ? 0 (1-? 2 L 2 F ) + 1- ? ? 0 ? ? 0 .</formula><p>Consider the sequence (z k ) k? generated by Algorithm 2. Then, the following estimate holds for all z ? S</p><formula xml:id="formula_15">E[dist(0, T zk ) 2 ] ? E[ z 0 -z 2 ] + ?E[ h -1 -Hz -1 2 ] + C? 2 ? 2 F K j=0 ? 2 j ? 2 ? K j=0 ? j where C = 1 + 2?(1 + ? 2 L 2 F ) + 2? 0 ? and k is chosen from {0, 1, . . . , K} according to probability P[k = k] = ? k K j=0 ? j .</formula><p>Remark 3. The condition on ? in (7.1) reduces to ? &gt; -? /2 when ? 0 ? 0 as in the deterministic case. As oppose to Theorem 6.3 which tracks Fz k 2 , the convergence measure of Theorem 7.1 reduces to dist(0, T zk ) 2 = F zk 2 when A ? 0. Since Algorithm 1 and Algorithm 2 coincide when A ? 0, Theorem 7.1 also applies to Algorithm 1 in the unconstrained case. Consequently, we obtain rates for both F zk 2 and Fz k 2 in the unconstrained smooth case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Asymmetric &amp; nonlinear preconditioning</head><p>In this section we show that the family of stochastic algorithms which converges under weak MVI can be expanded beyond Algorithm 2. This is achieved by extending (CEG+) through introducing Algorithm 3 Nonlinearly preconditioned primal dual extragradient (NP-PDEG) for solving (8.5)</p><formula xml:id="formula_16">Require z -1 = z 0 = (x 0 , y 0 ) with x 0 , x -1 , x-1 , x-1 ? n , y 0 , y -1 ? r , ? ? [0, ?), ? 1 0, ? 2 0 Repeat for k = 0, 1, . . . until convergence 3.1: ? k ? P 3.2: xk = x k -? 1 ? x ?(z k , ? k ) + (1 -? k ) xk-1 -x k-1 + ? 1 ? x ?(x k-1 , y k-1 , ? k ) 3.3: xk = prox ? -1 1 f xk 3.4: ? k ? P 3.5: ?k = y k + ? 2 ?? y ?( xk , y k , ? k ) + (1 -?)? y ?(z k , ? k ) 3.6: +(1 -? k ) ?k-1 -y k-1 -? 2 ?? y ?( xk-1 , y k-1 , ? k ) + (1 -?)? y ?(z k-1 , ? k ) 3.7: ?k = prox ? -1 2 g ?k 3.8: ?k ? P 3.9: x k+1 = x k + ? k xk -xk -? 1 ? x ?(z k , ?k ) 3.10: y k+1 = y k + ? k ?k -?k + ? 2 ? y ?(z k , ?k ) Return z k+1 = (x k+1 , y k+1 )</formula><p>a nonlinear and asymmetrical preconditioning. Asymmetrical preconditioning has been used in the literature to unify a large range of algorithm in the monotone setting <ref type="bibr" target="#b22">Latafat &amp; Patrinos (2017)</ref>. A subtle but crucial difference, however, is that the preconditioning considered here depends nonlinearly on the current iterate. As it will be shown in Section 8.1 this nontrivial feature is the key for showing convergence for primal-dual algorithms in the nonmonotone setting.</p><p>Consider the following generalization of (CEG+) by introducing a potentially asymmetric nonlinear preconditioning P z k that depends on the current iterate z k .</p><p>find zk such that</p><formula xml:id="formula_17">H z k (z k ) ? P z k (z k ) + A(z k ), (8.1a) update z k+1 = z k + ?? H z k (z k ) -H z k (z k ) . (8.1b) where H u (v) P u (v) -F(v)</formula><p>and ? is some positive definite matrix. The iteration independent and diagonal choice P z k = ? -1 I and ? = ?I correspond to the basic (CEG+). More generally we consider</p><formula xml:id="formula_18">P u (z) ? -1 z + Q u (z) (8.</formula><p>2) where Q u (z) captures the nonlinear and asymmetric part, which ultimately enables alternating updates and relaxing the Lipschitz conditions (see Remark 8.1(ii)). Notice that the iterates above does not always yield well-defined updates and one must inevitably impose additional structures on the preconditioner (we provide sufficient condition in Appendix F.1). Consistently with (8.2), in the stochastic case we define Pu (z, ?) ? -1 z + Qu (z, ?).</p><p>(8.</p><p>3) The proposed stochastic scheme, which introduces a carefully chosen bias-correction term, is summarized as compute</p><formula xml:id="formula_19">h k = Pz k (z k , ? k ) -F(z k , ? k ) + (1 -? k ) h k-1 -Pz k-1 (z k-1 , ? k ) + F(z k-1 , ? k ) (8.4a) -Qz k-1 (z k-1 , ? k-1 ) + Qz k-1 (z k-1 , ? k ) with ? k , ? k ? P find zk such that h k ? Pz k (z k , ? k ) + Az k (8.4b) update z k+1 = z k + ? k ? Pz k (z k , ?k ) -F(z k , ?k ) -h k with ?k ? P (8.4c)</formula><p>Remark 4. The two additional terms in (8.4a) are due to the interesting interplay between weak MVI and stochastic feedback, which forces a change of variables (see Appendix F.4).</p><p>To make a concrete choice of Qu (z, ?) we will consider a minimax problem as a motivating example (see Appendix F.1 for a more general setup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Nonlinearly preconditioned primal dual hybrid gradient</head><p>We consider the problem of minimize</p><formula xml:id="formula_20">x? n maximize y? r f (x) + ?(x, y) -g(y). (8.5) where ?(x, y) := E ? [ ?(x, y, ?)].</formula><p>The first order optimality conditions may be written as the inclusion 0</p><formula xml:id="formula_21">? T z Az + Fz, where A = (? f, ?g), F(z) = (? x ?(z), -? y ?(z)), (8.6)</formula><p>while the algorithm only has access to the stochastic estimates F(z, ?) (? x ?(z, ?), -? y ?(z, ?)).</p><p>Assumption IV. For problem (8.5), let the following hold with a stepsize matrix ? = blkdiag(? 1 , ? 2 ) where ? 1 ? n and ? 2 ? r are symmetric positive definite matrices:</p><p>(i) f , g are proper lsc convex (ii) ? : n+r ? is continuously differentiable and for some symmetric positive definite matrices D xx , D xy , D yx , D yy , the following holds for all z = (x, y), z = (x , y ) ? n+r</p><formula xml:id="formula_22">? x ?(z ) -? x ?(z) 2 ? 1 ? L 2 xx x -x 2 D xx + L 2 xy y -y 2 D xy , ? y ?(z ) -?? y ?(x , y) -(1 -?)? y ?(z) 2 ? 2 ? L 2 yx x -x 2 D yx + L 2 yy y -y 2 D yy . (iii) Stepsize condition: L 2 xx D xx + L 2 yx D yx ? ? -1 1 and L 2 xy D xy + L 2 yy D yy ? ? -1 2 .</formula><p>(iv) Bounded variance:</p><formula xml:id="formula_23">E ? F(z, ?) -F(z , ?) 2 ? ? ? 2 F ?z, z ? n .</formula><p>(v) ?(?, ?) : n+r ? is continuously differentiable and for some symmetric positive definite matrices D xz , D yz , D yx , D yy , the following holds for all z = (x, y), z = (x , y ) ? n+r and v, v</p><formula xml:id="formula_24">? n for ? ? [0, ?): E ? ? x ?(z , ?) -? x ?(z, ?) 2 ? 1 ? L 2 xz z -z 2 D xz if ? 1: E ? ? y ?(z, ?) -? y ?(z , ?) 2 ? 2 ? L 2 yz z -z 2 D yz if ? 0: E ? ? y ?(v , y , ?) -? y ?(v, y, ?) 2 ? 2 ? L 2 yx v -v 2 D yx + L 2 yy y -y 2 D yy . Remark 8.1.</formula><p>In Algorithm 3 the choice of ? ? [0, ?) leads to different algorithmic oracles and underlying assumptions in terms of Lipschitz continuity in Assumptions IV(ii) and IV(v).</p><p>(i) If ? = 0 then the first two steps may be computed in parallel and we recover Algorithm 2.</p><p>Moreover, to ensure Assumption IV(ii) in this case it suffices to assume for</p><formula xml:id="formula_25">L x , L y ? [0, ?), ? x ?(z ) -? x ?(z) ? L x z -z , ? y ?(z ) -? y ?(z) ? L y z -z .</formula><p>(ii) Taking ? = 1 leads to Gauss-Seidel updates and a nonlinear primal dual extragradient algorithm with sufficient Lipschitz continuity assumptions for some</p><formula xml:id="formula_26">L x , L y ? [0, ?), ? x ?(z ) -? x ?(z) ? L x z -z , ? y ?(z ) -? y ?(x , y) ? L y y -y .</formula><p>Algorithm 3 is an application of (8.4) applied for solving (8.6). In order to cast the algorithm as an instance of the template algorithm (8.4), we choose the positive definite stepsize matrix as ? = blkdiag(? 1 , ? 2 ) with ? 1 0, ? 2 0, and the nonlinear part of the preconditioner as Qu (z, ?) 0, -?? y ?( x, y, ?) , and</p><formula xml:id="formula_27">Q u (z) 0, -?? y ?( x, y) (8.7)</formula><p>where u = (x, y) and z = ( x, ?). Recall H u (z)</p><formula xml:id="formula_28">P u (z) -F(z) and define S u (z; z) H u (z) -Q u (z).</formula><p>The convergence in Theorem 8.2 depends on the distance between the initial estimate ? -1 ?-1 with ?-1 = ( x-1 , ?-1 ) and the deterministic S z -1 (z -1 ; z-1 ). See Appendix B for additional notation. Theorem 8.2. Suppose that Assumption I(iii) to II(ii) and IV hold. Moreover, suppose that ? k ? (0, 1), ? ? [0, ?) and the following holds,</p><formula xml:id="formula_29">? 1- ? ? 0 1+ ? ? 0 + 2? ? -? 0 -2? 0 (? 1 + 2? 2 (1 + ?3 ))? &gt; 0 and 1 -4? 2 ? 0 &gt; 0 (8.8)</formula><p>where ? denotes the smallest eigenvalue of ?, ? ? (1</p><formula xml:id="formula_30">+ 4? 2 ? 2 0 )( 1 ? ? 0 (1-L M ) 2 + 1- ? ? 0 ? ? 0 )/(1 -4? 2 ? 0 ) and ?1 L 2 xz ?D xz + 2(1 -?) 2 L 2 yz ?D yz + 2? 2 L 2 yy ? 2 D yy , ?2 2? 2 L 2 yx ? 1 D yx , ?3 L 2 xz ?D xz , L 2 M max L 2 xx D xx ? 1 + L 2 yx D yx ? 1 , L 2 xy D xy ? 2 + L 2 yy D yy ? 2 .</formula><p>Consider the sequence (z k ) k? generated by Algorithm 3. Then, the following holds for all z ? S where C 2(? + ? 0 ( 1</p><formula xml:id="formula_31">E[dist ? (0, T zk ) 2 ] ? E[ z 0 -z 2 ? -1 ] + ?E[ ? -1 ?-1 -S z -1 (z -1 ; z-1 ) 2 ? ] + C? 2 F K j=0 ? 2 j ? K j=0 ? j</formula><formula xml:id="formula_32">? ? 0 (1-L M ) 2 + 1- ? ? 0 ? ? 0 ))(1 + 2? 2 ) + 1 + 2(? 1 + 2? 2 (? + ?3 ))? with ? = (1 -?) 2 + 2? 2 and k is chosen from {0, 1, . . . , K} according to probability P[k = k] = ? k K j=0 ? j .</formula><p>Remark 5. When ? 0 ? 0 the conditions in (8.2) reduces to 1 + 2? ? &gt; 0 as in the deterministic case. Almost sure convergence is provided in Theorem F.6 of the appendix.</p><p>For ? = 0 Algorithm 3 reduces to Algorithm 2. With this choice Theorem 8.2 simplifies, since the constant ?2 = 0, and we recover the convergence result of Theorem 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Experiments</head><p>We compare BC-SEG+ and BC-PSEG+ against (EG+) using stochastic feedback (which we refer to as (SF-EG+)) and (SEG) in both an unconstrained setting and a constrained setting introduced in <ref type="bibr" target="#b29">Pethick et al. (2022)</ref>. See Appendix H.2 for the precise formulation of the projected variants which we denote (SF-PEG+) and (PSEG) respectively. In the unconstrained example we control all problem constant and set ? = -1 /10L F , while the constrained example is a specific minimax problem where ? &gt; -1 /2L F holds within the constrained set for a Lipschitz constant L F restricted to the same constrained set. To simulate a stochastic setting in both examples, we consider additive Gaussian noise, i.e. F(z, ?) = Fz + ? where ? ? N(0, ? 2 I). In the experiments we choose ? = 0.1 and ? k ? 1 /k, which ensures almost sure convergence of BC-(P)SEG+. For a more aggressive stepsize choice ? k ? 1 / ? k see Figure <ref type="figure" target="#fig_11">4</ref>. Further details can be found in Appendix H. The results are shown in Figure <ref type="figure" target="#fig_0">2</ref>. The sequence generated by (SEG) and (PSEG) diverges for the unconstrained problem and cycles in the constrained problem respectively. In comparison (SF-EG+) and (SF-PEG+) gets within a neighborhood of the solutions but fails to converge due to the nondiminishing stepsize, while BC-SEG+ and BC-PSEG+ converges in the examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>This paper shows that nonconvex-nonconcave problems characterize by the weak Minty variational inequality can be solved efficiently even when only stochastic gradients are available. The approach crucially avoids increasing batch sizes by instead introducing a bias-correction term. We show that convergence is possible for the same range of problem constant ? ? (-? /2, ?) as in the deterministic case. Rates are established for a random iterate, which matches those of stochastic extragradient in the monotone case, and the result is complemented with almost sure convergence, thus providing asymptotic convergence for the last iterate. We show that the idea extends to a family of extragradient-type methods which includes a nonlinear extension of the celebrated primal dual hybrid gradient (PDHG) algorithm. For future work it is interesting to see if the rate can be improved by considering accelerated methods such as Halpern iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Preliminaries</head><p>Given a psd matrix V we define the inner product as</p><formula xml:id="formula_33">?, ? V ?, V? and the corresponding norm ? ? ?, ? V . The distance from u ? n to a set U ? n with respect to a positive definite matrix V is defined as dist V (u, U)</formula><p>min u ?U uu V , which we simply denote dist(u, U) when V = I. The norm X refers to spectral norm when X is a matrix.</p><p>We summarize essential definitions from operator theory, but otherwise refer to Bauschke &amp; Combettes (2017); Rockafellar (1970) for further details.  <ref type="formula">2021</ref>)). An operator A : n ? n is said to be ?-monotone for some ? ? , if for all (x, y), (x , y ) ? gph A yy , xx ? ? xx 2 , and it is said to be ?-comonotone if for all (x, y), (x , y ) ? gph A yy , xx ? ? yy 2 . The operator A is said to be maximally (co)monotone if there exists no other (co)monotone operator B for which gph A ? gph B properly.</p><p>If A is 0-monotone we simply say it is monotone. When ? &lt; 0, ?-comonotonicity is also referred to as |?|-cohypomonotonicity. Definition B.2 (Lipschitz continuity and cocoercivity). Let D ? n be a nonempty subset of n . A single-valued operator A : D ? n is said to be L-Lipschitz continuous if for any x, x ? D Ax -Ax ? L xx , and ?-cocoercive if</p><p>xx , Ax -Ax ? ? Ax -Ax 2 . Moreover, A is said to be nonexpansive if it is 1-Lipschitz continuous, and firmly nonexpansive if it is 1-cocoercive.</p><p>A ?-cocoercive operator is also ? -1 -Lipschitz continuity by direct implication of Cauchy-Schwarz. The resolvent operator J A = (id + A) -1 is firmly nonexpansive (with dom J A = n ) if and only if A is (maximally) monotone.</p><p>We will make heavy use of the Fenchel-Young inequality. For all a, b ? n and e &gt; 0 we have,</p><formula xml:id="formula_34">2 a, b ? e a 2 + 1 e b 2 (B.1) a + b 2 ? (1 + e) a 2 + (1 + 1 e ) b 2 (B.2) -a -b 2 ? -1 1+e a 2 + 1 e b 2 (B.3)</formula><p>C Proof for SEG+ Proof of Theorem 5.1. Following <ref type="bibr" target="#b19">(Hsieh et al., 2020)</ref> closely, define the reference state zk := z k -?Fz k to be the exploration step using the deterministic operator and denote the second stepsize as ? k := ? k ?. We will let ? denote the additive noise term, i.e. F(z, ?) := F(z) + ?. Expanding the distance to solution,</p><formula xml:id="formula_35">z k+1 -z 2 = z k -? k F(z k , ?k ) -z 2 = z k -z 2 -2? k F(z k , ?k ), z k -z + ? 2 k F(z k , ?k ) 2 = z k -z 2 -2? k F(z k , ?k ), zk -z -2?? k F(z k , ?k ), F(z k ) + ? 2 k F(z k , ?k ) 2 . (C.1)</formula><p>Recall that the operator is assumed to be linear Fz = Bz + v in which case we have,</p><formula xml:id="formula_36">F(z k , ?k ) = Bz k + v + ?k =B(z k -? F(z k , ? k )) + v + ?k =B(z k -?Bz k -?v -?? k ) + v + ?k =B(z k -?(Bz k + v)) + v -?B? k + ?k =F( zk ) -?B? k + ?k . (C.2)</formula><p>The two latter terms are zero in expectation due to the unbiasedness from Assumption II(ii), which lets us write the terms on the RHS of (C.1) as,</p><formula xml:id="formula_37">-E k F(z k , ?k ), zk -z = -F( zk ), zk -z (C.3) -E k F(z k , ?k ), F(z k ) = -F( zk ), F(z k ) (C.4) E k F(z k , ?k ) 2 = F( zk ) 2 + E k ?B? k 2 + E k ?k 2 .</formula><p>(C.5) We can bound (C.3) directly through the weak MVI in Assumption I(iii) which might still be positive,</p><p>-</p><formula xml:id="formula_38">F( zk ), zk -z ? -? F( zk ) 2 . (C.6)</formula><p>For the latter two terms of (C.5) we have</p><formula xml:id="formula_39">E k ?B? k 2 + E k ?k 2 = ? 2 E k F(? k ) -F(0) 2 + E k ?k 2 ? (? 2 L 2 F + 1)? 2 F , (C.7</formula><p>) where the last inequality follows from Lipschitz in Assumption I(i) and bounded variance in Assumption II(iii).</p><p>Combining everything into (C.1) we are left with</p><formula xml:id="formula_40">E k z k+1 -z 2 ? z k -z 2 + ? 2 k (? 2 L 2 F + 1)? 2 F -2?? k F( zk ), F(z k ) + (? 2 k -2? k ?) F( zk ) 2 (C.8)</formula><p>By assuming the stepsize condition, ? ? (? k -?)/2, we have ? 2 k -2? k ? ? ?? k . This allows us to complete the square,</p><formula xml:id="formula_41">-2?? k F( zk ), F(z k ) + (? 2 k -2? k ?) F( zk ) 2 ? -2?? k F( zk ), F(z k ) + ?? k F( zk ) 2 = ?? k ( F(z k ) -F( zk ) 2 -F(z k ) 2 ) ? ?? k (? 2 L 2 F -1) F(z k ) 2 , (C.9)</formula><p>where the last inequality follows from Lipschitzness of F and the definition of the update rule. Plugging into (C.8) we are left with</p><formula xml:id="formula_42">E k z k+1 -z 2 ? z k -z 2 + ? 2 k (? 2 L 2 F + 1)? 2 F -?? k (1 -? 2 L 2 F ) F(z k ) 2 .</formula><p>(C.10) The result is obtained by total expectation and summing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof for smooth unconstrained case</head><formula xml:id="formula_43">Lemma D.1. Consider the recurrent relation B k+1 = ? k B k + d k such that ? k &gt; 0 for all k ? 0. Then B k+1 = ? k p=0 ? p ? ? ? ? ? ? ? ? B 0 + k =0 d ? p=0 ? p ? ? ? ? ? ? ? ? . Assumption V. ? ? ( -2? + , 1 /L F ) and for positive real valued b, ? ? 2 (1 -? 2 L 2 F (1 + b -1 )) &gt; 0. (D.1)</formula><p>Theorem D.2. Suppose that Assumptions I to III hold. Suppose in addition that Assumption V holds and that (? k ) k? ? (0, 1) is a diminishing sequence such that</p><formula xml:id="formula_44">2?L F ? ? 0 + 1 + (b + 1)? 2 L 2 F ? 2 L 2 F ? 0 ? 1 + 2? ? . (D.2)</formula><p>Consider the sequence (z k ) k? generated by Algorithm 1. Then, the following estimate holds</p><formula xml:id="formula_45">K k=0 ? k K j=0 ? j E[ F(z k ) 2 ] ? z 0 -z 2 + ?? 2 F(z 0 ) 2 + C? 2 F ? 2 K j=0 ? 2 j ? K j=0 ? j , (D.3) where C = 1 + 2? (? 2 L 2 F + 1) + 2? 0 and ? = 1 2 (b + 1)? 2 L 2 F + 1 ?L F ? ? 0 .</formula><p>Proof of Theorem D.2. The proof relies on establishing a (stochastic) descent property on the following potential function</p><formula xml:id="formula_46">U k+1 z k+1 -z 2 + A k+1 u k 2 + B k+1 z k+1 -z k 2 .</formula><p>where u k zkz k + ?F(z k ) measures the difference of the bias-corrected step from the deterministic exploration step, and (A k ) k? , (B k ) k? are positive scalar parameters to be identified. We proceed to consider each term individually.</p><p>Let us begin by quantifying how well zk estimates z k -?F(z k ).</p><formula xml:id="formula_47">u k = zk -z k + ?F(z k ) = ?F(z k ) -? F(z k , ? k ) + (1 -? k )(z k-1 -z k-1 + ? F(z k-1 , ? k )).</formula><p>Therefore,</p><formula xml:id="formula_48">u k 2 = ?F(z k ) -? F(z k , ? k ) + (1 -? k )(? F(z k-1 , ? k ) -?F(z k-1 )) 2 + (1 -? k ) 2 u k-1 2 + 2(1 -? k ) zk-1 -z k-1 + ?F(z k-1 ), ?F(z k ) -? F(z k , ? k ) + (1 -? k )(? F(z k-1 , ? k ) -?F(z k-1 )) .</formula><p>Conditioned on F k , in the inner product the left term is known and the right term has an expectation that equals zero. Therefore, we obtain</p><formula xml:id="formula_49">E[ u k 2 |F k ]=E[ (1-? k ) ?F(z k )-? F(z k ,? k )+? F(z k-1 ,? k )-?F(z k-1 ) +? k ?F(z k )-? F(z k ,? k ) 2 |F k ] +(1-? k ) 2 u k-1 2 ?(1-? k ) 2 u k-1 2 +2(1-? k ) 2 ? 2 E[ F(z k ,? k )-F(z k-1 ,? k ) 2 |F k ] +2? 2 k ? 2 E[ F(z k )-F(z k ,? k ) 2 |F k ] ?(1-? k ) 2 u k-1 2 +2(1-? k ) 2 ? 2 L 2 F z k -z k-1 2 +2? 2 k ? 2 ? 2 F (D.4)</formula><p>where in the first inequality we used Young inequality and the fact that the second moment is larger than the variance, and Assumptions II(iii) and III were used in the second inequality.</p><p>By step 1.4, the equality</p><formula xml:id="formula_50">z k+1 -z 2 = z k -z 2 -2? k ? F(z k , ?k ), z k -z + ? 2 k ? 2 F(z k , ?k ) 2 , (D.5)</formula><p>holds. The inner product in (D.5) can be upper bounded using Young inequalities with positive parameters ? k , k ? 0, and b as follows.</p><formula xml:id="formula_51">E[ -? F(z k , ?k ), z k -z | Fk ] = -? F(z k ), z k -zk -? F(z k ), zk -z = -? 2 F(z k ), F(z k ) + ? F(z k ), zk -z k + ?F(z k ) -? F(z k ), zk -z ? ? 2 1 2 F(z k ) -F(z k ) 2 - 1 2 F(z k ) 2 - 1 2 F(z k ) 2 + ? 2 ? k 2 F(z k ) 2 + 1 2? k zk -z k + ?F(z k ) 2 -?? F(z k ) 2 ? ? 2 L 2 F 1 + b 2 u k 2 + 1 + b -1 2 ? 4 L 2 F F(z k ) 2 - ? 2 2 F(z k ) 2 - ? 2 2 F(z k ) 2 + ? 2 ? k 2 F(z k ) 2 + 1 2? k u k 2 -?? F(z k ) 2 = ? 2 L 2 F 1 + b 2 + 1 2? k u k 2 + ? 2 (? 2 L 2 F (1 + b -1 ) -1) 2 F(z k ) 2 + ? 2 (? k -1) 2 -?? F(z k ) 2 . (D.6) Conditioning (D.6) with E ? | F k = E E ? | Fk | F k , since F k ? Fk , yields 2E[ -? F(z k , ?k ), z k -z | F k ] ? ? 2 L 2 F (1 + b) + 1 ? k E[ u k 2 | F k ] -? F(z k ) 2 + ? 2 (? k -1) -2?? E F(z k ) 2 | F k , (D.7)</formula><p>where ? was defined in (D.1).</p><p>The condition expectation of the third term in (D.5) is bounded through Assumption II(iii) by</p><formula xml:id="formula_52">E F(z k , ?k ) 2 | F k = E E[ F(z k , ?k ) 2 | Fk ] | F k ? F(z k ) 2 + ? 2 F , which in turn implies E z k+1 -z k 2 | F k = ? 2 k ? 2 E F(z k , ?k ) 2 | F k ? ? 2 k ? 2 E F zk 2 | F k + ? 2 k ? 2 ? 2 F (D.8)</formula><p>Combining (D.7), (D.8), and (D.5) yields</p><formula xml:id="formula_53">E[ z k+1 -z 2 + A k+1 u k 2 + B k+1 z k+1 -z k 2 | F k ] ? z k -z 2 + A k+1 + ? k ? 2 L 2 F (1 + b) + 1 ? k E[ u k 2 | F k ] -? k ? F(z k ) 2 + ? k ? 2 (? k -1) -2?? + ? 2 k ? 2 E F(z k ) 2 | F k + ? 2 k ? 2 ? 2 F + B k+1 ? 2 k ? 2 E F zk 2 | F k + B k+1 ? 2 k ? 2 ? 2 F . (D.9)</formula><p>Further using (D.4) and denoting</p><formula xml:id="formula_54">X k 1 ? k ? 2 L 2 F (1 + b) + 1 ? k + A k+1 , X k 2 ? k ? 2 (? k -1) -2?? + ? k ? 2 leads to E[U k+1 | F k ] -U k ? -? k ? F(z k ) 2 + X k 1 (1 -? k ) 2 -A k u k-1 2 + 2X k 1 (1 -? k ) 2 ? 2 L 2 F -B k z k -z k-1 2 + X k 2 + B k+1 ? 2 k ? 2 E F(z k ) 2 | F k + B k+1 ? 2 k + ? 2 k + 2X k 1 ? 2 k ? 2 ? 2 F . (D.10)</formula><p>Having established (D.10), set A k = A, B k = 2A? 2 L 2 F , and ? k = ? to obtain by the law of total expectation that</p><formula xml:id="formula_55">E[U k+1 ] -E[U k ] ? -? k ?E F(z k ) 2 + X k 1 (1 -? k ) 2 -A E u k-1 2 + 2? 2 L 2 F X k 1 (1 -? k ) 2 -A E z k -z k-1 2 + X k 2 + 2A? 4 L 2 F ? 2 k E F(z k ) 2 + 2A? 2 L 2 F + 1 + 2X k 1 ? 2 k ? 2 ? 2 F . (D.11)</formula><p>To get a recursion we require</p><formula xml:id="formula_56">X k 1 (1 -? k ) 2 -A ? 0 and X k 2 + 2A? 4 L 2 F ? 2 k ? 0.</formula><p>(D.12) By developing the first requirement of (D.12) we have,</p><formula xml:id="formula_57">0 ? X k 1 (1 -? k ) 2 -A = ? k (1 -? k ) 2 ? 2 L 2 F (1 + b) + 1 ? + ? k (? k -2)A. (D.13)</formula><p>Equivalently, A needs to satisfy</p><formula xml:id="formula_58">A ? (1 -? k ) 2 2 -? k ? 2 L 2 F (1 + b) + 1 ? . (D.14) for any ? k ? (0, 1). Since (1-? k ) 2 2-? k ? 1 2 given ? k ? (0, 1) it suffice to pick A = 1 2 (b + 1)? 2 L 2 F + 1 ? . (D.15)</formula><p>For the second requirement of (D.12) note that we can equivalently require that the following quantity is negative 1</p><formula xml:id="formula_59">? k ? 2 X k 2 + 2A? 4 L 2 F ? 2 k = ? -1 -2? ? + ? k + 2A? 2 L 2 F ? k ? ? -1 -2? ? + 1 + (b + 1)? 2 L 2 F + 1 ? ? 2 L 2</formula><p>F ? 0 where we have used that ? k ? ? 0 and the choice of A from (D.15). Setting the Young parameter</p><formula xml:id="formula_60">? = ?L F ? ? 0 we obtain that X k 2 + 2A? 4 L 2 F ? 2 k ? 0 owing to (D.</formula><p>2). On the other hand, the last term in (D.11) may be upper bounded by</p><formula xml:id="formula_61">2A? 2 L 2 F + 1 + 2X k 1 = 1 + (b + 1)? 2 L 2 F + 1 ?L F ? ? 0 (? 2 L 2 F + 1) + 2? k ? 1 + (b + 1)? 2 L 2 F + 1 ?L F ? ? 0 (? 2 L 2 F + 1) + 2? 0 = C. Thus, it follows from (D.11) that E[U k+1 ] -E[U k ] ? -? k ?E F(z k ) 2 + C? 2 k ? 2 ? 2 F .</formula><p>Telescoping the above inequality completes the proof.</p><p>Proof of Theorem 6.1. The theorem is obtained as a particular instantiation of Theorem D.2.</p><p>The condition in (D.1) can be rewritten as b &gt;</p><formula xml:id="formula_62">? 2 L 2 F 1-? 2 L 2 F . A reasonable choice is b = 2? 2 L 2 F 1-? 2 L 2 F . Substituting back into ? we obtain ? = ? 2 (1 -? 2 L 2 F (1 + 1-? 2 L 2 F 2? 2 L 2 F )) = ? 2 (1-? 2 L 2 F ) 2 &gt; 0. (D.16)</formula><p>Similarly, the choice of b is substituted into ? and (D.2) of Theorem D.2.</p><p>The rate in (D.2) is further simplified by applying Lipschitz continuity of F from Assumption I(i) to Fz 0 2 = Fz 0 -Fz 2 . The proof is complete by observing that the guarantee on the weighted sum can be converted into an expectation over a sampled iterate in the style of <ref type="bibr" target="#b15">Ghadimi &amp; Lan (2013)</ref>.</p><p>Assumption VI (almost sure convergence). Let d ? [0, 1], b &gt; 0. Suppose that the following holds (i) the diminishing sequence (? k ) k? ? (0, 1) satisfies the classical conditions</p><formula xml:id="formula_63">? k=0 ? k = ?, ? ? k=0 ? 2 k &lt; ?; (ii) letting c k (1 + b)? 2 L 2 F + 1 ?L F ? -d k for all k ? 0 ? k ? =k c l ? l ? p=0 (1 -? p ) 2 &lt; ?, ? ? k=0 ? k+1 ? 2 k ? k p=0 1 (1-? p ) 2 &lt; ?, (D.17) and ?L F ? d k + ? k + 2? 2 L 2 F ? k ? k+1 ? k p=0 1 (1-? p ) 2 ? 1 + 2? ? . (D.18)</formula><p>Although at first look the above assumptions may appear involved, as shown in Theorem D.3 classical stepsize choice of ? 0 k+1 is sufficient to satisfy (D.17), and to ensure almost sure convergence provided that instead (D.20) holds. Note that with this choice as k goes to infinity, ? k 0 and the deterministic range ? + 2? &gt; 0 is obtained. Theorem D.3 (almost sure convergence). Suppose that Assumptions I to III hold. Additionally, suppose the stepsize conditions in Assumptions V and VI. Then, the sequence (z k ) k? generated by Algorithm 1 converges almost surely to some z ? zer T . Moreover, the following estimate holds</p><formula xml:id="formula_64">K k=0 ? k K j=0 ? j E[ F(z k ) 2 ] ? z 0 -z 2 + ? 0 ? 2 F(z 0 ) 2 + C ? K j=0 ? j , (D.19) where C = 2? 2 ? 2 F (? 2 L 2 F + 1)? + ? 1 2 + (b + 1)? 2 L 2 F + 1 ?L F is finite.</formula><p>In particular, if ? k = 1 k+r for any positive natural number r, and d = 1, then Assumption VI(ii) can be replaced by</p><formula xml:id="formula_65">(?L F + 1)? k + 2 (1 + b)? 4 L 2 F L 2 F ? k+1 + ?L F (? k+1 + 1)? k+1 ? 1 + 2? ? . (D.20)</formula><p>Proof of Theorem D.3 (almost sure convergence). Having established (D.10), let</p><formula xml:id="formula_66">B k = 2A k ? 2 L 2 F such that 2X k 1 (1 -? k ) 2 ? 2 L 2 F -B k z k -z k-1 2 = 2? 2 L 2 F X k 1 (1 -? k ) 2 -A k z k -z k-1 2 . (D.21)</formula><p>In what follows we show that it is sufficient to ensure </p><formula xml:id="formula_67">X k 1 (1 -? k ) 2 ? A k , X k 2 + 2A k+1 ? 4 L 2 F ? 2 k ? 0, (D.22) resulting in the inequality E[U k+1 | F k ] -U k ? -? k ? F(z k ) 2 + 2A k+1 ? 2 L 2 F + 1 + 2X k 1 ? 2 k ? 2 ? 2 F .<label>(</label></formula><formula xml:id="formula_68">A k+1 = ? k p=0 1 (1 -? p ) 2 ? ? ? ? ? ? ? ? A 0 - k =0 c l ? l ? p=0 (1 -? p ) 2 ? ? ? ? ? ? ? ? = ? k+1 ? k p=0 1 (1 -? p ) 2 (D.26)</formula><p>which would ensure A k ? 0 for all k. Therefore, assumptions (D.17) and (D.18) (which is a restatement of the conditions in (D.22)) are sufficient for ensuring (D.23). Substituting X k 1 and A k+1 in (D.23) yields</p><formula xml:id="formula_69">E[U k+1 | F k ] -U k ? -? k ? F(z k ) 2 + ? k , (D.27)</formula><p>where</p><formula xml:id="formula_70">? k = 2 A k+1 (? 2 L 2 F + 1) + 1 2 + (b + 1)? 2 L 2 F ? k + 1 ?L F ? 1-d k ? 2 k ? 2 ? 2 F . By Assumption VI we have that ? k=0 ? k = 2? 2 ? 2 F ? ? ? ? ? ? ? (? 2 L 2 F + 1) ? k=0 A k+1 ? 2 k + ? k=0 ? 2 k 2 + (b + 1)? 2 L 2 F ? k=0 ? 3 k + 1 ?L F ? k=0 ? 3-d k ? ? ? ? ? ? ? ? 2? 2 ? 2 F ? ? ? ? ? ? ? (? 2 L 2 F + 1) ? k=0 A k+1 ? 2 k + 1 2 + (b + 1)? 2 L 2 F + 1 ?L F ? k=0 ? 2 k ? ? ? ? ? ? ? &lt; ?</formula><p>where we used the fact that ? 3 k ? ? 2 k and d ? 1 in the first inequality, while the second inequality uses (D.25), and Assumption VI(i). The claimed convergence result follows by the Robbins-Siegmund supermartingale theorem <ref type="bibr">(Bertsekas, 2011, Prop.</ref> 2) and standard arguments as in <ref type="bibr">(Bertsekas, 2011, Prop. 9)</ref>.</p><p>The claimed rate follows by taking total expectation and summing the above inequality over k and noting that initial iterates were set as z-1 = z -1 = z 0 .</p><p>To provide an instance of the sequence (? k ) k? that satisfy the assumptions, let r denote a positive natural number and set ? k = 1 k+r .</p><p>(D.28)</p><p>Then, ? p=0 (1 -? p ) 2 = ? p=0 ( p+r-1 p+r ) 2 = (r-1) 2 ( +r) 2 = (r -1) 2 ? 2 , and for any K ? 0</p><formula xml:id="formula_71">K =0 c ? ? p=0 (1 -? p ) 2 = K =0 (r-1) 2 ( +r) 3 c .</formula><p>Plugging the value of c and ? k from Assumption VI(ii) and (D.24) we obtain that A 0 is finite valued since</p><formula xml:id="formula_72">? =0 1 ( +r) 3 ? = ? =0 1 ( +r) 3-d &lt; ? owing to the fact that d ? 1. Moreover, A k+1 = (k + r) 2 (r -1) 2 ? ? ? ? ? ? ? ? A 0 - k =0 (r-1) 2 ( +r) 3 c ? ? ? ? ? ? ? ? = (k + r) 2 ? =k+1 1 ( +r) 3 c = 1 ? 2 k ? =k+1 ? 3 c (D.29)</formula><p>On the other hand, for e &gt; 1 we have the following bound Therefore, it follows from (D.29) that</p><formula xml:id="formula_73">A k+1 ? k = 1 ? k ? =k+1 ? 3 (1 + b)? 2 L 2 F + 1 ?L F ? 3-d (D.30) ? (1 + b)? 2 L 2 F 1 2(k+1+r) 2 k+1+r + 1 1 k+1+r + 1 ?L F 1 (2-d)(k+1+r) 1-d 1 k+1+r + 1 1 k+1+r = 1+b 2 ? 2 L 2 F ? k+1 (2? k+1 + 1)? k+1 + 1 ?L F (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 ? (1 + b)? 2 L 2 F ? k+1 + 1 ?L F (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 (D.31)</formula><p>In turn, this inequality ensures that ? as defined in Assumption VI(ii) is finite. To see this note that</p><formula xml:id="formula_74">? = ? k=0 A k+1 ? 2 k (D.31) ? ? k=0 (1 + b)? 2 L 2 F ? k+1 + 1 ?L F (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 ? k ? ? ? k=0 ? 2 k &lt; ?,</formula><p>where in the last two inequalities Assumption VI(i) was used.</p><p>It remains to confirm the second inequality in <ref type="bibr">(D.22)</ref>. With the choice of ? k and ? k as in (D.28) and (D.24) we have 1</p><formula xml:id="formula_75">? k ? 2 X 2 + 2A k+1 ? 4 L 2 F ? 2 k = ?L F ? d k -1 -2? ? + ? k + 2A k+1 ? 2 L 2 F ? k (D.31) ? ?L F ? d k + ? k + 2? 2 L 2 F (1 + b)? 2 L 2 F ? k+1 + 1 ?L F (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 -1 -2? ? .</formula><p>It follows that with d = 1 the assumption (D.20) is sufficient to ensure that the second condition in (D.22) holds.</p><p>Proof of Theorem 6.3 (almost sure convergence). The result is a restatement of the special case in Theorem D.3 where ? k = 1 k+r . We proceed similarly to the proof of Theorem 6.1.</p><p>The condition in (D.1) can be rewritten as b &gt; </p><formula xml:id="formula_76">? 2 L 2 F 1-? 2 L 2 F . A reasonable choice is b = 2? 2 L 2 F 1-? 2 L 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof for constrained case</head><p>We will rely on two well-known and useful properties of the deterministic operator H = id -?F from <ref type="bibr">(Pethick et al., 2022, Lm. A.</ref>3) that we restate here for convenience.</p><p>Lemma E.1. Let F : n ? n be a L F -Lipschitz operator and H = id -?F with ? ? (0, 1 /L F ]. Then, (i) The operator H is 1 /2-cocoercive.</p><p>(ii) The operator H is (1 -?L F )-monotone, and in particular</p><formula xml:id="formula_77">Hz -Hz ? (1 -?L F ) z -z ?z, z ? n . (E.1)</formula><p>Proof. The first claim follows from direct computation Hz -Hz , zz = Hz -Hz , Hz -Hz + ?Fz -?Fz</p><formula xml:id="formula_78">= 1 2 Hz -Hz 2 -? 2 2 Fz -Fz 2 + 1 2 z -z 2 ? 1 2 Hz -Hz 2 , (E.2)</formula><p>where the last inequality is due to Lipschitz continuity and ? ? 1 /L F . The strongly monotonicity of H is a consequence of Cauchy-Schwarz and Lipschitz continuity of F,</p><formula xml:id="formula_79">Hz -Hz, z -z = z -z 2 -? Fz -Fz, z -z ? (1 -?L) z -z 2 .</formula><p>The last claim follows from the Cauchy-Schwarz inequality.</p><p>Theorem E.2. Suppose that Assumptions I to III hold. Moreover, suppose that ? k ? (0, 1), ? ? ( -2? + , 1 /L F ) and for positive parameters ? and b the following holds,</p><formula xml:id="formula_80">? 1 1+b (1 - 1 ?(1-?L F ) 2 ) -? 0 (1 + 2? 2 L 2 F A) + 2? ? &gt; 0 and 1 - 1 ?(1-?L F ) 2 ? 0 (E.3) where A ? ? + 1 b (1 - 1 ?(1-?L F ) 2</formula><p>). Consider the sequence (z k ) k? generated by Algorithm 2. Then, the following estimate holds for all z ? S K k=0</p><formula xml:id="formula_81">? k K j=0 ? j E[ h k -Hz k 2 ] ? E[ z 0 -z 2 ] + AE[ h -1 -Hz -1 2 ] + C? 2 ? 2 F K j=0 ? 2 j ? K j=0 ? j (E.4) where C = 1 + 2A(1 + ? 2 L 2 F ) + 2? 0 A.</formula><p>Proof of Theorem E.2. We rely on the following potential function,</p><formula xml:id="formula_82">U k+1 z k+1 -z 2 + A k+1 h k -Hz k 2 + B k+1 z k+1 -z k 2 ,</formula><p>where (A k ) k? and (B k ) k? are positive scalar parameters to be identified.</p><p>We will denote ?k := zk -? F(z k , ?k ), so that z k+1 = z k? k (h k -?k ). Then, expanding one step,</p><formula xml:id="formula_83">z k+1 -z 2 = z k -z 2 -2? k h k -?k , z k -z + ? 2 k h k -?k 2 . (E.5)</formula><p>Recall that Hz z -?Fz in the deterministic case. In the Algorithm 2, h k estimates Hz k . Let us quantify how good this estimation is.</p><formula xml:id="formula_84">h k -Hz k = ?Fz k -? F(z k , ? k ) + (1 -? k-1 )(h k-1 -z k-1 + ? F(z k-1 , ? k )) h k -Hz k 2 = (1 -? k-1 ) 2 h k-1 -z k-1 + ?Fz k-1 2 + ?Fz k -? F(z k , ? k ) + (1 -? k-1 )(? F(z k-1 , ? k ) -?Fz k-1 ) 2 + 2(1 -? k-1 ) h k-1 -z k-1 + ?Fz k-1 , ?Fz k -? F(z k , ? k ) + (1 -? k-1 )(? F(z k-1 , ? k ) -?Fz k-1 )</formula><p>In the scalar product, the left term is known when z k is known and the right term has an expectation equal to 0 by Assumption II(ii) when z k is known. Thus, taking conditional expectation and using the fact that the second moment is larger than the variance, we can go on as</p><formula xml:id="formula_85">E[ h k -Hz k 2 | F k ] ? (1 -? k ) 2 h k-1 -Hz k-1 2 + E[2(1 -? k ) 2 ? 2 F(z k , ? k ) -F(z k-1 , ? k ) 2 | F k ] + E[2? 2 k ? 2 Fz k -F(z k , ? k ) 2 | F k ] ? (1 -? k ) 2 h k-1 -Hz k-1 2 + 2(1 -? k ) 2 L 2 F ? 2 z k -z k-1 2 + 2? 2 k ? 2 ? 2 F (E.6)</formula><p>where we have used Assumption II(iii) and Assumption III.</p><p>We continue with the conditional expectation of the inner term in (E.5).</p><p>-</p><formula xml:id="formula_86">E[ h k -?k , z k -z | F k ] = -h k -Hz k , z k -z = -h k -Hz k , z k -zk -h k -Hz k , zk -z = -h k -Hz k , z k -zk -Hz k -Hz k , z k -zk -h k -Hz k , zk -z ? -h k -Hz k , z k -zk -1 2 Hz k -Hz k 2 -h k -Hz k , zk -z (E.7)</formula><p>where the last inequality uses 1 /2-cocoercivity of H from Lemma F.2(i) under Assumption I(i) and the choice ? ? 1/L F .</p><p>By definition of zk in Step 2.3, we have</p><formula xml:id="formula_87">h k ? zk + ?A(z k ), so that 1 ? (h k -Hz k ) ? F(z k ) + A(z k</formula><p>). Hence, using the weak MVI from Assumption I(iii),</p><formula xml:id="formula_88">h k -Hz k , zk -z ? ? ? h k -Hz k 2 . (E.8)</formula><p>Using (E.8) in (E.7) leads to the following inequality, true for any ? k &gt; 0:</p><p>-</p><formula xml:id="formula_89">E[ h k -?k , z k -z | F k ] ? ? k 2 h k -Hz k 2 + 1 2? k zk -z k 2 -1 2 Hz k -Hz k 2 -? ? h k -Hz k 2 .</formula><p>To majorize the term zkz k 2 , we use Lemma F.2(ii) to get</p><formula xml:id="formula_90">Hz k -Hz k 2 ? (1 -?L F ) 2 zk -z k 2 .</formula><p>Hence, as long as ?L F &lt; 1, then</p><formula xml:id="formula_91">-E[ h k -?k , z k -z | F k ] ? ? k 2 h k -Hz k 2 + 1 2? k (1-?L F ) 2 -1 2 Hz k -Hz k 2 -? ? h k -Hz k 2 . (E.9)</formula><p>The third term in (E.5) is bounded by</p><formula xml:id="formula_92">? 2 k E[ h k -?k 2 | F k ] = ? 2 k h k -Hz k 2 + ? 2 k ? 2 E[ F zk -F(z k , ?k ) 2 | F k ] ? ? 2 k h k -Hz k 2 + ? 2 k ? 2 ? 2 F (E.10)</formula><p>Combined with the update rule, (E.10) can also be used to bound the difference of iterates</p><formula xml:id="formula_93">E[ z k+1 -z k 2 | F k ] = E[? 2 k h k -?k 2 | F k ] ? ? 2 k h k -Hz k 2 + ? 2 k ? 2 ? 2 F (E.11)</formula><p>Using (E.5), (E.9), (E.10) and (E.11) we have,</p><formula xml:id="formula_94">E[U k+1 | F k ] ? z k -z 2 + (A k+1 + ? k ? k ) h k -Hz k 2 -? k 1 - 1 ? k (1-?L F ) 2 Hz k -Hz k 2 + ? k (? k -2? ? + ? k B k+1 ) h k -Hz k 2 + ? 2 k (1 + B k+1 )? 2 ? 2 F ? z k -z 2 + A k+1 + ? k (? k + 1 b (1 - 1 ? k (1-?L F ) 2 )) h k -Hz k 2 + ? k ? k -2? ? + ? k B k+1 -1 1+b (1 - 1 ? k (1-?L F ) 2 ) h k -Hz k 2 + ? 2 k (1 + B k+1 )? 2 ? 2 F , (E.12)</formula><p>where the last inequality follows from Young's inequality with positive b and requiring 1 -</p><formula xml:id="formula_95">1 ? k (1-?L F ) 2 ? 0 as also stated in (E.3). By defining X 1 k A k+1 + ? k (? k + 1 b (1 - 1 ? k (1-?L F ) 2 )) X 2 k ? k ? k -2? ? + ? k B k+1 -1 1+b (1 - 1 ? k (1-?L F ) 2 ) (E.13)</formula><p>and applying (E.6), we finally obtain</p><formula xml:id="formula_96">E[U k+1 | F k ] -U k ? X 2 k h k -Hz k 2 + (X 1 k (1 -? k ) 2 -A k ) h k-1 -Hz k-1 2 + (2X 1 k (1 -? k ) 2 ? 2 L 2 F -B k ) z k -z k-1 2 + 2X 1 k ? 2 k ? 2 ? 2 F + ? 2 k (1 + B k+1 )? 2 ? 2 F , (E.14)</formula><p>We can pick B k = 2? 2 L 2 F A k in which case, to get a recursion, we only require the following.</p><formula xml:id="formula_97">X 1 k (1 -? k ) 2 -A k ? 0 and X 2 k &lt; 0 (E.15) Set A k = A, ? k = ?. For the first requirement of (E.15), X 1 k (1 -? k ) 2 -A k = ? k (1 -? k ) 2 (? + 1 b (1 - 1 ?(1-?L F ) 2 )) + (1 -? k ) 2 A -A ? ? k (? + 1 b (1 - 1 ?(1-?L F ) 2 )) + (1 -? k ) 2 A -A ? ? k (? + 1 b (1 - 1 ?(1-?L F ) 2 )) + (1 -? k )A -A = ? k (? + 1 b (1 - 1 ?(1-?L F ) 2 )) -? k A (E.16)</formula><p>where the first inequality follows from (1 -? k ) 2 ? 1 and the second inequality follows from (1 -? k ) 2 ? (1 -? k ). Thus, to satisfy the first inequality of (E.15) it suffice to pick</p><formula xml:id="formula_98">A ? ? + 1 b (1 - 1 ?(1-?L F ) 2 ). (E.17)</formula><p>The noise term in (E.14) can be made independent of k by using ? k ? ? 0 and (E.17) as follows</p><formula xml:id="formula_99">2X 1 k + 1 + B k+1 = 1 + 2A(1 + ? 2 L 2 F ) + 2? k (? + 1 b (1 - 1 ?(1-?L F ) 2 )) ? 1 + 2A(1 + ? 2 L 2 F ) + 2? 0 A = C. (E.18)</formula><p>Thus it follows from (E.14) and ? k ? ? 0 that</p><formula xml:id="formula_100">E[U k+1 | F k ] -U k ? ? k ? 0 -2? ? + 2? 0 ? 2 L 2 F A -1 1+b (1 - 1 ? k (1-?L F ) 2 ) h k -Hz k 2 + ? 2 k C? 2 ? 2 F . (E.19)</formula><p>The result is obtained by total expectation and summing the above inequality while noting that the initial iterate were set as z -1 = z 0 .</p><p>Proof of Theorem 7.1. The theorem is a specialization of Theorem E.2 with a particular a choice of b and ?. The second requirement in (E.3) can be rewritten as,</p><formula xml:id="formula_101">? ? 1 (1-?L F ) 2 , (E.20) which is satisfied by ? = 1 ? ? 0 (1-?L F ) 2 .</formula><p>We substitute in the choice of ?, b = ? ? 0 and denotes ? A.</p><p>The weighted sum in (E.4) can be converted into an expectation over a sampled iterate in the style of <ref type="bibr" target="#b15">Ghadimi &amp; Lan (2013)</ref>,</p><formula xml:id="formula_102">E[ h k -Hz k 2 ] = K k=0 ? k K j=0 ? j E[ h k -Hz k 2 ]</formula><p>with k chosen from {0, 1, . . . , K} according to probability</p><formula xml:id="formula_103">P[k = k] = ? k K j=0 ? j . Noticing that h k -Hz k ? ?(F zk + Az k ) = ?T zk so E[ h k -Hz k 2 ] ? min u?T zk E[ ?u 2 ] ? E[ min u?T zk ?u 2 ] =: E[dist(0, ?T zk ) 2 ]</formula><p>where the second inequality follows from concavity of the minimum. This completes the proof.</p><p>F Proof for NP-PDEG through a nonlinear asymmetric preconditioner</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Preliminaries</head><p>Consider the decomposition z = (z 1 , . . . , z m ), u = (u 1 , . . . , u m ) with z i , u i ? n i and define the shorthand notation u ?i (u 1 , u 2 , . . . , u i ) and u ?i (u i , . . . , u m ) for the truncated vectors. Moreover sup-pose that A conforms to the decomposition Az = (A 1 , z 1 , . . . , A m z m ) with A i : n i ? n i maximally monotone. Consistently with the decomposition define ? = blkdiag(? 1 , . . . , ? m ) where ? i ? n i ?n i are positive definite matrices and let P u (z) ? -1 z + Q u (z), where Q u (z) = 0, q 1 (z 1 , u ?2 ), q 2 (z 1 , z 2 , u ?3 ), . . . , q m-1 (z ?m-1 , u m ) (F.1) When P u furnishes such an asymmetric structure the preconditioned resolvent has full domain, thus ensuring that the algorithm is well-defined.</p><p>In the following lemma we show that the iterates in (8.1) are well-defined for a particular choice of the preconditioner P u in (F.1). The proof is similar to that of <ref type="bibr">(Latafat &amp; Patrinos, 2017, Lem. 3.1)</ref> and is included for completeness.</p><p>Lemma F.1. Let z = (z 1 , . . . , z m ), u = (u 1 , . . . , u m ) be given vectors, suppose that A conforms to the decomposition Az = (A 1 , z 1 , . . . , A m z m ) with A i : n i ? n i maximally monotone, and let P u be defined as in (F.1). Then, the preconditioned resolvent (P u + A) -1 is Lipschitz continuous and has full domain. Moreover, the update z = (P u + A) -1 z reduces to the following update</p><formula xml:id="formula_104">zi = (? -1 1 + A 1 ) -1 z 1 if i = 1 (? -1 i + A i ) -1 (z i -q i-1 (z ?i-1 , u ?i ) if i = 2, . . . , m (F.2)</formula><p>Proof. Owing to the asymmetric structure (F.1), the resolvent may equivalently be expressed as z = (z 1 , . . . zm ) <ref type="figure">m</ref>, where q 0 ? 0. The Gauss-Seidel-type update in (F.2) is of immediate verification after noting that (? -1 i + A i ) -1 is single-valued (in fact Lipschitz continuous) since the sum of ? i 0 and A i is (maximally) strongly monotone. This also implies that ? -1 i + A i = ?i + ?I for some ? &gt; 0 and some maximally monotone operator ?i . Thus dom</p><formula xml:id="formula_105">= (P u + A) -1 z ?? ? -1 i zi + A i (z i ) ? z i -q i-1 (z ?i-1 , u ?i ), i = 1, . . . ,</formula><formula xml:id="formula_106">? -1 i + A i ) -1 = range(? -1 i + A i ) = range( 1 ? ? + I) = n</formula><p>, where we used Minty's theorem in the last equality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Deterministic lemmas</head><p>To eventually prove Theorem F.5 we will compare the stochastic algorithm (8.4) with its deterministic counterpart (8.1), so we introduce</p><formula xml:id="formula_107">H u (z) P u (z) -F(z) (F.3a) ?(z) (P z + A) -1 (H z (z)) (F.3b) G(z) z -? k ? H z (z) -H z ( ?(z)) . (F.3c)</formula><p>We first derive results for the deterministic operator G and then shows that z k+1 from the stochastic scheme behaves similarly to G(z k ) when ? k is small enough, even if ?, which also appears inside the preconditioner Pu (?, ?), remains large.</p><p>Instead of making assumptions on F directly, we instead consider the following important operator,</p><formula xml:id="formula_108">M u (z) := F(z) -Q u (z). (F.4) such that we can write (F.3b) as H u (z) = ? -1 z -M u (z). As a shorthand we write M(z) = M z (z). Assumption VII. The operator M u as defined in (F.4) is L M -Lipschitz with L M ? 1 with respect to a positive definite matrix ? ? n?n , i.e. M u (z) -M u (z ) ? ? L M z -z ? -1 ?z, z ? n . (F.5)</formula><p>Remark 6. This is satisfied by the choice of Q u in (8.7) and Assumptions IV(ii) and IV(iii).</p><p>With M u defined, it is straightforward to establish that H u is 1 /2-cocoercive and strongly monotone.</p><p>Lemma F.2. Suppose Assumption VII holds. Then,</p><formula xml:id="formula_109">(i) The mapping H u is 1 /2-cocoercive for all u ? n , i.e. H u (z ) -H u (z), z -z ? 1 2 H u (z ) -H u (z) 2 ? ?z, z ? n . (F.6) (ii) Furthermore, H u is (1 -L M )-monotone for all u ? n , and in particular H u (z ) -H u (z) ? ? (1 -L M ) z -z ? -1 ?z, z ? n . (F.7)</formula><p>Proof. By expanding using (F.4),</p><formula xml:id="formula_110">H u (z) -H u (z ) = ? -1 (z -z ) -(M u (z) -M u (z )). (F.8) Using this we can show cocoercivity, H u (z ) -H u (z), z -z = H u (z ) -H u (z), H u (z ) -H u (z) -(M u (z) -M u (z )) ? (F.8) = 1 2 H u (z ) -H u (z) 2 ? + 1 2 z -z 2 ? -1 -1 2 M u (z) -M u (z ) 2 ? Assumption VII ? 1 2 H u (z ) -H u (z) 2 ? (F.9</formula><p>) That H u is strongly monotone follows from Cauchy-Schwarz and Assumption VII,</p><formula xml:id="formula_111">H u (z ) -H u (z), z -z = z -z 2 ? -1 -M u (z ) -M u (z), z -z ? (1 -L M ) z -z 2 ? -1 .</formula><p>(F.10) The last claim follows from Cauchy-Schwarz and dividing by zz ? -1 .</p><p>We will rely on the resolvent remaining nonexpansive when preconditioned with a variable stepsize matrix.</p><p>Lemma F.3. Let ? ? n?n be positive definite and the operator A : n ? n be maximally monotone.</p><formula xml:id="formula_112">Then, R = (? -1 + A) -1 is nonexpansive, i.e. Rx -Ry ? -1 ? x -y ? for all x, y ? n . Proof. Let v ? Rx and u ? Ry. By maximal monotonicity of A, 0 ? v -? -1 x -u + ? -1 y, x -y = -x -y 2 ? -1 + v -u, x -y . Therefore, using the Cauchy-Schwarz inequality x -y 2 ? -1 ? v -u, x -y ? x -y ? -1 v -u ? (F.</formula><p>11) The proof is complete by rearranging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Stochastic results</head><p>The stochastic assumptions on F in Theorem F.5 propagates to M and Qu as captured by the following lemma.</p><p>Lemma F.4. Suppose Assumptions II(ii) and IV(iv) for F(z, ?) = (? x ?(z, ?), -? y ?(z, ?)) as defined in (8.6). Let M and M be as defined in (F.15) and Qu and Q u as in (8.7) with ? ? [0, ?). Then, the following holds for all z, z ? R n</p><formula xml:id="formula_113">(i) E ? [ M(z, ?)] = M(z) and E ? [ Qz (z, ?)] = Q z (z) (ii) E ? [ M(z) -M(z, ?) 2 ? ] ? ((1 -?) 2 + ? 2 )? 2 F and E ? [ Q z (z) -Qz (z, ?) 2 ? ] ? ? 2 ? 2 F .</formula><p>Proof. Unbiasedness follows immediately through Assumption II(ii). For the second claim we have for all (x, y)</p><formula xml:id="formula_114">= z ? n E ? [ M(z) -M(z, ?) 2 ? ] = E ? ? ? ? ? ? ? ? ? x ?(z, ?) -? x ?(z) (1 -?)(? y ?(z, ?) -? y ?(z )) 2 ? ? ? ? ? ? ? ? = E ? ? ? ? ? ? ? ? (1 -?)(? x ?(z, ?) -? x ?(z)) + ?(? x ?(z, ?) -? x ?(z)) (1 -?)(? y ?(z, ?) -? y ?(z)) 2 ? ? ? ? ? ? ? ? (Assumption II(ii)) ? (1 -?) 2 E ? ? ? ? ? ? ? ? ? x ?(z, ?) -? x ?(z) ? y ?(z, ?) -? y ?(z) 2 ? ? ? ? ? ? ? ? + ? 2 E ? ? ? ? ? ? ? ? ? x ?(z, ?) -? x ?(z) 0 2 ? ? ? ? ? ? ? ? (Assumption IV(iv)) ? ((1 -?) 2 + ? 2 )? 2 F .</formula><p>(F.12) The last claim follows directly through Assumption IV(iv). This completes the proof.</p><p>Theorem F.5. Suppose that Assumption I(iii) to II(ii) and IV hold. Moreover, suppose that ? k ? (0, 1), ? ? [0, ?) and for positive parameter b and ? the following holds, ?</p><formula xml:id="formula_115">1 1+b (1 - 1 ?(1-L M ) 2 ) + 2? ? -? 0 -2? 0 (? 1 + 2? 2 (1 + ?3 ))A &gt; 0, (F.13) 1 -4? 2 ? 0 &gt; 0 and 1 - 1 ?(1-L M ) 2 ? 0 where ? denotes the smallest eigenvalue of ?, A ? (1 + 4? 2 ? 2 0 )(? + 1 b (1 - 1 ?(1-L M ) 2 ))/(1 -4? 2 ? 0 ) and ?1 L 2 xz ?D xz + 2(1 -?) 2 L 2 yz ?D yz + 2? 2 L 2 yy ? 2 D yy , ?2 2? 2 L 2 yx ? 1 D yx , ?3 L 2 xz ?D xz , L 2 M max L 2 xx D xx ? 1 + L 2 yx D yx ? 1 , L 2 xy D xy ? 2 + L 2 yy D yy ? 2 .</formula><p>Consider the sequence (z k ) k? generated by Algorithm 3. Then, the following holds for all z ? S K k=0</p><formula xml:id="formula_116">? k K j=0 ? j E[ ? -1 ?k -S z k (z k ; zk ) 2 ? ] ? E[ z 0 -z 2 ? -1 ] + AE[ ? -1 ?-1 -S z -1 (z -1 ; z-1 ) 2 ? ] + C? 2 F K j=0 ? 2 j ? K j=0 ? j (F.14) where C 2(A + ? 0 (? + 1 b (1 - 1 ?(1-L M ) 2 )))(? + 2? 2 ) + 1 + 2(? 1 + 2? 2 (1 + ?3 ))A and ? = (1 -?) 2 + 2? 2 .</formula><p>Proof of Theorem F.5. The proof relies on tracking the two following important operators instead of F and F M(z)</p><formula xml:id="formula_117">:= F(z) -Q z (z) and M(z, ?) := F(z, ?) -Qz (z, ?). (F.15)</formula><p>We will denote ?k := Pk (z k , ?k ) -F(z k , ?k ), so that z k+1 = z k? k ?(h k -?k ). We will further need the following change of variables to later be able to apply weak MVI (see Appendix F.4):</p><formula xml:id="formula_118">s k = h k -Qz k (z k , ? k ) ? k = ?k -Qz k (z k , ? k ) (F.18)</formula><p>In the algorithm, s k estimates S z k (z k ; zk ). Let us quantify how good this estimation is. We will make use of the careful choice of the bias-correction term to shift the noise index by 1 in the second equality.</p><formula xml:id="formula_119">s k -S z k (z k ; zk ) = M(z k ) + Q z k (z k ) -M(z k , ? k ) -Qz k (z k , ? k ) + (1 -? k )(h k-1 -? -1 z k-1 + M(z k-1 , ? k ) -Qz k-1 (z k-1 , ? k-1 ) + Qz k-1 (z k-1 , ? k )) = M(z k ) + Q z k (z k ) -M(z k , ? k ) -Qz k (z k , ? k ) + (1 -? k )(s k-1 + Qz k-1 (z k-1 , ? k ) -? -1 z k-1 + M(z k-1 , ? k )) = M(z k ) + Q z k (z k ) -M(z k , ? k ) -Qz k (z k , ? k ) + (1 -? k )(s k-1 -S z k-1 (z k-1 ; zk-1 )) + (1 -? k )( M(z k-1 , ? k ) -M(z k-1 ) + Qz k-1 (z k-1 , ? k ) -Q z k-1 (z k-1 )) Using the shorthand notation sk s k -S z k (z k ; zk ), Qz k (z k , ? k ) Q z k (z k ) -Qz k (z k , ? k ), M(z k , ? k ) M(z k ) -M(z k , ? k ), it follows that sk 2 ? = (1 -? k ) 2 sk-1 2 ? + M(z k , ? k ) + Qz k (z k , ? k ) -(1 -? k )( M(z k-1 , ? k ) + Qz k-1 (z k-1 , ? k ) 2 ? + 2(1 -? k ) sk-1 , M(z k , ? k ) + Qz k (z k , ? k ) -(1 -? k ) M(z k-1 , ? k ) + Qz k-1 (z k-1 , ? k ) (F.19)</formula><p>In the scalar product, the left term is known when z k is known. Moreover, since</p><formula xml:id="formula_120">E ? | F k = E E ? | F k | F k , owing to F k ? F k , we have E M(z k , ? k ) + Qz k (z k , ? k ) -(1 -? k ) M(z k-1 , ? k ) + Qz k-1 (z k-1 , ? k ) | F k = E M(z k , ? k ) -(1 -? k ) M(z k-1 , ? k ) | F k = 0,</formula><p>where we use Assumption II(ii) through Lemma F.4(i).</p><p>Since the second moment is larger than the variance we have</p><formula xml:id="formula_121">E M(z k , ? k ) + Qz k (z k , ? k ) -M(z k-1 , ? k ) -Qz k-1 (z k-1 , ? k ) 2 ? | F k ? E M(z k , ? k ) -M(z k-1 , ? k ) + Qz k (z k , ? k ) -Qz k-1 (z k-1 , ? k ) 2 ? | F k (F.20) Using the Young inequality it follows from (F.19), (F.20) that E[ sk 2 ? | F k ] ? (1 -? k ) 2 sk-1 2 ? + 2? 2 k E[ M(z k , ? k ) + Qz k (z k , ? k ) 2 ? | F k ] + 2(1 -? k ) 2 E[ M(z k , ? k ) + Qz k (z k , ? k ) -M(z k-1 , ? k ) -Qz k-1 (z k-1 , ? k ) 2 ? | F k ] ? (1 -? k ) 2 sk-1 2 ? + 2? 2 k E[ M(z k ) -M(z k , ? k ) + Q z k (z k ) -Qz k (z k , ? k ) 2 ? | F k ] + E[2(1 -? k ) 2 M(z k , ? k ) -M(z k-1 , ? k ) + Qz k (z k , ? k ) -Qz k-1 (z k-1 , ? k ) 2 ? | F k ] (F.21)</formula><p>To bound the second last term of (F.21) we use unbiasedness due to Assumption II(ii) through Lemma F.4(i) and that</p><formula xml:id="formula_122">E ? | F k = E E ? | F k | F k , owing to F k ? F k E[ M(z k ) -M(z k , ? k ) + Q z k (z k ) -Qz k (z k , ? k ) 2 ? | F k ] = E[ M(z k ) -M(z k , ? k ) 2 ? | F k ] + E[E[ Q z k (z k ) -Qz k (z k , ? k ) 2 ? | F k ] | F k ] ? ?? 2 F (F.22) with ? = (1 -?) 2 + 2? 2 .</formula><p>where the last inequality follows from Assumptions II(ii) and IV(iv) through Lemma F.4(ii).</p><p>To bound the last term of (F.21) we use the particular choice of</p><formula xml:id="formula_123">Q u , M(z k , ? k ) -M(z k-1 , ? k ) + Qz k (z k , ? k ) -Qz k-1 (z k-1 , ? k ) = ? x ?(z k , ? k ) -? x ?(z k-1 , ? k ) (1 -?)(? y ?(z k-1 ? k ) -? y ?(z k , ? k )) -?(? y ?( xk , y k , ? k ) -? y ?( xk , y k , ? k ) . (F.23)</formula><p>So Assumption IV(v) applies after application of Young's inequality and the tower rule, leading to the following bound</p><formula xml:id="formula_124">E[ M(z k , ? k ) -M(z k-1 , ? k ) + Qz k (z k , ? k ) -Qz k-1 (z k-1 , ? k ) 2 ? | F k ] = E[ ? x ?(z k , ? k ) -? x ?(z k-1 , ? k ) 2 ? 1 | F k ] + E[ (1 -?)(? y ?(z k-1 ? k ) -? y ?(z k , ? k )) -?? y ?( xk , y k , ? k ) -? y ?( xk , y k , ? k ) 2 ? 2 | F k ] ? E[ ? x ?(z k , ? k ) -? x ?(z k-1 , ? k ) 2 ? 1 | F k ] + 2(1 -?) 2 E[ (? y ?(z k-1 ? k ) -? y ?(z k , ? k )) 2 ? 2 | F k ] + 2? 2 E E[ ? y ?( xk , y k , ? k ) -? y ?( xk , y k , ? k ) 2 ? 2 | F k ] | F k Assumption IV(v) ? L 2 xz z k -z k-1 2 D xz + 2(1 -?) 2 L 2 yz z k -z k-1 2 D yz + 2? 2 L 2 yy y k -y k-1 2 D yy + 2? 2 L 2 yx xk -xk-1 2 D yx ? ?1 z k -z k-1 2 ? -1 + ?2 xk -xk-1 2 ? -1 (F.24)</formula><p>where ?1 L 2 xz ?D xz + 2(1 -?) 2 L 2 yz ?D yz + 2? 2 L 2 yy ? 2 D yy and ?2 2? 2 L 2 yx ? 1 D yx . Using (F.24) and (F.22) in (F.21) yields,</p><formula xml:id="formula_125">E[ sk 2 ? | F k ] ? (1 -? k ) 2 sk-1 2 ? + 2? 2 k ?? 2 F + 2(1 -? k ) 2 ?1 z k -z k-1 2 ? -1 + ?2 xk -xk-1 2 ? -1 1 . (F.25)</formula><p>To majorize xkxk-1</p><formula xml:id="formula_126">? -1</formula><p>Published as a conference paper at ICLR 2023 of A 1 we have through Lemma F.3 that xkxk-1</p><formula xml:id="formula_127">? -1 1 ? s k x -s k-1 x ? 1 . (F.26)</formula><p>We can go on as</p><formula xml:id="formula_128">s k x -s k-1 x ? 1 = ? -1 1 x k -? x ?(z k , ? k ) + (1 -? k ) ? -1 1 (x k-1 -x k-1 ) + ?x ?(z k-1 , ? k ) -s k-1 x ? 1 ? (1 -? k ) x k -x k-1 ? -1 x ? 1 (Assumption IV(v)) ? (1 -? k ) x k -x k-1 ? -1 1 + (1 -? k )L xz z k -z k-1 D xz + ? k ? -1 1 x k -? x ?(z k , ? k ) -s k-1 x ? 1 = (1 -? k ) x k -x k-1 ? -1 1 + (1 -? k )L xz z k -z k-1 D xz + ? k s k x -s k-1 x ? -1 1 + ? k (1 -? k ) ? -1 1 x k-1 -? x ?(z k-1 , ? k ) -s k-1</formula><p>x ? 1 , where the last equality uses ab 2 = a 2 + b 2 -2 a, b and unbiasedness from Assumption II(ii) to conclude that the inner product is zero.</p><p>Hence, by subtracting ? k s k</p><p>xs k-<ref type="foot" target="#foot_2">1</ref> </p><p>x ? -1 1 and diving by 1 -? k , we get</p><formula xml:id="formula_129">E[ s k x -s k-1 x 2 ? -1 1 | F k ] ? 2(1 + ?3 ) x k -x k-1 2 ? -1 1 + 2? 2 k E[ ? -1 x k-1 -? x ?(z k-1 , ? k ) -s k-1 x 2 ? 1 | F k ]</formula><p>Assumptions II(ii) and IV(iv</p><formula xml:id="formula_130">) ? 2(1 + ?3 ) x k -x k-1 2 ? -1 1 + 2? 2 k E[ ? -1 x k-1 -? x ?(z k-1 ) -s k-1 x 2 ? 1 | F k ] + 2? 2 k ? 2 F ? 2(1 + ?3 ) z k -z k-1 2 ? -1 + 2? 2 k E[ S z k-1 (z k-1 ; zk-1 ) -s k-1 2 ? | F k ] + 2? 2 k ? 2 F</formula><p>where ?3 L 2 xz ?D xz and the last inequality reintroduces the y-components. We finally obtain</p><formula xml:id="formula_131">E[ xk -xk-1 2 ? -1 | F k ] ? 2(1 + ?3 ) z k -z k-1 2 ? -1 + 2? 2 k E[ s k-1 -S z k-1 (z k-1 ; zk-1 ) 2 ? | F k ] + 2? 2 k ? F .</formula><p>(F.27) Introducing (F.27) into (F.25) yields</p><formula xml:id="formula_132">E[ s k -S z k (z k ; zk ) 2 ? | F k ] ? (1 -? k ) 2 (1 + 4? 2 ? 2 k ) s k-1 -S z k-1 (z k-1 ; zk-1 ) 2 ? + 2(1 -? k ) 2 (? 1 + 2? 2 (1 + ?3 )) z k -z k-1 2 ? -1 + 2? 2 k (? + (1 -? k ) 2 2? 2 )? 2 F .</formula><p>(F.28)</p><p>We continue with the inner term in (F.18) under conditional expectation.</p><formula xml:id="formula_133">-E[ s k -? k , z k -z ? | F k ] = -s k -S z k (z k ), z k -z = -s k -S z k (z k ), z k -zk -s k -S z k (z k ), zk -z = -s k -S z k (z k ; zk ), z k -zk -S z k (z k ; zk ) -S z k (z k ), z k -zk -s k -S z k (z k ), zk -z = -s k -S z k (z k ; zk ), z k -zk -H z k (z k ) -H z k (z k ), z k -zk -s k -S z k (z k ), zk -z</formula><p>where the last equality uses that</p><formula xml:id="formula_134">S z k (z k ; zk ) -S z k (z k ) = H z k (z k ) -H z k (z k ).</formula><p>By definition of zk in (8.4b), we have</p><formula xml:id="formula_135">s k = h k -Qz k (z k , ? k ) ? ? -1 zk + A(z k ), so that s k -S z k (z k ) ? F(z k ) + A(z k</formula><p>). Hence, using the weak MVI from Assumption I(iii),</p><formula xml:id="formula_136">s k -S z k (z k ), zk -z ? ? s k -S z k (z k ) 2 .</formula><p>(F.29) Using also cocoercivity of H u from Lemma F.2(i), this leads to the following inequality, true for any ? k &gt; 0:</p><formula xml:id="formula_137">-E[ s k -? k , z k -z | F k ] ? ? k 2 s k -S z k (z k ; zk ) 2 ? + 1 2? k zk -z k 2 ? -1 -1 2 H z k (z k ) -H z k (z k ) 2 ? -? s k -S z k (z k ) 2 .</formula><p>To majorize the term zkz k 2</p><p>? -1 , we may use Lemma F.2(ii) for which we need to determind L M . For the particular choice of Q u , we have through Assumption IV(ii) that</p><formula xml:id="formula_138">M(z ) -M(z) 2 ? ? L 2 M z -z 2 ? -1 (F.30)</formula><p>Published as a conference paper at ICLR 2023</p><formula xml:id="formula_139">with L 2 M max L 2 xx D xx ? 1 + L 2 yx D yx ? 1 , L 2 xy D xy ? 2 + L 2 yy D yy ? 2 .</formula><p>By the stepsize choice Assumption IV(iii), L M &lt; 1, which will be important promptly.</p><p>From Lemma F.2(ii) it then follows that</p><formula xml:id="formula_140">H z k (z k ) -H z k (z k ) 2 ? ? (1 -L M ) 2 z k -zk 2 ? -1 . Hence, given L M &lt; 1, -E[ s k -? k , z k -z ? | F k ] ? ? k 2 s k -S z k (z k ; zk ) 2 ? + 1 2? k (1-L M ) 2 -1 2 H z k (z k ) -H z k (z k ) 2 ? -? s k -S z k (z k ) 2 = ? k 2 s k -S z k (z k ; zk ) 2 ? + 1 2? k (1-L M ) 2 -1 2 S z k (z k ; zk ) -S z k (z k ) 2 ? -? s k -S z k (z k ) 2 . (F.</formula><p>31) The conditional expectation of the third term in (F.18) is bounded by</p><formula xml:id="formula_141">? 2 k E[ s k -? k 2 ? | F k ] = ? 2 k s k -S z k (z k ) 2 ? + ? 2 k E[ F(z k ) -F(z k , ?k ) 2 ? | F k ] ? ? 2 k s k -S z k (z k ) 2 ? + ? 2 k ? 2 F (F.</formula><p>32) where we have used Assumption IV(iv).</p><p>Combined with the update rule, (F.32) can also be used to bound the conditional expectation of the difference of iterates</p><formula xml:id="formula_142">E[ z k+1 -z k 2 ? -1 | F k ] = E[? 2 k s k -? k 2 ? | F k ] ? ? 2 k s k -S z k (z k ) 2 ? + ? 2 k ? 2 F (F.33)</formula><p>Using (F.18), (F.31), (F.32), (F.33) and that -? <ref type="bibr">(F.34)</ref> where the last inequality follows from Young's inequality with positive b as long as 1-</p><formula xml:id="formula_143">s k -S z k (z k ) 2 ? -? ? s k -S z k (z k ) 2 ? with ? denoting the smallest eigenvalue of ? we have, E[U k+1 | F k ] ? z k -z 2 ? -1 + (A k+1 + ? k ? k ) s k -S z k (z k ; zk ) 2 ? -? k 1 - 1 ? k (1-L M ) 2 S z k (z k ; zk ) -S z k (z k ) 2 ? + ? k (? k -2? ? + ? k B k+1 ) s k -S z k (z k ) 2 ? + ? 2 k (1 + B k+1 )? 2 F ? z k -z 2 ? -1 + A k+1 + ? k (? k + 1 b (1 - 1 ? k (1-L M ) 2 )) s k -S z k (z k ; zk ) 2 ? + ? k ? k -2? ? + ? k B k+1 -1 1+b (1 - 1 ? k (1-L M ) 2 ) H z k (z k ) -H z k (z k ) 2 ? + ? 2 k (1 + B k+1 )? 2 F ,</formula><formula xml:id="formula_144">1 ? k (1-L M ) 2 ? 0. By defining X 1 k A k+1 + ? k (? k + 1 b (1 - 1 ? k (1-L M ) 2 )) X 2 k 2? ? -? k -? k B k+1 + 1 1+b (1 - 1 ? k (1-L M ) 2 ) (F.35)</formula><p>and applying (F.28), we finally obtain</p><formula xml:id="formula_145">E[U k+1 | F k ] -U k ? -? k X 2 k s k -S z k (z k ) 2 ? + (X 1 k (1 -? k ) 2 (1 + 4? 2 ? 2 k ) -A k ) s k-1 -S z k-1 (z k-1 ; zk-1 ) 2 ? + (2X 1 k (1 -? k ) 2 (? 1 + 2? 2 (1 + ?3 )) -B k ) z k -z k-1 2 ? -1 + 2X 1 k ? 2 k (? + (1 -? k ) 2 2? 2 )? 2 F + ? 2 k (1 + B k+1 )? 2 F , (F.36) If A k ? X 1 k (1 -? k ) 2 (1 + 4? 2 ? 2 k ), then it suffice to pick B k as 2X 1 k (1 -? k ) 2 (? 1 + 2? 2 (1 + ?3 )) = 2(? 1 +2? 2 (1+? 3 ))A k 1+4? 2 ? 2 k ? 2(? 1 + 2? 2 (1 + ?3 ))A k =: B k . (F.37)</formula><p>To get a recursion, we then only require the following conditions</p><formula xml:id="formula_146">X 1 k (1 -? k ) 2 (1 + 4? 2 ? 2 k ) ? A k and X 2 k &gt; 0. (F.38) Set A k = A, ? k = ?.</formula><p>For the first inequality of (F.38), since (1</p><formula xml:id="formula_147">-? k ) 2 ? (1 -? k ), the terms involving A are bounded as (1 -? k ) 2 (1 + 4? 2 ? 2 k )A -A ? (1 -? k )(1 + 4? 2 ? 2 k )A -A = -? k A + (1 -? k )(4? 2 ? 2 k )A ? -? k (1 -4? 2 ? 0 )A (F.39)</formula><p>where the last inequality follows from (1 -? k ) ? 1 and ? k ? ? 0 . Thus to satisfy the first inequality of (F.38) it suffice to pick</p><formula xml:id="formula_148">A ? (1 + 4? 2 ? 2 0 )(? + 1 b (1 - 1 ?(1-L M ) 2 )) 1 -4? 2 ? 0 (F.40)</formula><p>where 1 -4? 2 ? 0 &gt; 0 is required.</p><p>The second equality of (F.38) is satisfied owing to (F.13).</p><p>The noise term in (F.36) can be made independent of k by using</p><formula xml:id="formula_149">? k ? ? 0 , 2X 1 k (1 + (1 -? k ) 2 2? 2 ) + 1 + B k+1 = 2(A + ? k (? + 1 b (1 - 1 ?(1-L M ) 2 )))(? + (1 -? k ) 2 2? 2 ) + 1 + 2(? 1 + 2? 2 (1 + ?3 ))A ? 2(A + ? 0 (? + 1 b (1 - 1 ?(1-L M ) 2 )))(? + 2? 2 ) + 1 + 2(? 1 + 2? 2 (1 + ?3 ))A =: C. (F.41)</formula><p>Thus, it follows from (F.36) that</p><formula xml:id="formula_150">E[U k+1 | F k ] -U k ? ? k ? 0 -2? ? + 2? 0 (? 1 + 2? 2 (1 + ?3 ))A -1 1+b (1 - 1 ?(1-L M ) 2 ) s k -S z k (z k ) 2 ? + ? 2 k C? 2 F . (F.42)</formula><p>The result is obtained by total expectation and summing the above inequality while noting that the initial iterate were set as z -1 = z 0 .</p><p>Proof of Theorem 8.2. The theorem is a specialization of Theorem F.5 for a particular a choice of b and ?. The third requirement of (F.13) can be rewritten as,</p><formula xml:id="formula_151">? ? 1 (1-L M ) 2 , (F.43) which is satisfied by ? = 1 ? ? 0 (1-L M ) 2 .</formula><p>We substitute in the choice of ?, b = ? ? 0 and denotes ? A.</p><p>The weighted sum in (F.14) is equivalent to an expectation over a sampled iterate in the style of <ref type="bibr" target="#b15">Ghadimi &amp; Lan (2013)</ref>,</p><formula xml:id="formula_152">E[ ? -1 ?k -S z k (z k ; zk ) 2 ? ] = K k=0 ? k K j=0 ? j E[ ? -1 ?k -S z k (z k ; zk ) 2 ? ].</formula><p>with k chosen from {0, 1, . . . , K} according to probability P</p><formula xml:id="formula_153">[k = k] = ? k K j=0 ? j . Noticing that ? -1 ?k -S z k (z k ; zk ) ? F zk + Az k = T zk so E[ ? -1 ?k -S z k (z k ; zk ) 2 ? ] ? min u?T zk E[ u 2 ? ] ? E[ min u?T zk u 2 ? ] =: E[dist ? (0, T zk ) 2 ]</formula><p>where the second inequality follows from concavity of the minimum. This completes the proof.</p><p>Theorem F.6 (almost sure convergence). Suppose that Assumption I(iii) to II(ii) and IV hold. Moreover, suppose that ? k ? (0, 1), ? ? [0, ?) and the following holds for positive parameter b,</p><formula xml:id="formula_154">? k ? =k c l ? l ? p=0 (1 -? p ) 2 ? p &lt; ?, ? ? k=0 ? 2 k ? k+1 ? k p=0 1 (1-? p ) 2 ? p &lt; ? and (F.44) 2? ? -? k -? k 2(? 1 + 2? 2 (1 + ?3 ))? k+1 ? k p=0 1 (1 -? p ) 2 ? p + 1 1+b (1 -? d k ) &gt; 0 (F.45)</formula><p>where ? denotes the smallest eigenvalue of ?, d ? [0, 1] and</p><formula xml:id="formula_155">? k = 1 + 4? 2 ? 2 k , c k = 1 ? d k (1-L M ) 2 + 1 b (1 -? d k ), ?1 L 2 xz ?D xz + 2(1 -?) 2 L 2 yz ?D yz + 2? 2 L 2 yy ? 2 D yy , ?2 2? 2 L 2 yx ? 1 D yx , ?3 L 2 xz ?D xz , L 2 M max L 2 xx D xx ? 1 + L 2 yx D yx ? 1 , L 2 xy D xy ? 2 + L 2 yy D yy ? 2 .</formula><p>Then, the sequence (z k ) k? generated by Algorithm 3 converges almost surely to some z ? zer T . Furthermore, if ? k = 1 k+r for some positive natural number r, then (F.44) is satisfied and it suffice to assume</p><formula xml:id="formula_156">2? ? -? k -2(? 1 + 2? 2 (1 + ?3 )) 1 b (1 -? k )? k+1 + 1 (1-L M ) 2 (? k+1 + 1)? k+1 + 1 1+b (1 -? k ) &lt; 0. (F.46)</formula><p>Proof of Theorem F.6 (almost sure convergence). We continue from the conditions in (F.38) which we restate here for convenience:</p><formula xml:id="formula_157">X 1 k (1 -? k ) 2 (1 + 4? 2 ? 2 k ) ? A k and X 2 k &gt; 0, (F.47) where X 1 k A k+1 + ? k (? k + 1 b (1 - 1 ? k (1-L M ) 2 )) X 2 k 2? ? -? k -? k 2(? 1 + 2? 2 (1 + ?3 ))A k+1 + 1 1+b (1 - 1 ? k (1-L M ) 2 ).</formula><p>(F.48) Under (F.47) the descent inequality (F.36) reduces to</p><formula xml:id="formula_158">E[U k+1 | F k ] -U k ? -? k X 2 k s k -S z k (z k ) 2 ? + ? 2 k ? k , (F.49)</formula><p>where</p><formula xml:id="formula_159">? k = 2X 1 k (? + (1 -? k ) 2 2? 2 ) + (1 + 2(? 1 + 2? 2 (1 + ?3 ))A k+1 ) ? 2 F .</formula><p>To show almost sure convergence through the Robbins-Siegmund supermartingale theorem (Bertsekas, 2011, Prop. 2), we need 0</p><formula xml:id="formula_160">? A k &lt; ? (inside U k ), X 2</formula><p>k and ? k to be nonnegative and furthermore</p><formula xml:id="formula_161">? k=0 ? 2 k ? k &lt; ?.</formula><p>To make a concrete choice of A k we solve the first (linear) inequality of (F.47) to equality with Lemma D.1. Letting</p><formula xml:id="formula_162">? k 1 + 4? 2 ? 2 k and c k ? k + 1 b (1 - 1 ? k (1-L M ) 2 ) we have A k+1 = ? k p=0 1 (1 -? p ) 2 ? p ? ? ? ? ? ? ? ? A 0 - k =0 c l ? l ? p=0 (1 -? p ) 2 ? p ? ? ? ? ? ? ? ? = ? k+1 ? k p=0 1 (1 -? p ) 2 ? p (F.50)</formula><p>where the last equality follows from picking</p><formula xml:id="formula_163">A 0 ? =0 c l ? l ? p=0 (1 -? p ) 2 ? p and ? k ? =k c l ? l ? p=0 (1 -? p ) 2 ? p (F.44) &lt; ?.</formula><p>This choice ensures A k ? 0 for all k and consequently ? k ? 0 and X 2 k ? 0 as long as (1-</p><formula xml:id="formula_164">1 ? k (1-L M ) 2 ) &gt; 0. It remains to show boundedness of the cumulative noise terms. ? k=0 ? 2 k ? k = 2? 2 F ? k=0 (? + (1 -? k ) 2 2? 2 )? 2 k A k+1 + ? 2 F (1 + 2(? 1 + 2? 2 (1 + ?3 ))) ? k=0 ? 2 k A k+1 + ? k=0 ? 3 k ? 2 F (? k + 1 b (1 - 1 ? k (1-L M ) 2 ))(? + (1 -? k ) 2 2? 2 ) ? 2? 2 F (? + 2? 2 ) ? k=0 ? 2 k A k+1 + ? 2 F (1 + 2(? 1 + 2? 2 (1 + ?3 ))) ? k=0 ? 2 k A k+1 + ? 2 F 1 (1-L M ) 2 (? + 2? 2 ) ? k=0 ? 3-d k + ? 2 F 1 b (? + 2? 2 ) ? k=0 ? 3 k (1 -? d k ) (F.51)</formula><p>where the last inequality follows from (1 -? k ) 2 ? 1 and picking</p><formula xml:id="formula_165">? k = 1 ? d k (1-L M ) 2 with d ? [0, 1] to ensure 1 - 1 ? k (1-L M ) 2 &gt; 0. Assuming ? ? k=0 ? 2 k A k+1 &lt; ? as in (F.44</formula><p>) is sufficient to show that (F.51) is bounded. This finishes the proof for the first claim of Theorem F.6.</p><p>To provide an instance of the sequence (? k ) k? that satisfy the assumptions, let r denote a positive natural number and set</p><formula xml:id="formula_166">? k = 1 k+r . (F.52) Then, ? p=0 (1 -? p ) 2 ? p = ? p=0 ( p+r-1 p+r ) 2 (1 + 4? 2 ? 2 p ) = (r-1) 2 ( +r) 2 (1 + 4? 2 ? p=0 1 (p+r) 2 ) = (r-1) 2 ( +r) 2 ? with ? = (1 + r 2(1-) 4? 2 ) ? [1, 1 + 4? 2 ] owing to the fact that ? p=0 1 p+r = r 1-. It follows that, for any K ? 0 K =0 c ? ? p=0 (1 -? p ) 2 ? p = K =0 c (r-1) 2 ( +r) 3 ? .</formula><p>Plugging the value of c and ? k from (F.50) and (F.51) we obtain that A 0 is finite valued since b &gt; 0,</p><formula xml:id="formula_167">L M &lt; 1 and ? =0 ? ( +r) 3 = ? =0 1 ( +r) 3-d (1-L M ) 2 &lt; ? owing to the fact that d ? 1.</formula><p>Published as a conference paper at ICLR 2023 Moreover,</p><formula xml:id="formula_168">A k+1 = (k + r) 2 (r -1) 2 ? k ? ? ? ? ? ? ? ? A 0 - k =0 (r-1) 2 ? ( +r) 3 c ? ? ? ? ? ? ? ? = (k + r) 2 ? =k+1 ? ( +r) 3 ? k c = 1 ? 2 k ? =k+1 ? 3 c ? ? k ? 1 ? 2 k ? =k+1 ? 3 c (F.53)</formula><p>where the last inequality follows from ? i ? j ? 1 1+4? 2 ? 1 for any i, j ? . On the other hand, for e &gt; 1 we have the following bound (F.54) Therefore, recalling</p><formula xml:id="formula_169">c k = ? -d k 1 (1-L M ) 2 + 1 b (1 -? d k ), it follows from (F.53) that ? k A k+1 ? 1 ? k ? =k+1 ? 2 k 1 b (1 -? d k ) + ? 3-d k 1 (1-L M ) 2 (F.54) ? 1 b (1 -? d k ) 1 2(k+1+r) + 1 (1-L M ) 2 1 (2-d)(k+1+r) 1-d 1 k+1+r + 1 1 k+1+r = 1 b (1 -? d k )? k+1 + 1 (1-L M ) 2 (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 . (F.55)</formula><p>In turn, this inequality ensures that ? as defined in eq. (F.44) is finite. To see this note that</p><formula xml:id="formula_170">? = ? k=0 A k+1 ? 2 k (F.55) ? ? k=0 1 b (1 -? d k )? k+1 + 1 (1-L M ) 2 (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 ? k ? ? ? k=0 ? 2 k &lt; ?.</formula><p>It remains to simplify the remaining condition (F.45). Using (F.55</p><formula xml:id="formula_171">) 2? ? -? k -? k 2(? 1 + 2? 2 (1 + ?3 ))A k+1 + 1 1+b (1 -? d k ) ? 2? ? -? k -2(? 1 + 2? 2 (1 + ?3 )) 1 b (1 -? d k )? k+1 + 1 (1-L M ) 2 (2-d) ? 1-d k+1 (? k+1 + 1)? k+1 + 1 1+b (1 -? d k ) = 2? ? -? k -2(? 1 + 2? 2 (1 + ?3 )) 1 b (1 -? k )? k+1 + 1 (1-L M ) 2 (? k+1 + 1)? k+1 + 1 1+b (1 -? k ) (F.46)</formula><p>&lt; 0 where the equality follows from choosing d = 1. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Explanation of bias-correction term</head><p>Consider the naive analysis which would track h k . By the definition of zk in (8.4b) and H k (z k ) we would have h k -H k (z k ) + P k (z k ) -Pk (z k , ?k ) ? F(z k ) + A(z k ). Hence, assuming zero mean and using the weak MVI from Assumption I(iii),</p><formula xml:id="formula_172">E[ h k -H k (z k ), zk -z | F k ] = E[ h k -H k (z k ) + P k (z k ) -Pk (z k , ? k ), zk -z | F k ] ? E[? h k -H k (z k ) + P k (z k ) -Pk (z k , ? k ) 2 | F k ] . (F.56)</formula><p>To proceed we could apply Young's inequality, but this would produce a noise term, which would propagate to the descent inequality in (F.36) with a ? k factor in front. To show convergence we would instead need a smaller factor of ? 2 k . To avoid this error term entirely we instead do a change of variables with s k h k -Pz k (z k , ? k ) such that,</p><formula xml:id="formula_173">h k ? Pz k (z k , ? k ) + Az k ? h k -Pz k (z k , ? k ) ? Az k ? s k ? Az k . (F.57)</formula><p>This make application of Assumption I(iii) unproblematic, but affects the choice of the biascorrection term, since the analysis will now apply to s k . If we instead of the careful choice of h k in (8.4a) had made the choice</p><formula xml:id="formula_174">h k = Pz k (z k , ? k ) -F(z k , ? k ) + (1 -? k )(h k-1 -Pz k-1 (z k-1 , ? k ) + F(z k-1 , ? k )) (F.58) then s k = Pz k (z k , ? k ) -F(z k , ? k ) -Pz k (z k , ? k ) + (1 -? k )(s k-1 -Pz k-1 (z k-1 , ? k ) + F(z k-1 , ? k ) -Pz k-1 (z k-1 , ? k-1 )).</formula><p>Notice how the latter term is evaluated under ? k-1 instead of ? k . The choice in (8.4a) resolves this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Negative weak Minty variational inequality</head><p>In this section we consider the problem of finding a zero of the single-valued operator F (with the set-valued operator A ? 0). Observe that the weak MVI in Assumption I(iii), Fz, zz ? ? Fz 2 , for all z ? n , is not symmetric and one may instead consider that the assumption holds for -F. As we will see below this simple observation leads to nontrivial problem classes extending the reach of extragradient-type methods both in the deterministic and stochastic settings.</p><p>Assumption VIII (negative weak MVI). There exists a nonempty set S ? zer T such that for all z ? S and some ? ? (-?, 1 /2L) Fz, zz ? ? Fz 2 , for all z ? n . (G.1)</p><p>Under this assumption the algorithm of <ref type="bibr" target="#b29">Pethick et al. (2022)</ref> leads to the following modified iterates: zk = z k +? k Fz k , (G.2)</p><formula xml:id="formula_175">z k+1 = z k + ? k ? k (H k zk -H k z k ) = z k +? k ? k ? k F zk , where H k id+? k F (G.3)</formula><p>We next consider the lower bound example of <ref type="bibr">(Pethick et al., 2022, Ex. 5)</ref> to show that despite the condition for weak MVI being violated for b smaller than a certain threshold, the negative weak MVI in Assumption VIII holds for any negative b and thus the extragradient method applied to -F is guaranteed to converge. ). The range for ? is nonempty if b &gt; -a ? 3 while this is not an issue for ? which allows any negative b.</p><p>We complete this section with a corollary to Theorem 6.3 when replacing weak MVI assumption with Assumption VIII.</p><p>Corollary G.1. Suppose that Assumptions I(i) and I(ii), Assumptions II, III and VIII hold. Let (z k ) k? denote the sequence generated by Algorithm 1 applied to -F. Then, the claims of Theorem 6.3 hold true.  The (projected) (SEG+) method needs to take ? arbitrarily small to guarantee convergence to an arbitrarily small neighborhood. We show an instance satisfying the weak MVI where ? cannot be taken arbitrarily small. The objective is ?(x, y) = ?(x -0.9, y -0.9) under box constraints (x, y) ? ? 1 with ? from Example 2 where L = 1 and ? = -1 /10L. The unique stationary point (x , y ) = (0.9, 0.9) lies in the interior, so even Fz can be driven to zero. Taking ? smaller does not make the neighborhood smaller as oppose to the monotone case in Figure <ref type="figure" target="#fig_6">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Experiments</head><p>Example 3 (Constrained minimax <ref type="bibr">(Pethick et al., 2022, Ex. 4</ref>)). Consider minimize where ?(z) = 2z 6 21 -z 4 3 + z 2 3 .</p><p>In both Example 2 and Example 3 the operator F is defined as Fz = (? x ?(x, y), -? y ?(x, y)).</p><p>To simulate a stochastic setting in all examples, we consider additive Gaussian noise, i.e. F(z, ?) = Fz + ? where ? ? N(0, ? 2 I). We choose ? = 0.1 and initialize with z 0 = 1 if not specified otherwise. The default configuration is ? = 1 /2L F with ? k = 1 /18?(k/c+1), c = 100 and ? k = ? k for diminishing stepsize schemes and ? = 1 /18 for fixed stepsize schemes. We make two exceptions: Figure <ref type="figure" target="#fig_6">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Additional algorithmic details</head><p>For the constrained setting in Figure <ref type="figure" target="#fig_6">1</ref>, we consider two extensions of (SEG+). One variant uses a single application of the resolvent as suggested by <ref type="bibr" target="#b29">Pethick et al. (2022)</ref>, zk = (id + ?A) -1 (z k? F(z k , ? k )) with ? k ? P z k+1 = z k + ? k (z kz k ) -?( F(z k , ?k ) -F(z k , ? k )) with ?k ? P (P 1 SEG+)</p><p>The other variant applies the resolvent twice as in stochastic Mirror-Prox <ref type="bibr" target="#b20">(Juditsky et al., 2011)</ref>, zk = (id + ?A) -1 (z k? F(z k , ? k )) with ? k ? P z k+1 = (id + ? k ?A) -1 (z k? k ? F(z k , ?k )) with ?k ? P (P 2 SEG+)</p><p>When applying (SEG) to constrained settings we similarly use the following projected variants: zk = (id + ? k ?A) -1 (z k? k ? F(z k , ? k )) with ? k ? P z k+1 = (id + ? k ?A) -1 (z k? k ? F(z k , ?k )) with ?k ? P (PSEG)   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of methods in the unconstrained setting of Example 2 (left) and the constrained setting of Example 3 (right). Notice that only BC-SEG+ and BC-PSEG+ converges properly while (SEG) diverges, (PSEG) cycles and both (SF-EG+) and (SF-PEG+) only converge to a neighborhood. BC-(P)SEG+ is guaranteed to converge with probability 1 as established through Theorems 6.3 and F.6.</figDesc><graphic url="image-3.png" coords="9,108.00,66.92,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>An operator A : n ? d maps each point x ? n to a subset Ax ? d , where the notation A(x) and Ax will be used interchangably. We denote the domain of A by dom A {x ? n | Ax ?}, its graph by gph A {(x, y) ? n ? d | y ? Ax}. The inverse of A is defined through its graph, gph A -1 {(y, x) | (x, y) ? gph A} and the set of its zeros by zer A {x ? n | 0 ? Ax}. Definition B.1 ((co)monotonicity Bauschke et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>D.23) A reasonable choice for the Young parameter ? k is to choose ? k = ?L F ? d k for some d ? [0, 1]. (D.24) The rational for this choice will become more clear in what follows. The first inequality in (D.22) is linear and we can solve it to equality by Lemma D.1. Let A 0 ? =0 c l ? l ? p=0 (1 -? p ) 2 = ? 0 c k and ? k be as in Assumption VI(ii). Then, Lemma D.1 yields</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>. The choice of b is substituted into (D.1), (D.20) and C of Theorem D.3. This completes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 1 .</head><label>1</label><figDesc>Consider(Pethick et al., 2022, Ex. 5) b &lt; 0 and a &gt; 0. The associated F is a linear mapping. For a linear mapping M, Assumption VIII holds if1 2 (M + M ) -?M M 0, ? ? (-?, 1 /2L) While Assumption I(iii) holds if 1 2 (M + M ) -?M M 0, ? ? (-1 /2L, ?).For this example L = ? a 2 + b 2 and F(z) = Mz (bx + ay, -ax + by). Since M is a bisymmetric linear mapping, M M = (a 2 + b 2 )I which according to the above characterizations implies ? ? (-1 2L , b a 2 +b 2 ], ? ? [ b a 2 +b 2 , 1 2L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>H. 1</head><label>1</label><figDesc>Synthetic exampleExample 2 (Unconstrained quadratic game(Pethick et al., 2022, Ex.  5)). Consider, minimize x?R maximize y?R ?(x, y) := axy + b 2 x 2 -b 2 y 2 , (H.1) where a ? R + and b ? R. The problem constants in Example 2 can easily be computed as ? = b a 2 +b 2 and L = ? a 2 + b 2 . We can rewrite Example 2 in terms of L and ? by choosing a = L 2 -L 4 ? 2 and b = L 2 ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>Figure3: The (projected) (SEG+) method needs to take ? arbitrarily small to guarantee convergence to an arbitrarily small neighborhood. We show an instance satisfying the weak MVI where ? cannot be taken arbitrarily small. The objective is ?(x, y) = ?(x -0.9, y -0.9) under box constraints (x, y) ? ? 1 with ? from Example 2 where L = 1 and ? = -1 /10L. The unique stationary point (x , y ) = (0.9, 0.9) lies in the interior, so even Fz can be driven to zero. Taking ? smaller does not make the neighborhood smaller as oppose to the monotone case in Figure1.</figDesc><graphic url="image-5.png" coords="34,146.35,81.86,316.81,237.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>y) := xy + ?(x) -?(y), (GlobalForsaken)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>uses the slower decay c = 1000 when ? = 0.1 and Figure 3 uses c = 5000 for ? = 0.01 (and otherwise c = 1000) to ensure fast enough convergence. When the aggressive stepsize schedule is used then ? k = 1 /18? ? k/100+1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Instead of taking ? k ? 1 /k (for which almost sure convergence is established through Theorems 6.3 and F.6) we take ? k ? 1 / ? k as permitted in Theorems 6.1 and 7.1. We consider the example provided in Figure 1 (top row) and the two examples from Figure 2 (bottom row). Under this more aggressive stepsize schedule the guarantee is only in expectation over the iterates which is also apparent from the relatively large volatility in comparison with Figures 1 and 2.</figDesc><graphic url="image-8.png" coords="35,108.00,231.36,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>and (EG+) (using stochastic feedback denoted SF) zk = (id + ?A) -1 (z k? F(z k , ? k )) with ? k ? Pz k+1 = (id + ??A) -1 (z k?? F(z k , ?k )) with ?k ? P (SF-PEG+)which we in the unconstrained case (A ? 0) refer to as (SF-EG+) as defined below.zk = z k? F(z k , ? k ) with ? k ? P z k+1 = z k?? F(z k , ?k ) with ?k ? P (SF-EG+)I Comparison with variance reductionConsider the case where the expectation comes in the form a finite sum, case the averaged Lipschitz constant F F scales proportionally to the number of elements N squared, i.e. L F = ?( ? NL F ). It is easy to construct such an example by taking one elements to have Lipschitz constant NL while letting the remaining elements have Lipschitz constant L. Recalling the definition in Assumption III,L 2 F = N 2 L 2 N + N-1 N L 2 ? NL 2 while the average becomes L F = NL N + N-1 N L ? 2L so L F ? ? N /2L F .Thus, L F can be ? N times larger than L F , leading to a potentially strict requirement on the weak MVI parameter ? &gt; -L F/2 for variance reduction methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the results. The second row is obtained as special cases of the first row. the unconstrained and smooth setting Appendix C treats convergences of (SEG+) for the restricted case where F is linear. Appendix D shows both random iterate results and almost sure convergence of Algorithm 1. Theorems 6.1 and 6.3 in the main body are implied by the more general results in this section, which preserves certain free parameters and more general stepsize requirements. Appendices E and F moves beyond the unconstrained and smooth case by showing convergence for instances of the template scheme (8.1). Almost sure convergence is established in Theorem F.6. The analysis of Algorithm 3 in Appendix F applies to Algorithm 2, but for completeness we establish convergence for general F separately in Appendix E. The relationship between the theorems are presented in Table1.</figDesc><table><row><cell></cell><cell cols="2">Unconstrained &amp; smooth (A ? 0)</cell><cell cols="2">Constrained (A 0)</cell></row><row><cell></cell><cell>Random iterate</cell><cell>Last iterate</cell><cell>BC-PSEG+</cell><cell>NP-PDHG</cell></row><row><cell>Appendix</cell><cell>Theorem D.2</cell><cell>Theorem D.3</cell><cell cols="2">Theorem E.2 Theorem F.5</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Main paper</cell><cell>Theorem 6.1</cell><cell>Theorem 6.3</cell><cell cols="2">Theorem 7.1 Theorem 8.2</cell></row><row><cell>A Prelude</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>For</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S u (z) = H u (z) -Q u (z) S u (z; z) = H u (z) -Q u (z) (F.16)where Q u (z) and H u are as defined in Section 8.In contrast with the unconstrained smooth case we will rely on a slightly different potential function, namely,U k+1 z k+1z 2 ? -1 + A k+1 s k -S z k (z k ; zk ) 2 ? + B k+1 z k+1z k 2 ?where (A k ) k? and (B k ) k? are positive scalar parameters to be identified.We start by writing out one step of the updatez k+1z 2 ? -1 = z kz 2 ? -1 -2? k h k -?k , z kz + ? 2 k h k -?k 2 ? (F.17) = z kz 2 ? -1 -2? k s k -? k , z kz + ? 2 k s k -? k 2 ?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>in (F.25) let s k x be the primal components of s k in what follows. Recall that A decomposes as specified in Section 8, such that we can write s kx ? ? -11 xk + A 1 ( xk ). By monotonicity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>+ (1 -? k ) ? x ?(z k , ? k ) -? x ?(z k-1 , ? k ) ? 1 + ? k ? -1 1 x k -? x ?(z k , ? k )s k-1</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>11 Acknowledgments and disclosure of funding This project has received funding from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement n?725594 -time-data). This work was supported by the <rs type="funder">Swiss National Science Foundation (SNSF)</rs> under grant number <rs type="grantNumber">200021_205011</rs>. The work of the third and fourth author was supported by the <rs type="funder">Research Foundation Flanders (FWO)</rs> postdoctoral grant <rs type="grantNumber">12Y7622N</rs> and research projects <rs type="grantNumber">G081222N</rs>, <rs type="grantNumber">G033822N</rs>, <rs type="grantNumber">G0A0920N</rs>; <rs type="funder">Research Council KU Leuven</rs> <rs type="grantNumber">C1</rs> project No. <rs type="grantNumber">C14/18/068</rs>; European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Sk?odowska-Curie</rs> grant agreement No. <rs type="grantNumber">953348</rs>. The work of <rs type="person">Olivier Fercoq</rs> was supported by the <rs type="funder">Agence National de la Recherche</rs> grant <rs type="grantNumber">ANR-20-CE40-0027</rs>, <rs type="funder">Optimal Primal-Dual Algorithms (APDO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Um4uSZg">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_Tu8qrya">
					<idno type="grant-number">200021_205011</idno>
				</org>
				<org type="funding" xml:id="_Uhkj4Tg">
					<idno type="grant-number">12Y7622N</idno>
				</org>
				<org type="funding" xml:id="_6uGPXgJ">
					<idno type="grant-number">G081222N</idno>
				</org>
				<org type="funding" xml:id="_ZMYeFzM">
					<idno type="grant-number">G033822N</idno>
				</org>
				<org type="funding" xml:id="_AqnKh2m">
					<idno type="grant-number">G0A0920N</idno>
				</org>
				<org type="funding" xml:id="_e4XFTvU">
					<idno type="grant-number">C1</idno>
				</org>
				<org type="funding" xml:id="_xHbqJKc">
					<idno type="grant-number">C14/18/068</idno>
					<orgName type="grant-name">Marie Sk?odowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_jma8ahw">
					<idno type="grant-number">953348</idno>
				</org>
				<org type="funding" xml:id="_aCUbtkD">
					<idno type="grant-number">ANR-20-CE40-0027</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table of Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Comparison with variance reduction</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Alacaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Malitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08352</idno>
		<title level="m">Stochastic variance reduction for variational inequality methods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Arjevani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02365</idno>
		<title level="m">Lower bounds for non-convex stochastic optimization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convex analysis and monotone operator theory in Hilbert spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">L</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName><surname>Combettes</surname></persName>
		</author>
		<idno>978-3-319-48310-8</idno>
	</analytic>
	<monogr>
		<title level="m">CMS Books in Mathematics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalized monotone operators and their averaged resolvents</title>
		<author>
			<persName><forename type="first">Walaa</forename><forename type="middle">M</forename><surname>Heinz H Bauschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfu</forename><surname>Moursi</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="74" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental proximal methods for large scale convex optimization</title>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="163" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Beznosikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Gorbunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Loizou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07262</idno>
		<title level="m">Stochastic gradient descent-ascent: Unified theory and new efficient methods</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Solving nonconvex-nonconcave min-max problems exhibiting weak minty solutions</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>B?hm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12247</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minibatch forwardbackward-forward methods for solving stochastic variational inequalities</title>
		<author>
			<persName><forename type="first">Panayotis</forename><surname>Radu Ioan Bo?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phan</forename><forename type="middle">Tu</forename><surname>Staudigl</surname></persName>
		</author>
		<author>
			<persName><surname>Vuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="139" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A stochastic Halpern iteration with variance reduction for stochastic monotone inclusion problems</title>
		<author>
			<persName><forename type="first">Xufeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaobing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crist?bal</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Diakonikolas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09436</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm for convex problems with applications to imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="145" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="4937" to="4948" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proximal methods for cohypomonotone operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teemu</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><surname>Pennanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="731" to="742" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The complexity of constrained min-max optimization</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Skoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Zampetakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 53rd Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1466" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient methods for structured nonconvex-nonconcave min-max optimization</title>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">Junchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic extragradient: General analysis and improved rates</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Gorbunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Loizou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7865" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exponential lower bounds for finding Brouwer fixed points</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vavasis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Symposium on Foundations of Computer Science</title>
		<meeting>the 28th Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the convergence of single-call stochastic extra-gradient methods</title>
		<author>
			<persName><forename type="first">Yu-Guan</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Iutzeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Malick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayotis</forename><surname>Mertikopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling</title>
		<author>
			<persName><forename type="first">Yu-Guan</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Iutzeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Malick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panayotis</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10162</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Solving variational inequalities with stochastic mirror-prox algorithm</title>
		<author>
			<persName><forename type="first">Anatoli</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Tauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="58" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal stochastic extragradient schemes for pseudomonotone stochastic variational inequality problems and their variants</title>
		<author>
			<persName><forename type="first">Aswin</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Uday</surname></persName>
		</author>
		<author>
			<persName><surname>Shanbhag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="779" to="820" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asymmetric forward-backward-adjoint splitting for solving monotone inclusions involving three operators</title>
		<author>
			<persName><forename type="first">Puya</forename><surname>Latafat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Patrinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="93" />
			<date type="published" when="2017-09">Sep 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fast extra gradient methods for smooth structured nonconvexnonconcave minimax problems</title>
		<author>
			<persName><forename type="first">Sucheol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02326</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging</title>
		<author>
			<persName><forename type="first">Chris Junchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9793" to="9826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic hamiltonian gradient methods for smooth games</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6370" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">19095-19108, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting stochastic extragradient</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Shulgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Malitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4573" to="4582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Marten</forename><surname>Lam M Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><surname>Dzung T Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Ha Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayant</forename><forename type="middle">R</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><surname>Kalagnanam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07648</idno>
		<title level="m">Finite-sum smooth optimization with SARAH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pethick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puya</forename><surname>Latafat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Patrinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Tyrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rockafellar</forename></persName>
		</author>
		<title level="m">Convex analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimistic dual extrapolation for coherent non-monotone variational inequalities</title>
		<author>
			<persName><forename type="first">Chaobing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14303" to="14314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A modified forward-backward splitting method for maximal monotone mappings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="446" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems</title>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09621</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
