<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Graph Encoder for Attributed Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-03">3 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>zhoujie18@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Graph Encoder for Attributed Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-03">3 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403140</idno>
					<idno type="arXiv">arXiv:2007.01594v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>attributed graph embedding</term>
					<term>graph convolutional networks</term>
					<term>Laplacian smoothing</term>
					<term>adaptive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attributed graph embedding, which learns vector representations from graph topology and node features, is a challenging task for graph analysis. Recently, methods based on graph convolutional networks (GCNs) have made great progress on this task. However, existing GCN-based methods have three major drawbacks. Firstly, our experiments indicate that the entanglement of graph convolutional filters and weight matrices will harm both the performance and robustness. Secondly, we show that graph convolutional filters in these methods reveal to be special cases of generalized Laplacian smoothing filters, but they do not preserve optimal low-pass characteristics. Finally, the training objectives of existing algorithms are usually recovering the adjacency matrix or feature matrix, which are not always consistent with real-world applications. To address these issues, we propose Adaptive Graph Encoder (AGE), a novel attributed graph embedding framework. AGE consists of two modules: (1) To better alleviate the high-frequency noises in the node features, AGE first applies a carefully-designed Laplacian smoothing filter. (2) AGE employs an adaptive encoder that iteratively strengthens the filtered features for better node embeddings. We conduct experiments using four public benchmark datasets to validate AGE on node clustering and link prediction tasks. Experimental results show that AGE consistently outperforms state-of-the-art graph embedding methods considerably on these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b14">[15]</ref><p>. The components we argue about are marked in red blocks: Entanglement of the filters and weight matrices, design of the filters, and the reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Attributed graphs are graphs with node attributes/features and are widely applied to represent network-structured data in social networks <ref type="bibr" target="#b11">[12]</ref>, citation networks <ref type="bibr" target="#b15">[16]</ref>, recommendation systems <ref type="bibr" target="#b36">[37]</ref>, etc. For tasks analyzing attributed graphs, including node classification, link prediction and node clustering, plenty of machine learning techniques are developed. However, because of the complex high-dimensional non-Euclidean graph structure and various node features, this task imposes the challenge of jointly capturing structure and feature information on machine learning approaches.</p><p>Representation learning methods on graphs, also known as graph embedding methods, have emerged as general approaches in graph learning area. This kind of approaches aims to learn lowdimensional representations to encode graph structural information. Early graph embedding approaches are based on Laplacian eigenmaps <ref type="bibr" target="#b20">[21]</ref>, matrix factorization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, and random walks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. However, these methods are also limited because of their shallow architecture.</p><p>More recently, there has been a surge of approaches that focus on deep learning on graphs. Specifically, approaches from the family of graph convolutional networks (GCNs) <ref type="bibr" target="#b15">[16]</ref> have made great progress in many graph learning tasks <ref type="bibr" target="#b38">[39]</ref> and strengthen the representation power of graph embedding algorithms. In this paper, we will study the attributed graph embedding problem, which is one of the most important problems in deep graph learning and GCN-based methods have also made great progress on it. Among these methods, most of them are based on graph autoencoder (GAE) and variational graph autoencoder (VGAE) <ref type="bibr" target="#b14">[15]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, they comprise a GCN encoder and a reconstruction decoder. Nevertheless, these GCN-based methods have three major drawbacks:</p><p>Firstly, a GCN encoder consists of multiple graph convolutional layers, and each layer contains a graph convolutional filter (H in Figure <ref type="figure" target="#fig_0">1</ref>), a weight matrix (W 1 , W 2 in Figure <ref type="figure" target="#fig_0">1</ref>) and an activation function. However, previous work <ref type="bibr" target="#b34">[35]</ref> demonstrates that the entanglement of the filters and weight matrices provides no performance gain for semi-supervised graph representation learning, and even harms training efficiency since it deepens the paths of back-propagation. In this work, we further extend this conclusion to unsupervised scenarios by controlled experiments, showing that our disentangled architecture performs better and more robust than entangled models (Section 5.3).</p><p>Secondly, considering the graph convolutional filters, previous research <ref type="bibr" target="#b17">[18]</ref> shows in theory that they are actually Laplacian smoothing filters <ref type="bibr" target="#b27">[28]</ref> applied on the feature matrix for low-pass denoising. But we show that existing graph convolutional filters are not optimal low-pass filters since they can not filter out noises in some high-frequency intervals. Thus, they can not reach the best smoothing effect (Section 3.3.3).</p><p>Thirdly, we also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> or feature matrix <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>) are not compatible with real-world applications. To be specific, reconstructing adjacency matrix literally sets the adjacency matrix as the ground truth pairwise similarity, while it is not proper for the lack of feature information. Recovering the feature matrix, however, will force the model to remember high-frequency noises in features, and thus be inappropriate as well.</p><p>Motivated by such observations, we propose Adaptive Graph Encoder (AGE), a unified framework for attributed graph embedding. To disentangle the filters and weight matrices, AGE consists of two modules: (1) A well-designed non-parametric Laplacian smoothing filter to perform low-pass filtering in order to get smoothed features. (2) An adaptive encoder to learn more representative node embeddings. To replace the reconstruction training objectives, we employ adaptive learning <ref type="bibr" target="#b5">[6]</ref> in this step, which selects training samples from the pairwise similarity matrix and finetunes the embeddings iteratively. The code and data are available on https://github.com/thunlp/AGE.</p><p>Our contributions can be summarized as follows:</p><p>• Analysis: We make a detailed analysis of the mechanism of graph convolutional filters from the perspective of signal smoothing on graphs and Laplacian smoothing. The analysis helps us design a proper Laplacian smoothing filter to better alleviate high-frequency noises. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Conventional Graph Embedding</head><p>Early researches on graph embedding merely focus on finding node similarity with graph structure. Methods based on dimension reduction aim to project the high-dimensional adjacency matrix to low-dimensional latent embedding space. Laplacian eigenmaps <ref type="bibr" target="#b20">[21]</ref> and matrix factorization <ref type="bibr" target="#b2">[3]</ref> are two widely used algorithms for these methods. Another line of researches manages to learn node embeddings with a particular objective function. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> learn node embeddings by generating random walks and input the sequences into SkipGram model <ref type="bibr" target="#b16">[17]</ref>, assuming that similar nodes tend to cooccur in same sequences. Other models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> can be concluded by an encoder-decoder framework <ref type="bibr" target="#b10">[11]</ref>, while they differ from model structure and training objectives.</p><p>Taking node features into account, there are several works make adjustments to encode structural and content information simultaneously. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> are matrix factorization extensions that add feature-related regularization terms. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> model features as latent variables in Bayesian networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GCN-based Graph Embedding</head><p>As mentioned in the introduction, due to the strong representation power of graph convolutional networks (GCNs) <ref type="bibr" target="#b15">[16]</ref>, there are several GCN-based approaches for attributed graph embedding and they have achieved state-of-the-art. For unsupervised graph embedding that lacks label information, GCN-based methods can be categorized into two groups by their optimization objectives.</p><p>Reconstruct the adjacency matrix. This kind of approaches forces the learned embeddings to recover their localized neighborhood structure. Graph autoencoder (GAE) and variational graph autoencoder (VGAE) <ref type="bibr" target="#b14">[15]</ref> learn node embeddings by using GCN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type="bibr" target="#b22">[23]</ref> exploits adversarially regularized method to learn more robust node embeddings. <ref type="bibr" target="#b30">[31]</ref> further employs graph attention networks <ref type="bibr" target="#b29">[30]</ref> to differentiate the importance of the neighboring nodes to a target node.</p><p>Reconstruct the feature matrix. This kind of models is autoencoders for the node feature matrix while the adjacency matrix merely serves as a filter. <ref type="bibr" target="#b31">[32]</ref> leverages marginalized denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type="bibr" target="#b23">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplacian smoothing in the encoder. The authors claim that Laplacian sharpening is a process that makes the reconstructed feature of each node away from the centroid of its neighbors to avoid over-smoothing. However, as we will show in the next section, there exists high-frequency noises in raw node features, which harm the quality of learned embeddings.</p><p>In this section, we first formalize the embedding task on attributed graphs. Then we present our proposed Adaptive Graph Encoder (AGE) algorithm. Specifically, we first design an effective graph filter to perform Laplacian smoothing on node features. Given the smoothed node features, we further develop a simple node representation learning module based on adaptive learning <ref type="bibr" target="#b5">[6]</ref>. Finally, the learned node embeddings are used for downstream tasks such as node clustering and link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formalization</head><p>Given an attributed graph G = (V, E, X), where</p><formula xml:id="formula_0">V = {v 1 , v 2 , • • • , v n }</formula><p>is the vertex set with n nodes in total, E is the edge set, and</p><formula xml:id="formula_1">X = [x 1 , x 2 . • • • , x n ]</formula><p>T is the feature matrix. The topology structure of graph G can be denoted by an adjacency matrix A = {a i j } ∈ R n×n , where</p><formula xml:id="formula_2">a i j = 1 if (v i , v j ) ∈ E, indicating there is an edge from node v i to node v j . D = diag(d 1 , d 2 , • • • , d n ) ∈ R n×n denotes the degree matrix of A, where d i = v j ∈V a i j is the degree of node v i . The graph Laplacian matrix is defined as L = D − A.</formula><p>The purpose of attributed graph embedding is to map nodes to low-dimensional embeddings. We take Z as the embedding matrix and the embeddings should preserve both the topological structure and feature information of graph G.</p><p>For downstream tasks, we consider node clustering and link prediction. The node clustering task aims to partition the nodes into</p><formula xml:id="formula_3">m disjoint groups {G 1 , G 2 , • • • , G m },</formula><p>where similar nodes should be in the same group. The link prediction task requires the model to predict whether there is a potential edge existing between two given nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>The framework of our model is shown in Figure <ref type="figure" target="#fig_1">2</ref>. It consists of two parts: a Laplacian smoothing filter and an adaptive encoder.</p><p>• Laplacian Smoothing Filter: The designed filter H serves as a low-pass filter to denoise the high-frequency components of the feature matrix X. The smoothed feature matrix X is taken as input of the adaptive encoder. • Adaptive Encoder: To get more representative node embeddings, this module builds a training set by adaptively selecting node pairs which are highly similar or dissimilar. Then the encoder is trained in a supervised manner. After the training process, the learned node embedding matrix Z is used for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Laplacian Smoothing Filter</head><p>The basic assumption for graph learning is that nearby nodes on the graph should be similar, thus node features are supposed to be smooth on the graph manifold. In this section, we first explain what smooth means. Then we give the definition of the generalized Laplacian smoothing filter and show that it is a smoothing operator.</p><p>Finally, we answer how to design an optimal Laplacian smoothing filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Analysis of Smooth Signals.</head><p>We start with interpreting smooth from the perspective of graph signal processing. Take x ∈ R n as a graph signal where each node is assigned with a scalar. Denote the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laplacian Smoothing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head><p>Select training pairs S u p e r v i s e d l e a r n i n g S i m i l a r i t y m a t r i x filter matrix as H. To measure the smoothness of graph signal x, we can calculate the Rayleigh quotient <ref type="bibr" target="#b12">[13]</ref> over the graph Laplacian L and x:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Encoder</head><formula xml:id="formula_4">R(L, x) = x ⊺ Lx x ⊺ x = (i, j)∈E (x i − x j ) 2 i ∈V x 2 i . (<label>1</label></formula><formula xml:id="formula_5">)</formula><p>This quotient is actually the normalized variance score of x. </p><formula xml:id="formula_6">= diag(λ 1 , λ 2 , • • • , λ n )</formula><p>is a diagonal matrix of eigenvalues. Then the smoothness of eigenvector u i is given by</p><formula xml:id="formula_7">R(L, u i ) = u ⊺ i Lu i u ⊺ i u i = λ i .<label>(2)</label></formula><p>Eq. ( <ref type="formula" target="#formula_7">2</ref>) indicates that smoother eigenvectors are associated with smaller eigenvalues, which means lower frequencies. Thus we decompose signal x on the basis of L based on Eq. ( <ref type="formula" target="#formula_4">1</ref>) and Eq. ( <ref type="formula" target="#formula_7">2</ref>):</p><formula xml:id="formula_8">x = Up = n i=1 p i u i .<label>(3)</label></formula><p>where p i is the coefficient of eigenvector u i . Then the smoothness of x is actually</p><formula xml:id="formula_9">R(L, x) = x ⊺ Lx x ⊺ x = n i=1 p 2 i λ i n i=1 p 2 i .<label>(4)</label></formula><p>Therefore, to get smoother signals, the goal of our filter is filtering out high-frequency components while preserving low-frequency components. Because of its high computational efficiency and convincing performance, Laplacian smoothing filters <ref type="bibr" target="#b27">[28]</ref> are often utilized for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Generalized Laplacian Smoothing</head><p>Filter. As stated by <ref type="bibr" target="#b27">[28]</ref>, the generalized Laplacian smoothing filter is defined as</p><formula xml:id="formula_10">H = I − kL,<label>(5)</label></formula><p>where k is real-valued. Employ H as the filter matrix, the filtered signal x is present by</p><formula xml:id="formula_11">x = Hx = U(I − kΛ)U −1 Up = n i=1 (1 − kλ i )p i u i = n i=1 p′ i u i . (6)</formula><p>Hence, to achieve low-pass filtering, the frequency response function 1 −kλ should be a decrement and non-negative function. Stacking up t Laplacian smoothing filters, we denote the filtered feature matrix X as X = H t X.</p><p>(7) Note that the filter is non-parametric at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>The Choice of k. In practice, with the renormalization trick Ã = I + A, we employ the symmetric normalized graph Laplacian</p><formula xml:id="formula_12">Lsym = D− 1 2 L D− 1 2 ,<label>(8)</label></formula><p>where D and L are degree matrix and Laplacian matrix corresponding to Ã. Then the filter becomes</p><formula xml:id="formula_13">H = I − k Lsym .<label>(9)</label></formula><p>Notice that if we set k = 1, the filter becomes the GCN filter.</p><p>For selecting optimal k, the distribution of eigenvalues Λ (obtained from the decomposition of Lsym = Ũ Λ Ũ−1 ) should be carefully discovered.</p><p>The smoothness of x is</p><formula xml:id="formula_14">R(L, x) = x⊺ L x x⊺ x = n i=1 p′ 2 i λ i n i=1 p′ 2 i . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Thus p′ 2 i should decrease as λ i increases. We denote the maximum eigenvalue as λ max . Theoretically, if k &gt; 1/λ max , the filter is not low-pass in the (1/k, λ max ] interval because p′ 2 i increases in this interval; Otherwise, if k &lt; 1/λ max , the filter can not denoise all the high-frequency components. Consequently, k = 1/λ max is the optimal choice.</p><p>It has been proved that the range of Laplacian eigenvalues is between 0 and 2 <ref type="bibr" target="#b6">[7]</ref>, hence GCN filter is not low-pass in the (1, 2] interval. Some work <ref type="bibr" target="#b30">[31]</ref> accordingly chooses k = 1/2. However, our experiments show that after renormalization, the maximum eigenvalue λ max will shrink to around 3/2, which makes 1/2 not optimal as well. In experiments, we calculate λ max for each dataset and set k = 1/λ max . We further analyse the effects of different k values (Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adaptive Encoder</head><p>Filtered by t-layer Laplacian smoothing, the output features are smoother and preserve abundant attribute information.</p><p>To learn better node embeddings from the smoothed features, we need to find an appropriate unsupervised optimization objective. To this end, we manage to utilize pairwise node similarity inspired by Deep Adaptive Learning <ref type="bibr" target="#b5">[6]</ref>. For attributed graph embedding task, the relationship between two nodes is crucial, which requires the training targets to be suitable similarity measurements. GAE-based methods usually choose the adjacency matrix as true labels of node pairs. However, we argue that the adjacency matrix only records one-hop structure information, which is insufficient. Meanwhile, we address that the similarity of smoothed features or trained embeddings are more accurate since they incorporate structure and features together. To this end, we adaptively select node pairs of high similarity as positive training samples, while those of low similarity as negative samples.</p><p>Given filtered node features X, the node embeddings are encoded by linear encoder f :</p><formula xml:id="formula_16">Z = f ( X; W) = XW, (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>where W is the weight matrix. We then scale the embeddings to the [0, 1] interval by min-max scaler for variance reduction. To measure the pairwise similarity of nodes, we utilize cosine function to implement our similarity metric. The similarity matrix S is given by</p><formula xml:id="formula_18">S = ZZ ⊺ ∥Z∥ 2 2 .<label>(12)</label></formula><p>Next, we describe our training sample selection strategy in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Training Sample Selection.</head><p>After calculating the similarity matrix, we rank the pairwise similarity sequence in the descending order. Here r i j is the rank of node pair (v i , v j ). Then we set the maximum rank of positive samples as r pos and the minimum rank of negative samples as r neд . Therefore, the generated label of node pair (v i , v j ) is</p><formula xml:id="formula_19">l i j =          1 r i j ≤ r pos 0 r i j &gt; r neд None otherwise . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>In this way, a training set with r pos positive samples and n 2 − r neд negative samples is constructed. Specially, for the first time we construct the training set, since the encoder is not trained, we directly employ the smoothed features for initializing S:</p><formula xml:id="formula_21">S = X X⊺ ∥ X∥ 2 2 . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>After construction of the training set, we can train the encoder in a supervised manner. In real-world graphs, there are always far more dissimilar node pairs than positive pairs, so we select more than r pos negative samples in the training set. To balance positive/negative samples, we randomly choose r pos negative samples in every epoch. The balanced training set is denoted by O. Accordingly, our cross entropy loss is given by</p><formula xml:id="formula_23">L = (v i ,v j )∈ O −l i j log(s i j ) − (1 − l i j ) log(1 − s i j ).<label>(15)</label></formula><p>3.4.2 Thresholds Update. Inspired by the idea of curriculum learning <ref type="bibr" target="#b0">[1]</ref>, we design a specific update strategy for r pos and r neд to control the size of training set. At the beginning of training process, more samples are selected for the encoder to find rough cluster patterns. After that, samples with higher confidence are remained for training, forcing the encoder to capture refined patterns. In practice, r pos decreases while r neд increases linearly as the training procedure goes on. We set the initial threshold as r st pos and r st neд , together with the final threshold as r ed pos and r ed neд . We have r ed pos ≤ r st pos and r ed neд ≥ r st neд . Suppose the thresholds are updated T times, we present the update strategy as</p><formula xml:id="formula_24">r ′ pos = r pos + r ed pos − r st pos T ,<label>(16)</label></formula><formula xml:id="formula_25">r ′ neд = r neд + r ed neд − r st neд T . (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>As the training process goes on, every time the thresholds are updated, we reconstruct the training set and save the embeddings. For node clustering, we perform Spectral Clustering <ref type="bibr" target="#b21">[22]</ref> on the similarity matrices of saved embeddings, and select the best epoch by DaviesâĂŞBouldin index <ref type="bibr" target="#b7">[8]</ref> (DBI), which measures the clustering quality without label information. For link prediction, we select the best performed epoch on validation set. Algorithm 1 presents the overall procedure of computing the embedding matrix Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS</head><p>We evaluate the benefits of AGE against a number of state-of-the-art graph embedding approaches on node clustering and link prediction tasks. In this section, we introduce our benchmark datasets, baseline methods, evaluation metrics, and parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct node clustering and link prediction experiments on four widely used network datasets (Cora, Citeseer, Pubmed <ref type="bibr" target="#b25">[26]</ref> and Wiki <ref type="bibr" target="#b35">[36]</ref>). Features in Cora and Citeseer are binary word vectors, while in Wiki and Pubmed, nodes are associated with tf-idf weighted word vectors. The statistics of the four datasets are shown in Table <ref type="table" target="#tab_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Adaptive Graph Encoder</head><p>Input: Adjacency matrix A, feature matrix X, filter layer number t, iteration number max_iter and threshold update times T Output: Node embedding matrix Z 1: Obtain graph Laplacian Lsym from Eq. ( <ref type="formula" target="#formula_12">8</ref>); 2: k ← 1/λ max ; 3: Get filter matrix H from Eq. ( <ref type="formula" target="#formula_13">9</ref>); 4: Get smoothed feature matrix X from Eq. ( <ref type="formula">7</ref>); 5: Initialize similarity matrix S and training set O by Eq. ( <ref type="formula" target="#formula_21">14</ref>); 6: for iter = 1 to max_iter do 7:</p><p>Compute Z with Eq. ( <ref type="formula" target="#formula_16">11</ref>);</p><p>8:</p><p>Train the adaptive encoder with loss in Eq. ( <ref type="formula" target="#formula_23">15</ref>); 9:</p><p>if iter mod (max_iter /T ) == 0 then 10:</p><p>Update thresholds with Eq. ( <ref type="formula" target="#formula_24">16</ref>) and ( <ref type="formula" target="#formula_25">17</ref>);</p><p>11:</p><p>Calculate the similarity matrix S with Eq. ( <ref type="formula" target="#formula_18">12</ref>);</p><p>12:</p><p>Select training samples from S by Eq. ( <ref type="formula" target="#formula_19">13</ref>); </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>For attributed graph embedding methods, we include 5 baseline algorithms in our comparisons: GAE and VGAE <ref type="bibr" target="#b14">[15]</ref> combine graph convolutional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type="bibr" target="#b22">[23]</ref> add adversarial constraints to GAE and VGAE respectively, enforcing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type="bibr" target="#b23">[24]</ref> proposes a symmetric graph convolutional autoencoder recovering the feature matrix. The encoder is based on Laplacian smoothing while the decoder is based on Laplacian sharpening.</p><p>On the node clustering task, we compare our model with 8 more algorithms. The baselines can be categorized into three groups:</p><p>(1) Methods using features only. Kmeans <ref type="bibr" target="#b19">[20]</ref> and Spectral Clustering <ref type="bibr" target="#b21">[22]</ref> are two traditional clustering algorithms. Spectral-F takes the cosine similarity of node features as input.</p><p>(2) Methods using graph structure only. Spectral-G is Spectral Clustering with the adjacency matrix as the input similarity matrix. DeepWalk <ref type="bibr" target="#b24">[25]</ref> learns node embeddings by using SkipGram on generated random walk paths on graphs.</p><p>(3) Methods using both features and graph. TADW <ref type="bibr" target="#b35">[36]</ref> interprets DeepWalk as matrix factorization and incorporates node features under the DeepWalk framework. MGAE <ref type="bibr" target="#b31">[32]</ref> is a denoising marginalized graph autoencoder. Its training objective is reconstructing the feature matrix. AGC <ref type="bibr" target="#b37">[38]</ref> exploits high-order graph convolution to filter node features. The number of graph convolution layers are selected for different datasets. DAEGC <ref type="bibr" target="#b30">[31]</ref> employs For representation learning algorithms including DeepWalk, TADW, GAE and VGAE which do not specify on the node clustering problem, we apply Spectral Clustering on their learned representations. For other works that conduct experiments on benchmark datasets, the original results in the papers are reported.</p><p>AGE variants. We consider 4 variants of AGE to compare various optimization objectives. The Laplacian smoothing filters in these variants are the same, while the encoder of LS+RA aims at reconstructing the adjacency matrix. LS+RX, respectively, reconstructs the feature matrix. LS only preserves the Laplacian smoothing filter, the smoothed features are taken as node embeddings. AGE is our proposed model with adaptive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics &amp; Parameter Settings</head><p>To measure the performance of node clustering methods, we employ three metrics: Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI) <ref type="bibr" target="#b8">[9]</ref>. For link prediction, we partition the datasets following GAE, and report Area Under Curve (AUC) and Average Precision (AP) scores. For all the metrics, a higher value indicates better performance.</p><p>For the Laplacian smoothing filter, we find the maximum eigenvalues of the four datasets are all around 3/2. Thus we set k = 2/3 universally. For the adaptive encoder, we train the MLP encoder for 400 epochs with a 0.001 learning rate by the Adam optimizer <ref type="bibr" target="#b13">[14]</ref>. The encoder consists of a single 500-dimensional embedding layer, and we update the thresholds every 10 epochs. We tune other hyperparameters including Laplacian smoothing filter layers t, r st pos , r ed pos , r st neд and r ed neд based on DBI. The detailed hyperparameter settings are reported in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we show and analyse the results of our experiments. Besides the main experiments, we also conduct auxiliary experiments to answer the following hypotheses:</p><p>H1: Entanglement of the filters and weight matrices has no improvement for embedding quality.</p><p>H2: Our adaptive learning strategy is effective compared to reconstruction losses, and each mechanism has its own contribution.</p><p>H3: k = 1/λ max is the optimal choice for Laplacian smoothing filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Node Clustering Results</head><p>The node clustering results are presented in Table <ref type="table" target="#tab_3">2</ref>, where bold and underlined values indicate the highest scores in all methods and all baselines respectively. Our observations are as follows:</p><p>Algorithms using both feature and graph information usually achieve better performance than methods leveraging information from single source. This investigation demonstrates that features and graph structure contribute to clustering from different perspectives.</p><p>AGE shows superior performance to baseline methods by a considerable margin, especially on Cora and Wiki datasets. Competing with the strongest baseline GALA, our model outperforms it by 2.95%, 5.20% and 6.20% on Cora, by 12.29%, 18.45% and 13.11% on Wiki with respect to ACC, NMI and ARI. Such results show strong evidence advocating our proposed framework. For Citeseer and Pubmed, we give further analysis in section 5.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link Prediction Results</head><p>In this section, we evaluate the quality of node embeddings on the link prediction task. Following the experimental settings of GALA, we conduct experiments on Cora and Citeseer, removing 5% edges for validation and 10% edges for test. The training procedure and hyper-parameters remain unchanged. Given the node embedding matrix Z, we use a simple inner product decoder to get the predicted adjacency matrix</p><formula xml:id="formula_27">Â = σ (ZZ ⊺ ) (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>where σ is the sigmoid function.</p><p>The experimental results are reported in Table <ref type="table" target="#tab_4">3</ref>. Compared with state-of-the-art unsupervised graph representation learning models, AGE outperforms them on both AUC and AP. It is worth noting that the training objectives of GAE/VGAE and ARGA/ARVGA are the adjacency matrix reconstruction loss. GALA also adds reconstruction loss for the link prediction task, while AGE does not utilize explicit links for supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GAE v.s. LS+RA</head><p>We use controlled experiments to verify hypothesis H1, evaluating the influence of entanglement of the filters and weight matrices. The compared methods are GAE and LS+RA, where the only difference between them is the position of the weight matrices. GAE, as we show in Figure <ref type="figure" target="#fig_0">1</ref>, combines the filter and weight matrix in each layer. LS+RA, however, moves weight matrices after the filter.</p><p>Specifically, GAE has multiple GCN layers where each one contains a 64-dimensional linear layer, a ReLU activition layer and a graph convolutional filter. LS+RA stacks multiple graph convolutional filters and after which is a 1-layer 64-dimensional perceptron. Both embedding layers of the two models are 16-dimensional. Rest of the parameters are set to the same.</p><p>We report the NMI scores for node clustering on the four datasets with different number of filter layers in Figure <ref type="figure" target="#fig_2">3</ref>. The results show that LS+RA outperforms GAE under most circumstances with fewer parameters. Moreover, the performance of GAE decreases significantly as the filter layer increases, while LS+RA is relatively stable. A reasonable explanation to this phenomenon is stacking multiple graph convolution layers makes it harder to train all the weight matrices well. Also, the training efficiency will be affected by the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To validate H2, we first compare the four variants of AGE on the node clustering task. Our findings are listed below:</p><p>(1) Compared with raw features (Spectral-F), smoothed features (LS) integrate graph structure, thus perform better on node clustering. The improvement is considerable.</p><p>(2) The variants of our model, LS+RA and LS+RX, also show powerful performances compared with baseline methods, which results from our Laplacian smoothing filter. At the same time, AGE still outperforms the two variants, demonstrating that the adaptive optimization target is superior.</p><p>(3) Comparing the two reconstruction losses, reconstructing the adjacency matrix (LS+RA) performs better on Cora, Wiki and Pubmed, while reconstructing the feature matrix (LS+RX) performs better on Citeseer. Such difference illustrates that structure information and feature information are of different importance across datasets, therefore either of them is not optimal universally. Furthermore, on Citeseer and Pubmed, the reconstruction losses contribute negatively to the smoothed features.</p><p>Then, we conduct ablation study on Cora to manifest the efficacy of four mechanisms in AGE. We set five variants of our model for comparison.</p><p>All five variants cluster nodes by performing Spectral Clustering on the cosine similarity matrix of node features or embeddings. "Raw features" simply performs Spectral Clustering on raw node features; "+Filter" clusters nodes using smoothed node features; "+Encoder" initializes training set from the similarity matrix of smoothed node features, and learns node embeddings via the fixed training set; "+Adaptive" selects training samples adaptively with   fixed thresholds; "+Thresholds Update" further adds thresholds update strategy and is exactly the full model.</p><p>In Table <ref type="table" target="#tab_6">4</ref>, it is obviously noticed that each part of our model contributes to the final performance, which evidently states the effectiveness of them. Additionally, we can observe that model supervised by the similarity of smoothed features ("+Encoder") outperforms almost all the baselines, giving verification to the rationality of our adaptive learning training objective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Selection of k</head><p>As stated in section 3.3.3, we select k = 1/λ max while λ max is the maximum eigenvalue of the renormalized Laplacian matrix. To verify the correctness of our hypothesis (H3), we first plot the eigenvalue distributions of the Laplacian matrix for benchmark datasets in Figure <ref type="figure" target="#fig_3">4</ref>. Then, we perform experiments with different k and the results are report in Figure <ref type="figure" target="#fig_4">5</ref>. From the two figures,we can make the following observations:</p><p>(1) The maximum eigenvalues of the four datasets are around 3/2, which supports our selecting k = 2/3.</p><p>(2) In Figure <ref type="figure" target="#fig_4">5</ref>, it is clear that filters with k = 2/3 work best for Cora and Wiki datasets, since all three metrics reach the highest scores at k = 2/3. For Citeseer and Pubmed, there is little difference for various k.</p><p>(3) To further explain why some datasets are sensitive to k while some are not, we can look back into Figure <ref type="figure" target="#fig_3">4</ref>. Obviously, there are more high-frequency components in Cora and Wiki than Citeseer and Pubmed. Therefore, for Citeseer and Pubmed, filters with different k achieve similar effects.</p><p>Overall, for Laplacian smoothing filters, we can conclude that k = 1/λ max is the optimal choice for Laplacian smoothing filters (H3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization</head><p>To intuitively show the learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type="bibr" target="#b28">[29]</ref>. The figures are shown in Figure <ref type="figure" target="#fig_6">6</ref> and each subfigure corresponds to a variant in the ablation study. From the visualization, we can see that AGE can well cluster the nodes according to their corresponding classes. Additionally, as the model gets complete gradually, there are fewer overlapping areas and nodes belong to the same group gather together.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper we propose AGE, a unified unsupervised graph representation learning model. We investigate the graph convolution operation in view of graph signal smoothing, and then design a non-parametric Laplacian smoothing filter which preserves optimal denoising properties to filter out high-frequency noises. In the encoder part, we find adaptive learning is more appropriate for embedding. Experiments on standard benchmarks demonstrate our model has outperformed state-of-the-art baseline algorithms.</p><p>For future work, an intriguing direction is to improve the computational efficiency of adaptive learning by avoiding the full computation of the pairwise similarity matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The architecture of graph autoencoder<ref type="bibr" target="#b14">[15]</ref>. The components we argue about are marked in red blocks: Entanglement of the filters and weight matrices, design of the filters, and the reconstruction loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our AGE framework. Given the raw feature matrix X, we first perform t-layer Laplacian smoothing using filter H t to get the smoothed feature matrix X (Top). Then the node embeddings are encoded by the adaptive encoder which utilizes the adaptive learning strategy: (1) Calculate the pairwise node similarity matrix. (2) Select positive and negative training samples of high confidence (red and green squares). (3) Train the encoder by a supervised loss (Bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Controlled experiments comparing GAE and LS+RA</figDesc><graphic url="image-1.png" coords="7,60.52,83.69,121.04,90.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The eigenvalue distributions of benchmark datasets. λ max is the maximum eigenvalue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of k on the three metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 2D visualization of node representations on Cora using t-SNE. The different colors represent different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Furthermore, instead of the reconstruction loss, we apply a novel adaptive learning strategy to train node embeddings.• Experiment: We conduct extensive experiments on node clustering and link prediction tasks with real-world benchmark datasets. The results demonstrate that AGE outperforms state-of-the-art attributed graph embedding methods.</figDesc><table /><note>• Model: We propose AGE, a general model for attributed graph embedding. Our two-fold model disentangles the filters and weight matrices. The filters we adopt preserve the optimal low-pass properties.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell>13:</cell><cell>end if</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">14: end for</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Dataset # Nodes # Edges # Features # Classes</cell></row><row><cell></cell><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell></cell><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell></cell><cell>Wiki</cell><cell>2,405</cell><cell>17,981</cell><cell>4,973</cell><cell>17</cell></row><row><cell></cell><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of node clustering.</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Wiki</cell><cell></cell><cell></cell><cell>Pubmed</cell></row><row><cell></cell><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Kmeans</cell><cell>F</cell><cell cols="11">0.503 0.317 0.244 0.544 0.312 0.285 0.417 0.440 0.151 0.580 0.278 0.246</cell></row><row><cell>Spectral-F</cell><cell>F</cell><cell cols="11">0.347 0.147 0.071 0.441 0.203 0.183 0.491 0.464 0.254 0.602 0.309 0.277</cell></row><row><cell>Spectral-G</cell><cell>G</cell><cell cols="11">0.342 0.195 0.045 0.259 0.118 0.013 0.236 0.193 0.017 0.528 0.097 0.062</cell></row><row><cell>DeepWalk</cell><cell>G</cell><cell cols="11">0.484 0.327 0.243 0.337 0.089 0.092 0.385 0.324 0.173 0.543 0.102 0.088</cell></row><row><cell>TADW</cell><cell cols="12">F&amp;G 0.560 0.441 0.332 0.455 0.291 0.228 0.310 0.271 0.045 0.511 0.244 0.217</cell></row><row><cell>GAE</cell><cell cols="12">F&amp;G 0.611 0.482 0.302 0.456 0.221 0.191 0.379 0.345 0.189 0.632 0.249 0.246</cell></row><row><cell>VGAE</cell><cell cols="12">F&amp;G 0.592 0.408 0.347 0.467 0.261 0.206 0.451 0.468 0.263 0.619 0.216 0.201</cell></row><row><cell>MGAE</cell><cell cols="12">F&amp;G 0.681 0.489 0.436 0.669 0.416 0.425 0.529 0.510 0.379 0.593 0.282 0.248</cell></row><row><cell>ARGA</cell><cell cols="12">F&amp;G 0.640 0.449 0.352 0.573 0.350 0.341 0.381 0.345 0.112 0.681 0.276 0.291</cell></row><row><cell>ARVGA</cell><cell cols="12">F&amp;G 0.638 0.450 0.374 0.544 0.261 0.245 0.387 0.339 0.107 0.513 0.117 0.078</cell></row><row><cell>AGC</cell><cell cols="12">F&amp;G 0.689 0.537 0.486 0.670 0.411 0.419 0.477 0.453 0.343 0.698 0.316 0.319</cell></row><row><cell>DAEGC</cell><cell cols="12">F&amp;G 0.704 0.528 0.496 0.672 0.397 0.410 0.482 0.448 0.331 0.671 0.266 0.278</cell></row><row><cell>GALA</cell><cell cols="12">F&amp;G 0.746 0.577 0.532 0.693 0.441 0.446 0.545 0.504 0.389 0.694 0.327 0.321</cell></row><row><cell>LS</cell><cell cols="12">F&amp;G 0.638 0.493 0.373 0.677 0.419 0.433 0.515 0.534 0.317 0.656 0.300 0.315</cell></row><row><cell>LS+RA</cell><cell cols="12">F&amp;G 0.742 0.580 0.545 0.658 0.410 0.403 0.552 0.566 0.382 0.652 0.291 0.301</cell></row><row><cell>LS+RX</cell><cell cols="12">F&amp;G 0.647 0.479 0.423 0.674 0.416 0.424 0.553 0.543 0.365 0.645 0.285 0.251</cell></row><row><cell>AGE</cell><cell cols="12">F&amp;G 0.768 0.607 0.565 0.702 0.448 0.457 0.612 0.597 0.440 0.711 0.316 0.334</cell></row></table><note>graph attention network to capture the importance of the neighboring nodes, then co-optimize reconstruction loss and KL-divergencebased clustering loss.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of link prediction.Compared with GCN-based methods, AGE has simpler mechanisms than those in baselines, such as adversarial regularization or attention. The only trainable parameters are in the weight matrix of the 1-layer perceptron, which minimizes memory usage and improves training efficiency.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell></row><row><cell></cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell>GAE</cell><cell cols="4">0.910 0.920 0.895 0.899</cell></row><row><cell>VGAE</cell><cell cols="4">0.914 0.926 0.908 0.920</cell></row><row><cell>ARGA</cell><cell cols="4">0.924 0.932 0.919 0.930</cell></row><row><cell cols="5">ARVGA 0.924 0.926 0.924 0.930</cell></row><row><cell>GALA</cell><cell cols="4">0.921 0.922 0.944 0.948</cell></row><row><cell>AGE</cell><cell cols="4">0.957 0.952 0.964 0.968</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study.</figDesc><table><row><cell>Model Variants</cell><cell></cell><cell>Cora</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Raw features</cell><cell cols="3">0.347 0.147 0.071</cell></row><row><cell>+Filter</cell><cell cols="3">0.638 0.493 0.373</cell></row><row><cell>+Encoder</cell><cell cols="3">0.728 0.558 0.521</cell></row><row><cell>+Adaptive</cell><cell cols="3">0.739 0.585 0.544</cell></row><row><cell cols="4">+Thresholds Update 0.768 0.607 0.565</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Key Research and Development Program of China (No. 2018YFB1004503) and the National Natural Science Foundation of China (NSFC No. 61772302, 61732008).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE DETAILS ABOUT THE EXPERIMENTS</head><p>Here we describe more details about the experiments to help in reproducibility.</p><p>A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Settings</head><p>We report our hyperparameter settings in Table <ref type="table">5</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian robust attributed graph clustering: Joint learning of partial anomalies and group structure</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2738" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relational topic models for document networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan</title>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
	<note>Deep adaptive image clustering</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A cluster separation measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><surname>Bouldin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979. 1979</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Data clustering: Theory, algorithms, and applications</title>
				<meeting><address><addrLine>Siam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data(base) Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Community detection as an inference problem</title>
		<author>
			<persName><forename type="first">Hastings</forename><surname>Matthew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="35" to="102" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
				<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR. 15</title>
				<meeting>ICLR. 15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Community detection in attributed graphs: an embedding approach</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="338" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="36" to="104" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Jiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A signal processing approach to fair surface design</title>
		<author>
			<persName><surname>Gabriel Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attributed graph clustering: A deep attentional embedding approach</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3670" to="3676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic community identification in large attribute networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attributed graph clustering via adaptive graph convolution</title>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
