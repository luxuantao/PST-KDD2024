<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Olfoto: Designing a Smell-Based Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
							<email>stephen@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Glasgow Interactive Systems Group</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Mcgookin</surname></persName>
							<email>mcgookdk@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Glasgow Interactive Systems Group</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Glasgow Interactive Systems Group</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Olfoto: Designing a Smell-Based Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18D872DC10CAA8E0CB35CE8CA3CC84D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Olfaction</term>
					<term>Smell</term>
					<term>Digital Photographs</term>
					<term>Searching</term>
					<term>Tagging H5.m. Information interfaces and presentation: Miscellaneous</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a study into the use of smell for searching digital photo collections. Many people now have large photo libraries on their computers and effective search tools are needed. Smell has a strong link to memory and emotion so may be a good way to cue recall when searching. Our study compared text and smell based tagging. For the first stage we generated a set of smell and tag names from user descriptions of photos. Participants then used these to tag photos, returning two weeks later to answer questions on their photos. Results showed that participants could tag effectively with text labels, as this is a common and familiar task. Performance with smells was lower but participants performed significantly above chance, with some participants using smells well. This suggests that smell has potential. Results also showed that some smells were consistently identified and useful, but some were not and highlighted issues with smell delivery devices. We also discuss some practical issues of using smell for interaction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Smell output is an under-explored presentation technique in human-computer interaction. Our sense of smell is very rich, but not often used. It is a powerful alerting mechanism and has strong links to both emotion and memory. These make it an interesting area of study for future user interfaces.</p><p>One reason for the lack of research is that there are few effective computer controlled smell devices which can emit a range of odours. Commercial devices are beginning to appear, but are not common. Many companies have proposed systems, but few have come to market. It is, however, still possible to investigate interactions using very simple smell technologies. Our aim in this work is to explore how smell might be used in a real application, if it is effective, and to discover the issues and practical problems that must be dealt with when designing smell-based interactions.</p><p>Our application focus is on searching digital photographs. Many people now have very large digital photograph collections with thousands of images. There are tools to support browsing and searching in many consumer-oriented applications (for example, Adobe Photoshop Elementswww.adobe.com, or Corel Photo Album -www.corel. com). These do not always provide the easiest ways to find photographs and users may not always use the tagging and searching features as they are too complex <ref type="bibr" target="#b10">[11]</ref>. Our aim is to examine the use of smell and its link to memory and recall to aid photo searching by allowing users to tag photos with particular smells and then use those smells to help recall the photos later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMELL RESEARCH</head><p>The whole area of smell is much less well understood than vision or audition. We have no detailed understanding of how we perceive smell. We cannot easily build up complex smells from simpler components. This makes designing interfaces that use them more difficult, requiring rigorous evaluation to ensure usability.</p><p>As Kaye suggests in his thorough review of the whole topic <ref type="bibr" target="#b8">[9]</ref>, there are no good classification or description schemes for smells. There are specific ones for the perfume, wine and beer industries, for example, but these do not apply to the wide range of smells that we might want to use in a user interface. This makes it difficult to describe what is being used or to know what a smell you might want to use actually smells like.</p><p>Anosmia (the inability to smell one or more specific odours) also has a big impact. Large groups of the population are unable to smell particular smells with reasons for this only now being understood. Care must be taken to avoid these and evaluation is needed to ensure people can smell all of the smells you choose to use.</p><p>One of the key features of smell is its close link with memory. You may smell something and it takes you right back to the time and place that you smelled it before. Neuroscience research has shown strong links between smell and working memory, attention, reaction times, mood and emo-tion <ref type="bibr" target="#b9">[10]</ref>. If we can tap into these links then there is potential to use smell as an aid to searching or other interactions.</p><p>Cann and Ross <ref type="bibr" target="#b5">[6]</ref> report that there is a strong link between smell and memory "One experience related to the influence of odors that seems nearly universal, at least within the folklore surrounding odors, is the occurrence of vivid memories, often involving events from the distant past, precipitated by exposure to an odor associated with the original experience". They report a study by Laird in which 80% of men and 90% of women reported multiple odourrevived memories. The memories were often emotionally charged, rather than neutral. Cann and Ross say "… it is clear that people can respond to olfactory cues as memory devices, describing events associated with odors, and that they believe odors often lead to the recall of past events". The kinds of memories evoked are often complex images or experiences rather than objects. This suggests that there is a strong response to smells that we could make use of for photo recall. It would seem essential to have a range of smells, both good and bad, to suit a wide emotional range.</p><p>Aggleton and Waskett <ref type="bibr" target="#b0">[1]</ref> found a similar powerful effect with smells used in the Jorvik Viking Museum in York, UK with visitors recalling information about the museum significantly better in the presence of the Jorvik smells over six years later (smells at the museum are from Dale Air). <ref type="bibr" target="#b5">[6]</ref> studied the use of odour to cue recognition of images. Participants were exposed to one of two odours when learning images and then asked to recognise them with and without the same odours present. They used a pleasant (flowery) or unpleasant (ammonium sulphide) smell during the learning phase. Results showed that recognition of the images 48 hours later was significantly better when the same smell was present as the learning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cann and Ross</head><p>The idea of such state-dependent recall is a useful one for our application as it gives us two choices of smells (abstract and representational) and two opportunities for associating a smell with a photo. Using representational smells that embody the original photo means that we can help people recall that original event. However, learning will also occur at the tagging stage where people associate smells with photos. Smells in this case could be either representational or more abstract (which might be easier to differentiate). Both types, when smelled again, would help recall of photos. This gives a real application the possibility of using a larger range of smells as we are just not tied to ones representing the actual contents of the photos, although the representational ones may be the most effective as they would be reinforced at the tagging stage, giving users more chance to associate them with the photo.</p><p>Cain <ref type="bibr" target="#b4">[5]</ref> showed that certain smells appear to be more identifiable than others: coffee, peanut butter, Vicks, and chocolate were shown to be the most easy to identify, with cough syrup, cleaning fluid, and lighter fluid being the hardest. There is little practical work to help the interface designer choose smells. Much of the psychophysical work has used very simple compounds, a problem then being naming the thing being smelled. For our purposes we wanted more 'natural' smells to represent the real-world things that people might have in their photographs.</p><p>There are few studies into the perception of intensity of odours. Engen and Pfaffman <ref type="bibr" target="#b6">[7]</ref> showed that people could recognise three levels of the smells they used. This was less important for our study as the technology we used to deliver our smells had no control over intensity. With more sophisticated output devices one could perhaps use intensity to give some indication of the importance of the thing tagged, but this would be hard to control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smell Delivery</head><p>One important issue is smell delivery. As discussed, there is no simple way to mix complex smells from atomic ones, therefore most systems just use pre-packaged smells. The quality of the smells is often very variable; some smell very little like the thing they are supposed to, are quite synthetic smelling and hard to give a name to. This is a problem for interface designers as smells are required that people can identify. Kaye offers some help as he reports that "of personal memories evoked by an odor, 32% were evoked without an odor name". This may mean that lower quality smells may still elicit the memories we need, even if they are not good enough for people to be able to name.</p><p>One key issue is that smells may diffuse across a room and thus be smelled by many people. This may be beneficial for an ambient display, but problematic for more specific ones. Yanagida et al. <ref type="bibr" target="#b15">[16]</ref> looked at using an air cannon that tracks the user's nose to present smells. This fires a focused vortex of air so that others nearby do not smell anything. Their initial prototype shows promise but is not commercially available. There are several examples of other devices that have been proposed but have never made it to market (DigiScents and TriSenx are well know examples).</p><p>For situations where there is just one user in an office or the smell can be displayed to a larger group of people things are easier. Simple fans can blow across the surface of scented material to spread it across a room, or the smell may just diffuse naturally.</p><p>For our study we used smell cubes from Dale Air, UK (www.daleair.com). These are 50mm 3 green plastic cubes that contain an oil-based aroma on cotton wool. When the plastic tape covering the holes on top is removed, the smell diffuses out into the surrounding air. These are very cheap and are available in a wide range of different odours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uses of smell for interaction</head><p>There is little work in the area of smell and HCI. It has been used for ambient displays <ref type="bibr" target="#b12">[13]</ref>, but most uses have focused on notifications. Kaye has done some of the first work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. He has proposed and built several systems that have investigated the use of olfaction for providing ambient notification information. His first system, inStink, provided a sense of presence in a remote location. A spice rack with sensors attached to each spice jar was linked to a set of spray guns at a remote location. These were filled with essential oils which smelled like the spices in the spice rack. When a spice jar was removed from the rack, the corresponding essential oil was released at the remote location, allowing users there to be aware of activity at the location of the spice rack. Although Kaye did not perform a detailed evaluation, he noted that when the essential oils combined they produced an unpleasant smell. This further highlights that care must be taken when using multiple scents.</p><p>Kaye's second system, Dollars &amp; Scents, presented information about the current value of the NASDAQ stock exchange using two scents. In this system a device was constructed using solenoids and perfume bottles that would emit a mint smell if the market was rising, or lemon if it was falling. Kaye reports no particular reason for the choice of these two scents other than they "leverage linguistic idioms as mnemonics". He also noted that the original system was to have used rose as the scent for a rising market, but was changed since one of the residents at MIT Media Lab (where the system was installed) was allergic to roses. Although Kaye reports no specific evaluation of Dollars &amp; Scents, there was a generally positive response and the smells quickly were incorporated into the culture of the environment. For example people passing and saying "Smelling of mint today".</p><p>Scent Reminder used the same technology as Dollars &amp; Scents but was integrated with Microsoft Outlook. Appointments could be set and scents released as reminders. Unfortunately, as with Kaye's other systems. there were no detailed evaluation so it is difficult to determine the actual usefulness or issues with the systems he proposes. However, his applications do show the potential that smell has to be a useful addition to computer interfaces.</p><p>Arroyo et al. <ref type="bibr" target="#b1">[2]</ref> looked at interruption modalities (heat, light, sound, vibration and smell) to see which caused the greatest disruption to their ongoing work. For their smells they chose glue and soy sauce. No justification is given for the smells used. Their results were inconclusive; individual differences masked the effects of particular modalities. People's backgrounds made them respond very differently to the different modalities.</p><p>In a similar study Bodnar et al. <ref type="bibr" target="#b3">[4]</ref> looked at auditory, smell and visual stimuli for interruptions. They used extract of cloves and artificial eucalyptus smells. These were chosen from an initial set of 10 in a pilot study. They could be easily differentiated at intervals of 30 seconds. Their results showed that olfactory cues were less effective than audio or visual ones, causing participants to reduce the number of questions completed during the study. Some interesting issues arose from participants' comments. Many felt that the smells used were too similar and thus easy to confuse. Smells lingering in the air and confusing participants was also a problem. As with Arroyo et al., the background of the participants played a role. An aromatherapist had no problem with the smell cues, whereas other participants said lack of experience with smell cues made the task difficult. These are serious issues for a photo searching application as users will have a wide range of backgrounds. One potential solution is to provide a wide range of different smells so that users can choose ones that suit them best.</p><p>Smell has been proposed in several VR systems to improve fidelity <ref type="bibr" target="#b14">[15]</ref>. For example Tominaga et al. <ref type="bibr" target="#b13">[14]</ref> used an air pump system to present aromas in virtual environments. They suggest two types of smells: scent of the environment and scent of the object. The environmental odours provide the smell of a place, to help the users feel like they are in a particular environment. The objects' odours give information about a particular object in the environment. Results showed very positive results with 5 aromas. People responded more positively about being in particular locations (forest, bar) when smells were present.</p><p>Tillotson from the Royal College of Art in the UK has investigated the use of smell for fashion, health and wellbeing, using smell emitted from clothing to create a 'scent bubble' around the wearer that "enhances the visual message of fashion with medical, sensory &amp; psychological wellbeing for the wearer" (www.smartsecondskin.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PHOTOGRAPH CATALOGUING AND SEARCHING</head><p>Over recent years there has been a rapid growth in the use of digital cameras and many people have large numbers of photos on home computers which they need to search. There have been few published studies that have investigated the management of personal photos and people's cataloguing, searching and browsing habits. Rodden and Wood <ref type="bibr" target="#b10">[11]</ref> undertook one of the most detailed studies to date using their ShoeBox digital photo cataloguing system with 13 users. They found a range of organizing strategies, from users putting images into different folders, to all images being in one large folder. Some users spent more time organising their photos, others spent very little time. Those who spent more time often changed the name of the folder to help them remember what was in it. Eight of their participants added extra text or spoken audio annotations to help searching. The spoken annotations were generally of people and places. One issue with the spoken annotations was the problem with speech recognition errors that could then make searching a problem. The methods for adding annotations or meta tags to facilitate searching need to be simple and effective otherwise people may not use them.</p><p>Rodden and Wood found users made three main types of queries:</p><p>• The set of photos from a particular event, e.g. holiday;</p><p>• An individual remembered photo; • A set of photos taken at different events, but all sharing a property, such as containing a certain person.</p><p>Users in their study tended to do much more browsing than querying. One reason for this could have been that the users were all novices with small collections of photos taken specifically for the study. As Rodden and Wood say "…queries might start to seem more important as a collection grows, and the photos get older and less familiar".</p><p>Participants in our study brought their existing photos with them to avoid this problem. We also wanted to make our tagging process as simple as possible, to avoid complexity that might put people off from using smell.</p><p>Shneiderman's PhotoFinder <ref type="bibr" target="#b11">[12]</ref> looked at simpler and more effective ways to tag photos, particularly people in photos. These were very low effort, simply requiring a user to drag a label onto a photo. These techniques are now common in commercial tools. We aimed to build on the simplicity of this approach in our photo tagging tool to make adding tags as simple as possible.</p><p>Bedersen's PhotoMesa <ref type="bibr" target="#b2">[3]</ref> provides sophisticated layout and visualisation mechanisms to help users find images from large sets of thumbnails. But with a large number of photos this can still be a problem. Alternative mechanisms to help cut down the number of thumbnails that needed to be presented would ease the problem.</p><p>We attempted to build on these previous systems, but use smell as a tool to tag and then retrieve photos. We also wanted to conduct an initial user study as many systems in this area have had little user evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OLFOTO</head><p>We developed a simple olfactory photo browsing and searching tool, Olfoto, based on the freeware Ekspos photo viewer (www.kiyut.com/products/ekspos). This provides basic thumbnail/photo viewing capabilities and, as it is written in Java, we could easily add our own code to allow smell and text tagging and searching (see Figure <ref type="figure" target="#fig_1">1</ref>). It has a standard scrolling thumbnail pane occupying most of the screen, with a larger version of the chosen thumbnail shown top left in the photo viewing pane. Below this is the tagging pane and below that the search pane. Clicking on any of the thumbnails in the thumbnail pane shows it in the photo viewing pane. To present the odours we used smell cubes from Dale Air. An RFID disc tag was attached to the base of each of the cubes (Figure <ref type="figure" target="#fig_0">2</ref>). We used Phidgets for this (www.phidgets .com) as they provide a very simple way of using RFID in Java. In tagging mode, users add one or more tags to a photo by moving the appropriate smell cube over the RFID reader. Multiple selections could be made in the thumbnail window so that the same smell could be added to multiple photos. Multiple smell tags could by added to a photo by moving more cubes over the reader. To remove a tag the cube was moved over the reader again. We tried to create a simple, tangible interaction that was easy to do, to avoid the problems highlighted by Rodden and Wood.</p><p>In search mode, the user moved a cube over the reader and all of the photos with that smell were displayed in the thumbnail pane. Multiple tags could be used for searching by passing the cubes of interest over the reader.</p><p>Text tagging and searching were done in a similar way; a user could choose one or more of the text tags from a drop down list and apply them to the selected photo(s). For searching the user chose the text tags from a drop down list to display the thumbnails with that tag attached. The aim of the studies was to find out if smell could play a role in photo searching, how it compared to text tagging, if users would consider using it, and for us to understand how to use smell in a practical setting. This was our first study in this area and we needed to understand how to manage the practicalities of smell-based interaction.</p><p>Our study was undertaken in a usability lab which had two doors -one into the building and one to the outside -so we could get a good air flow through the room (to avoid the problems smell mixing found by Bodnar et al. <ref type="bibr" target="#b3">[4]</ref>).</p><p>Our participants were all undergraduates, PhD students or members of staff in our Department. This was intentionally not a wide range. We needed to keep the likely range of tags small enough that we could cover them with a small selection of different smells to make the study feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorisation Study</head><p>In this phase we wanted to generate a list of smell and text labels that could be used to tag photos for the main two studies. We used three participants, plus the three authors of the paper. Each was asked to bring along 300-500 photos from their collection. The experimenter then went through the photos asking each participant what smells labels he/she would use to describe the images; the process was then repeated for text labels. They were free to use whatever words came to mind and these could be different for the smell and text labels. We then grouped these words to come up with an overall set of smell and text categories. Table <ref type="table" target="#tab_1">1</ref> shows the category names we chose based on the words generated by the participants for six of the smell categories.</p><p>We came up with 16 smell categories (Alcohol, Pine, Food, Fresh, Beach, Smoke, Garden, Musty, Grass, Floral, Sea, Sweat, River, Perfume, Petrol, Chocolate). These covered all of the words used, except for those that would have formed single word categories (i.e. words that could not be grouped with any others). We wanted to keep the number of categories small enough to make the rest of the study feasible. There were 12 text categories (City, Colour, Countryside, Family, Friends, Hobby, Holiday, Home, Party, Pub, Season, Sport, Travel, University, Work), again avoiding single word categories. These are by no means a complete classification of general photo searching keywords (which would be an interesting study). We used a focused group of users to make a set of tags that would likely be useful for a similar group of main test participants.</p><p>We then used the smell categories to buy a set of smell cubes from Dale Air. For this study we chose to use representational smells as a starting point given the research presented above. A major problem was taking the smell categories and matching them to a smell produced by Dale Air. The company produces around 200 different odours, and without smelling them, it was hard to know which were the ones most useful to us. This is an example of the problem that Kaye identified above -there is no standard classification scheme for smell. A floral smell from one company may be completely different to that of another (and still not like a real floral smell). In addition, some of the names are not very informative (Dragons breath, for example); it is difficult to know what the smell is like and if it matched the category we had, but we did our best. The smells chosen from the Dale Air website to match our categories were: Brewery, Alpine, Bread, Ozone, Sea Shore, Smoke, Farmyard, Dusty, Grass, Floral, Sea Breeze, Sweaty Feet, Riverbank, Unisex Perfume, Machine Oil, Dark Chocolate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagging Study</head><p>The next stage of the work involved participants tagging photographs with the smell and text labels generated in the previous stage. We used 12 new participants who were paid £10 if they completed both this and the following Recall Study. There were four women and eight men, ages ranging from 20-45, six were from the UK, two were Indian, one each were from China, Mexico, Greece and South Africa.</p><p>All were asked to bring along around 500 digital photos from their collections. This gave us a wide enough range of material for them to be able to tag and also to create a large enough set for the Recall Study.</p><p>We asked what software they used to manage their photo collections. Six did nothing, just putting them into a folder on their PC, two used the online photo service Fickr (www.flickr.com), two used iPhoto and one used the Canon software that came with his camera. We asked if they tagged their photos in any way. Five did nothing, four renamed files with suitable labels, one Flickr user and one iPhoto user used text annotations and one iPhoto user created albums, used text annotations, created titles and keywords for his photos. This is generally in line with the results of Rodden and Wood <ref type="bibr" target="#b10">[11]</ref>.</p><p>Participants either did the text tagging first or worked through the smell cubes and assigned a name to each one and then did the smell tagging (in a counterbalanced order). Each tagging phase took around 20 minutes. Finally, we interviewed the participants about the process of tagging and general issues raised. The whole process took one hour.</p><p>When tagging with the smells participants were free to pick up any cube they desired and smell it by lifting the tape. The cubes were not labelled in any way. The cubes were on the table next to the PC running our software and they could smell (and tag with) as many different cubes as they wanted. If they were happy with a chosen smell they could move it over the RFID tag reader to tag the photo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Table <ref type="table">2</ref> shows the names participants gave to 10 of the 16 smells. The first five were less well identified; the last five were much more consistently named. One problem was that some of the smells were very synthetic smelling. Floral, for example, had a very soapy, bathroom cleaning product smell and many participants identified it as that rather than flowers. As suggested by Cain <ref type="bibr" target="#b4">[5]</ref> chocolate was well recognised. His list also included Vicks (a gel rubbed on the chest to ease a blocked nose). It is interesting to note that four of the participants identified Riverbank as having that smell. Again this was due to its very synthetic nature.</p><p>These results show that some smells could work really well, some do not smell like their name but are consistently identified by participants, and some are not consistently identified at all. It is useful to know consistency for providing a general set of smells for an olfactory photo tool. It means that a set of easily differentiable smells could be provided with the system. However, a more important issue is how consistently people can name the smells over time, which was investigated in the Recall Study below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smell Name</head><p>Further work is needed to build up a set of good smells. The only way this can be done is empirically because, as Kaye reports, there is very little research to help choose a wide range of uniquely identifiable smells.</p><p>Participants completed NASA TLX subjective workload scales after both conditions. These give subjective data on how demanding people feel a task to be and are a good complement to quantitative measures of error rates, giving a more complete picture of usability. We added a question on fatigue (to see how tiring the task was) and on which method was preferred (participant 6 failed to complete the smell part of the subjective scales, so has been excluded from the analysis). Figure <ref type="figure" target="#fig_2">3</ref> shows the results. An analysis of Overall Workload (using, as for all the analyses in the paper, a paired two-tailed T-test) showed a significant reduction for the Text condition (Smell=10.4, Text=6.8, T 10 =3.84, p=0.001, Eta 2 = 0.24). We then performed a detailed analysis of each of the individual factors. Results showed a significant reduction in workload in the Text Condition for: Mental demand (p=0.005), Physical demand (p=0.006), Perceived performance (p&lt;0.001) and Fatigue (p=0.01). Temporal demand, Effort expended, Frustration and Preference showed no effect.</p><p>These results show that it was easier for people to tag photos with text labels than with the smells. Participants found the task mentally demanding as they were not used to using smells in this way and it required some concentration. This was reflected in the physical demand, fatigue and the low level of perceived performance. It was interesting to note that there was no effect for frustration or preference given the above results. This suggests that people might persevere with the smells, and with some practice, they would become easier to use and less demanding. One of the most raised issues in the informal comments from participants was that the smells did not always match their photos. Many wanted to choose their own smells, which was not practical for this study. Some also said that certain of the smells were hard to tell apart (Ozone, Sea shore and Sea breeze were most commonly mentioned). This was a problem as we could not smell the cubes before we bought them. Participants also wanted to be able to create their own text tags (which all real photo-cataloguing systems do).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall Study</head><p>Two weeks after the Tagging Study participants returned to perform the second part of the task to see if smell helped them recall photos, and how this compared to the use of text labels. Only six of the original twelve were able to return for this second phase due to other commitments.</p><p>Participants performed the two conditions in the same order as the previous study. The first step got them to go through the smell cubes again and re-label them. This allowed us to get an idea of the consistency of their naming. They then answered three types of questions in both conditions. The range of questions tried to gauge their ability to use smell and text labels in a range of different ways and were motivated by the queries identified by Rodden and Wood <ref type="bibr" target="#b10">[11]</ref>.</p><p>1. Smell/text ⇒ photo: Participants were shown 4 photos and given one smell or text tag. They had to identify which photo had that tag. Only one photo in the set of four had the tag and the three distracters were chosen randomly from the rest of their photo collection. They had to do this 15 times with different photos. We measured the number of correctly identified photos. text tags and one photo. They had to identify which tag was attached to the photo. This was done 15 times. We measured the number of correctly identified tags.</p><p>3. Search: Participants then performed a more general search task. They were given three key features of a photo and then asked to find it in their photo catalogue using smell or text tags. We chose the photos at random and picked the key features ourselves. For example, we might ask for photo with a building, people and palm trees, or an arch, angels and a roof. They did this five times. In the Smell Condition, the cubes were on the table next to the PC and participants were free to smell them as they liked. If they were happy with a chosen smell they could move it over the RFID tag reader to search with it. We measured the number of tags that they tried before finding the right photo. The fewer tags used would suggest that that type of tag helped them find the particular photo better. It was possible for a number of photos to match the three criteria we gave the participants. If the photo identified matched the criteria, they were given a mark.</p><p>They were presented with similar questions for both conditions. Participants filled in subjective workload assessment sheets as before after each condition. Finally, we interviewed them about the study as a whole to get their views.</p><p>The study took around 90 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Table <ref type="table">3</ref> shows the same example smells as Table <ref type="table">2</ref>. Chocolate was still well identified in this second phase. Sweaty feet and farmyard were much less well identified. Floral still had a very perfumed smell that was commonly identified by participants as was the Vicks/minty smell of Riverbank.</p><p>These results show that there is some similarity in the judgements made by the participants, but there is less consensus in this second phase (partly due to the smaller number of participants). It is not clear why they would be less good in this second phase, except if the smells were beginning to fade. By the time the second study took place, the cubes had been open for a month and a half. Some of the strong initial smells had definitely mellowed and changed. This makes it hard to assess our participants' abilities to reidentify the smells as they may not have been the same. For future work it would be worth using a set of new cubes for the second study to see if the effect disappeared. This also had an impact on their answers to the questions we set in this phase. If the smells were different then people were likely to do worse as the correct memories would not be evoked. There was no variation in the text tags, so performance should be better. The results for question type 1 showed no significant difference between conditions (mean correct score of 7.8 (out of 15) with Smell and 11.8 with Text, T 5 =1.64, p=0.16, Eta 2 = 0.27). Formal statistical analysis is limited due to the small number of participants. Looking at the data in Figure <ref type="figure" target="#fig_3">4</ref> a trend towards text tagging can be seen, with participant 4 behaving somewhat differently to the rest. This suggests we too were seeing the marked individual differences that Arroyo et al. <ref type="bibr" target="#b1">[2]</ref> found. However, participants were right using the smells 52% of the time, chance would be 25%. This shows that they were able to use the smell tags. With new cubes and better smells we might improve upon this performance.</p><p>The results for question type 2 showed the text tags being significantly better than the smell ones (mean number of correct identifications was 8 for Smell and 14 The results for question type 3 (Figure <ref type="figure" target="#fig_5">6</ref>) showed no significant difference between smell and text tagging. All users used the same or fewer tags to find the photos in the Text Condition (mean of 3.6 tags used in the Smell Condition and 1.5 in the Text, T 5 =2.13, p=0.08, Eta 2 = 0.27). Participants again showed marked individual differences, with some participants using many more tags in the Smell Condition, and others using the same number as for Text. We need a study with a larger number of participants to understand these effects fully. The results of the subjective measures are shown in Figure <ref type="figure" target="#fig_6">7</ref>. Participant 4 failed to complete one part of the subjective scales, so was removed from the analysis. Analysis showed no effect for overall workload (Smell=12.3, Text=5.9, T 4 =2.5, p=0.06, Eta 2 = 0.46), so no further analysis was undertaken. However, looking at the figure there seems to be a clear trend in favour of the Text Condition (with all categories in the Text Condition showing a reduction) and the pattern of the data is very similar to that of the Tagging Study, with perhaps a larger difference between the perceived performance with smell and text tagging. With a larger number of participants it is likely the differences would be significant. There were no significant differences for Fatigue or Preference. The scores for fatigue were almost identical to the Tagging Study, preference for the text tagging had increased a little from the earlier study. From informal comments many participants felt that the whole interaction with the smells was good and easy to use. However, two people said that they felt they were trying to remember the smells, rather than being 'taken back' to the photos they were looking for. This may have been because the smells changed slightly between the two stages of the Study. Differentiability of the smells was also mentioned again -people again found it difficult to tell some apart, so this affected their searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>In general, participants were able to use the smells to identify pictures. Participants were performing at greater than chance levels with the smells. They were less effective than text tags, but this is perhaps not surprising. Using smells in this way is an unusual thing for people to do, whereas text labels are much more familiar. With greater usage performance is likely to improve. An added difficulty was that some of the smells may have changed since the participants learned them, reducing their power for recall. Even with these problems, participants were able to use the smells.</p><p>Is smell tagging really practical? One key issue would be getting hold of the smells you wanted to tag with. We provided a small set to our group of users and these did not cover all the things they wanted to tag. There would need to be some simple way to buy appropriate smells, perhaps in cartridges like printer ink, dispensed by machines located by digital photo kiosks (this was the method suggested by DigiScents for their device, but it never came to market). Lower odour intensity output devices are also needed, suitable for a single user type environment. Dale Air recently released their Vortex USB device (www.daleair.com) that uses much less powerful smell discs. They do not saturate the environment in the same way as the cubes do, so are much more suitable for single user environments. We are using these for our next set of studies.</p><p>We tried to allow users to make a natural mapping between photo and tag. This is difficult as a large number of different smells would be required. An alternative would be to use an abstract or semi-abstract mapping. One might tag photos of the beach with chocolate, which has no relevance to the scene, but is easily identified. This would remove the emotional link but might still help recall, as Cann and Ross showed <ref type="bibr" target="#b5">[6]</ref>. We found this behaviour as one participant informally reported that he was using 'nice' smells to tag 'nice' photos and 'bad' smells for 'bad' photos when no suitable representational one was available.</p><p>For a further study it would be useful to try and get participants to describe the types of photos a smell brings to mind in the Recall Study, rather than just doing searching with the tags. This would give some more general, subjective information about the effect of the smells.</p><p>Much work needs to go into the choice of smells. As Kaye suggests <ref type="bibr" target="#b8">[9]</ref>, there is little practical research of use to interaction designers that can help in the choice. Some of the smells we ordered smelled quite similar (Sea Shore and Sea Breeze for example), others smelled little like we imagined when we ordered them. There are some (chocolate for example) that really worked well; we need further research to identify more of this type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some Practical Considerations When Using Smell</head><p>There turned out to be many practical issues to deal with when running our study. Some have been mentioned before and some occurred because of our design. We took care to make sure the room was well ventilated so that smells did not linger. Some participants did complain that the smells got mixed with each other. Due to our task users needed to switch between smells frequently. Even if the top was only off a cube for a short while it did get out into the room. The use of a fan might alleviate this problem, as would the use of less powerful smell discs.</p><p>When we first received the sixteen cubes the smells were very powerful and created a nasty mixture (which infused our whole department). We put the cubes in zip-loc bags and then in a sealed plastic box to stop the smells escaping.</p><p>In the future we may leave new cubes open outside for a few days to let them 'cool off' before using them so that we can get an even level of intensity for our studies. After a week or so, the smells had evened out so that we could store them with no problems. Some of the smells were of very different intensity levels. Sweaty Feet was initially very strong, masking the other smells in early prototypes.</p><p>Smells rapidly diffuse across wide areas and can reach people for whom they were not intended. Keeping them under control is difficult. This affects the uses to which they can be put. The cubes we were using were not designed for single users, they were meant for larger rooms, shops or museum exhibits so had to be quite intense. For a real photo searching application much less powerful smells would be more useful. Since our study was completed, the Vortex device has become available and solves many such problems.</p><p>After some pilot studies we took care to wipe the sides of the cubes. Pilot users had complained of getting smells on their fingers so that one smell then interfered with the next. At the start of each day we wiped the cubes so that there was no residue on them.</p><p>Related to the above is the important issue of consistency of the smell delivery device. Our smell cubes were perhaps starting to run out by the second study which meant that the smells had changed (some of the more volatile components may have evaporated, making the odours smell different). A computer-controlled device that kept the smells sealed and only opened them when needed might solve this problem.</p><p>Two participants said their sense of smell was impaired (and did not return for the Recall study). One had hay fever and one a cold. This is a particular issue for our olfactory sense as it is more susceptible to day-to-day variation than our sight or hearing. A good screening program for experimental participants might avoid this problem, but it would not deal with the issues of using a smell-based interface as part of a normal interaction with a computer. As with all such issues, a good multimodal system that used several different senses would be best. Users could then pick the interaction techniques most suitable; if they had a cold they could use text or audio searches instead that day. We will investigate the use of smell in a multimodal context in our next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>The studies presented here have shown that smell can be used to aid photo recall. Participants performed significantly better than chance when using the smells, and several participants used the same number of smell and text tags to find the photos for which they were searching. Text tagging did work better overall, but the results were very positive for the first smell-based study of this type.</p><p>Many issues remain about the use of smell in HCI. Choosing an appropriate set of smells is difficult and more research needs to be done into creating an easily differentiable set. Individual differences also seem to play a large role; some people are better at using smells than others. A bigger study is needed to investigate this further. If some of these issues can be addressed, the research presented here shows that smell has a valuable role alongside the other senses in future user interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A smell cube and the RFID tag reader.</figDesc><graphic coords="4,347.04,285.36,180.73,135.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Photo viewing pane</figDesc><graphic coords="4,185.52,53.88,274.96,180.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Means and standard deviations of subjective measures in the Tagging Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Number of correct answers for question type 1 in the Recall Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Number of correct answers for question type 2 in the Recall Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Mean number of tags used for question type 3 in the Recall Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Means and standard deviations of subjective measures in the Recall Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>CHI 2006 Proceedings • Media April 22-27, 2006 • Montréal, Québec, Canada</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Six categories of smells with the number of occurrences of each of the words used.</head><label>1</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This project was funded by the Department of Computing Science and EPSRC Advanced Research Fellowship GR/S53244. Thanks go to Dale Air for their help with the smell cubes.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The ability of odours to serve as state-dependent cues for real-world memories: Can Viking smells aid the recall of Viking experience?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Aggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waskett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interruptions as multimodal outputs: which are the less disruptive?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stouffs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimodal Interfaces (ICMI&apos;02</title>
		<meeting>IEEE International Conference on Multimodal Interfaces (ICMI&apos;02</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="479" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PhotoMesa: a zoomable image browser using quantum treemaps and bubblemaps</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM UIST</title>
		<meeting>ACM UIST<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AROMA: ambient awareness through olfaction in a messaging application</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nekrasovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimodal Interfaces (ICMI&apos;04)</title>
		<meeting>ACM International Conference on Multimodal Interfaces (ICMI&apos;04)<address><addrLine>State College, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cain</surname></persName>
		</author>
		<title level="m">What we remember about odors. Perfumer and Flavorist</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Olfactory stimuli as context cues in human memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Absolute judgements of odor intensity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Engen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pfaffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="26" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aromatic Output for HCI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactions</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Symbolic Olfactory Display. MIT, Media, Arts and Science, School of Architecture and Planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Masters Thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ambient odors modulate visual attentional capture</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacquot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Millot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience Letters</title>
		<imprint>
			<biblScope unit="issue">352</biblScope>
			<biblScope unit="page" from="221" to="225" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How Do People Manage Their Digital Photographs?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rodden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI 2003</title>
		<meeting>ACM CHI 2003<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press Addison Wesley</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Direct annotation: A drag-and-drop strategy for labelling photos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Information Visualisation</title>
		<meeting>IEEE Information Visualisation</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feather,Scent and Shaker: Supporting Simple Intimacy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CSCW &apos;96</title>
		<meeting>CSCW &apos;96<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Friend Park&quot;-expression of the wind and the scent on virtual space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tominaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ohsawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shigeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Virtual Systems and Multimedia &apos;01</title>
		<meeting>Virtual Systems and Multimedia &apos;01</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Could olfactory displays improve data visualization?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Washburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Unencumbering, Localized Olfactory Display</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yanagida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tetsutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CHI 2003</title>
		<meeting>ACM CHI 2003<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="988" to="989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
