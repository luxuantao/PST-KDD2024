<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-10">10 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
							<email>huangjizhou01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yibo</forename><surname>Sun</surname></persName>
							<email>sunyibo@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
							<email>shiyunsheng01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
							<email>huangzhengjie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Zhuo</surname></persName>
							<email>zhuoan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-10">10 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539021</idno>
					<idno type="arXiv">arXiv:2203.09127v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pre-training</term>
					<term>heterogeneous graph</term>
					<term>graph neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained models (PTMs) have become a fundamental backbone for downstream tasks in natural language processing and computer vision. Despite initial gains that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps, a clear performance plateau over time was observed. One of the main reasons for this plateau is the lack of readily available geographic knowledge in generic PTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a geography-and-language pre-trained model designed and developed for improving the georelated tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to learn a universal representation of geography-language by pretraining on large-scale data generated from a heterogeneous graph that contains abundant geographic knowledge. Extensive quantitative and qualitative experiments conducted on large-scale realworld datasets demonstrate the superiority and effectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production at Baidu Maps since April 2021, which significantly benefits the performance of various downstream tasks. This demonstrates that ERNIE-GeoL can serve as a fundamental backbone for a wide range of geo-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained models (PTMs) are designed to learn a universal representation from large-scale raw text <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, unlabeled images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Data Spatial Data</head><p>Text Images [25], or videos <ref type="bibr" target="#b29">[30]</ref>, which have become a fundamental backbone for downstream tasks in natural language processing (NLP) and computer vision (CV) <ref type="bibr" target="#b2">[3]</ref>. The most common paradigm for adapting PTMs to downstream tasks is sequential transfer learning <ref type="bibr" target="#b27">[28]</ref> via supervised fine-tuning on labeled data. In this paradigm, downstream tasks can benefit from the knowledge learned by PTMs, which brings significant improvements <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic PTMs Geographic PTMs</head><note type="other">Speech Spatial Location Human Mobility Spatial Correlation</note><p>The web mapping services provided by Baidu Maps, such as point of interest (POI) retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, POI recommendation <ref type="bibr" target="#b3">[4]</ref>, POI information page <ref type="bibr" target="#b30">[31]</ref>, and intelligent voice assistant <ref type="bibr" target="#b11">[12]</ref>, have shown improved performance by applying PTMs. However, a clear performance plateau over time was observed in our practice, i.e., the performance gain remains marginal w.r.t. the optimization of generic PTMs. One of the main reasons for this plateau is the lack of geographic knowledge, which plays a vital role in improving tasks that necessitate computational support for geographic information (hereafter referred to as geo-related tasks). In this work, we focus on two types of geographic knowledge. (1) Toponym knowledge. A toponym refers to the name of a geo-located entity, such as a POI, a street, and a district. Toponym resolution <ref type="bibr" target="#b19">[20]</ref>, which aims at identifying and extracting toponyms from text, is a fundamental necessity for a wide range of geo-related tasks. However, the semantic meaning of most toponyms can hardly be captured by generic PTMs, because toponym knowledge is largely absent from or rarely seen in their training data. (2) Spatial knowledge. Spatial knowledge mainly includes the geographic coordinates of a geo-located entity and the spatial relationships between different geo-located entities, which is indispensable for geo-related tasks such as geocoding <ref type="bibr" target="#b8">[9]</ref> and georeferencing <ref type="bibr" target="#b10">[11]</ref>. However, the generic PTMs are incapable of handling geo-related tasks effectively, due to the absence of spatial knowledge and the lack of pre-training tasks for incorporating spatial knowledge.</p><p>To effectively learn a geography-and-language pre-trained model from large-scale spatial data, we need to address two key challenges.</p><p>(1) Heterogeneous data integration. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the main difference between the training data used by generic and geographic PTMs. Generic PTMs are typically learned from multimodal data, including text, images, and speech. By contrast, the spatial data mainly include spatial location (a single POI), spatial correlation (a triplet of POIs), and human mobility (a sequence of POIs). However, the way to effectively integrate the three sources of spatial data with text data for training geographic PTMs has been little explored and remains a challenge. (2) Geography-language pre-training. Different from existing language model pre-training <ref type="bibr" target="#b5">[6]</ref> and visionlanguage (image-text <ref type="bibr" target="#b24">[25]</ref> and video-text <ref type="bibr" target="#b29">[30]</ref>) pre-training that is designed to learn the semantic correlations between vision and language, geography-language pre-training necessitates learning the associations between geography and language, e.g., learning to associate "Beijing railway station" in text with its real-world geo-location information in the form of coordinates. To learn a cooperative knowledge of geography and language from unlabeled data, it is important to design effective backbone networks, pretraining tasks, and learning objectives.</p><p>In this paper, we present our efforts toward designing and implementing ERNIE-GeoL, which is a geography-and-language pretrained model designed for improving a wide range of geo-related downstream tasks at Baidu Maps. Specifically, we first construct a heterogeneous graph that contains POI nodes and query nodes, using the POI database and search logs of Baidu Maps. To integrate spatial information with text, we construct edges between two nodes based on spatial correlation and human mobility data, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, which enables knowledge transfer between different modalities. To generate each input sequence for training ERNIE-GeoL, we use the random walk algorithm to sample a sequence of nodes as an input document. In this way, we can automatically build large-scale training data, which facilitates comprehensive knowledge transfer and bridges the modality gap. Second, we use Transformer as the backbone network to learn the representations of each node. To incorporate an input document's graph information, we employ a Transformer-based aggregation layer to encode the relations between multiple nodes in the document. To effectively learn comprehensive knowledge, we adopt masked language modeling and geocoding as the pre-training tasks, which are elaborated to simultaneously learn toponym and spatial knowledge, as well as to balance both knowledge explorations. As such, we can learn a universal representation of geography-language by pre-training on domain-specific and cross-modality data.</p><p>We evaluate ERNIE-GeoL on five geo-related tasks. Extensive experiments show that ERNIE-GeoL significantly outperforms the generic PTMs when applied to all five tasks. ERNIE-GeoL has already been deployed in production at Baidu Maps since April 2021, which significantly benefits the performance of a wide range of downstream tasks. This demonstrates that ERNIE-GeoL can serve as a fundamental backbone for geo-related tasks.</p><p>Our contributions can be summarized as follows:</p><p>? Potential impact: We suggest a practical and robust solution for training a geography-and-language pre-trained model, named ERNIE-GeoL, which can serve as a fundamental backbone for geo-related tasks. We document our efforts and findings on designing and developing ERNIE-GeoL, and we hope that it could be of potential interest to practitioners working with pre-trained models and geo-related problems. ? Novelty: The design and development of ERNIE-GeoL are driven by the novel idea that learns a universal representation of geography-language by pre-training on large-scale graph data with both toponym and spatial knowledge. To the best of our knowledge, this is the first attempt to design and build a geography-and-language pre-trained model. ? Technical quality: Extensive quantitative and qualitative experiments, conducted on large-scale, real-world datasets, demonstrate the superiority and effectiveness of ERNIE-GeoL.</p><p>The successful deployment of ERNIE-GeoL at Baidu Maps further shows that it is a practical and fundamental backbone for a wide range of geo-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ERNIE-GeoL</head><p>In this section, we introduce the design and implementation details of ERNIE-GeoL, which mainly contains three parts: training data construction, model architecture, and pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Data Construction</head><p>Our previous work <ref type="bibr" target="#b14">[15]</ref> has demonstrated that the heterogeneous graph is able to significantly benefit POI retrieval task. Motivated by this, we construct large-scale training data based on a heterogeneous graph that contains both toponym and spatial knowledge for pre-training ERNIE-GeoL. Specifically, we first construct a unified heterogeneous graph that contains POI nodes and query nodes using the POI database and search logs of Baidu Maps. Then, we construct edges between two nodes based on spatial relationships between POIs to integrate spatial information with text. The toponym data mainly include POI names and addresses, which are derived from the POI database and are stored in unstructured text format. The spatial data consist of POI geographic coordinates, POIs that co-locate within individual geographic regions, and POIs that co-occur in same sessions from search logs, which are stored using numerical digits or in triplet format (i.e., non-text format).</p><p>To bridge the gap between the text and non-text representations, we build a heterogeneous graph G = (V, E, ? V , ? E ), where V denotes the set of nodes, E the set of edges, ? V the set of node types, and ? E the set of edge types. Each node ? ? V and each edge ? ? E are associated with their corresponding mapping functions ? (?) : V ? ? V and ? (?) : E ? ? E . As shown by Figure <ref type="figure">2</ref>, the node types ? V include POI and query. The edge types ? E include Query-click-POI, Origin-to-Destination, and POI-(co-locate with)-POI. Next, we detail each element of the heterogeneous graph.</p><p>2.1.1 POI Node and Query Node. Assigning a unique ID to each POI node is a straightforward way to represent it. However, this approach is unable to represent newly emerging POIs. To address this problem, we uniformly use text rather than fixed IDs to represent all nodes. Specifically, we organize each POI node in the form of the concatenation of the following three types of textual information:</p><p>(1) the full POI name, (2) the POI address, and (3) the POI type. We separate each type of textual information with a special token "[SEP]". We also equip each POI node with its real-world location information, i.e., the geographic coordinates of it. See Appendix A.2 for a detailed description of the node representation method. search engine suggested. This process produces large-scale query-POI pairs, where the different expressions of each POI can bridge the semantic gap between queries and POIs. For example, users usually make spelling errors or use abbreviations, which would lead to poor results when directly matching query and POI text information. Motivated by this observation, we model the relations between queries and POIs using the Query-click-POI edge. Specifically, we connect Query-click-POI edges between each POI node and its historical query nodes. To speed up training with large-scale data, we select the top 4 searched queries for each POI by following <ref type="bibr" target="#b14">[15]</ref>.</p><p>2.1.3 Origin-to-Destination Edge. A user's mobility behavior <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref> produces a visited POI sequence in search logs. From which, the origin POI and destination POI can be extracted to construct the Origin-to-Destination edge between two POIs. Specifically, we perform a 2-gram sliding window on the POI sequences and link the adjacent POIs with such edge type.  <ref type="figure">2</ref>, we quantize the Earth's surface as a grid using a Discrete Global Grid<ref type="foot" target="#foot_0">1</ref> (DGG) system and construct the co-location edge between POIs that lie in the same cell of the grid. This can be regarded as an analogy to the surrounding words, where each word in a sentence is surrounded by a textual context. In this way, each POI node in the heterogeneous graph has a spatial context representing the real-world spatial distribution. Specifically, we use the S2 geometry<ref type="foot" target="#foot_1">2</ref> as the DGG system. See Appendix A.1 for a detailed comparison of recently proposed DGG systems and the reason that we choose S2 geometry. The S2 geometry library supports 31 levels of hierarchy, where its grid's cells have different area coverage. For simplicity, we use the term "S2 cell" to refer to the cells generated by the S2 geometry library. We  for ? = 0 to ? do  end for 8: end for construct the co-location edges between the POIs within the same S2 cell of level 15, which covers an area of ~200 m 2 .</p><p>2.1.5 Random Walk Sampling. Upon G, we use the random walk algorithm to sample a sequence of nodes as an input document</p><formula xml:id="formula_0">? = {? 1 , ? 2 , ? ? ? , ? ? }, where ? is the length of ?. Each node ? ? has a text representation consisting of a sequence of words ? ? = {? ? 1 , ? ? ? , ? ? ? , ? ? ? , ? ? ? }.</formula><p>During random walk sampling, at time step ?, we sample the node by considering the influence of different edges. Specifically, we use the weighted probability distributed over the neighbors of ? ? as the transition probability ? (? |?):   </p><formula xml:id="formula_1">? (? |?) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 * 1 |? (? ?,? )| , ?? (? ?,? ) ? Query-click-POI ???? ? 2 * 1 |? (? ?,? )| , ?? (? ?,? ) ? Origin-to-Destination ???? ? 3 * 1 |? (? ?,? )| , ?? (? ?,? ) ? POI-(co-locate with)-POI ????<label>(</label></formula><formula xml:id="formula_2">h CLS ~1 H context 1 h CLS ~2 H context 2 h CLS ~3 H context 3 h CLS ~4 H context 4 h CLS ~5 H context 5 h CLS ~1 H context 1 h CLS ~2 H context 2 h CLS ~3 H context 3 h CLS ~4 H context 4 h CLS ~5 H context 5 h CLS 1 H context 1 h CLS 2 H context 2 h CLS 3 H context 3 h CLS 4 H context</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>As shown in Figure <ref type="figure" target="#fig_8">3</ref>, the two major components in ERNIE-GeoL's model architecture are a multi-layer bidirectional Transformer <ref type="bibr" target="#b33">[34]</ref> encoder and a transformer-based aggregation (TranSAGE) layer. The document in traditional NLP tasks consists of multiple sentences, where discourse structure of the document should be considered. By contrast, the input document ? for pre-training ERNIE-GeoL consists of a sequence of nodes, and there is no discourse structure in ?. Therefore, instead of concatenating all the nodes and modeling them as one text sequence, we use the Transformer encoder to get each node's hidden vector separately, and employ the TranSAGE layer to capture the relations between each node and its neighbors. As a graph-contextualized representation, the output of the TranSAGE is fused with each node's vector. Then, each node's fused representation is used for the pre-training tasks. Formally, for each node ? ? in document ?, we first use the sentence-piece algorithm to tokenize its text representation ? ? into a sub-word sequence</p><formula xml:id="formula_3">? ? = {? ? 1 , ? ? ? , ? ? ? , ? ? ? , ? ? ? }</formula><p>, where ? ? ? denotes a sub-word token and ? is the length of the sub-word sequence. Then, we insert a "[CLS]" token to the head of the tokenized sequence and use a multi-layer bidirectional Transformer encoder to get the node ? ? 's vector representation {h ? ??? , H ? ??????? } as follows:</p><formula xml:id="formula_4">{h ? ??? , H ? ??????? } = Transformer({[???], ? ? 1 , ? ? ? , ? ? ? , ? ? ? , ? ? ? }),<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">H ? ??????? = {h ? 0 , ? ? ? , h ? ? , ? ? ? , h ? ? } and h ? ? ? R ? ? is the hidden vector of ? ? ? . h ? ??? ? R ? ?</formula><p>is the final hidden vector of the special token "[CLS]", which is the aggregated representation of ? ? .</p><p>After we obtain each node's vector representation, we need to model the relations between different nodes. A straightforward way to model such relations is directly adopting the vanilla multi-head attention <ref type="bibr" target="#b33">[34]</ref> mechanism to project the input sequence into a key matrix and a value matrix with corresponding weights. The attention scores are computed by the dot product of these two matrices. However, such attention mechanism is agnostic of the different node's type in the heterogeneous graph. Taking node type into consideration, we propose a TranSAGE layer, which use different weights for the projection based on the node's type. Specifically, we first pack the aggregated representations of all nodes together into a matrix</p><formula xml:id="formula_6">H = {h 1 ??? , ? ? ? , h ? ??? , ? ? ? , h ? ??? }.</formula><p>Then, we use the TranSAGE layer to compute the output matrix H as follows:</p><formula xml:id="formula_7">H = concat(???? 1 , ? ? ? , ???? ? )? ? , ???? ? = softmax( ? ? ? ? ? ? ? )H, ? ? = Q-Linear ? (? ? ) (H), ? ? = K-Linear ? (? ? ) (H),<label>(3)</label></formula><p>where Q-Linear ? (? ? ) : R ? ? ? R ? ? ? is a linear projection indexed by each node's type, and ? is the number of the head. We use this projection layer to convert H into a query matrix ? ? for the ?-th head, where nodes with different types are computed with unique parameters. Similarly, we use another linear projection K-Linear ? (? ? ) to compute a key matrix ? ? for the ?-th head.</p><p>Finally, we apply another attention-based module to each subword representation h ? ? with its corresponding graph contextualized representation h? ??? ? H. Specifically, as shown in the right part of Figure <ref type="figure" target="#fig_8">3</ref>, we replace h ? ??? in Equation 2 with h? ??? and use a new Transformer layer to get each node's representation for computing the pre-training objectives as follows:</p><formula xml:id="formula_8">{ h ? ??? , H ? ??????? } = Transformer({ h? ??? , H ? ??????? }),<label>(4)</label></formula><p>where h ? ??? is used for training the geocoding task and H ? ??????? is used for training the masked language modeling (MLM) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-training ERNIE-GeoL</head><p>2.3.1 Masked Language Modeling. We use the whole word mask (WWM) strategy to make predictions for the phrases in each document. We use a query component analysis module deployed at Baidu Maps to split each document at the granularity of geographic entities. Each geographic entity in a document has a 15% probability of being masked and predicted by the language model during the training process. For each word in the selected entity, we replace the word with a "[MASK]" token with 70% probability, replace the word with a misspelled word with 10% probability, replace the word with a random word with 10% probability, and leave the word unchanged with 10% probability. The words in a query that do not match any words in the target POI name are treated as misspelled words. With this training procedure, we can learn four types of toponym knowledge in the masked language modeling (MLM) task as follows. <ref type="bibr" target="#b0">(1)</ref> The natural language descriptions of POI name and address. <ref type="bibr" target="#b1">(2)</ref> The relationships between POI name, address, and type.</p><p>(3) The relationships between query, POI name, and address. (4) The possible misspelling of POI name and address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Geocoding.</head><p>We provide new insights into learning the relations between text and geographic coordinates of a POI during pre-training. Specifically, we adopt a feed-forward layer for each POI node to predict the IDs for multi-level S2 cells converted from the POI's coordinates. In order to model the relations between text and coordinates in a more fine-grained manner, we set the highest level of S2 cell to 22, which covers an area approximate to 2 m 2 . However, in the granularity of level 22, the Earth's surface is divided into 105 trillion S2 cells. Directly predicting these S2 cell's IDs will introduce an overwhelming number of output classes. To address this problem, we first encode multi-level S2 tokens into a sequence, and then make predictions on it. Figure <ref type="figure" target="#fig_9">4</ref> shows an example of our proposed geocoding task, where the goal is to predict the sequence "453 cf5 41f 450 475" for the input text ? ? .</p><p>For the same coordinates, the S2 tokens of two consecutive levels 2? +1 and 2? +2 (? &gt; 0) are designed to have a same length. Suppose their length is ? ? , then the S2 tokens of the levels 2?-1 and 2? would have a length of ? ? -1. Moreover, the tokens of the levels 2? -1 and 2? differ only in the last character. To represent the multi-level S2 tokens of a POI as a single sequence efficiently and obtain the ability of parallel computing, we propose a new encoding/decoding scheme called 2Lt3C. For simplicity, we use the term "(2?)'s token" to denote "the S2 token of the level 2?". For encoding, we use the last character of (2? -1)'s token, the last character of (2?)'s token, and the shared penultimate character between (2? -1)'s and (2?)'s tokens, to generate a sub-sequence with three characters. For example, the sub-sequence "475" is generated from the L9's token "35f054" and the L10's token "35f057". Figure <ref type="figure" target="#fig_9">4</ref> shows an example of the encoding scheme for levels from 1 to 10. In this way, sufficient information of the multi-level S2 tokens can be encoded into a single sequence, which is also necessary to decode all levels.</p><p>Specifically, we cast the task of predicting the sequence encoded by 2Lt3C as ? independent classification problems, where each classifier learns to predict a character in this sequence. Although this approach ignores potential correlations among characters, it offers the opportunity for parallel computing, and improves computational efficiency. For each classifier, we use a softmax layer to compute the probability of classifying ? ? as a certain character by:</p><formula xml:id="formula_9">?? (? ? |? ? ) = softmax( h ? ??? W ? ),<label>(5)</label></formula><p>where </p><formula xml:id="formula_10">W ? ? R ? ? ?16 is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we present the results of ERNIE-GeoL on five georelated tasks and the ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Geo-related Tasks</head><p>Task #1: Query Intent Classification. The query intent classification <ref type="bibr" target="#b21">[22]</ref> task aims at predicting the intent behind a query, which plays an important role in the POI search engine of Baidu Maps. We define four intents, including the search for a specific POI, for a specific type of POI, for addresses, and for bus routes.</p><p>To evaluate its performance, we randomly sample 60,000 queries from the search logs of Baidu Maps and manually annotate them.</p><p>In our experiments, we use h ? ??? (see Equation <ref type="formula" target="#formula_4">2</ref>) as the input representation, based on which a linear layer is adopted to perform the classification. We use accuracy as the evaluation metric, which represents the proportion of correctly predicted queries.</p><p>Task #2: Query-POI Matching. The query-POI matching <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> task aims at identifying the more relevant POI for a given query from a list of POIs. There are four levels of relevance: the POI is exactly matched, highly relevant, weakly relevant, and irrelevant to the query. The relevance score predicted by the matching model is an important feature for ranking the candidate POIs in the search engine of Baidu Maps.</p><p>To construct the dataset for this task, we randomly sample realworld queries from the search logs of Baidu Maps. For each query, we randomly sample 6 POIs from the top 10 POI candidates ranked by the POI search engine of Baidu Maps, as well as another four random POIs. Then, we ask annotators to manually examine the 10 candidate POIs of each query and assign relevance levels to them.</p><p>In our experiments, we also use h ? ??? to perform this task and employ accuracy as the evaluation metric. To build the dataset for this task, we utilize two data sources. One data source is the anonymized query logs of the geocoding service of Baidu Maps. The other data source is the anonymized POI search logs of Baidu Maps. Then, we ask the annotators to annotate every possible chunk of the addresses and queries.</p><p>We formulate this task as a sequence labeling problem and use ERNIE-GeoL + CRF as the model architecture. We employ the entitylevel ? 1 score to evaluate the performance of chunk detection.</p><p>Task #4: Geocoding. Given a geo-located entity reference in text, the geocoding task <ref type="bibr" target="#b8">[9]</ref> aims at resolving the input to a corresponding location on the Earth. As described in Section 2.3.2, we directly predict S2 tokens for the input text. The geocoding task is an essential service of mapping applications, its output is also a crucial feature required by other services like POI retrieval.</p><p>To collect the dataset for geocoding task, we use the same data built for the above-mentioned address parsing task. For each address ?? ? , we first obtain its geographic coordinates ?? ? predicted by an in-house geocoding service, and then correlate ?? ? with an S2 token converted from ?? ? . In this way, we can automatically construct large amounts of training data. Since the accuracy of our existing geocoding service cannot reach 100%, we manually annotate 2,000 addresses as the development set and test set, respectively. Different from the well-formatted address descriptions derived from our POI database for pre-training ERNIE-GeoL, the address descriptions in this dataset are generated by different users with varied knowledge and are not well formatted. As such, the data leakage problem can be avoided. Therefore, we can use this dataset to sufficiently validate a model's ability to correlate text with geographic coordinates.</p><p>The model architecture used for fine-tuning this task is the same as that described in Section 2.3.2. We use "Accuracy@N km" as the evaluation metric, which measures the percentage of predicted locations that are apart with a distance less than N km to their actual physical locations. In our experiments, we set N to 3.</p><p>Task #5: Next POI Recommendation. Given a user's sequence of historical POI visits, the task of next POI recommendation (NPR) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> aims at recommending a list of POIs that the user is most likely to visit consequently. NPR is an essential feature in Baidu Maps, which can help users explore new POIs.</p><p>To construct the dataset for NPR task, we use the anonymized POI visiting data in Beijing within a 6-month period from Baidu Maps. For each POI sequence, we process a sliding window with a randomly selected width from 3 to 6 to get some sub-sequences, where the last POI is regarded as the label and the rest of POIs are taken as the historical visits. For evaluation, we use 6 million POIs in Beijing as candidate POIs for retrieval.</p><p>In the fine-tuning phase, we use a two-tower approach similar to that used in our previous work <ref type="bibr" target="#b14">[15]</ref>. Taking ERNIE-GeoL as a feature encoder, we calculate the similarity between the graph-based visiting sequence representation and the target POI representation.</p><p>The key difference between pre-training and fine-tuning is that the input graph only contains POI-POI edges constructed with the POI visiting sequences. In the evaluation phase, we first generate vectors for all the input sequences and candidate POIs. Then, we use the HNSW <ref type="bibr" target="#b25">[26]</ref> algorithm to process an approximate K-nearest neighbor search for retrieving the target POI. We use Acc@K as the evaluation metric, which calculates the proportion of the recommended POI sequences where the visited POI appears within the top K positions. In our experiments, we set K to 50.</p><p>The five tasks and datasets used to evaluate ERNIE-GeoL are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Datasets.</head><p>In our experiments, we construct the heterogeneous graph using search logs within a 3-month period from Baidu Maps. The heterogeneous graph contains 40 million POI nodes, 120 million query nodes, 175 million Query-click-POI edges, 1,574 million Origin-to-Destination edges, and 363 million POI-(co-locate with)-POI edges. We use the random walk algorithm on the heterogeneous graph to sample a sequence of nodes as an input document. The sampling weights of Query-click-POI, Origin-to-Destination, and POI-(co-locate with)-POI edges are set as ? 1 = 0.5, ? 2 = 0.25, and ? 3 = 0.25, respectively. We sample 800 million documents from the graph, which contain 400 billion words. Each document contains an average of 10 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Baselines.</head><p>We evaluate ERNIE-GeoL against three strong generic PTMs as follows:</p><p>? BERT [6] is a Transformer-based pre-trained language model, which has made impressive gains on many NLP tasks. ? RoBERTa <ref type="bibr" target="#b23">[24]</ref> is a variant of BERT with enhanced strategies, which improves the performance on several NLP tasks. ? ERNIE 2.0 <ref type="bibr" target="#b31">[32]</ref> is another variant of BERT, which facilitates continuous learning of multiple pre-training tasks. It outperforms BERT and RoBERTa on many Chinese NLP tasks. We also perform ablation experiments over a number of facets of ERNIE-GeoL to figure out their relative importance, which include: when constructing the heterogeneous graph, respectively. All the pre-training and fine-tuning procedures are implemented using the PaddlePaddle deep learning framework. We use Adam optimizer <ref type="bibr" target="#b17">[18]</ref>, with the learning rate initialized to 5 ? 10 -5 and gradually decreased during the process of training. The hyperparameters of all PTMs are the same as those used in BERT_BASE <ref type="bibr" target="#b5">[6]</ref> (number of hidden layers = 12, hidden layer size = 768, number of attention heads = 12, number of total parameters = 110M). The training takes about one week on 16 Nvidia A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Analysis</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the main experiment results. The last column "Average" presents the averaged score of the five tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Overall</head><p>Performance. We first evaluate whether ERNIE-GeoL can improve the performance of the five geo-related tasks. Before ERNIE-GeoL, we have applied ERNIE to improve these tasks at Baidu Maps and obtained initial gains. However, a clear gain plateau over time was observed due to the lack of geographic domain knowledge in ERNIE. This motivated us to design and develop a geography-and-language pre-trained model ERNIE-GeoL for improving the geo-related tasks at Baidu Maps. The results in Table <ref type="table" target="#tab_2">2</ref> show that ERNIE-GeoL significantly outperforms all three generic PTMs (i.e., BERT, RoBERTa, and ERNIE) by a large margin, and achieves a highest average score of 0.6878. This demonstrates that our model is more effective in dealing with geo-realted tasks. One of the main reasons for this superiority is that ERNIE-GeoL has comprehensively learned the geographic knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Ablation</head><p>Studies. We perform ablation experiments to understand the relative importance of different facets of ERNIE-GeoL.</p><p>First, we study the effect of pre-training tasks. Compared with ERNIE-GeoL, the average score of "ERNIE-GeoL w/o geocoding task" dramatically decreases by an absolute 3.71%, which is the largest drop observed among all ablation experiments. This demonstrates the impact brought by the geocoding task. The main reason is that geo-information plays an vital role in geo-related tasks. Therefore, the ability to learn a universal representation of geographylanguage is crucial for pre-training a geographic model.</p><p>Second, we evaluate the effect of heterogeneous graph. The results in Table <ref type="table" target="#tab_2">2</ref> show that removing the graph (ERNIE-GeoL w/o heterogeneous graph) hurts performance significantly on all tasks. This demonstrates the significance of graph structure in the training data, which can effectively integrate spatial knowledge with text.</p><p>Third, we examine the impact of different edge types. The results show that removing individual edges (ERNIE-GeoL w/o P-c-P edge, w/o O-t-D edge, and w/o Q-c-P edge) hurts performance on all tasks. This indicates that all types of edges are essential for pre-training a geographic model, and they can work together as complements. Removing the O-t-D edge (ERNIE-GeoL w/o O-t-D edge) leads to the largest drops among the three ablations of edges, this demonstrates the importance of human mobility data in geo-related tasks.</p><p>Finally, we evaluate the performance of individual tasks. (1) Compared with ERNIE 2.0 (the best performing generic PTM), the performance of geocoding trained with ERNIE-GeoL achieves the largest increase of 19.09% (by absolute value). We also showcase how this model can robustly deal with different types of input text in Appendix A.4. Moreover, the "ERNIE-GeoL w/o geocoding task" model performs worse than the ERNIE-GeoL model on the geocoding task, with a large drop of 11.09% (by absolute value). The main reason is that the performance of a geocoding model heavily relies on its ability to correlate text with geographic coordinates. This demonstrates that ERNIE-GeoL has learned sufficient geographic knowledge about text and coordinates during pre-training. <ref type="bibr" target="#b1">(2)</ref> The task that has the second highest benefits among five tasks is next POI recommendation, with an absolute improvement of 3.58% over ERNIE. Moreover, removing the O-t-D edge (ERNIE-GeoL w/o O-t-D edge) leads to a drop of 1.53% on the next POI recommendation task. The main reason is that prior knowledge of the distribution of human mobility data is crucial for the next POI recommendation task. This indicates that ERNIE-GeoL has learned the distribution of human mobility from the training data. (3) Among the generic PTMs, ERNIE 2.0 outperforms BERT and RoBERTa on five tasks. This shows that the optimization made by ERNIE 2.0 for dealing with Chinese NLP tasks, such as the masking strategy that masks   phrases and entities rather than individual sub-words, can be a benefit for Chinese toponym masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Qualitative Study on Geo-Linguistic</head><p>Knowledge ERNIE-GeoL Has Learned 3.4.1 Embedding Projection. For an intuitive understanding of the geo-linguistic knowledge ERNIE-GeoL has learned, we encode POIs to embeddings and project them into a two-dimensional space using the t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b32">[33]</ref> method. We select the top 500 frequently searched POIs at Baidu Maps in 31 provinces excluding Hong Kong, Macao, and Taiwan of China. To get each POI's embedding, we first concatenate its name and address as the input for PTMs. Then, we use the output hidden state of the "[CLS]" token as its embedding. Figure <ref type="figure" target="#fig_12">5</ref> illustrates the t-SNE visualization of embeddings predicted by BERT and ERNIE-GeoL. From which, we can clearly see that the t-SNE embeddings predicted by ERNIE-GeoL (Figure <ref type="figure" target="#fig_12">5b</ref>) are more discriminative than those predicted by BERT (Figure <ref type="figure" target="#fig_12">5a</ref>), which shows that POIs located in different provinces can be geographically differentiated by our ERNIE-GeoL model. Moreover, in Figure <ref type="figure" target="#fig_12">5b</ref>, geographically adjacent provinces tend to be adjacent to one another. For example, the points of Heilongjiang, Jilin, and Liaoning provinces co-locate in the upper-right corner of Figure <ref type="figure" target="#fig_12">5b</ref>. In reality, the three provinces are also adjacent to each other in the northeast part of China. These observations show that ERNIE-GeoL has successfully learned the relations between text and their real-world geographic locations.</p><p>3.4.2 Geographic Analogy. Following the famous ???? -??? + ????? ? ????? example <ref type="bibr" target="#b26">[27]</ref>, we curate two test cases to examine how well ERNIE-GeoL can learn the geographic analogy. As shown in Figure <ref type="figure">6</ref> and Figure <ref type="figure" target="#fig_13">7</ref>, we test the geographic analogy of "district of a city" and "capital city of a province", respectively. We show the top 10 neighbors ranked by their cosine similarity to the query. In Figure <ref type="figure">6</ref>, the query is set to "Huangpu District -Shanghai + Beijing",  and the candidate neighbors are set to district names of all Chinese cities. We can observe that ERNIE-GeoL recalls more Beijing's districts than BERT. In Figure <ref type="figure" target="#fig_13">7</ref>, the query is set to "Guangdong Province -Guangzhou + Kunming", and the candidate neighbors are set to all Chinese province names. We can observe that ERNIE-GeoL recalls the target neighbor, "Yunnan Province", with the highest score. Moreover, in both Figures, the cosine similarity predicted by BERT is less discriminative than that by ERNIE-GeoL. Such observations show that ERNIE-GeoL has learned the spatial relationships between different geo-located entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Here we briefly review the closely related work in the fields of domain-specific PTMs and PTMs utilizing multi-source data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domain-specific PTMs</head><p>Existing domain-specific PTMs mainly lie in the domain of healthcare <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, biomedical <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>, and academic &amp; research <ref type="bibr" target="#b1">[2]</ref>. Most of them learn the domain-specific knowledge by pre-training on domain-specific corpora with the MLM pre-training task. The most relevant work to ours is OAG-BERT <ref type="bibr" target="#b22">[23]</ref>, which is an academic PTM pre-trained using the heterogeneous knowledge from an academic knowledge graph. However, they do not model the graph structure explicitly, like our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PTMs with Multi-source Data</head><p>Most existing multimodal PTMs are designed to model the relations between text and image <ref type="bibr" target="#b24">[25]</ref>, video <ref type="bibr" target="#b29">[30]</ref>, and audio <ref type="bibr" target="#b4">[5]</ref>. However, pre-training a geographic PTM requires modeling the relations between text and geographic coordinates (in the form of numerical data). Such an intersection of multiple modalities of text and numbers has not been well explored in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>This paper presents an industrial solution for building a geographyand-language pre-trained model that has already been deployed at Baidu Maps. We propose a framework, named ERNIE-GeoL, that comprehensively learns geographic domain knowledge. Sampled from a heterogeneous graph constructed upon the POI database and the search logs of Baidu Maps, the documents used for pre-training ERNIE-GeoL are injected with toponym and spatial knowledge. The backbone network of ERNIE-GeoL contains an aggregation layer for modeling the graph structure entailed in the input documents. ERNIE-GeoL adopts two pre-training objectives, including masked language modeling and geocoding, for guiding the model to learn the toponym and spatial knowledge, respectively. We evaluate ERNIE-GeoL on a benchmark built based on five tasks that provide fundamental support for an essential mapping service. The experiment results and ablation studies show that ERNIE-GeoL outperforms previous generic pre-trained models, demonstrating that ERNIE-GeoL can serve as a promising foundation for a wide range of geo-related tasks.</p><p>In future work, to make ERNIE-GeoL capable of handling a more wide range of geographic applications, we plan to enhance ERNIE-GeoL with satellite images and street views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In this section, we first compare different DGG systems, and detail the node representation method. Then, we showcase an example of the generated pre-training data, and how ERNIE-GeoL can help the geocoding model to effectively handle different types of input. We also show how to fine-tune downstream tasks with ERNIE-GeoL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Comparison of Different DGG Systems</head><p>Table <ref type="table" target="#tab_5">3</ref> shows the comparison of three recently proposed DGG systems including Geohash<ref type="foot" target="#foot_2">3</ref> , H3 system<ref type="foot" target="#foot_3">4</ref> , and S2 geometry <ref type="foot" target="#foot_4">5</ref> . We compare them in terms of projection method, geographic containment, and the number of levels.</p><p>First, modern DGG systems usually adopt a projection method to transform a three-dimensional location on the Earth into a twodimensional point on a map. Compared with Geohash, H3 and S2 use spherical projections, which significantly reduce the distortion brought by the Mercator projection. Second, all three systems are hierarchical systems in which each cell is a "box reference" to a subset of cells. Here, we use "geographic containment" to indicate whether all child cells are perfectly contained within a parent cell. Compared with Geohash and S2, H3's geographic containment is approximate since it uses hexagons for tilling. Third, the more levels a system supports, the greater is its flexibility. S2 supports 31 levels of hierarchy, while Geohash/H3 only supports 12/15. We choose S2 as our DGG system based on the following three considerations. First, it uses the hexahedral projection scheme to avoid distortion, which is able to preserve the correct topology of the Earth. Second, S2 supports 31 levels of hierarchy, which offers the best flexibility. Third, S2's geographic containment is accurate, which underpins the design, development, and implementation of the proposed geocoding pre-training task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Node Representation</head><p>As described in Section 2.1.1, we uniformly use the textual information of each node to represent the query nodes and POI nodes.</p><p>For a query node, since the query used to find a user's desired POI is already in text format, we use its text to represent the query node. For example, in Figure <ref type="figure" target="#fig_15">8</ref>, the text "No.1 Songzhuang Road Yichi Food" is used to represent the query node "Q1".</p><p>For a POI node, we first extract the POI's text fields of name, address, and type from the POI database of Baidu Maps. Then, we generate a text sequence by joining the three fields with a special token "[SEP]", to represent the POI node. For example, in Figure <ref type="figure" target="#fig_15">8</ref>, the text sequence "Yizi Food (Suzhou) Co. [SEP] No.1, Songxiang, Suzhou Industrial Park, Suzhou, Jiangsu Province [SEP] Company" is used to represent the POI node "01", where "Yizi Food (Suzhou) Co.", "No.1, Songxiang, Suzhou Industrial Park, Suzhou, Jiangsu Province", and "Company" are the name, address, and type of POI "01", respectively.  For toponym knowledge, we can see that a POI's text sequence contains the exact toponyms like the POI name and the address that the POI locates in. In addition, the queries often contain different formulations of the same toponym. On the one hand, different formulations can reveal multiple aliases of the same POI, which can help the model learn diversified expressions of a formal toponym. For example, we can observe from Figure <ref type="figure" target="#fig_15">8</ref> that users have used three different queries "Q1", "Q2", and "Q3" to search the same POI "01". On the other hand, large-scale formulations uncover the majority of the frequent misspelling patterns, from which we can learn misspelling regularities that make the pre-training model to robustly generalize to unseen misspelling patterns. For example, the yellow-colored formulation "Yichi Food" is a misspelled query of the POI name "Yizi Food" as shown in Figure <ref type="figure" target="#fig_15">8</ref>.</p><p>For spatial knowledge, we can observe from Figure <ref type="figure" target="#fig_15">8</ref> that the graph contains human mobility patterns. For example, the POI visit sequence "02-to-03" represents a mobility pattern of "workplaceto-market". These patterns are valuable for geo-related tasks that need to take human mobility into consideration, such as the next POI recommendation task. Moreover, we can see that the graph also contains the spatial correlation information. For example, the POIs "01" and "02" are linked by a POI-(co-located with)-POI edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Typical Examples of the Geocoding Task</head><p>As described in Section 3.1, given a geo-located entity reference in text as input, the geocoding task aims at resolving the input to the corresponding location on the Earth. Figure <ref type="figure">9</ref> illustrates four typical examples of the geocoding task. Given four different input text (D1, D2, D3, and D4) that attempt to describe the target POI (Vanke City #2, Jingyue Street, Nanguan District, Changchun City, Jilin Province), the coordinates predicted by our model are represented as blue pins in the figure. First, we use the ERNIE-GeoL enhanced geocoding model to generate an S2 token for each input text. Then, we take as output the coordinates located in the centroid of the territory represented by the generated S2 token.</p><p>We can observe from Figure <ref type="figure">9</ref> that our model can make accurate predictions for different variety of descriptions of the same POI. These examples show that our model is able to robustly handle the following three categories of descriptions. (1) Incomplete description. In this category, the description of an address usually omits some essential address elements. For example, in the input D1 (Vanke City #2, Jingyue Street), the descriptions of province and city are both omitted. (2) Informal description. In this category, some address elements are described informally. For example, in the input D4, the street and district name "Jingyue Street, Nanguan District" is informally abbreviated as "Jingyue Nanguan". (3) Geographically irrelevant description. In this category, the description</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the multimodal data and spatial data used for training generic and geographic PTMs, respectively.</figDesc><graphic url="image-1.png" coords="1,411.13,170.25,157.79,104.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 1 . 2 Figure 2 :</head><label>122</label><figDesc>Figure 2: The process of constructing the heterogeneous graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Random Walk Sampling on Heterogeneous Graph Input: Heterogeneous Graph G = (V, E, ? V , ? E ); ? walk length Output: Sequence of text nodes ? = {? 1 , ? 2 , ? ? ? , ? ? } 1: for each ? ? G do 2: ? = [?, ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1) where |? (? ?,? )| denotes the number of ?'s neighborhood with corresponding edge, and ? ? is the weight corresponding to different edges. Algorithm 1 shows the details. See Appendix A.3 for an illustration of the generated training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of ERNIE-GeoL.</figDesc><graphic url="image-159.png" coords="4,251.34,99.42,293.73,196.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the geocoding task. Each parallelogram represents an S2 cell of the corresponding level.</figDesc><graphic url="image-175.png" coords="5,317.84,84.08,117.75,139.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a trainable parameter, and ?? (? ? |? ? ) is the probability vector of a category ? ? ? {0, ? ? ? , 15} (6 English lettersa to f, and 10 Arabic numerals-0 to 9). The number of classifiers (?) for S2 cells with level 22 is 33 determined by the design of 2Lt3C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) The t-SNE visualization of embeddings produced by BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A 2D t-SNE projection of top 500 searched POIs in 31 provinces excluding Hong Kong, Macao, and Taiwan of China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Nearest Neighbors of "Guangdong Province -Guangzhou + Kunming". The darker bar represents Yunnan Province, the capital city of which is Kunming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>A. 3 A</head><label>3</label><figDesc>Case Study on the Pre-training Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8</head><label>8</label><figDesc>Figure 8  showcases an example of the heterogeneous graph and a training example generated from the graph. We show how toponym knowledge and spatial knowledge (see Section 1 for details) are entailed by the generated training example.For toponym knowledge, we can see that a POI's text sequence contains the exact toponyms like the POI name and the address that the POI locates in. In addition, the queries often contain different formulations of the same toponym. On the one hand, different formulations can reveal multiple aliases of the same POI, which can help the model learn diversified expressions of a formal toponym. For example, we can observe from Figure8that users have used three different queries "Q1", "Q2", and "Q3" to search the same POI "01". On the other hand, large-scale formulations uncover the majority of the frequent misspelling patterns, from which we can learn misspelling regularities that make the pre-training model to robustly generalize to unseen misspelling patterns. For example, the yellow-colored formulation "Yichi Food" is a misspelled query of the POI name "Yizi Food" as shown in Figure8.For spatial knowledge, we can observe from Figure8that the graph contains human mobility patterns. For example, the POI visit sequence "02-to-03" represents a mobility pattern of "workplaceto-market". These patterns are valuable for geo-related tasks that need to take human mobility into consideration, such as the next POI recommendation task. Moreover, we can see that the graph also contains the spatial correlation information. For example, the POIs "01" and "02" are linked by a POI-(co-located with)-POI edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-205.png" coords="11,54.41,301.43,502.11,220.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Tasks and Datasets used to evaluate ERNIE-GeoL. #Dev = the number of samples in development set.</figDesc><table><row><cell>Task</cell><cell>Problem Formulation</cell><cell>Applicable Service</cell><cell>#Train</cell><cell>#Dev</cell><cell>#Test</cell><cell>Metric</cell></row><row><cell>Query intent classification</cell><cell>Sequence classification</cell><cell>POI search engine</cell><cell>50,000</cell><cell>5,000</cell><cell>5,000</cell><cell>Accuracy</cell></row><row><cell>Query-POI matching</cell><cell>Sequence pair classification</cell><cell>POI search engine</cell><cell>140,614</cell><cell>4,000</cell><cell>4,000</cell><cell>Accuracy</cell></row><row><cell>Address parsing</cell><cell>Sequence labeling</cell><cell cols="2">POI information processing 125,009</cell><cell>10,000</cell><cell cols="2">10,000 Entity-level ? 1</cell></row><row><cell>Geocoding</cell><cell>Sequence classification</cell><cell cols="2">POI information processing 2,171,114</cell><cell>1,000</cell><cell>1,000</cell><cell>Acc@N km</cell></row><row><cell>Next POI recommendation</cell><cell>Relevance ranking</cell><cell>POI recommendation</cell><cell cols="3">6,398,231 300,000 300,000</cell><cell>Acc@K</cell></row><row><cell cols="3">Task #3: Address Parsing. The task of address parsing [21] aims</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">at parsing an address into a sequence of fine-grained geo-related</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">chunks. In this work, we designed 22 chunk types, among which</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">9 are for different levels of geographic areas, 2 for roads, 3 for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">different types of POI, 5 for details of POI location, and 3 for the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">auxiliary words that describe the location. Plenty of modules in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Baidu Maps, including a rule-based geocoding framework and the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">query understanding module of the POI search engine, rely on the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">output of the address parsing model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of pre-trained models on five geo-related tasks. Average means the averaged score of five tasks. GeoL is the complete model depicted in Section 2. ? ERNIE-GeoL w/o geocoding task. In this setting, we remove the pre-training task of geocoding (see Section 2.3.2). ? ERNIE-GeoL w/o heterogeneous graph. In this setting, we remove all edges in the heterogeneous graph and pre-train ERNIE-GeoL using the text representation of each node, as described in Section 2.1.1. ? ERNIE-GeoL w/o a specific type of edge. In this group of</figDesc><table><row><cell>Pre-trained Model</cell><cell>Query Intent Classification</cell><cell>Query-POI Matching</cell><cell cols="2">Address Parsing Geocoding</cell><cell>Next POI Recommendation</cell><cell>Average</cell></row><row><cell>BERT [6]</cell><cell>0.8875</cell><cell>0.8279</cell><cell>0.8452</cell><cell>0.4592</cell><cell>0.1092</cell><cell>0.6258</cell></row><row><cell>RoBERTa [24]</cell><cell>0.8907</cell><cell>0.8285</cell><cell>0.8497</cell><cell>0.4618</cell><cell>0.1115</cell><cell>0.6284</cell></row><row><cell>ERNIE 2.0 [32]</cell><cell>0.8919</cell><cell>0.8290</cell><cell>0.8511</cell><cell>0.4636</cell><cell>0.1198</cell><cell>0.6311</cell></row><row><cell>ERNIE-GeoL</cell><cell>0.9161</cell><cell>0.8332</cell><cell>0.8794</cell><cell>0.6545</cell><cell>0.1556</cell><cell>0.6878</cell></row><row><cell>-w/o geocoding task</cell><cell>0.9068</cell><cell>0.8050</cell><cell>0.8682</cell><cell>0.5436</cell><cell>0.1301</cell><cell>0.6507</cell></row><row><cell>-w/o heterogeneous graph</cell><cell>0.9101</cell><cell>0.8025</cell><cell>0.8688</cell><cell>0.5809</cell><cell>0.1359</cell><cell>0.6596</cell></row><row><cell>-w/o O-t-D edge</cell><cell>0.9076</cell><cell>0.8129</cell><cell>0.8715</cell><cell>0.6273</cell><cell>0.1403</cell><cell>0.6719</cell></row><row><cell>-w/o Q-c-P edge</cell><cell>0.9155</cell><cell>0.8072</cell><cell>0.8784</cell><cell>0.6381</cell><cell>0.1413</cell><cell>0.6761</cell></row><row><cell>-w/o P-c-P edge</cell><cell>0.9148</cell><cell>0.8305</cell><cell>0.8780</cell><cell>0.6164</cell><cell>0.1458</cell><cell>0.6771</cell></row><row><cell>? ERNIE-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>settings, we remove the edge of Origin-to-Destination (O-t-D), query-click-POI (Q-c-P), and POI-(co-locate with)-POI (P-c-P)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of three DGG systems.</figDesc><table><row><cell>Geocode System</cell><cell>Projection Method</cell><cell>Geographic Containment</cell><cell>#Levels</cell></row><row><cell>Geohash</cell><cell>Mercator</cell><cell>Accurate</cell><cell>12</cell></row><row><cell>H3</cell><cell cols="2">Icosahedron Approximate</cell><cell>15</cell></row><row><cell>S2</cell><cell>Hexahedron</cell><cell>Accurate</cell><cell>31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Discrete_global_grid</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://s2geometry.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://en.wikipedia.org/wiki/Geohash</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://eng.uber.com/h3/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://s2geometry.io</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>usually contains some elements that are irrelevant to geographic information. For example, the "One large closet" in input D3 is totally irrelevant to the POI. In our practice, such descriptions often confuse the model and lead to inaccurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 How to Fine-tune Tasks with ERNIE-GeoL</head><p>In general, we can use ERNIE-GeoL as a feature encoder for finetuning downstream tasks. There are two options for encoding the input of the downstream tasks.</p><p>(1) If the input of the downstream tasks contains a graph structure, we can encode each node ? ? in the input as { h ? ??? , H ? ??????? } calculated by the Equation <ref type="formula">4</ref>. In our practice, typical tasks that have an input with a graph structure include POI recommendation, context-aware POI retrieval, and task-oriented dialogue generation in our intelligent voice assistant.</p><p>(2) If the input of the downstream tasks is a text sequence, we can regard the input as a single node and encode it as {h ? ??? , H ? ??????? } calculated by the Equation <ref type="formula">2</ref>. For example, the tasks that only have a text sequence as input include context-insensitive query intent classification, address parsing, and geocoding.</p><p>Then, the encoded input can be further handled by the taskspecific neural network layers to obtain the final output for training or inferencing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Publicly Available Clinical BERT Embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ClinicalNLP. 72-78</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum Meta-Learning for Next POI Recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2692" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SpeechBERT: Cross-Modal Pre-trained Language Model for End-to-end Spoken Question Answering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Meta-Learned Spatial-Temporal POI Auto-Completion for the Search Engine at Baidu Maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>KDD. 2822-2830</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometry-enhanced molecular representation learning for property prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From text to geographic coordinates: the current state of geocoding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">URISA journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="33" to="46" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Georeferencing: The geographic associations of information</title>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DuIVA: An Intelligent Voice Assistant for Hands-free and Eyes-free Voice Interaction with the Baidu Maps App</title>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Personalized Prefix Embedding for POI Auto-Completion in the Search Engine of Baidu Maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding the Impact of the COVID-19 Pandemic on Transportation-Related Behaviors with Human Mobility Data</title>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3443" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual POI Retrieval at Baidu Maps</title>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Li</surname></persName>
		</author>
		<idno>KDD. 3032-3040</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Quantifying the Economic Impact of COVID-19 in Mainland China Using Human Mobility Data</title>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2020. 2005. 2020</date>
			<biblScope unit="page">3010</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toponym Resolution in Text: Annotation, Evaluation and Applications of Spatial Grounding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName><surname>Leidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="126" />
			<date type="published" when="2007-12">2007. dec 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Chinese Address Parsing</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3421" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Query Intent from Regularized Click Graphs</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv abs/2103.02410</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<idno>ArXiv abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName><surname>Da Y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VideoBERT: A Joint Model for Video and Language Representation Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vndrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GEDIT: Geographic-Enhanced and Dependency-Guided Tagging for Joint POI and Accessibility Extraction at Baidu Maps</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4135" to="4144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Continual Pre-training Framework for Language Understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>2020. ERNIE 2.0</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Haoyi Xiong, and Dejing Dou. 2021. C-Watcher: A Framework for Early Detection of High-Risk Neighborhoods Ahead of COVID-19 Outbreak</title>
		<author>
			<persName><forename type="first">Congxi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page" from="4892" to="4900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of point-of-interest recommendation in location-based social networks</title>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingguo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
