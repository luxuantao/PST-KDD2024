<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OuluVS2: a multi-view audiovisual database for non-rigid mouth motion analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Anina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision Research</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziheng</forename><surname>Zhou</surname></persName>
							<email>zhouzh@ee.oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision Research</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>gyzhao@ee.oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision Research</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision Research</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OuluVS2: a multi-view audiovisual database for non-rigid mouth motion analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">344ACEB6BA234FA7E9B64934142EDF19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual speech constitutes a large part of our nonrigid facial motion and contains important information that allows machines to interact with human users, for instance, through automatic visual speech recognition (VSR) and speaker verification. One of the major obstacles to research of non-rigid mouth motion analysis is the absence of suitable databases. Those available for public research either lack a sufficient number of speakers or utterances or contain constrained view points, which limits their representativeness and usefulness.</p><p>This paper introduces a newly collected multi-view audiovisual database for non-rigid mouth motion analysis. It includes more than 50 speakers uttering three types of utterances and more importantly, thousands of videos simultaneously recorded by six cameras from five different views spanned between the frontal and profile views. Moreover, a simple VSR system has been developed and tested on the database to provide some baseline performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual speech constitutes a large part of our non-rigid facial motion and contains important information that allows machines to interact with human users, for instance, through automatic visual speech recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and speaker verification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. On one hand, speech perception is a bimodal process that makes use of information related both to what we hear (acoustic) and what we see (visual) <ref type="bibr" target="#b5">[6]</ref>. There is clear evidence that visual cues of speech play an important role in automatic speech recognition when audio is corrupted or even inaccessible <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. On the other hand, visual speech contains unique information related to speakers' identities and may be used as some ideal facial biometrics since it allows us to conduct liveness detection that is critical to a verification system.</p><p>Despite the apparent motivation, the problem of non-rigid mouth motion analysis remains relatively under-studied. One of the major obstacles is the lack of suitable databases available for public research. By 'suitable', we mean that the database should include a relatively large number of speakers for sufficient training and testing, various types of utterances, such as digits, phrases and sentences, for building various applications and, last but not least, multiple views of talking mouths that represent the large variation we may encounter in a real-world situation since we cannot assume that users would face the video camera all the time during their interaction with machines. At the moment, there are only few audiovisual databases <ref type="bibr" target="#b1">[2]</ref> available for public research and none of them satisfies the above criteria.</p><p>Motivated by the demand for representative datasets, we collected a multi-view audiovisual database, named OuluVS2, for non-rigid mouth motion analysis. It contains videos of more than 50 speakers speaking various types of utterances, simultaneously recorded by six cameras from five different views and will be available for downloading in near future. This paper provides a comprehensive introduction to the OuluVS2 database. Moreover, we conducted VSR experiments in a setting that has been widely used in previous studies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The results reported in this paper can serve as the baseline performance for future research. They show that the best VSR performance does not come from frontal view videos, somehow contradicting our expectation that the frontal view may provide the most useful information for VSR. Such results could give us insight into future studies of non-rigid mouth motion analysis.</p><p>The rest of this paper is organized as follows: in Section 2 audio-visual databases available for public research are briefly described. Section 3 introduces the newly collected multi-view database, including the details of data collection, video format and preprocessing. Section 4 presents VSR experiments conducted on part of the OuluVS2 database as well as discussion about results. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we briefly describe all the publicly available audiovisual databases.</p><p>The XM2VTSDB database designed for person authentication contains 295 speakers pronouncing two sequences of digits and one phonetically balanced sentence <ref type="bibr" target="#b11">[12]</ref>. The AVLetters database, which includes ten speakers uttering isolated letters of English alphabet, was created for lipreading purpose <ref type="bibr" target="#b12">[13]</ref>. The AVLetters2 dataset <ref type="bibr" target="#b13">[14]</ref>, a higher definition version of AVLetters, contains 5 speakers uttering isolated A-Z letters seven times. The OuluVS database was created for VSR <ref type="bibr" target="#b6">[7]</ref>. It includes 20 speakers uttering 10 small English phrases of everyday use.</p><p>The CUAVE database geared toward audio-visual speech recognition includes 36 speakers <ref type="bibr" target="#b14">[15]</ref>. Speakers pronouncing isolated digits and connected-digit sequences were framed from the frontal view and then both profile views. Simultaneous speech of pair speakers and speaker movements are the special features of CUAVE database.</p><p>The AVICAR database was recorded in a moving car <ref type="bibr" target="#b15">[16]</ref>. There were 100 speakers involved uttering isolated digits and letters, phone numbers and randomly chosen TIMIT sentences <ref type="bibr" target="#b16">[17]</ref>. The database consists of videos recorded by four cameras from different views simultaneously. However since the cameras were located in a lateral array in front of the speaker in a limited space, the actual angles between the views are unknown, and all of them appear near-frontal.</p><p>The Grid audio-visual corpus <ref type="bibr" target="#b17">[18]</ref> involved 34 speakers in the recording, but due to some technical reasons the video data of only 13 speakers is available. Each speaker pronounced 1000 synthetic sentences constructed combining a small number of keywords.</p><p>Table <ref type="table" target="#tab_0">I</ref> summarizes the above databases. It lists the number of subjects, types of utterances and camera views of each database. OuluVS2 database presented in this paper is also placed in the same table for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE OULUVS2 DATABASE A. Utterances</head><p>There were three phases in each data collection session. In phase 1, a subject was asked to utter continuously ten digit sequences. Each sequence consisted of ten randomly generated digits and was repeated three times during recording. The talking speed choice was up to the subject. The ten digit sequences were generated just once and stayed the same for all the subjects. In phase 2 the subject pronounced ten daily-use short English phrases such as "Hello" and "Nice to meet you". The same set of phrases was used in the OuluVS database <ref type="bibr" target="#b6">[7]</ref> that had been widely used for VSR studies. Every phrase was uttered three times. In phase 3 the subject was asked to read ten randomly chosen TIMIT sentences <ref type="bibr" target="#b16">[17]</ref>. Every sentence was read only once. A separate set of sentences was generated for every subject. Table <ref type="table" target="#tab_0">II</ref> shows examples of utterances used in different phases of data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subject Population</head><p>Among 53 subjects taking part in our data collection there were 40 males and 13 females. Fig. <ref type="figure" target="#fig_0">1</ref> shows some </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database</head><p>Subj. Utterances Views XM2VTSDB <ref type="bibr" target="#b11">[12]</ref> 295 continuous digits, frontal sentences AVLetters <ref type="bibr" target="#b12">[13]</ref> 10 isolated letters frontal CUAVE <ref type="bibr" target="#b14">[15]</ref> 36 isolated digits, frontal &amp; profile, continous digits Isolated digits, AVICAR <ref type="bibr" target="#b15">[16]</ref> 100 continuous digits, four near isolated letters, frontal views sentences Grid <ref type="bibr" target="#b17">[18]</ref> 34 Sentences frontal AVLetters2 <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Collection</head><p>Fig. <ref type="figure">3</ref> illustrates the setting for the data collection. As can be seen, a subject was asked to sit on a chair in front of six cameras facing HD camera 1 and high speed (HS) camera (0°). Other four HD cameras were located in the following positions: 30°, 45°, 60°and 90°(profile view) to the subject's right hand side. Digits, phrases and TIMIT sentences were shown to the subject on a computer monitor located slightly to the left behind the frontal cameras. Subjects were asked to keep the head still and facial expression neutral during recording. Of course, natural uncontrolled head movements  The relation between appearance types in OuluVS2 subject population and body position changes can still be found in the recorded videos.</p><p>The recording was made in an ordinary office condition with mixed lighting (professional studio lighting intermixed with ordinary office illumination and natural daylight falling through the window) and possible background sounds (e.g., human conversations). For the video and audio recording, five GoPro Hero3 Black Edition cameras were used (video resolution 1920×1080, 30 fps, audio bit rate 128 kbps). The "fisheye" effect was reduced by using "narrow" recording mode. From the frontal view the 100 fps HS video was also recorded using PuxeLink PL-B774U camera (resolution 640×480). It could be used to investigate the influence of video frame rates.</p><p>At the beginning of every recording session, we placed a white cardboard in front of the subject and projected a periodic flash light on it. By doing so we got a periodic signal in front of all the six cameras for a short period. Frames recorded within such a signal were later used to synchronize Tex t Fig. <ref type="figure">3</ref>. OuluVS2 recording system setup videos of different views. In our case, audio synchronization for all six cameras was not possible due to the absence of microphone in the HS camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Synchronization</head><p>Videos produced in the same session by different cameras were synchronized using the periodic flash-light signal recorded at the beginning of every video. A semi-automatic procedure was developed to synchronize HD videos. We first manually located the flash-light-spot and roughly marked the period when the white cardboard appeared in every video sequence. The average grayscale signal was calculated over the small neighborhood (5 × 5) around to the center of the located light-spot for every frame. Fig. <ref type="figure" target="#fig_2">4</ref> shows the resulting signal curve for every camera view. It can be seen that there are rapid changes caused by the flash light periodically switched on and off in the curves for all the videos. After matching those periodic signals we obtained time shifts (in frames) between videos obtained for different views. The matching was performed through minimizing the following heuristic function:</p><formula xml:id="formula_0">h = t |f i t -f j t+∆t | (1)</formula><p>where f i is a grayscale signal calculated for the ith video and ∆t is the time shift between the two videos. Synchronization results were also checked by eye at the end.</p><p>The same approach can be used to synchronize HS video with HD videos taking into account the difference in the frame rate. This kind of synchronization is planned to be done on the later stage of database preprocessing work which is being continued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Video Preprocessing</head><p>Preprocessing of HD videos were conducted semiautomatically. Given a video recorded in one recording  session, we used its audio to locate all the utterances and cropped off video segments for them. For each video segment, we first detected a bounding box for the face in the first frame <ref type="bibr" target="#b18">[19]</ref>. Images within the box were cropped off across the sequence. We then calculated the SURF features from images, matched the feature points and estimated the image transformation matrix for each pair of consecutive images <ref type="bibr" target="#b19">[20]</ref>. After that, all images were aligned to the first one.</p><p>We next performed facial landmark (e.g., eye corners, nose tip and lip corners) localization using the method described in <ref type="bibr" target="#b20">[21]</ref>, checked the detected facial points by eye and manually marked the landmarks if they were far away from the true position. After that, images containing the talking mouth were cropped off according to some fixed heightwidth ratio. Fig. <ref type="figure" target="#fig_3">5</ref> shows us an example of the synchronous cropped images of a talking mouth. Images were resized to have the same height for the purpose of illustration.</p><p>All the HS videos were roughly segmented in time and converted to MP4 format. The original raw videos are too large (about 30 GB per video) but can be made available on demand. The above procedure will be used to preprocess these videos in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VSR EXPERIMENTS</head><p>In this work, we considered VSR as the application for testing. A simple VSR system was developed and tested in an experimental setting that had been widely used in previous VSR studies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The purpose was to simply provide some baseline performance (results for the frontal view comparable to previous work) rather than achieving the state of the art. Moreover, we wanted to test each view under the simple system and carry out comparison that might give us insight into designing better systems for analyzing non-rigid mouth motion.</p><p>Following <ref type="bibr" target="#b6">[7]</ref>, the task was to recognize ten phrases using only visual information. To do that, we first normalized videos such that images from the same view had the same dimension. The image sizes together with some other details can be found in Table <ref type="table" target="#tab_2">III</ref>. Due to data-collection errors (e.g., the subject wrongly positioned him/herself such that his/her talking mouth was not included in the recorded video sequences), not all the videos were used in the experiments.</p><p>The number of videos used for training varied from 1560 (frontal and 60°views) to 1374 (profile view). For feature extraction, our VSR system computed 2D DCT features from each image and performed PCA to reduce the feature dimension to 100 <ref type="bibr" target="#b21">[22]</ref>. For recognition, a whole-word hidden Markov model (HMM) <ref type="bibr" target="#b22">[23]</ref> was constracted for classification for each phrase. The numbers of states and Gaussian mixtures were determined empirically to maximize recognition performance.</p><p>The experiments were designed to be speaker independent using leave-one-speaker-out cross validation. In other words, we chose one speaker as the test subject and used corresponding data only for testing. System training was carried out using data from all the other speakers. We first conducted VSR experiments for every single view. After that, all the views were mixed together. During training, we randomly sampled the view for every phrase of every subject and picked the corresponding video into the training corpus. The testing data was chosen the same way. The recognition rate results for different views are shown in Fig. <ref type="figure">6</ref> It is interesting to see that the best recognition rate (47%), 6% higher than that of the frontal view, comes from the 60°view and the second best (42%), 1% higher than that Fig. <ref type="figure">6</ref>. Recognition results for different views of the frontal view, from the profile view, which somehow contradicts our expectation that the frontal or those close-tofrontal views should provide the most useful information for VSR. For the test involving all the views, there is no surprise that the outcome recognition rate (23%) is significantly lower than those of single views. It shows that the standard way of extracting features and constructing classifiers cannot cope with the large variations of the mouth appearance caused by camera-view changes. The experimental results highlight the need for more research effort to better understand visual speech, especially under various views, so as to develop more effective methods to model such variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We have presented our newly recorded multi-view audiovisual database named OuluVS2. It contains thousands of videos of more than 50 speakers speaking three types of utterances, recorded simultaneously by six cameras from five different views, which makes OuluVS2 an appropriate corpus for non-rigid mouth motion analysis. In addition, synchronized multi-view videos may also be useful for research of 3D face analysis and synthesis.</p><p>We have given details about the speaker population, utterances, data video synchronization and video preprocessing. Baseline VSR experiments have been carried out on the database based on a widely used setting. Recognition results show that the best VSR performance does not come from the frontal view or those close-to-frontal views. They highlight the need for more research effort to better understand visual speech especially under various camera views.</p><p>The OuluVS2 database will be available for downloading 1 . Synchronization information and preprocessed data will also be provided soon. Based on the database, our future research will be focused on developing models to cope with the large variations caused by camera-view changes and applying the models in the applications of visual-only or audiovisual speech recognition and speaker verification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample images of the speakers framed from different views</figDesc><graphic coords="2,314.64,511.40,241.91,204.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.</figDesc><graphic coords="3,55.44,54.00,241.92,109.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of gray-level change at the center of the light-spot used for video synchronization. Periodic parts correspond to flash-light switching and mark the same time moments.</figDesc><graphic coords="3,313.20,505.21,246.97,194.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of the synchronous preprocessed images of a talking mouth. All images are resized to have the same height for the purpose of illustration.</figDesc><graphic coords="4,55.44,54.00,241.93,163.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc></figDesc><table /><note><p>OF THE PUBLICLY AVAILABLE AUDIO-VISUAL DATABASES</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>images of the speakers framed from different views. Most of the participants were university students and staff. There were no native English speakers among them. According to appearance subjects can be conventionally grouped into five following appearance types: European, Chinese, Indian/Pakistani, Arabian and African. The relation between these types in subject population of OuluVS2 database is shown in Fig.2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ABLE II</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EXAMPLE UTTERANCES USED IN DIFFERENT PHASES OF DATA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>COLLECTION</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Phase 1: digit sequences</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 7 3 5 1 6 2 6 6 7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4 0 2 9 1 8 5 9 0 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 9 0 7 8 8 0 3 2 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Phase 2: phrases</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Thank you</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Have a good time</cell></row><row><cell></cell><cell></cell><cell></cell><cell>You are welcome</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Phase 3: TIMIT sentances [17]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Military personnel are expected to obey government orders.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chocolate and roses never fail as a romantic gift.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Agricultural products are unevenly distributed.</cell></row><row><cell>14]</cell><cell>5</cell><cell>isolated letters</cell><cell>frontal</cell></row><row><cell>OuluVS [7]</cell><cell>20</cell><cell>Phrases</cell><cell>frontal</cell></row><row><cell></cell><cell></cell><cell cols="2">continuous digits, Five views:</cell></row><row><cell>OuluVS2</cell><cell>53</cell><cell>phrases,</cell><cell>frontal, profile,</cell></row><row><cell></cell><cell></cell><cell>sentences</cell><cell>30°, 45°, 60°T</cell></row></table><note><p>sample</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III VSR</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">EXPERIMENT DATA DETAILS</cell><cell></cell></row><row><cell>Camera</cell><cell>Aspect</cell><cell>Downsampled</cell><cell>Subjects</cell><cell>Videos</cell></row><row><cell>View</cell><cell>Ratio</cell><cell>Resolution</cell><cell>involved</cell><cell>involved</cell></row><row><cell cols="2">0°1:0.8</cell><cell>56x45</cell><cell>52</cell><cell>1560</cell></row><row><cell cols="2">30°1:0.8</cell><cell>56x45</cell><cell>51</cell><cell>1530</cell></row><row><cell cols="2">45°1:0.9</cell><cell>56x50</cell><cell>51</cell><cell>1525</cell></row><row><cell cols="2">60°1:1</cell><cell>56x56</cell><cell>52</cell><cell>1560</cell></row><row><cell cols="2">90°1:1.25</cell><cell>45x56</cell><cell>46</cell><cell>1374</cell></row><row><cell>mixed</cell><cell>1:1</cell><cell>56x56</cell><cell>52</cell><cell>1560</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This work was supported by Infotech Oulu, University of Oulu and Academy of Finland.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perrier</surname></persName>
		</author>
		<title level="m">Issues in audio-visual speech processing</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>ch. Audio-visual automatic speech recognition: an overview</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust biometric person identification using automatic classifier fusion of speech, mouth, and face experts</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal biometric human recognition for perceptual human-computer interaction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sadka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="676" to="681" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning multi-boosted hmms for lippasswordbased speaker verication</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5588</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a practical lipreading system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning sequential patterns for lipreading</title>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised random forest manifold alignment for lipreading</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A compact representation of visual speech data using latent variables</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: The extended m2vts database</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Audio, Video-Based Biometrics Person Authentication (AVBPA)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extraction of visual features for lipreading</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bangham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The challenge of multispeaker lip-reading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Auditory-Visual Speech Process</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cuave: A new audio-visual database for multimodal humancomputer interface research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurbuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gowdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2017" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Avicar: Audio-visual speech corpus in a car environment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goudeseune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Conf. Int. Speech Commun. Assoc. (INTER-SPEECH</title>
		<imprint>
			<biblScope unit="page" from="380" to="383" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech database development at mit: Timit and beyond</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="356" />
			<date type="published" when="1990-08">Aug. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An audiovisual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DBN based multi-stream models for audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gowdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="993" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
