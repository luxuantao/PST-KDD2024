<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Debiasing Sentence Representations</title>
				<funder>
					<orgName type="full">AFRL</orgName>
				</funder>
				<funder ref="#_zP3EnkW #_B3AUkB4">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">US Army, ONR</orgName>
				</funder>
				<funder ref="#_Q79jMxV">
					<orgName type="full">Apple</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder>
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
							<email>pliang@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irene</forename><forename type="middle">Mengze</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><forename type="middle">Chong</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department and Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Debiasing Sentence Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, SENT-DEBIAS, to reduce these biases. We show that SENT-DEBIAS is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning tools for learning from language are increasingly deployed in real-world scenarios such as healthcare <ref type="bibr" target="#b40">(Velupillai et al., 2018)</ref>, legal systems <ref type="bibr" target="#b9">(Dale, 2019)</ref>, and computational social science <ref type="bibr" target="#b2">(Bamman et al., 2016)</ref>. Key to the success of these models are powerful embedding layers which learn continuous representations of input information such as words, sentences, and documents from large amounts of data <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>. Although word-level embeddings <ref type="bibr" target="#b30">(Pennington et al., 2014;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref> are highly informative features useful for a variety of tasks in Natural Language Processing (NLP), recent work has shown that word-level embeddings reflect and propagate social biases present in training corpora <ref type="bibr" target="#b18">(Lauscher and Glava?, 2019;</ref><ref type="bibr" target="#b8">Caliskan et al., 2017;</ref><ref type="bibr" target="#b38">Swinger et al., 2019;</ref><ref type="bibr" target="#b6">Bolukbasi et al., 2016)</ref>. Machine learning systems that incorporate these word embeddings can further amplify biases <ref type="bibr">(Sun et al., 2019b;</ref><ref type="bibr" target="#b48">Zhao et al., 2017;</ref><ref type="bibr" target="#b3">Barocas and Selbst, 2016)</ref> and unfairly discriminate against users, particularly those from disadvantaged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref> and multiclass <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref> bias attributes such as gender, race, and religion.</p><p>More recently, sentence-level representations such as ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, and GPT <ref type="bibr" target="#b33">(Radford et al., 2019)</ref> have become the preferred choice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP <ref type="bibr" target="#b44">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type="bibr" target="#b45">(Zellers et al., 2019;</ref><ref type="bibr">Sun et al., 2019a)</ref>, and grounded language learning <ref type="bibr" target="#b39">(Urbanek et al., 2019)</ref>. As their usage proliferates across various real-world applications <ref type="bibr" target="#b14">(Huang et al., 2019;</ref><ref type="bibr" target="#b1">Alsentzer et al., 2019)</ref>, it becomes necessary to recognize the role they play in shaping social biases and stereotypes.</p><p>Debiasing sentence representations is difficult for two reasons. Firstly, it is usually unfeasible to fully retrain many of the state-of-the-art sentencebased embedding models. In contrast with conventional word-level embeddings such as GloVe <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref> and word2vec <ref type="bibr" target="#b27">(Mikolov et al., 2013)</ref> which can be retrained on a single machine within a few hours, the best sentence encoders such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, and GPT <ref type="bibr" target="#b33">(Radford et al., 2019)</ref> are trained on massive amounts of text data over hundreds of machines for several weeks. As a result, it is difficult to retrain a new sentence encoder whenever a new source of bias is uncovered from data. We therefore focus on post-hoc debiasing techniques which add a post-training debiasing step to these sentence representations before they are used in downstream tasks <ref type="bibr" target="#b6">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b23">Manzini et al., 2019)</ref>. Secondly, sentences display large variety in how they are composed from individual words. This variety is driven by many factors such as topics, individuals, settings, and even differences between spoken and written text. As a result, it is difficult to scale traditional word-level debiasing approaches (which involve bias-attribute words such as man, woman) <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref> to sentences.</p><p>Related Work: Although there has been some recent work in measuring the presence of bias in sentence representations <ref type="bibr" target="#b24">(May et al., 2019;</ref><ref type="bibr" target="#b4">Basta et al., 2019)</ref>, none of them have been able to successfully remove bias from pretrained sentence representations. In particular, <ref type="bibr" target="#b47">Zhao et al. (2019)</ref>, <ref type="bibr" target="#b28">Park et al. (2018), and</ref><ref type="bibr" target="#b11">Garg et al. (2019)</ref> are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. <ref type="bibr" target="#b7">Bordia and Bowman (2019)</ref> only study word-level language models and also requires re-training. Finally, <ref type="bibr" target="#b16">Kurita et al. (2019)</ref> only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT) <ref type="bibr" target="#b8">(Caliskan et al., 2017)</ref> metric in a manner similar to <ref type="bibr" target="#b24">May et al. (2019)</ref>.</p><p>In this paper, as a compelling step towards generalizing debiasing methods to sentence representations, we capture the various ways in which biasattribute words can be used in natural sentences. This is performed by contextualizing bias-attribute words using a diverse set of sentence templates from various text corpora into bias-attribute sentences. We propose SENT-DEBIAS, an extension of the HARD-DEBIAS method <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref>, to debias sentences for both binary<ref type="foot" target="#foot_0">1</ref> and multiclass bias attributes spanning gender and religion. Key to our approach is the contextualization step in which bias-attribute words are converted into bias-attribute sentences by using a diverse set  of sentence templates from text corpora. Our experimental results demonstrate the importance of using a large number of diverse sentence templates when estimating bias subspaces of sentence representations. Our experiments are performed on two widely popular sentence encoders BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, showing that our approach reduces the bias while preserving performance on downstream sequence tasks. We end with a discussion about possible shortcomings and present some directions for future work towards accurately characterizing and removing social biases from sentence representations for fairer NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Debiasing Sentence Representations</head><p>Our proposed method for debiasing sentence representations, SENT-DEBIAS, consists of four steps: 1) defining the words which exhibit bias attributes, 2) contextualizing these words into bias attribute sentences and subsequently their sentence representations, 3) estimating the sentence representation bias subspace, and finally 4) debiasing general sentences by removing the projection onto this bias subspace. We summarize these steps in Algorithm 1 and describe the algorithmic details in the following subsections. 1) Defining Bias Attributes: The first step involves identifying the bias attributes and defining a set of bias attribute words that are indicative of these attributes. For example, when characterizing bias across the male and female genders, we use the word pairs (man, woman), (boy, girl) that are indicative of gender. When estimating the 3class religion subspace across the Jewish, Christian, and Muslim religions, we use the tuples (Judaism, Christianity, Islam), (Synagogue, Church, Mosque). Each tuple should consist of words that have an equivalent meaning except for the bias attribute. In general, for d-class bias attributes, the set of words forms a dataset D = {(w</p><formula xml:id="formula_0">(i) 1 , ..., w (i) d )} m</formula><p>i=1 of m entries where each entry (w 1 , ..., w d ) is a d-tuple of words that are each representative of a particular Algorithm 1 SENT-DEBIAS: a debiasing algorithm for sentence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENT-DEBIAS:</head><p>1: Initialize (usually pretrained) sentence encoder M ? . 2: Define bias attributes (e.g. binary gender g m and g f ).</p><p>3: Obtain words D = {(w</p><formula xml:id="formula_1">(i) 1 , ..., w (i) d )} m</formula><p>i=1 indicative of bias attributes (e.g. Table <ref type="table" target="#tab_1">1</ref>). 4: S = ? m i=1 CONTEXTUALIZE(w </p><formula xml:id="formula_2">(i) 1 , ..., w (i) d ) = {(s (i) 1 , ..., s (i) d )} n i=1 //</formula><formula xml:id="formula_3">h V = ? k</formula><p>j=1 ?h, v j ?v j // project onto bias subspace 11: ? = hh V // subtract projection 12: end for bias attribute (we drop the superscript (i) when it is clear from the context). Table <ref type="table" target="#tab_1">1</ref> shows some bias attribute words that we use to estimate the bias subspaces for binary gender and multiclass religious attributes (full pairs and triplets in appendix).</p><p>Existing methods that investigate biases tend to operate at the word-level which simplifies the problem since the set of tokens is bounded by the vocabulary size <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref>. This simple approach has the advantage of identifying the presence of biases using predefined sets of word associations, and estimate the bias subspace using the predefined bias word pairs. On the other hand, the potential number of sentences are unbounded which makes it harder to precisely characterize the sentences in which bias is present or absent. Therefore, it is not trivial to directly convert these words to sentences to obtain a representation from pretrained sentence encoders. In the subsection below, we describe our solution to this problem.</p><p>2) Contextualizing Words into Sentences: A core step in our SENT-DEBIAS approach involves contextualizing the predefined sets of bias attribute words to sentences so that sentence encoders can be applied to obtain sentence representations. One option is to use a simple template-based design to simplify the contextual associations a sentence encoder makes with a given term, similar to how <ref type="bibr" target="#b24">May et al. (2019)</ref> proposed to measure (but not remove) bias in sentence representations. For example, each word can be slotted into templates such as "This is &lt;word&gt;.", "I am a &lt;word&gt;.". We take an alternative perspective and hypothesize that for a given bias attribute (e.g. gender), a single bias subspace exists across all possible sentence representations. For example, the bias subspace should be the same in the sentences "The boy is coding.", "The girl is coding.", "The boys at the playground.", and "The girls at the playground.". In order to estimate this bias subspace accurately, it becomes important to use sentence templates that are as diverse as possible to account for all occurrences of that word in surrounding contexts. In our experiments, we empirically demonstrate that estimating the bias subspace using a large and diverse set of templates from text corpora leads to improved bias reduction as compared to using simple templates.</p><p>To capture the variety in syntax across sentences, we use large text corpora to find naturally occurring sentences. These naturally occurring sentences therefore become our sentence "templates". To use these templates to generate new sentences, we replace words representing a single class with another. For example, a sentence containing a male term "he" is used to generate a new sentence but replacing it with the corresponding female term "she". This contextualization process is repeated for all word tuples in the bias attribute word dataset D, eventually contextualizing the given set of bias attribute words into bias attribute sentences. Since there are multiple templates which a bias attribute word can map to, the contextualization process results in a bias attribute sentence dataset S which is substantially larger in size:  where CONTEXTUALIZE(w 1 , ..., w d ) is a function which returns a set of sentences obtained by matching words with naturally-occurring sentence templates from text corpora.</p><formula xml:id="formula_4">S = m ? i=1 CONTEXTUALIZE(w (i) 1 , ..., w (i) d ) (1) = {(s (i) 1 , ..., s (i) d )} n i=1 , S &gt; D (2)</formula><p>Our text corpora originate from the following five sources: 1) WikiText-2 <ref type="bibr">(Merity et al., 2017a)</ref>, a dataset of formally written Wikipedia articles (we only use the first 10% of WikiText-2 which we found to be sufficient to capture formally written text), 2) Stanford Sentiment Treebank <ref type="bibr" target="#b35">(Socher et al., 2013)</ref>, a collection of 10000 polarized written movie reviews, 3) Reddit data collected from discussion forums related to politics, electronics, and relationships, 4) MELD <ref type="bibr" target="#b32">(Poria et al., 2019)</ref>, a large-scale multimodal multi-party emotional dialog dataset collected from the TV-series Friends, and 5) POM <ref type="bibr" target="#b29">(Park et al., 2014)</ref>, a dataset of spoken review videos collected across 1,000 individuals spanning multiple topics. These datasets have been the subject of recent research in language understanding <ref type="bibr">(Merity et al., 2017b;</ref><ref type="bibr" target="#b21">Liu et al., 2019;</ref><ref type="bibr" target="#b42">Wang et al., 2019)</ref> and multimodal human language <ref type="bibr" target="#b20">(Liang et al., 2018</ref><ref type="bibr" target="#b19">(Liang et al., , 2019))</ref>. Table <ref type="table" target="#tab_4">2</ref> summarizes these datasets. We also give some examples of the diverse templates that occur naturally across various individuals, settings, and in both written and spoken text.</p><p>3) Estimating the Bias Subspace: Now that we have contextualized all m word d-tuples in D into n sentence d-tuples S, we pass these sentences through a pre-trained sentence encoder (e.g. BERT, ELMo) to obtain sentence representations. Suppose we have a pre-trained encoder M ? with parameters ?. Define R j , j ? [d] as sets that collect all sentence representations of the j-th entry in the d-tuple, R j = {M ? (s</p><formula xml:id="formula_5">(i) j )} n i=1 .</formula><p>Each of these sets R j defines a vector space in which a specific bias attribute is present across its contexts. For example, when dealing with binary gender bias, R 1 (likewise R 2 ) defines the space of sentence representations with a male (likewise female) context. The only difference between the representations in R 1 versus R 2 should be the specific bias attribute present. Define the mean of set j as ? j = 1 R j ? w?R j w. The bias subspace V = {v 1 , ..., v k } is given by the first k components of principal component analysis (PCA) <ref type="bibr" target="#b0">(Abdi and Williams, 2010)</ref>:</p><formula xml:id="formula_6">V = PCA k ? ? d ? j=1 ? w?R j (w -? j ) ? ? . (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>k is a hyperparameter in our experiments which determines the dimension of the bias subspace. Intuitively, V represents the top-k orthogonal directions which most represent the bias subspace.</p><p>4) Debiasing: Given the estimated bias subspace V, we apply a partial version of the HARD-DEBIAS algorithm <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref> to remove bias from new sentence representations. Taking the example of binary gender bias, the HARD-DEBIAS algorithm consists of two steps: 4a) Neutralize: Bias components are removed from sentences that are not gendered and should not contain gender bias (e.g., I am a doctor., That nurse is taking care of the patient.) by removing the projection onto the bias subspace. More formally, given a representation h of a sentence and the previously estimated gender subspace V = {v 1 , ..., v k }, the debiased representation ? is given by first obtaining h V , the projection of h onto the bias subspace V before subtracting h V from h. This results in a vector that is orthogonal to the bias subspace V and therefore contains no bias:</p><formula xml:id="formula_8">h V = k j=1 ?h, v j ?v j ,<label>(4)</label></formula><formula xml:id="formula_9">? = h -h V .</formula><p>(5) 4b) Equalize: Gendered representations are centered and their bias components are equalized (e.g. man and woman should have bias components in opposite directions, but of the same magnitude). This ensures that any neutral words are equidistant to biased words with respect to the bias subspace. In our implementation, we skip this Equalize step because it is hard to identify all or even the majority of sentence pairs to be equalized due to the complexity of natural sentences. For example, we can never find all the sentences that man and woman appear in to equalize them appropriately. Note that even if the magnitudes of sentence representations are not normalized, the debiased representations are still pointing in directions orthogonal to the bias subspace. Therefore, skipping the equalize step still results in debiased sentence representations as measured by our definition of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We test the effectiveness of SENT-DEBIAS at removing biases and retaining performance on downstream tasks. All experiments are conducted on English terms and downstream tasks. We acknowledge that biases can manifest differently across different languages, in particular gendered languages <ref type="bibr" target="#b50">(Zhou et al., 2019)</ref>, and emphasize the need for future extensions in these directions. Experimental details are in the appendix and code is released at https://github.com/pliang279/ sent_debias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluating Biases</head><p>Biases are traditionally measured using the Word Embedding Association Test (WEAT) <ref type="bibr" target="#b8">(Caliskan et al., 2017)</ref>. WEAT measures bias in word embeddings by comparing two sets of target words to two sets of attribute words. For example, to measure social bias surrounding genders with respect to careers, one could use the target words programmer, engineer, scientist, and nurse, teacher, librarian, and the attribute words man, male, and woman, female. Unbiased word representations should display no difference between the two target words in terms of their relative similarity to the two sets of attribute words. The relative similarity as measured by WEAT is commonly known as the effect size. An effect size with absolute value closer to 0 represents lower bias.</p><p>To measure the bias present in sentence representations, we use the method as described in <ref type="bibr" target="#b24">May et al. (2019)</ref> which extended WEAT to the Sentence Encoder Association Test (SEAT). For a given set of words for a particular test, words are converted into sentences using a template-based method. The WEAT metric can then be applied for fixed-length, pre-trained sentence representations. To measure bias over multiple classes, we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref>. For the binary gender setting, we use words from the Caliskan Tests <ref type="bibr" target="#b8">(Caliskan et al., 2017)</ref> which measure biases in common stereotypes surrounding gendered names with respect to careers, math, and science <ref type="bibr" target="#b13">(Greenwald et al., 2009)</ref>. To evaluate biases in the multiclass religion setting, we modify the Caliskan Tests used in <ref type="bibr" target="#b24">May et al. (2019)</ref> with lexicons used by <ref type="bibr" target="#b23">Manzini et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Debiasing Setup</head><p>We first describe the details of applying SENT-DEBIAS on two widely-used sentence encoders: BERT<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>. Note that the pre-trained BERT encoder must be fine-tuned on task-specific data. This implies that the final BERT encoder used during debiasing changes from task to task. To account for these differences, we report two sets of metrics: 1) BERT: simply debiasing the pre-trained BERT encoder, and 2) BERT post task: first fine-tuning BERT and post-processing (i.e. normalization) on a specific task before the final BERT representations are debiased. We apply SENT-DEBIAS on BERT fine-tuned on two single sentence datasets, Stanford Sentiment Treebank (SST-2) sentiment classification <ref type="bibr" target="#b35">(Socher et al., 2013)</ref> and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment <ref type="bibr" target="#b43">(Warstadt et al., 2018)</ref>. It is also possible to apply BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> on downstream tasks that involve two sentences. The output sentence pair representation can also be debiased (after fine-tuning and normalization). We test the effect of SENT-DEBIAS on Question Natural Language Inference (QNLI) <ref type="bibr" target="#b41">(Wang et al., 2018)</ref> which converts the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b34">(Rajpurkar et al., 2016)</ref> into a binary classification task. These results are   <ref type="formula">2017</ref>) row N . The last row measures bias in a multiclass religion setting using MAC <ref type="bibr" target="#b23">(Manzini et al., 2019)</ref> before and after debiasing. MAC score ranges from 0 to 2 and closer to 1 represents lower bias. Results are reported as x 1 ? x 2 where x 1 represents score before debiasing and x 2 after, with lower bias score in bold. Our method reduces bias of BERT and ELMo for the majority of binary and multiclass tests.</p><p>reported as BERT post SST-2, BERT post CoLA, and BERT post QNLI respectively.</p><p>For ELMo, the encoder stays the same for downstream tasks (no fine-tuning on different tasks) so we just debias the ELMo sentence encoder. We report this result as ELMo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Debiasing Results</head><p>We present these debiasing results in Table <ref type="table" target="#tab_5">3</ref>, and see that for both binary gender bias and multiclass religion bias, our proposed method reduces the amount of bias as measured by the given tests and metrics. The reduction in bias is most pronounced when debiasing the pre-trained BERT encoder. We also observe that simply fine-tuning the BERT encoder for specific tasks also reduces the biases present as measured by the Caliskan tests, to some extent. However, fine-tuning does not lead to consistent decreases in bias and cannot be used as a standalone debiasing method. Furthermore, finetuning does not give us control over which type of bias to control for and may even amplify bias if the task data is skewed towards particular biases. For example, while the bias effect size as measured by Caliskan test C7 decreases from +0.542 to -0.033 and +0.288 after fine-tuning on SST-2 and CoLA respectively, the effect size as measured by the multiclass Caliskan test increases from +0.035 to +1.200 and +0.243 after fine-tuning on SST-2 and CoLA respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with Baselines</head><p>We compare to three baseline methods for debiasing: 1) FastText derives debiased sentence embeddings using an average of debiased FastText word embeddings <ref type="bibr" target="#b5">(Bojanowski et al., 2016</ref>) using wordlevel debiasing methods <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref>, 2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Debiasing Method</head><p>Ave. Abs. Effect Size BERT original <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> +0.354 FastText <ref type="bibr" target="#b5">(Bojanowski et al., 2016)</ref> +0.565 BERT word <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref> +0.861 BERT simple <ref type="bibr" target="#b24">(May et al., 2019)</ref> +0.298 SENT-DEBIAS BERT (ours) +0.256</p><p>Table <ref type="table">4</ref>: Comparison of various debiasing methods on sentence embeddings. FastText <ref type="bibr" target="#b5">(Bojanowski et al., 2016)</ref> (and BERT word) derives debiased sentence embeddings with an average of debiased FastText (and BERT) word embeddings using word-level debiasing methods <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref>. BERT simple adapts <ref type="bibr" target="#b24">May et al. (2019)</ref> by using simple templates to debias BERT representations. SENT-DEBIAS BERT represents our method using diverse templates. We report the average absolute effect size across all Caliskan tests. Average scores closer to 0 represent lower bias.</p><p>BERT word obtains a debiased sentence representation from average debiased BERT word representations, again debiased using word-level debiasing methods <ref type="bibr" target="#b6">(Bolukbasi et al., 2016)</ref>, and 3) BERT simple adapts <ref type="bibr" target="#b24">May et al. (2019)</ref> by using simple templates to debias BERT sentence representations.</p><p>From Table <ref type="table">4</ref>, SENT-DEBIAS achieves a lower average absolute effect size and outperforms the baselines based on debiasing at the word-level and averaging across all words. This indicates that it is not sufficient to debias words only and that biases in a sentence could arise from their debiased word constituents. In comparison with BERT simple, we observe that using diverse sentence templates obtained from naturally occurring written and spoken text makes a difference on how well we can remove biases from sentence representations. This supports our hypothesis that using increasingly diverse templates estimates a bias subspace that generalizes to different words in their context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effect of Templates</head><p>We further test the importance of sentence templates through two experiments.</p><p>1) Same Domain, More Quantity: Firstly, we ask: how does the number of sentence templates impact debiasing performance? To answer this, we begin with the largest domain WikiText-2 (13750 templates) and divide it into 5 partitions each of size 2750. We collect sentence templates using all possible combinations of the 5 partitions and apply these sentence templates in the contextualization step of SENT-DEBIAS. We then estimate the corresponding bias subspace, debias, and measure the average absolute values of all 6 SEAT effect sizes. Since different combinations of the 5 partitions result in a set of sentence templates of different sizes (20%, 40%, 60%, 80%, 100%), this allows us to see the relationship between size and debiasing performance. Combinations with the same percentage of data are grouped together and for each group we compute the mean and standard deviation of the average absolute effect sizes. We perform the above steps to debias BERT fine-tuned on SST-2 and QNLI and plot these results in Figure <ref type="figure" target="#fig_1">1</ref>. Please refer to the appendix for experiments with BERT fine-tuned on CoLA, which show similar results.</p><p>For BERT fine-tuned on SST-2, we observe a decreasing trend in the effect size as increasing subsets of the data is used. For BERT fine-tuned on QNLI, there is a decreasing trend that quickly tapers off. However, using a larger number of templates reduces the variance in average absolute effect size and improves the stability of the SENT-DEBIAS algorithm. These observations allow us to conclude the importance of using a large number of templates from naturally occurring text corpora.</p><p>2) Same Quantity, More Domains: How does the number of domains that sentence templates are extracted from impact debiasing performance? We fix the total number of sentence templates to be 1080 and vary the number of domains these templates are drawn from. Given a target number k, we first choose k domains from our Reddit, SST, POM, WikiText-2 datasets and randomly sample 1080 k templates from each of the k selected domains. We construct 1080 templates using all possible subsets of k domains and apply them in the contextualization step of SENT-DEBIAS. We estimate the corresponding bias subspace, debias and measure the average absolute SEAT effect sizes. To see the relationship between the number of domains k and debiasing performance, we group combinations with the same number of domains (k) and for each group compute the mean and standard deviation of the average absolute effect sizes. This experiment is also performed for BERT fine-tuned on SST-2 and QNLI datasets. Results are plotted in Figure <ref type="figure">2</ref>. We draw similar observations: there is a decreasing trend in effect size as templates are drawn from more domains. For BERT fine-tuned on QNLI, using a larger number of domains reduces the variance in effect size and improves stability of the algorithm. Therefore, it is important to use a large variety of templates across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Visualization</head><p>As a qualitative analysis of the debiasing process, we visualize how the distances between sentence representations shift after the debiasing process is performed. We average the sentence representations of a concept (e.g. man, woman, science, art) across its contexts (sentence templates) and plot the t-SNE (van der <ref type="bibr" target="#b22">Maaten and Hinton, 2008)</ref> Figure <ref type="figure">2</ref>: Influence of the number of template domains on the effectiveness of bias removal on BERT fine-tuned on SST-2 (left) and BERT fine-tuned on QNLI (right). The domains span the Reddit, SST, POM, WikiText-2 datasets. The solid line is the mean over different combinations of domains and the shaded area is the standard deviation. As more domains are used, we observe a decreasing trend and lower variance in average absolute effect size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained BERT embeddings</head><p>Debiased BERT embeddings embeddings of these points in 2D space. From Figure <ref type="figure" target="#fig_2">3</ref>, we observe that BERT average representations of science and technology start off closer to man while literature and art are closer to woman.</p><p>After debiasing, non gender-specific concepts (e.g science, art) become more equidistant to both man and woman average concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Performance on Downstream Tasks</head><p>To ensure that debiasing does not hurt the performance on downstream tasks, we report the performance of our debiased BERT and ELMo on SST-2 and CoLA by training a linear classifier on top of debiased BERT sentence representations. From Table <ref type="table" target="#tab_6">5</ref>, we observe that downstream task performance show a small decrease ranging from 1 -3% after the debiasing process. However, the performance of ELMo on SST-2 increases slightly from 89.6 to 90.0. We hypothesize that these differences in performance are due to the fact that CoLA tests for linguistic acceptability so it is more concerned with low-level syntactic structure such as verb usage, grammar, and tenses. As a result, changes in sentence representations across bias directions may impact its performance more. For example, sentence representations after the gender debiasing steps may display a mismatch between gendered pronouns and the sentence context. For SST, it has been shown that sentiment analysis datasets have labels that correlate with gender information and therefore contain gender bias <ref type="bibr" target="#b15">(Kiritchenko and Mohammad, 2018)</ref>. As a result, we do expect possible decreases in accuracy after debiasing. Finally, we test the effect of SENT-DEBIAS on QNLI by training a classifier on top of debiased BERT sentence pair representations. We observe little impact on task performance: our debiased BERT fine-tuned on QNLI achieves 90.6% performance as compared to the 91.3% we obtained without debiasing.</p><p>4 Discussion and Future Work</p><p>Firstly, we would like to emphasize that both the WEAT, SEAT, and MAC metrics are not perfect since they only have positive predictive ability: they can be used to detect the presence of biases but not their absence <ref type="bibr" target="#b12">(Gonen and Goldberg, 2019)</ref>. and paired sentence (BERT on QNLI) downstream tasks. The performance (higher is better) of debiased BERT and ELMo sentence representations on downstream tasks is not hurt by the debiasing step.</p><p>spoken and written text. We believe that our positive results regarding contextualizing words into sentences implies that future work can build on our algorithms and tailor them for new metrics.</p><p>Secondly, a particular bias should only be removed from words and sentences that are neutral to that attribute. For example, gender bias should not be removed from the word "grandmother" or the sentence "she gave birth to me". Previous work on debiasing word representations tackled this issue by listing all attribute specific words based on dictionary definitions and only debiasing the remaining words. However, given the complexity of natural sentences, it is extremely hard to identify the set of neutral sentences and its complement. Thus, in downstream tasks, we removed bias from all sentences which could possibly harm downstream task performance if the dataset contains a significant number of non-neutral sentences.</p><p>Finally, a fundamental challenge lies in the fact that these representations are trained without explicit bias control mechanisms on large amounts of naturally occurring text. Given that it becomes infeasible (in standard settings) to completely retrain these large sentence encoders for debiasing <ref type="bibr" target="#b49">(Zhao et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al., 2018)</ref>, future work should focus on developing better post-hoc debiasing techniques. In our experiments, we need to re-estimate the bias subspace and perform debiasing whenever the BERT encoder was fine-tuned. It remains to be seen whether there are debiasing methods which are invariant to fine-tuning, or can be efficiently re-estimated as the encoders are fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper investigated the post-hoc removal of social biases from pretrained sentence representations. We proposed the SENT-DEBIAS method that accurately captures the bias subspace of sentence representations by using a diverse set of templates from naturally occurring text corpora. Our experiments show that we can remove biases that occur in BERT and ELMo while preserving performance on downstream tasks. We also demonstrate the importance of using a large number of diverse sentence templates when estimating bias subspaces. Leveraging these developments will allow researchers to further characterize and remove social biases from sentence representations for fairer NLP. that debiasing performance improves and stabilizes with the number of sentence templates as well as the number of domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Influence of the number of templates on the effectiveness of bias removal on BERT fine-tuned on SST-2 (left) and BERT fine-tuned on QNLI (right). All templates are from WikiText-2. The solid line represents the mean over different combinations of domains and the shaded area represents the standard deviation. As increasing subsets of data are used, we observe a decreasing trend and lower variance in average absolute effect size.</figDesc><graphic url="image-1.png" coords="7,127.33,62.81,170.08,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE plots of average sentence representations of a word across its sentence templates before (left) and after (right) debiasing. After debiasing, non gender-specific concepts (in black) are more equidistant to genders.</figDesc><graphic url="image-6.png" coords="8,303.31,267.05,155.42,116.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of Bias Removal on BERT fine-tuned on CoLA with varying percentage of data from a single domain (left) and varying number of domains with fixed total size (right).</figDesc><graphic url="image-7.png" coords="14,98.98,62.81,198.43,148.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of word pairs to estimate the binary gender bias subspace and the 3-class religion bias subspace in our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>"the mailing contained information about their history and advised people to read several books, which primarily focused on {jewish/christian/muslim} history" SST written movie reviews informal 19.2 "{his/her} fans walked out muttering words like horrible and terrible, but had so much fun dissing the film that they didn't mind the ticket cost."</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Topics</cell><cell cols="2">Formality Length</cell><cell>Samples</cell></row><row><cell cols="2">WikiText-2 written</cell><cell>everything</cell><cell>formal</cell><cell>24.0</cell></row><row><cell></cell><cell></cell><cell>politics,</cell><cell></cell><cell></cell><cell>"roommate cut my hair without my consent,</cell></row><row><cell>Reddit</cell><cell>written</cell><cell>electronics,</cell><cell cols="2">informal 13.6</cell><cell>ended up cutting {himself /herself } and is threatening to</cell></row><row><cell></cell><cell></cell><cell>relationships</cell><cell></cell><cell></cell><cell>call the police on me"</cell></row><row><cell>MELD</cell><cell cols="4">spoken comedy TV-series informal 8.1</cell><cell>"that's the kind of strength that I want in the {man/woman} I love!"</cell></row><row><cell>POM</cell><cell cols="4">spoken opinion videos informal 16.0</cell><cell>"and {his/her} family is, like, incredibly confused"</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the various datasets used to find natural sentence templates. Length represents the average length measured by the number of words in a sentence. Words in italics indicate the words used to estimating the binary gender or multiclass religion subspaces, e.g. (man, woman), (jewish, christian, muslim). This demonstrates the variety in our naturally occurring sentence templates in terms of topics, formality, and spoken/written text.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Debiasing results on BERT and ELMo sentence representations. First six rows measure binary SEAT effect sizes for sentence-level tests, adapted from Caliskan tests. SEAT scores closer to 0 represent lower bias. CN : test from Caliskan et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>This calls for new metrics that evaluate biases and can scale to the various types of sentences appearing across different individuals, topics, and in both We test the effect of SENT-DEBIAS on both single sentence (BERT and ELMo on CoLA)   </figDesc><table><row><cell>Test</cell><cell cols="4">BERT debiased BERT ELMo debiased ELMo</cell></row><row><cell cols="2">SST-2 92.7</cell><cell>89.1</cell><cell>89.6</cell><cell>90.0</cell></row><row><cell cols="2">CoLA 57.6</cell><cell>55.4</cell><cell>39.1</cell><cell>37.1</cell></row><row><cell cols="2">QNLI 91.3</cell><cell>90.6</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Although we recognize that gender is non-binary and there are many important ethical principles in the design, ascription of categories/variables to study participants, and reporting of results in studying gender as a variable in NLP (Lar-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p> son, 2017), for the purpose of this study, we follow existing research and focus on female and male gendered terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We used uncased BERT-Base throughout all experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>PPL and LPM were supported in part by the <rs type="funder">National Science Foundation</rs> (Awards #<rs type="grantNumber">1750439</rs>, #<rs type="grantNumber">1722822</rs>) and <rs type="funder">National Institutes of Health</rs>. RS was supported in part by <rs type="funder">US Army, ONR</rs>, <rs type="funder">Apple</rs>, and <rs type="grantNumber">NSF IIS1763562</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, <rs type="funder">National Institutes of Health</rs>, <rs type="funder">DARPA</rs>, and <rs type="funder">AFRL</rs>, and no official endorsement should be inferred. We also acknowledge NVIDIA's GPU support and the anonymous reviewers for their constructive comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zP3EnkW">
					<idno type="grant-number">1750439</idno>
				</org>
				<org type="funding" xml:id="_B3AUkB4">
					<idno type="grant-number">1722822</idno>
				</org>
				<org type="funding" xml:id="_Q79jMxV">
					<idno type="grant-number">NSF IIS1763562</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Debiasing Details</head><p>We provide some details on estimating the bias subspaces and debiasing steps.</p><p>Bias Attribute Words: Table <ref type="table">6</ref> shows the bias attribute words we used to estimate the bias subspaces for binary gender bias and multiclass religious biases.</p><p>Datasets: We provide some details on dataset downloading below:  <ref type="table">7</ref>.</p><p>For all three models, the second output "pooled output" of BERT is treated as the sentence embedding. The variant BERT is the pretrained model with weights downloaded from https: //s3.amazonaws.com/models.huggingface. co/bert/bert-base-uncased.tar.gz.</p><p>The variant BERT post SST is BERT after being finetuned on the Stanford Sentiment Treebank(SST-2) task, a binary single-sentence classification task <ref type="bibr" target="#b35">(Socher et al., 2013)</ref>. During fine-tuning, we first normalize the sentence embedding and then feed it into a linear layer for classification. The variant BERT post CoLA is BERT fine-tuned on the Corpus of Linguistic Acceptability (CoLA) task, a binary single-sentence classification task. Normalization and classification are done exactly the same as BERT post SST. All BERT models are fine-tuned for 3 epochs which is the default hyper-parameter in the huggingface transformers repository. Debiasing for BERT models that are fine-tuned is done just before the classification layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ELMo</head><p>We use the ElmoEmbedder from allennlp.commands.elmo. We perform summation over the aggregated layer outputs. The resulting sentence representation is a time sequence vector with data dimension 1024. When computing gender direction, we perform mean pooling over the time dimension to obtain a 1024-dimensional vector for each definitional sentence. In debiasing, we remove the gender direction from each time step of each sentence representation. We then feed the debiased representation into an LSTM with hidden size 512. Finally, the last hidden state of the LSTM goes through a fully connected layer to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>We also studied the effect of templates on BERT fine-tuned on CoLA as well. Steps taken are exactly the same as described in Effect of Templates: Same Domain, More Quantity and Effect of Templates: Same Quantity, More Domains. Results are plotted in Figure <ref type="figure">4</ref>. It shows</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">Herv?</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynne</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1002/wics.101</idno>
	</analytic>
	<monogr>
		<title level="j">WIREs Comput. Stat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="459" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>David Bamman</surname></persName>
		</author>
		<author>
			<persName><surname>Seza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dogru?z</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><surname>Volkova</surname></persName>
		</author>
		<title level="m">Proceedings of the first workshop on NLP and computational social science</title>
		<meeting>the first workshop on NLP and computational social science</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calif. L. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">671</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating the underlying gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Basta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Casas</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3805</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? Debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying and reducing gender bias in word-level language models</title>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-3002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Law and word order: Nlp in legal tech</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="217" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Counterfactual fairness in text classification through robustness</title>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3317950</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="609" to="614" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding and using the implicit association test: Iii. meta-analysis of predictive validity</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Poehlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahzarin</forename><surname>Luis Uhlmann</surname></persName>
		</author>
		<author>
			<persName><surname>Banaji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<idno>CoRR, abs/1904.05342</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="43" to="53" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gender as a variable in naturallanguage processing: Ethical considerations</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Larson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-1601</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Are we consistently biased? multidimensional analysis of biases in distributional word vectors</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<editor>*SEM</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strong and simple baselines for multimodal utterance embeddings</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2599" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal language analysis with recurrent multistage fusion</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lim</forename><surname>Yao Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On measuring social biases in sentence encoders</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Rudinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Workshop Track Proceedings</publisher>
			<date type="published" when="2013-05-02">2013. May 2-4, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing gender bias in abusive language detection</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2799" to="2804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach</title>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moitreya</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In ICMI</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Oliver Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>CoRR, abs/1904.01766</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mitigating gender bias in natural language processing: Literature review</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirlyn</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Elsherief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diba</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Belding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What are the biases in my word embedding?</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Swinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Mark Dm Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="305" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to speak and act in a fantasy text adventure game</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>CoRR, abs/1903.03094</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using clinical natural language processing for health outcomes research: Overview and actionable suggestions for future advances</title>
		<author>
			<persName><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Osborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Dutta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2018.10.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In Black-boxNLP</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Language models with transformers</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>CoRR, abs/1904.09408</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Amanpreet Singh, and Samuel R Bowman</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
	</analytic>
	<monogr>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hu Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In AIES</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1323</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2979" to="2989" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning gender-neutral word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Process<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Examining gender bias in languages with grammatical gender</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1531</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5276" to="5284" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
